Epoch: 0 
 Validation Loss: 4.407947985331218
---------------------------
Epoch: 0 Batch: 50
Training Loss: 4.289803128242493
Epoch: 0 Batch: 100
Training Loss: 1.3969761037826538
Epoch: 0 Batch: 150
Training Loss: 0.6327481102943421
Epoch: 0 Batch: 200
Training Loss: 0.43435540437698367
Epoch: 0 Batch: 250
Training Loss: 0.31648073959350587
Epoch: 0 Batch: 300
Training Loss: 0.24580886483192443
Epoch: 0 Batch: 350
Training Loss: 0.22223649723189218
Epoch: 0 Batch: 400
Training Loss: 0.19937893331050874
Epoch: 0 Batch: 450
Training Loss: 0.1760220483938853
Epoch: 0 Batch: 500
Training Loss: 0.15623863339424132
Epoch: 0 Batch: 550
Training Loss: 0.13408419197255914
Epoch: 0 Batch: 600
Training Loss: 0.12959241529305776
Epoch: 0 Batch: 650
Training Loss: 0.11836375172321613
Epoch: 0 Batch: 700
Training Loss: 0.10598940014839173
Epoch: 0 Batch: 750
Training Loss: 0.10107046190897624
Epoch: 0 Batch: 800
Training Loss: 0.09553163141012191
Epoch: 0 Batch: 850
Training Loss: 0.08911525053136489
Epoch: 0 Batch: 900
Training Loss: 0.08691619303491381
Epoch: 0 Batch: 950
Training Loss: 0.08392576180006328
Epoch: 0 Batch: 1000
Training Loss: 0.07546312600374222
Epoch: 0 Batch: 1050
Training Loss: 0.07148036854607719
Epoch: 0 Batch: 1100
Training Loss: 0.06602772550149398
Epoch: 0 Batch: 1150
Training Loss: 0.06543636736662492
Epoch: 0 Batch: 1200
Training Loss: 0.060053437103827795
Epoch: 0 Batch: 1250
Training Loss: 0.061763431692123415
Epoch: 0 Batch: 1300
Training Loss: 0.05689399233231178
Epoch: 0 Batch: 1350
Training Loss: 0.05667394187715318
Epoch: 0 Batch: 1400
Training Loss: 0.051487841265542164
Epoch: 0 Batch: 1450
Training Loss: 0.04995139553629119
Epoch: 0 Batch: 1500
Training Loss: 0.05211651742458343
Epoch: 0 Batch: 1550
Training Loss: 0.04767368478159751
Epoch: 0 Batch: 1600
Training Loss: 0.04797761790454388
Epoch: 0 Batch: 1650
Training Loss: 0.04506355170047644
Epoch: 0 Batch: 1700
Training Loss: 0.04165074404548196
Epoch: 0 Batch: 1750
Training Loss: 0.04372092158453805
Epoch: 0 Batch: 1800
Training Loss: 0.03924862010611428
Epoch: 0 Batch: 1850
Training Loss: 0.03846806381199811
Epoch: 0 Batch: 1900
Training Loss: 0.038498212130446186
Epoch: 0 Batch: 1950
Training Loss: 0.03596270625407879
Epoch: 0 Batch: 2000
Training Loss: 0.035545499950647354
Epoch: 0 Batch: 2050
Training Loss: 0.03420845203283356
Epoch: 0 Batch: 2100
Training Loss: 0.034790218869845076
Epoch: 0 Batch: 2150
Training Loss: 0.03351036365642104
Epoch: 0 Batch: 2200
Training Loss: 0.03232741729779677
Epoch: 0 Batch: 2250
Training Loss: 0.029365504450268216
Epoch: 0 Batch: 2300
Training Loss: 0.030298312435979427
Epoch: 0 Batch: 2350
Training Loss: 0.028438697951905272
Epoch: 0 Batch: 2400
Training Loss: 0.02951089009642601
Epoch: 0 Batch: 2450
Training Loss: 0.02757799839486881
Epoch: 0 Batch: 2500
Training Loss: 0.02754877588748932
Epoch: 0 Batch: 2550
Training Loss: 0.026999780243518307
Epoch: 0 Batch: 2600
Training Loss: 0.02527130358494245
Epoch: 0 Batch: 2650
Training Loss: 0.025689153671264647
Epoch: 0 Batch: 2700
Training Loss: 0.024084185472241153
Epoch: 0 Batch: 2750
Training Loss: 0.022389032472263683
Epoch: 0 Batch: 2800
Training Loss: 0.024067909164088113
Epoch: 0 Batch: 2850
Training Loss: 0.02257113718149955
Epoch: 0 Batch: 2900
Training Loss: 0.021818057935813377
Epoch: 0 Batch: 2950
Training Loss: 0.021772488759735885
Epoch: 0 Batch: 3000
Training Loss: 0.021152619361877442
Epoch: 0 Batch: 3050
Training Loss: 0.020191722971494082
Epoch: 0 Batch: 3100
Training Loss: 0.02064898787006255
Epoch: 0 Batch: 3150
Training Loss: 0.020109268559349906
Epoch: 0 Batch: 3200
Training Loss: 0.01943122858181596
Epoch: 1 
 Validation Loss: 0.9805214431550767
---------------------------
Epoch: 1 Batch: 50
Training Loss: 1.2481430542469025
Epoch: 1 Batch: 100
Training Loss: 0.6209583163261414
Epoch: 1 Batch: 150
Training Loss: 0.4055306565761566
Epoch: 1 Batch: 200
Training Loss: 0.3017216989398003
Epoch: 1 Batch: 250
Training Loss: 0.24321757125854493
Epoch: 1 Batch: 300
Training Loss: 0.19309000651041666
Epoch: 1 Batch: 350
Training Loss: 0.16680436372756957
Epoch: 1 Batch: 400
Training Loss: 0.15009574547410012
Epoch: 1 Batch: 450
Training Loss: 0.13820487830373976
Epoch: 1 Batch: 500
Training Loss: 0.1186330281496048
Epoch: 1 Batch: 550
Training Loss: 0.1110749976201491
Epoch: 1 Batch: 600
Training Loss: 0.09429122000932694
Epoch: 1 Batch: 650
Training Loss: 0.08466664442649255
Epoch: 1 Batch: 700
Training Loss: 0.0785557587657656
Epoch: 1 Batch: 750
Training Loss: 0.07615544128417968
Epoch: 1 Batch: 800
Training Loss: 0.07597308948636056
Epoch: 1 Batch: 850
Training Loss: 0.06556738804368412
Epoch: 1 Batch: 900
Training Loss: 0.06636375500096216
Epoch: 1 Batch: 950
Training Loss: 0.06011050136465775
Epoch: 1 Batch: 1000
Training Loss: 0.05412475669384003
Epoch: 1 Batch: 1050
Training Loss: 0.05340672135353088
Epoch: 1 Batch: 1100
Training Loss: 0.04971484325148843
Epoch: 1 Batch: 1150
Training Loss: 0.0481554056768832
Epoch: 1 Batch: 1200
Training Loss: 0.04435302053888639
Epoch: 1 Batch: 1250
Training Loss: 0.04462605757713318
Epoch: 1 Batch: 1300
Training Loss: 0.041839931469697215
Epoch: 1 Batch: 1350
Training Loss: 0.04004661330470333
Epoch: 1 Batch: 1400
Training Loss: 0.03823696549449648
Epoch: 1 Batch: 1450
Training Loss: 0.03723361693579575
Epoch: 1 Batch: 1500
Training Loss: 0.03692003194491068
Epoch: 1 Batch: 1550
Training Loss: 0.03610400995900554
Epoch: 1 Batch: 1600
Training Loss: 0.03449527066200972
Epoch: 1 Batch: 1650
Training Loss: 0.032955662951324925
Epoch: 1 Batch: 1700
Training Loss: 0.030803543574669783
Epoch: 1 Batch: 1750
Training Loss: 0.031727022068841114
Epoch: 1 Batch: 1800
Training Loss: 0.030192951824929978
Epoch: 1 Batch: 1850
Training Loss: 0.028596976286656146
Epoch: 1 Batch: 1900
Training Loss: 0.02795249314684617
Epoch: 1 Batch: 1950
Training Loss: 0.026832502438471868
Epoch: 1 Batch: 2000
Training Loss: 0.02643521746993065
Epoch: 1 Batch: 2050
Training Loss: 0.025840589854775404
Epoch: 1 Batch: 2100
Training Loss: 0.025360747093246098
Epoch: 1 Batch: 2150
Training Loss: 0.023500683862109517
Epoch: 1 Batch: 2200
Training Loss: 0.02373618177392266
Epoch: 1 Batch: 2250
Training Loss: 0.02345425396495395
Epoch: 1 Batch: 2300
Training Loss: 0.022564803932024084
Epoch: 1 Batch: 2350
Training Loss: 0.02249950665108701
Epoch: 1 Batch: 2400
Training Loss: 0.02201901045938333
Epoch: 1 Batch: 2450
Training Loss: 0.02108576200446304
Epoch: 1 Batch: 2500
Training Loss: 0.02173536057472229
Epoch: 1 Batch: 2550
Training Loss: 0.019355402553782743
Epoch: 1 Batch: 2600
Training Loss: 0.019948333914463337
Epoch: 1 Batch: 2650
Training Loss: 0.020753910743965293
Epoch: 1 Batch: 2700
Training Loss: 0.019121969585065488
Epoch: 1 Batch: 2750
Training Loss: 0.019209812597794966
Epoch: 1 Batch: 2800
Training Loss: 0.018286243272679192
Epoch: 1 Batch: 2850
Training Loss: 0.018343449726439358
Epoch: 1 Batch: 2900
Training Loss: 0.017722258917216595
Epoch: 1 Batch: 2950
Training Loss: 0.016798435388985327
Epoch: 1 Batch: 3000
Training Loss: 0.017253704567750296
Epoch: 1 Batch: 3050
Training Loss: 0.015554715961706442
Epoch: 1 Batch: 3100
Training Loss: 0.016410638305448717
Epoch: 1 Batch: 3150
Training Loss: 0.016293706912843008
Epoch: 1 Batch: 3200
Training Loss: 0.015816143006086348
Epoch: 2 
 Validation Loss: 0.7864207274383969
---------------------------
Epoch: 2 Batch: 50
Training Loss: 1.027458540201187
Epoch: 2 Batch: 100
Training Loss: 0.48230251252651213
Epoch: 2 Batch: 150
Training Loss: 0.3406773030757904
Epoch: 2 Batch: 200
Training Loss: 0.24461468786001206
Epoch: 2 Batch: 250
Training Loss: 0.18856423354148866
Epoch: 2 Batch: 300
Training Loss: 0.17492448687553405
Epoch: 2 Batch: 350
Training Loss: 0.14374265040670123
Epoch: 2 Batch: 400
Training Loss: 0.1234507481008768
Epoch: 2 Batch: 450
Training Loss: 0.10445626854896545
Epoch: 2 Batch: 500
Training Loss: 0.10038392686843872
Epoch: 2 Batch: 550
Training Loss: 0.09143073320388793
Epoch: 2 Batch: 600
Training Loss: 0.08139115025599798
Epoch: 2 Batch: 650
Training Loss: 0.07705024123191834
Epoch: 2 Batch: 700
Training Loss: 0.06793311059474945
Epoch: 2 Batch: 750
Training Loss: 0.06216988738377889
Epoch: 2 Batch: 800
Training Loss: 0.058900217190384865
Epoch: 2 Batch: 850
Training Loss: 0.05720699850250693
Epoch: 2 Batch: 900
Training Loss: 0.052928459776772394
Epoch: 2 Batch: 950
Training Loss: 0.05007694043611225
Epoch: 2 Batch: 1000
Training Loss: 0.04976688569784164
Epoch: 2 Batch: 1050
Training Loss: 0.045340816577275594
Epoch: 2 Batch: 1100
Training Loss: 0.04308221573179418
Epoch: 2 Batch: 1150
Training Loss: 0.04114735831385073
Epoch: 2 Batch: 1200
Training Loss: 0.04029330506920815
Epoch: 2 Batch: 1250
Training Loss: 0.03783668746948242
Epoch: 2 Batch: 1300
Training Loss: 0.03673385638457078
Epoch: 2 Batch: 1350
Training Loss: 0.03415919961752715
Epoch: 2 Batch: 1400
Training Loss: 0.03329888675894056
Epoch: 2 Batch: 1450
Training Loss: 0.03230764956309878
Epoch: 2 Batch: 1500
Training Loss: 0.030918982028961182
Epoch: 2 Batch: 1550
Training Loss: 0.03108121556620444
Epoch: 2 Batch: 1600
Training Loss: 0.029097106978297232
Epoch: 2 Batch: 1650
Training Loss: 0.029162145780794548
Epoch: 2 Batch: 1700
Training Loss: 0.028127493402537177
Epoch: 2 Batch: 1750
Training Loss: 0.027171625546046664
Epoch: 2 Batch: 1800
Training Loss: 0.026669338477982415
Epoch: 2 Batch: 1850
Training Loss: 0.025415497373890233
Epoch: 2 Batch: 1900
Training Loss: 0.023941485097533778
Epoch: 2 Batch: 1950
Training Loss: 0.024172554046679767
Epoch: 2 Batch: 2000
Training Loss: 0.023684340059757232
Epoch: 2 Batch: 2050
Training Loss: 0.023187862140376395
Epoch: 2 Batch: 2100
Training Loss: 0.02307509496098473
Epoch: 2 Batch: 2150
Training Loss: 0.021531932631204294
Epoch: 2 Batch: 2200
Training Loss: 0.021759355637160215
Epoch: 2 Batch: 2250
Training Loss: 0.021033618158764308
Epoch: 2 Batch: 2300
Training Loss: 0.020750184396038884
Epoch: 2 Batch: 2350
Training Loss: 0.020942631275095838
Epoch: 2 Batch: 2400
Training Loss: 0.02028491586446762
Epoch: 2 Batch: 2450
Training Loss: 0.017561177282917256
Epoch: 2 Batch: 2500
Training Loss: 0.018535966634750368
Epoch: 2 Batch: 2550
Training Loss: 0.018207799593607583
Epoch: 2 Batch: 2600
Training Loss: 0.01805153835278291
Epoch: 2 Batch: 2650
Training Loss: 0.016846839526914202
Epoch: 2 Batch: 2700
Training Loss: 0.01876371154078731
Epoch: 2 Batch: 2750
Training Loss: 0.01755207672986117
Epoch: 2 Batch: 2800
Training Loss: 0.016280691368239265
Epoch: 2 Batch: 2850
Training Loss: 0.016787683169047038
Epoch: 2 Batch: 2900
Training Loss: 0.015163099395817723
Epoch: 2 Batch: 2950
Training Loss: 0.015210792897111278
Epoch: 2 Batch: 3000
Training Loss: 0.01513052902619044
Epoch: 2 Batch: 3050
Training Loss: 0.014847289734199399
Epoch: 2 Batch: 3100
Training Loss: 0.014733028373410624
Epoch: 2 Batch: 3150
Training Loss: 0.013942439158757527
Epoch: 2 Batch: 3200
Training Loss: 0.014395336080342531
Epoch: 3 
 Validation Loss: 0.7038168854183621
---------------------------
Epoch: 3 Batch: 50
Training Loss: 0.8889353144168853
Epoch: 3 Batch: 100
Training Loss: 0.4617360121011734
Epoch: 3 Batch: 150
Training Loss: 0.29325889468193056
Epoch: 3 Batch: 200
Training Loss: 0.22339558303356172
Epoch: 3 Batch: 250
Training Loss: 0.18583079528808594
Epoch: 3 Batch: 300
Training Loss: 0.1522079175710678
Epoch: 3 Batch: 350
Training Loss: 0.1272666825566973
Epoch: 3 Batch: 400
Training Loss: 0.1145040239393711
Epoch: 3 Batch: 450
Training Loss: 0.10218390703201294
Epoch: 3 Batch: 500
Training Loss: 0.08661108303070068
Epoch: 3 Batch: 550
Training Loss: 0.07920796004208651
Epoch: 3 Batch: 600
Training Loss: 0.07400222301483154
Epoch: 3 Batch: 650
Training Loss: 0.06983489045730004
Epoch: 3 Batch: 700
Training Loss: 0.06649172595569065
Epoch: 3 Batch: 750
Training Loss: 0.06091636276245117
Epoch: 3 Batch: 800
Training Loss: 0.05543949656188488
Epoch: 3 Batch: 850
Training Loss: 0.05187729512943941
Epoch: 3 Batch: 900
Training Loss: 0.04834101312690311
Epoch: 3 Batch: 950
Training Loss: 0.04635328154814871
Epoch: 3 Batch: 1000
Training Loss: 0.04595003318786621
Epoch: 3 Batch: 1050
Training Loss: 0.04190317579678127
Epoch: 3 Batch: 1100
Training Loss: 0.039199101653966034
Epoch: 3 Batch: 1150
Training Loss: 0.03870893602785857
Epoch: 3 Batch: 1200
Training Loss: 0.03465319412449996
Epoch: 3 Batch: 1250
Training Loss: 0.03389715981483459
Epoch: 3 Batch: 1300
Training Loss: 0.03398091815985166
Epoch: 3 Batch: 1350
Training Loss: 0.03338534068178248
Epoch: 3 Batch: 1400
Training Loss: 0.03223063673291888
Epoch: 3 Batch: 1450
Training Loss: 0.030363444870915905
Epoch: 3 Batch: 1500
Training Loss: 0.028375586827596028
Epoch: 3 Batch: 1550
Training Loss: 0.02803068753211729
Epoch: 3 Batch: 1600
Training Loss: 0.027762344740331173
Epoch: 3 Batch: 1650
Training Loss: 0.027295180703654432
Epoch: 3 Batch: 1700
Training Loss: 0.026553008100565743
Epoch: 3 Batch: 1750
Training Loss: 0.024253750358309064
Epoch: 3 Batch: 1800
Training Loss: 0.023865751061174605
Epoch: 3 Batch: 1850
Training Loss: 0.023858547081818452
Epoch: 3 Batch: 1900
Training Loss: 0.02188204100257472
Epoch: 3 Batch: 1950
Training Loss: 0.023469907351029225
Epoch: 3 Batch: 2000
Training Loss: 0.021757922857999802
Epoch: 3 Batch: 2050
Training Loss: 0.020750499963760376
Epoch: 3 Batch: 2100
Training Loss: 0.020620474900518146
Epoch: 3 Batch: 2150
Training Loss: 0.02039808423020119
Epoch: 3 Batch: 2200
Training Loss: 0.019538568014448338
Epoch: 3 Batch: 2250
Training Loss: 0.01994443490770128
Epoch: 3 Batch: 2300
Training Loss: 0.018288051382355067
Epoch: 3 Batch: 2350
Training Loss: 0.017748365376858
Epoch: 3 Batch: 2400
Training Loss: 0.017880305983126163
Epoch: 3 Batch: 2450
Training Loss: 0.0179069392291867
Epoch: 3 Batch: 2500
Training Loss: 0.01686318497657776
Epoch: 3 Batch: 2550
Training Loss: 0.016788533481897093
Epoch: 3 Batch: 2600
Training Loss: 0.015967005537106442
Epoch: 3 Batch: 2650
Training Loss: 0.016468924081550453
Epoch: 3 Batch: 2700
Training Loss: 0.01624636182078609
Epoch: 3 Batch: 2750
Training Loss: 0.015208568984811956
Epoch: 3 Batch: 2800
Training Loss: 0.015575561757598605
Epoch: 3 Batch: 2850
Training Loss: 0.015536615994938633
Epoch: 3 Batch: 2900
Training Loss: 0.014756684467710298
Epoch: 3 Batch: 2950
Training Loss: 0.01424340023832806
Epoch: 3 Batch: 3000
Training Loss: 0.01505813992023468
Epoch: 3 Batch: 3050
Training Loss: 0.01405728600064262
Epoch: 3 Batch: 3100
Training Loss: 0.013310967280018715
Epoch: 3 Batch: 3150
Training Loss: 0.013768383245619516
Epoch: 3 Batch: 3200
Training Loss: 0.013236822932958603
Epoch: 4 
 Validation Loss: 0.6585969696442286
---------------------------
Epoch: 4 Batch: 50
Training Loss: 0.8885571789741517
Epoch: 4 Batch: 100
Training Loss: 0.4323027014732361
Epoch: 4 Batch: 150
Training Loss: 0.30097765604654947
Epoch: 4 Batch: 200
Training Loss: 0.21489308297634124
Epoch: 4 Batch: 250
Training Loss: 0.1744619836807251
Epoch: 4 Batch: 300
Training Loss: 0.14575745403766632
Epoch: 4 Batch: 350
Training Loss: 0.11339223197528295
Epoch: 4 Batch: 400
Training Loss: 0.10713572680950165
Epoch: 4 Batch: 450
Training Loss: 0.09715559932920668
Epoch: 4 Batch: 500
Training Loss: 0.08484327745437623
Epoch: 4 Batch: 550
Training Loss: 0.0754252547567541
Epoch: 4 Batch: 600
Training Loss: 0.06721220841010411
Epoch: 4 Batch: 650
Training Loss: 0.06359046193269582
Epoch: 4 Batch: 700
Training Loss: 0.0619118914433888
Epoch: 4 Batch: 750
Training Loss: 0.055695249557495115
Epoch: 4 Batch: 800
Training Loss: 0.053184914290905
Epoch: 4 Batch: 850
Training Loss: 0.04603351915583891
Epoch: 4 Batch: 900
Training Loss: 0.04751782629224989
Epoch: 4 Batch: 950
Training Loss: 0.043050817690397564
Epoch: 4 Batch: 1000
Training Loss: 0.04407244646549225
Epoch: 4 Batch: 1050
Training Loss: 0.037874659186317806
Epoch: 4 Batch: 1100
Training Loss: 0.0365923197702928
Epoch: 4 Batch: 1150
Training Loss: 0.03537414561147275
Epoch: 4 Batch: 1200
Training Loss: 0.03479824801286062
Epoch: 4 Batch: 1250
Training Loss: 0.03335985813140869
Epoch: 4 Batch: 1300
Training Loss: 0.0318445667395225
Epoch: 4 Batch: 1350
Training Loss: 0.030429574207023337
Epoch: 4 Batch: 1400
Training Loss: 0.030556848560060773
Epoch: 4 Batch: 1450
Training Loss: 0.027817547156892972
Epoch: 4 Batch: 1500
Training Loss: 0.026680177688598632
Epoch: 4 Batch: 1550
Training Loss: 0.028155747190598518
Epoch: 4 Batch: 1600
Training Loss: 0.026972516812384127
Epoch: 4 Batch: 1650
Training Loss: 0.025864423621784555
Epoch: 4 Batch: 1700
Training Loss: 0.025435710619477664
Epoch: 4 Batch: 1750
Training Loss: 0.024570429597582134
Epoch: 4 Batch: 1800
Training Loss: 0.02392683294084337
Epoch: 4 Batch: 1850
Training Loss: 0.022530564391935196
Epoch: 4 Batch: 1900
Training Loss: 0.021399140985388504
Epoch: 4 Batch: 1950
Training Loss: 0.021241968274116518
Epoch: 4 Batch: 2000
Training Loss: 0.020258108675479888
Epoch: 4 Batch: 2050
Training Loss: 0.020408160221285936
Epoch: 4 Batch: 2100
Training Loss: 0.01948542716957274
Epoch: 4 Batch: 2150
Training Loss: 0.018961639016173606
Epoch: 4 Batch: 2200
Training Loss: 0.019367513710802253
Epoch: 4 Batch: 2250
Training Loss: 0.018425332917107476
Epoch: 4 Batch: 2300
Training Loss: 0.017524598525918048
Epoch: 4 Batch: 2350
Training Loss: 0.017247557056711074
Epoch: 4 Batch: 2400
Training Loss: 0.01851563133299351
Epoch: 4 Batch: 2450
Training Loss: 0.01657822647873236
Epoch: 4 Batch: 2500
Training Loss: 0.01679635809659958
Epoch: 4 Batch: 2550
Training Loss: 0.016885207610971786
Epoch: 4 Batch: 2600
Training Loss: 0.016267895285899822
Epoch: 4 Batch: 2650
Training Loss: 0.01483391040900968
Epoch: 4 Batch: 2700
Training Loss: 0.015147239786607248
Epoch: 4 Batch: 2750
Training Loss: 0.014216307791796598
Epoch: 4 Batch: 2800
Training Loss: 0.013969197975737709
Epoch: 4 Batch: 2850
Training Loss: 0.014417685479448553
Epoch: 4 Batch: 2900
Training Loss: 0.013831156903299792
Epoch: 4 Batch: 2950
Training Loss: 0.013766700918391599
Epoch: 4 Batch: 3000
Training Loss: 0.013587646941343944
Epoch: 4 Batch: 3050
Training Loss: 0.013419516047493356
Epoch: 4 Batch: 3100
Training Loss: 0.012954179048538208
Epoch: 4 Batch: 3150
Training Loss: 0.013004213278255766
Epoch: 4 Batch: 3200
Training Loss: 0.013174536786973477
Epoch: 5 
 Validation Loss: 0.6292654997772641
---------------------------
Epoch: 5 Batch: 50
Training Loss: 0.8040104186534882
Epoch: 5 Batch: 100
Training Loss: 0.41235141932964325
Epoch: 5 Batch: 150
Training Loss: 0.26994398633639016
Epoch: 5 Batch: 200
Training Loss: 0.20020236313343048
Epoch: 5 Batch: 250
Training Loss: 0.16453566789627075
Epoch: 5 Batch: 300
Training Loss: 0.13615298132101694
Epoch: 5 Batch: 350
Training Loss: 0.11903987884521484
Epoch: 5 Batch: 400
Training Loss: 0.09960461899638176
Epoch: 5 Batch: 450
Training Loss: 0.08978871888584561
Epoch: 5 Batch: 500
Training Loss: 0.08075192308425903
Epoch: 5 Batch: 550
Training Loss: 0.07628084605390376
Epoch: 5 Batch: 600
Training Loss: 0.06673256898919741
Epoch: 5 Batch: 650
Training Loss: 0.062986238186176
Epoch: 5 Batch: 700
Training Loss: 0.05640480143683297
Epoch: 5 Batch: 750
Training Loss: 0.056129637161890664
Epoch: 5 Batch: 800
Training Loss: 0.049456807076931
Epoch: 5 Batch: 850
Training Loss: 0.04810828804969788
Epoch: 5 Batch: 900
Training Loss: 0.04330244249767727
Epoch: 5 Batch: 950
Training Loss: 0.041073528151763114
Epoch: 5 Batch: 1000
Training Loss: 0.04003289383649826
Epoch: 5 Batch: 1050
Training Loss: 0.03852801408086504
Epoch: 5 Batch: 1100
Training Loss: 0.03754344262860038
Epoch: 5 Batch: 1150
Training Loss: 0.03458049940026325
Epoch: 5 Batch: 1200
Training Loss: 0.032834052840868634
Epoch: 5 Batch: 1250
Training Loss: 0.03207011547088623
Epoch: 5 Batch: 1300
Training Loss: 0.030746096510153552
Epoch: 5 Batch: 1350
Training Loss: 0.029850582370051632
Epoch: 5 Batch: 1400
Training Loss: 0.02824365905353001
Epoch: 5 Batch: 1450
Training Loss: 0.0286601368312178
Epoch: 5 Batch: 1500
Training Loss: 0.02492512627442678
Epoch: 5 Batch: 1550
Training Loss: 0.026601875405157765
Epoch: 5 Batch: 1600
Training Loss: 0.025470065288245677
Epoch: 5 Batch: 1650
Training Loss: 0.024118627526543358
Epoch: 5 Batch: 1700
Training Loss: 0.022982614881852093
Epoch: 5 Batch: 1750
Training Loss: 0.022734471133777072
Epoch: 5 Batch: 1800
Training Loss: 0.022056277559863197
Epoch: 5 Batch: 1850
Training Loss: 0.02136744695740777
Epoch: 5 Batch: 1900
Training Loss: 0.02040017378957648
Epoch: 5 Batch: 1950
Training Loss: 0.02000355613537324
Epoch: 5 Batch: 2000
Training Loss: 0.020174473732709884
Epoch: 5 Batch: 2050
Training Loss: 0.019483442800800974
Epoch: 5 Batch: 2100
Training Loss: 0.01923126396678743
Epoch: 5 Batch: 2150
Training Loss: 0.019307284937348478
Epoch: 5 Batch: 2200
Training Loss: 0.01773295692422173
Epoch: 5 Batch: 2250
Training Loss: 0.01758834699789683
Epoch: 5 Batch: 2300
Training Loss: 0.01735524447067924
Epoch: 5 Batch: 2350
Training Loss: 0.017454019442517708
Epoch: 5 Batch: 2400
Training Loss: 0.016562024280428887
Epoch: 5 Batch: 2450
Training Loss: 0.015792999875788787
Epoch: 5 Batch: 2500
Training Loss: 0.015811148047447203
Epoch: 5 Batch: 2550
Training Loss: 0.016192837135464537
Epoch: 5 Batch: 2600
Training Loss: 0.015445831876534682
Epoch: 5 Batch: 2650
Training Loss: 0.014804613072917146
Epoch: 5 Batch: 2700
Training Loss: 0.01507969355141675
Epoch: 5 Batch: 2750
Training Loss: 0.014585926511070945
Epoch: 5 Batch: 2800
Training Loss: 0.015011737963982991
Epoch: 5 Batch: 2850
Training Loss: 0.013393985848677785
Epoch: 5 Batch: 2900
Training Loss: 0.013940021477896592
Epoch: 5 Batch: 2950
Training Loss: 0.013238331150200407
Epoch: 5 Batch: 3000
Training Loss: 0.01328039934237798
Epoch: 5 Batch: 3050
Training Loss: 0.013284723016082264
Epoch: 5 Batch: 3100
Training Loss: 0.01263092419793529
Epoch: 5 Batch: 3150
Training Loss: 0.012495696601413545
Epoch: 5 Batch: 3200
Training Loss: 0.012388533055782318
Epoch: 6 
 Validation Loss: 0.6089210192362468
---------------------------
Epoch: 6 Batch: 50
Training Loss: 0.8238733273744583
Epoch: 6 Batch: 100
Training Loss: 0.3926208126544952
Epoch: 6 Batch: 150
Training Loss: 0.27116616249084474
Epoch: 6 Batch: 200
Training Loss: 0.18836190462112426
Epoch: 6 Batch: 250
Training Loss: 0.15855308842658997
Epoch: 6 Batch: 300
Training Loss: 0.12827221433321634
Epoch: 6 Batch: 350
Training Loss: 0.10618760330336434
Epoch: 6 Batch: 400
Training Loss: 0.10016664654016495
Epoch: 6 Batch: 450
Training Loss: 0.08520933541986678
Epoch: 6 Batch: 500
Training Loss: 0.08233299779891967
Epoch: 6 Batch: 550
Training Loss: 0.0710922783613205
Epoch: 6 Batch: 600
Training Loss: 0.06785245011250178
Epoch: 6 Batch: 650
Training Loss: 0.06017737379440895
Epoch: 6 Batch: 700
Training Loss: 0.05616222321987152
Epoch: 6 Batch: 750
Training Loss: 0.05399132533868154
Epoch: 6 Batch: 800
Training Loss: 0.04792492248117924
Epoch: 6 Batch: 850
Training Loss: 0.04753224099383635
Epoch: 6 Batch: 900
Training Loss: 0.0456308803624577
Epoch: 6 Batch: 950
Training Loss: 0.04102064446399086
Epoch: 6 Batch: 1000
Training Loss: 0.03844252580404282
Epoch: 6 Batch: 1050
Training Loss: 0.037440332883880253
Epoch: 6 Batch: 1100
Training Loss: 0.03820201418616555
Epoch: 6 Batch: 1150
Training Loss: 0.03449515816958054
Epoch: 6 Batch: 1200
Training Loss: 0.03157152906060219
Epoch: 6 Batch: 1250
Training Loss: 0.030393782949447633
Epoch: 6 Batch: 1300
Training Loss: 0.02888609583561237
Epoch: 6 Batch: 1350
Training Loss: 0.028942184801454898
Epoch: 6 Batch: 1400
Training Loss: 0.027615451386996677
Epoch: 6 Batch: 1450
Training Loss: 0.026551992235512568
Epoch: 6 Batch: 1500
Training Loss: 0.026084905525048573
Epoch: 6 Batch: 1550
Training Loss: 0.02476483121995003
Epoch: 6 Batch: 1600
Training Loss: 0.024405954908579587
Epoch: 6 Batch: 1650
Training Loss: 0.022120618242205996
Epoch: 6 Batch: 1700
Training Loss: 0.022004961178583256
Epoch: 6 Batch: 1750
Training Loss: 0.021893144164766586
Epoch: 6 Batch: 1800
Training Loss: 0.0225616568658087
Epoch: 6 Batch: 1850
Training Loss: 0.021693919703767106
Epoch: 6 Batch: 1900
Training Loss: 0.021031266764590616
Epoch: 6 Batch: 1950
Training Loss: 0.020077899801425443
Epoch: 6 Batch: 2000
Training Loss: 0.01906997059285641
Epoch: 6 Batch: 2050
Training Loss: 0.018384346395004088
Epoch: 6 Batch: 2100
Training Loss: 0.01842166815485273
Epoch: 6 Batch: 2150
Training Loss: 0.018209919666135034
Epoch: 6 Batch: 2200
Training Loss: 0.017468304092233832
Epoch: 6 Batch: 2250
Training Loss: 0.01762176121605767
Epoch: 6 Batch: 2300
Training Loss: 0.01661232715067656
Epoch: 6 Batch: 2350
Training Loss: 0.016858917221109916
Epoch: 6 Batch: 2400
Training Loss: 0.01722166600326697
Epoch: 6 Batch: 2450
Training Loss: 0.015337409291948592
Epoch: 6 Batch: 2500
Training Loss: 0.015460530638694764
Epoch: 6 Batch: 2550
Training Loss: 0.014811684804804185
Epoch: 6 Batch: 2600
Training Loss: 0.015491450520662161
Epoch: 6 Batch: 2650
Training Loss: 0.015374090615308509
Epoch: 6 Batch: 2700
Training Loss: 0.014549138987505877
Epoch: 6 Batch: 2750
Training Loss: 0.014185098832303827
Epoch: 6 Batch: 2800
Training Loss: 0.014042056522199085
Epoch: 6 Batch: 2850
Training Loss: 0.013809118229046203
Epoch: 6 Batch: 2900
Training Loss: 0.013708228304468353
Epoch: 6 Batch: 2950
Training Loss: 0.012978465233818959
Epoch: 6 Batch: 3000
Training Loss: 0.013060291409492492
Epoch: 6 Batch: 3050
Training Loss: 0.012828133067146677
Epoch: 6 Batch: 3100
Training Loss: 0.012421367331858604
Epoch: 6 Batch: 3150
Training Loss: 0.012419285963452051
Epoch: 6 Batch: 3200
Training Loss: 0.012038443461060524
Epoch: 7 
 Validation Loss: 0.5945317536592484
---------------------------
Epoch: 7 Batch: 50
Training Loss: 0.7820371276140213
Epoch: 7 Batch: 100
Training Loss: 0.3959513568878174
Epoch: 7 Batch: 150
Training Loss: 0.25001924971739453
Epoch: 7 Batch: 200
Training Loss: 0.1948532435297966
Epoch: 7 Batch: 250
Training Loss: 0.1507852210998535
Epoch: 7 Batch: 300
Training Loss: 0.13092794835567476
Epoch: 7 Batch: 350
Training Loss: 0.11088236570358276
Epoch: 7 Batch: 400
Training Loss: 0.10255574271082878
Epoch: 7 Batch: 450
Training Loss: 0.08624120652675629
Epoch: 7 Batch: 500
Training Loss: 0.07468403565883637
Epoch: 7 Batch: 550
Training Loss: 0.07236209652640603
Epoch: 7 Batch: 600
Training Loss: 0.06627886647979418
Epoch: 7 Batch: 650
Training Loss: 0.061371941933265096
Epoch: 7 Batch: 700
Training Loss: 0.05412950958524432
Epoch: 7 Batch: 750
Training Loss: 0.05086607480049133
Epoch: 7 Batch: 800
Training Loss: 0.04792213037610054
Epoch: 7 Batch: 850
Training Loss: 0.0454578978173873
Epoch: 7 Batch: 900
Training Loss: 0.04319324135780334
Epoch: 7 Batch: 950
Training Loss: 0.039961568305366915
Epoch: 7 Batch: 1000
Training Loss: 0.03744022911787033
Epoch: 7 Batch: 1050
Training Loss: 0.035260498183114185
Epoch: 7 Batch: 1100
Training Loss: 0.03376794197342613
Epoch: 7 Batch: 1150
Training Loss: 0.03348084968069325
Epoch: 7 Batch: 1200
Training Loss: 0.03129711364706357
Epoch: 7 Batch: 1250
Training Loss: 0.02880472207069397
Epoch: 7 Batch: 1300
Training Loss: 0.030211372696436367
Epoch: 7 Batch: 1350
Training Loss: 0.029547773312639306
Epoch: 7 Batch: 1400
Training Loss: 0.027409086525440215
Epoch: 7 Batch: 1450
Training Loss: 0.02659252980659748
Epoch: 7 Batch: 1500
Training Loss: 0.02531272220611572
Epoch: 7 Batch: 1550
Training Loss: 0.024239291337228592
Epoch: 7 Batch: 1600
Training Loss: 0.024371172413229943
Epoch: 7 Batch: 1650
Training Loss: 0.02352503422534827
Epoch: 7 Batch: 1700
Training Loss: 0.02162359225399354
Epoch: 7 Batch: 1750
Training Loss: 0.021998255048479354
Epoch: 7 Batch: 1800
Training Loss: 0.020369826489024693
Epoch: 7 Batch: 1850
Training Loss: 0.020715722844407366
Epoch: 7 Batch: 1900
Training Loss: 0.019813046314214405
Epoch: 7 Batch: 1950
Training Loss: 0.020018825561572345
Epoch: 7 Batch: 2000
Training Loss: 0.020242475986480712
Epoch: 7 Batch: 2050
Training Loss: 0.01788005985864779
Epoch: 7 Batch: 2100
Training Loss: 0.017589073876539865
Epoch: 7 Batch: 2150
Training Loss: 0.017842616979465927
Epoch: 7 Batch: 2200
Training Loss: 0.017198447503826837
Epoch: 7 Batch: 2250
Training Loss: 0.01671192873848809
Epoch: 7 Batch: 2300
Training Loss: 0.017171926109687142
Epoch: 7 Batch: 2350
Training Loss: 0.015760082868819542
Epoch: 7 Batch: 2400
Training Loss: 0.01527359655747811
Epoch: 7 Batch: 2450
Training Loss: 0.01557362935980972
Epoch: 7 Batch: 2500
Training Loss: 0.015398468220233916
Epoch: 7 Batch: 2550
Training Loss: 0.014601287771673764
Epoch: 7 Batch: 2600
Training Loss: 0.014376953943417623
Epoch: 7 Batch: 2650
Training Loss: 0.015146453639246382
Epoch: 7 Batch: 2700
Training Loss: 0.014766187358785559
Epoch: 7 Batch: 2750
Training Loss: 0.014142115560444918
Epoch: 7 Batch: 2800
Training Loss: 0.014534427630049841
Epoch: 7 Batch: 2850
Training Loss: 0.013136606948417529
Epoch: 7 Batch: 2900
Training Loss: 0.013060926702515832
Epoch: 7 Batch: 2950
Training Loss: 0.012860726471674645
Epoch: 7 Batch: 3000
Training Loss: 0.012484288056691488
Epoch: 7 Batch: 3050
Training Loss: 0.01247562961500199
Epoch: 7 Batch: 3100
Training Loss: 0.011906886196905567
Epoch: 7 Batch: 3150
Training Loss: 0.012130843628020513
Epoch: 7 Batch: 3200
Training Loss: 0.011775765717029572
Epoch: 8 
 Validation Loss: 0.5823175473345651
---------------------------
Epoch: 8 Batch: 50
Training Loss: 0.7765225303173066
Epoch: 8 Batch: 100
Training Loss: 0.3774044644832611
Epoch: 8 Batch: 150
Training Loss: 0.248065252105395
Epoch: 8 Batch: 200
Training Loss: 0.1972891491651535
Epoch: 8 Batch: 250
Training Loss: 0.14722884821891785
Epoch: 8 Batch: 300
Training Loss: 0.12330456793308259
Epoch: 8 Batch: 350
Training Loss: 0.11058273834841592
Epoch: 8 Batch: 400
Training Loss: 0.09397578254342079
Epoch: 8 Batch: 450
Training Loss: 0.08488506701257494
Epoch: 8 Batch: 500
Training Loss: 0.0757219575047493
Epoch: 8 Batch: 550
Training Loss: 0.0703877959468148
Epoch: 8 Batch: 600
Training Loss: 0.06480172803004583
Epoch: 8 Batch: 650
Training Loss: 0.055379525331350474
Epoch: 8 Batch: 700
Training Loss: 0.053958309633391245
Epoch: 8 Batch: 750
Training Loss: 0.04925286205609639
Epoch: 8 Batch: 800
Training Loss: 0.047444441318511964
Epoch: 8 Batch: 850
Training Loss: 0.04485352894839118
Epoch: 8 Batch: 900
Training Loss: 0.04207683662573496
Epoch: 8 Batch: 950
Training Loss: 0.038842760638186806
Epoch: 8 Batch: 1000
Training Loss: 0.037895399630069736
Epoch: 8 Batch: 1050
Training Loss: 0.03558940739858718
Epoch: 8 Batch: 1100
Training Loss: 0.035154585459015585
Epoch: 8 Batch: 1150
Training Loss: 0.03472427186758622
Epoch: 8 Batch: 1200
Training Loss: 0.03037847568591436
Epoch: 8 Batch: 1250
Training Loss: 0.030745967984199523
Epoch: 8 Batch: 1300
Training Loss: 0.02980683711858896
Epoch: 8 Batch: 1350
Training Loss: 0.027948808073997498
Epoch: 8 Batch: 1400
Training Loss: 0.02659062419618879
Epoch: 8 Batch: 1450
Training Loss: 0.02554385561367561
Epoch: 8 Batch: 1500
Training Loss: 0.023971392631530763
Epoch: 8 Batch: 1550
Training Loss: 0.023961516138046018
Epoch: 8 Batch: 1600
Training Loss: 0.023273345604538916
Epoch: 8 Batch: 1650
Training Loss: 0.022197313019723604
Epoch: 8 Batch: 1700
Training Loss: 0.022748442660359774
Epoch: 8 Batch: 1750
Training Loss: 0.02102084047453744
Epoch: 8 Batch: 1800
Training Loss: 0.020336466398504047
Epoch: 8 Batch: 1850
Training Loss: 0.02039903238012984
Epoch: 8 Batch: 1900
Training Loss: 0.02009006205358003
Epoch: 8 Batch: 1950
Training Loss: 0.019574976716286097
Epoch: 8 Batch: 2000
Training Loss: 0.019294581413269042
Epoch: 8 Batch: 2050
Training Loss: 0.01841530168928751
Epoch: 8 Batch: 2100
Training Loss: 0.017195751127742586
Epoch: 8 Batch: 2150
Training Loss: 0.0172545558630034
Epoch: 8 Batch: 2200
Training Loss: 0.016948065852577035
Epoch: 8 Batch: 2250
Training Loss: 0.016495114088058472
Epoch: 8 Batch: 2300
Training Loss: 0.016880986068559728
Epoch: 8 Batch: 2350
Training Loss: 0.015866824477276904
Epoch: 8 Batch: 2400
Training Loss: 0.015014192139108975
Epoch: 8 Batch: 2450
Training Loss: 0.015333475275915496
Epoch: 8 Batch: 2500
Training Loss: 0.014867812418937683
Epoch: 8 Batch: 2550
Training Loss: 0.014552500738817102
Epoch: 8 Batch: 2600
Training Loss: 0.014580738590313839
Epoch: 8 Batch: 2650
Training Loss: 0.014912336232527247
Epoch: 8 Batch: 2700
Training Loss: 0.013628866860160122
Epoch: 8 Batch: 2750
Training Loss: 0.013316098895939914
Epoch: 8 Batch: 2800
Training Loss: 0.013190864654523985
Epoch: 8 Batch: 2850
Training Loss: 0.013042154333047699
Epoch: 8 Batch: 2900
Training Loss: 0.01237119624326969
Epoch: 8 Batch: 2950
Training Loss: 0.0129823835825516
Epoch: 8 Batch: 3000
Training Loss: 0.012392944832642873
Epoch: 8 Batch: 3050
Training Loss: 0.012279847166577323
Epoch: 8 Batch: 3100
Training Loss: 0.01183266625288994
Epoch: 8 Batch: 3150
Training Loss: 0.011671880871530564
Epoch: 8 Batch: 3200
Training Loss: 0.012004276318475605
Epoch: 9 
 Validation Loss: 0.5731735597054164
---------------------------
Epoch: 9 Batch: 50
Training Loss: 0.76948861181736
Epoch: 9 Batch: 100
Training Loss: 0.3600266388058662
Epoch: 9 Batch: 150
Training Loss: 0.2456443860133489
Epoch: 9 Batch: 200
Training Loss: 0.1875225505232811
Epoch: 9 Batch: 250
Training Loss: 0.1451294960975647
Epoch: 9 Batch: 300
Training Loss: 0.1228136529525121
Epoch: 9 Batch: 350
Training Loss: 0.10440936122621809
Epoch: 9 Batch: 400
Training Loss: 0.09352345712482929
Epoch: 9 Batch: 450
Training Loss: 0.08256317006217109
Epoch: 9 Batch: 500
Training Loss: 0.07318077796697617
Epoch: 9 Batch: 550
Training Loss: 0.06746351935646751
Epoch: 9 Batch: 600
Training Loss: 0.06043950840830803
Epoch: 9 Batch: 650
Training Loss: 0.05782843406383808
Epoch: 9 Batch: 700
Training Loss: 0.05744120542492185
Epoch: 9 Batch: 750
Training Loss: 0.04974366680781046
Epoch: 9 Batch: 800
Training Loss: 0.04518357325345278
Epoch: 9 Batch: 850
Training Loss: 0.0434578175053877
Epoch: 9 Batch: 900
Training Loss: 0.04113209621773826
Epoch: 9 Batch: 950
Training Loss: 0.037874723672866824
Epoch: 9 Batch: 1000
Training Loss: 0.037824995934963225
Epoch: 9 Batch: 1050
Training Loss: 0.03484129477114904
Epoch: 9 Batch: 1100
Training Loss: 0.03395413761789148
Epoch: 9 Batch: 1150
Training Loss: 0.03210695816122967
Epoch: 9 Batch: 1200
Training Loss: 0.031339978501200676
Epoch: 9 Batch: 1250
Training Loss: 0.028318790125846864
Epoch: 9 Batch: 1300
Training Loss: 0.02908611467251411
Epoch: 9 Batch: 1350
Training Loss: 0.027198487144929392
Epoch: 9 Batch: 1400
Training Loss: 0.027502833093915668
Epoch: 9 Batch: 1450
Training Loss: 0.024338567688547332
Epoch: 9 Batch: 1500
Training Loss: 0.025122262716293337
Epoch: 9 Batch: 1550
Training Loss: 0.02376075363928272
Epoch: 9 Batch: 1600
Training Loss: 0.023288535457104444
Epoch: 9 Batch: 1650
Training Loss: 0.022978622913360596
Epoch: 9 Batch: 1700
Training Loss: 0.02186716074452681
Epoch: 9 Batch: 1750
Training Loss: 0.02106984519958496
Epoch: 9 Batch: 1800
Training Loss: 0.020498808291223313
Epoch: 9 Batch: 1850
Training Loss: 0.020328851712716593
Epoch: 9 Batch: 1900
Training Loss: 0.019266398643192493
Epoch: 9 Batch: 1950
Training Loss: 0.01954570200198736
Epoch: 9 Batch: 2000
Training Loss: 0.018349116295576097
Epoch: 9 Batch: 2050
Training Loss: 0.018436883033775702
Epoch: 9 Batch: 2100
Training Loss: 0.018157573172024318
Epoch: 9 Batch: 2150
Training Loss: 0.016854767023130903
Epoch: 9 Batch: 2200
Training Loss: 0.01641020745038986
Epoch: 9 Batch: 2250
Training Loss: 0.01630975772274865
Epoch: 9 Batch: 2300
Training Loss: 0.015973320940266485
Epoch: 9 Batch: 2350
Training Loss: 0.014958382299605836
Epoch: 9 Batch: 2400
Training Loss: 0.015254980449875196
Epoch: 9 Batch: 2450
Training Loss: 0.014636226649187048
Epoch: 9 Batch: 2500
Training Loss: 0.014343882358074189
Epoch: 9 Batch: 2550
Training Loss: 0.014228770475761564
Epoch: 9 Batch: 2600
Training Loss: 0.013908143593714787
Epoch: 9 Batch: 2650
Training Loss: 0.013285683044847452
Epoch: 9 Batch: 2700
Training Loss: 0.013519895617608671
Epoch: 9 Batch: 2750
Training Loss: 0.013862064318223432
Epoch: 9 Batch: 2800
Training Loss: 0.01298675034727369
Epoch: 9 Batch: 2850
Training Loss: 0.012787955915718748
Epoch: 9 Batch: 2900
Training Loss: 0.012803178213793657
Epoch: 9 Batch: 2950
Training Loss: 0.012729240837743726
Epoch: 9 Batch: 3000
Training Loss: 0.012375307440757752
Epoch: 9 Batch: 3050
Training Loss: 0.012524205973890961
Epoch: 9 Batch: 3100
Training Loss: 0.012252199572901572
Epoch: 9 Batch: 3150
Training Loss: 0.011825622887838455
Epoch: 9 Batch: 3200
Training Loss: 0.011167475245893002
Epoch: 10 
 Validation Loss: 0.5650148941410913
---------------------------
Epoch: 10 Batch: 50
Training Loss: 0.7473677635192871
Epoch: 10 Batch: 100
Training Loss: 0.36717196434736254
Epoch: 10 Batch: 150
Training Loss: 0.23897971073786417
Epoch: 10 Batch: 200
Training Loss: 0.18708914160728454
Epoch: 10 Batch: 250
Training Loss: 0.14509321177005768
Epoch: 10 Batch: 300
Training Loss: 0.11889659682909648
Epoch: 10 Batch: 350
Training Loss: 0.10177639007568359
Epoch: 10 Batch: 400
Training Loss: 0.09076357416808606
Epoch: 10 Batch: 450
Training Loss: 0.07970702038870918
Epoch: 10 Batch: 500
Training Loss: 0.07141778928041458
Epoch: 10 Batch: 550
Training Loss: 0.06499979799444025
Epoch: 10 Batch: 600
Training Loss: 0.06008769581715266
Epoch: 10 Batch: 650
Training Loss: 0.055298177966704734
Epoch: 10 Batch: 700
Training Loss: 0.05288068303040096
Epoch: 10 Batch: 750
Training Loss: 0.04842352223396301
Epoch: 10 Batch: 800
Training Loss: 0.045338905900716785
Epoch: 10 Batch: 850
Training Loss: 0.04234998941421509
Epoch: 10 Batch: 900
Training Loss: 0.041047638456026714
Epoch: 10 Batch: 950
Training Loss: 0.0426615802237862
Epoch: 10 Batch: 1000
Training Loss: 0.03724050980806351
Epoch: 10 Batch: 1050
Training Loss: 0.03404242203349159
Epoch: 10 Batch: 1100
Training Loss: 0.033325304985046385
Epoch: 10 Batch: 1150
Training Loss: 0.0314464899768
Epoch: 10 Batch: 1200
Training Loss: 0.029968145514527957
Epoch: 10 Batch: 1250
Training Loss: 0.02989190137386322
Epoch: 10 Batch: 1300
Training Loss: 0.02801144239994196
Epoch: 10 Batch: 1350
Training Loss: 0.027449817613319116
Epoch: 10 Batch: 1400
Training Loss: 0.025468179498400006
Epoch: 10 Batch: 1450
Training Loss: 0.024986293295334125
Epoch: 10 Batch: 1500
Training Loss: 0.024999795794487
Epoch: 10 Batch: 1550
Training Loss: 0.0232522702024829
Epoch: 10 Batch: 1600
Training Loss: 0.02209699062630534
Epoch: 10 Batch: 1650
Training Loss: 0.0226526314200777
Epoch: 10 Batch: 1700
Training Loss: 0.02204612020184012
Epoch: 10 Batch: 1750
Training Loss: 0.020728734816823687
Epoch: 10 Batch: 1800
Training Loss: 0.02054116901424196
Epoch: 10 Batch: 1850
Training Loss: 0.02044469971914549
Epoch: 10 Batch: 1900
Training Loss: 0.019961304664611816
Epoch: 10 Batch: 1950
Training Loss: 0.019039299258818993
Epoch: 10 Batch: 2000
Training Loss: 0.018446795523166658
Epoch: 10 Batch: 2050
Training Loss: 0.01834520325428102
Epoch: 10 Batch: 2100
Training Loss: 0.01825900761854081
Epoch: 10 Batch: 2150
Training Loss: 0.01681184677190559
Epoch: 10 Batch: 2200
Training Loss: 0.01674242984164845
Epoch: 10 Batch: 2250
Training Loss: 0.015791952755716113
Epoch: 10 Batch: 2300
Training Loss: 0.015657746053260306
Epoch: 10 Batch: 2350
Training Loss: 0.016174657154590526
Epoch: 10 Batch: 2400
Training Loss: 0.015165757909417153
Epoch: 10 Batch: 2450
Training Loss: 0.015111714187933474
Epoch: 10 Batch: 2500
Training Loss: 0.01462282737493515
Epoch: 10 Batch: 2550
Training Loss: 0.014474762701520733
Epoch: 10 Batch: 2600
Training Loss: 0.01395973536830682
Epoch: 10 Batch: 2650
Training Loss: 0.013207954516950643
Epoch: 10 Batch: 2700
Training Loss: 0.013610549171765645
Epoch: 10 Batch: 2750
Training Loss: 0.013200719464908946
Epoch: 10 Batch: 2800
Training Loss: 0.013083229522619928
Epoch: 10 Batch: 2850
Training Loss: 0.01272078321691145
Epoch: 10 Batch: 2900
Training Loss: 0.012431519278164568
Epoch: 10 Batch: 2950
Training Loss: 0.012045851703417503
Epoch: 10 Batch: 3000
Training Loss: 0.012247110893328984
Epoch: 10 Batch: 3050
Training Loss: 0.011857849437682356
Epoch: 10 Batch: 3100
Training Loss: 0.011544578681069036
Epoch: 10 Batch: 3150
Training Loss: 0.011614045453450036
Epoch: 10 Batch: 3200
Training Loss: 0.010745067819952965
Epoch: 11 
 Validation Loss: 0.5582711107201046
---------------------------
Epoch: 11 Batch: 50
Training Loss: 0.7348616820573807
Epoch: 11 Batch: 100
Training Loss: 0.36793599486351014
Epoch: 11 Batch: 150
Training Loss: 0.24503695805867512
Epoch: 11 Batch: 200
Training Loss: 0.1749251028895378
Epoch: 11 Batch: 250
Training Loss: 0.1458226947784424
Epoch: 11 Batch: 300
Training Loss: 0.11267751187086106
Epoch: 11 Batch: 350
Training Loss: 0.10228168887751443
Epoch: 11 Batch: 400
Training Loss: 0.08918757699429988
Epoch: 11 Batch: 450
Training Loss: 0.08359535131189558
Epoch: 11 Batch: 500
Training Loss: 0.07294920217990876
Epoch: 11 Batch: 550
Training Loss: 0.06517183141274886
Epoch: 11 Batch: 600
Training Loss: 0.06102825840314229
Epoch: 11 Batch: 650
Training Loss: 0.055432237753501305
Epoch: 11 Batch: 700
Training Loss: 0.053564263071332656
Epoch: 11 Batch: 750
Training Loss: 0.04834258818626404
Epoch: 11 Batch: 800
Training Loss: 0.04332525454461575
Epoch: 11 Batch: 850
Training Loss: 0.04158997518174788
Epoch: 11 Batch: 900
Training Loss: 0.03912475426991781
Epoch: 11 Batch: 950
Training Loss: 0.037740807439151564
Epoch: 11 Batch: 1000
Training Loss: 0.03653860828280449
Epoch: 11 Batch: 1050
Training Loss: 0.03523261958644504
Epoch: 11 Batch: 1100
Training Loss: 0.03396084912798621
Epoch: 11 Batch: 1150
Training Loss: 0.03275261464326278
Epoch: 11 Batch: 1200
Training Loss: 0.029952440112829208
Epoch: 11 Batch: 1250
Training Loss: 0.030757618832588197
Epoch: 11 Batch: 1300
Training Loss: 0.028056364196997422
Epoch: 11 Batch: 1350
Training Loss: 0.02591073011910474
Epoch: 11 Batch: 1400
Training Loss: 0.025712801111595972
Epoch: 11 Batch: 1450
Training Loss: 0.025177007481969636
Epoch: 11 Batch: 1500
Training Loss: 0.023432239611943562
Epoch: 11 Batch: 1550
Training Loss: 0.023636310369737686
Epoch: 11 Batch: 1600
Training Loss: 0.02201433777809143
Epoch: 11 Batch: 1650
Training Loss: 0.021592972495339133
Epoch: 11 Batch: 1700
Training Loss: 0.020957925442387077
Epoch: 11 Batch: 1750
Training Loss: 0.020687740019389562
Epoch: 11 Batch: 1800
Training Loss: 0.01976653069257736
Epoch: 11 Batch: 1850
Training Loss: 0.019608504433889645
Epoch: 11 Batch: 1900
Training Loss: 0.019342816001490543
Epoch: 11 Batch: 1950
Training Loss: 0.018124186671697178
Epoch: 11 Batch: 2000
Training Loss: 0.018150715947151185
Epoch: 11 Batch: 2050
Training Loss: 0.018183037144381825
Epoch: 11 Batch: 2100
Training Loss: 0.017201114495595297
Epoch: 11 Batch: 2150
Training Loss: 0.016651488653449124
Epoch: 11 Batch: 2200
Training Loss: 0.016158704310655594
Epoch: 11 Batch: 2250
Training Loss: 0.015616133703125848
Epoch: 11 Batch: 2300
Training Loss: 0.016159504820471224
Epoch: 11 Batch: 2350
Training Loss: 0.015056024777128342
Epoch: 11 Batch: 2400
Training Loss: 0.014811582267284393
Epoch: 11 Batch: 2450
Training Loss: 0.01447942282472338
Epoch: 11 Batch: 2500
Training Loss: 0.01428963143825531
Epoch: 11 Batch: 2550
Training Loss: 0.014416626516510458
Epoch: 11 Batch: 2600
Training Loss: 0.014155694911113152
Epoch: 11 Batch: 2650
Training Loss: 0.012544193537730092
Epoch: 11 Batch: 2700
Training Loss: 0.013331970793229562
Epoch: 11 Batch: 2750
Training Loss: 0.01240232673558322
Epoch: 11 Batch: 2800
Training Loss: 0.013074661482657705
Epoch: 11 Batch: 2850
Training Loss: 0.012094804623670746
Epoch: 11 Batch: 2900
Training Loss: 0.01278042200310477
Epoch: 11 Batch: 2950
Training Loss: 0.012422425191281205
Epoch: 11 Batch: 3000
Training Loss: 0.0119415056904157
Epoch: 11 Batch: 3050
Training Loss: 0.011638425823117866
Epoch: 11 Batch: 3100
Training Loss: 0.012530252241319225
Epoch: 11 Batch: 3150
Training Loss: 0.011059884485744294
Epoch: 11 Batch: 3200
Training Loss: 0.011426061736419796
Epoch: 12 
 Validation Loss: 0.5532041897376379
---------------------------
Epoch: 12 Batch: 50
Training Loss: 0.7055215382575989
Epoch: 12 Batch: 100
Training Loss: 0.36343475013971327
Epoch: 12 Batch: 150
Training Loss: 0.24337869505087534
Epoch: 12 Batch: 200
Training Loss: 0.18267830401659013
Epoch: 12 Batch: 250
Training Loss: 0.14381907486915588
Epoch: 12 Batch: 300
Training Loss: 0.11511677453915278
Epoch: 12 Batch: 350
Training Loss: 0.09871115675994328
Epoch: 12 Batch: 400
Training Loss: 0.09193243287503719
Epoch: 12 Batch: 450
Training Loss: 0.08200500223371718
Epoch: 12 Batch: 500
Training Loss: 0.07172986543178558
Epoch: 12 Batch: 550
Training Loss: 0.06645230434157631
Epoch: 12 Batch: 600
Training Loss: 0.05984759196639061
Epoch: 12 Batch: 650
Training Loss: 0.056850865804232084
Epoch: 12 Batch: 700
Training Loss: 0.050686601528099604
Epoch: 12 Batch: 750
Training Loss: 0.04851534573237101
Epoch: 12 Batch: 800
Training Loss: 0.04668524116277695
Epoch: 12 Batch: 850
Training Loss: 0.04247575339149026
Epoch: 12 Batch: 900
Training Loss: 0.04097202257977592
Epoch: 12 Batch: 950
Training Loss: 0.036877661316018356
Epoch: 12 Batch: 1000
Training Loss: 0.038979081571102143
Epoch: 12 Batch: 1050
Training Loss: 0.03361162954852694
Epoch: 12 Batch: 1100
Training Loss: 0.034886056130582634
Epoch: 12 Batch: 1150
Training Loss: 0.030255527003951694
Epoch: 12 Batch: 1200
Training Loss: 0.029657894025246302
Epoch: 12 Batch: 1250
Training Loss: 0.02767219488620758
Epoch: 12 Batch: 1300
Training Loss: 0.028262803600384637
Epoch: 12 Batch: 1350
Training Loss: 0.02655284883799376
Epoch: 12 Batch: 1400
Training Loss: 0.025516489957060132
Epoch: 12 Batch: 1450
Training Loss: 0.024272736742578703
Epoch: 12 Batch: 1500
Training Loss: 0.02336145540078481
Epoch: 12 Batch: 1550
Training Loss: 0.023676656907604586
Epoch: 12 Batch: 1600
Training Loss: 0.02238001076504588
Epoch: 12 Batch: 1650
Training Loss: 0.022344817248257723
Epoch: 12 Batch: 1700
Training Loss: 0.02220029692439472
Epoch: 12 Batch: 1750
Training Loss: 0.020194240604128156
Epoch: 12 Batch: 1800
Training Loss: 0.01988571806086434
Epoch: 12 Batch: 1850
Training Loss: 0.020022417564649838
Epoch: 12 Batch: 1900
Training Loss: 0.01841283777826711
Epoch: 12 Batch: 1950
Training Loss: 0.0180349808320021
Epoch: 12 Batch: 2000
Training Loss: 0.01782259675860405
Epoch: 12 Batch: 2050
Training Loss: 0.01768808751571469
Epoch: 12 Batch: 2100
Training Loss: 0.01662096917629242
Epoch: 12 Batch: 2150
Training Loss: 0.017073106793470162
Epoch: 12 Batch: 2200
Training Loss: 0.016107026609507476
Epoch: 12 Batch: 2250
Training Loss: 0.016357419543796114
Epoch: 12 Batch: 2300
Training Loss: 0.015186615132767221
Epoch: 12 Batch: 2350
Training Loss: 0.015049424716766844
Epoch: 12 Batch: 2400
Training Loss: 0.015234906921784083
Epoch: 12 Batch: 2450
Training Loss: 0.014318854127611432
Epoch: 12 Batch: 2500
Training Loss: 0.013932110273838044
Epoch: 12 Batch: 2550
Training Loss: 0.013903694339826996
Epoch: 12 Batch: 2600
Training Loss: 0.013783700683942208
Epoch: 12 Batch: 2650
Training Loss: 0.01302913230545116
Epoch: 12 Batch: 2700
Training Loss: 0.01345574446298458
Epoch: 12 Batch: 2750
Training Loss: 0.012952961531552402
Epoch: 12 Batch: 2800
Training Loss: 0.012491452757801329
Epoch: 12 Batch: 2850
Training Loss: 0.012791874544662341
Epoch: 12 Batch: 2900
Training Loss: 0.011890111997209745
Epoch: 12 Batch: 2950
Training Loss: 0.011678965617034395
Epoch: 12 Batch: 3000
Training Loss: 0.01144283577799797
Epoch: 12 Batch: 3050
Training Loss: 0.0117653517742626
Epoch: 12 Batch: 3100
Training Loss: 0.011914939034369684
Epoch: 12 Batch: 3150
Training Loss: 0.01137162066641308
Epoch: 12 Batch: 3200
Training Loss: 0.011038925889879465
Epoch: 13 
 Validation Loss: 0.5485400620434019
---------------------------
Epoch: 13 Batch: 50
Training Loss: 0.7360829168558121
Epoch: 13 Batch: 100
Training Loss: 0.34575661182403566
Epoch: 13 Batch: 150
Training Loss: 0.2445227281252543
Epoch: 13 Batch: 200
Training Loss: 0.18044042512774466
Epoch: 13 Batch: 250
Training Loss: 0.1445107626914978
Epoch: 13 Batch: 300
Training Loss: 0.12197301377852758
Epoch: 13 Batch: 350
Training Loss: 0.10543887027672359
Epoch: 13 Batch: 400
Training Loss: 0.09006619520485401
Epoch: 13 Batch: 450
Training Loss: 0.08383756485250261
Epoch: 13 Batch: 500
Training Loss: 0.07198475217819214
Epoch: 13 Batch: 550
Training Loss: 0.06476940913633866
Epoch: 13 Batch: 600
Training Loss: 0.05820210407177607
Epoch: 13 Batch: 650
Training Loss: 0.05271846587841327
Epoch: 13 Batch: 700
Training Loss: 0.0498122861129897
Epoch: 13 Batch: 750
Training Loss: 0.046373303095499675
Epoch: 13 Batch: 800
Training Loss: 0.04513306830078363
Epoch: 13 Batch: 850
Training Loss: 0.04197300960035885
Epoch: 13 Batch: 900
Training Loss: 0.03962120347552829
Epoch: 13 Batch: 950
Training Loss: 0.03593778023594304
Epoch: 13 Batch: 1000
Training Loss: 0.03476303398609162
Epoch: 13 Batch: 1050
Training Loss: 0.03559322606949579
Epoch: 13 Batch: 1100
Training Loss: 0.03333346399393949
Epoch: 13 Batch: 1150
Training Loss: 0.030499955026999762
Epoch: 13 Batch: 1200
Training Loss: 0.029478011975685756
Epoch: 13 Batch: 1250
Training Loss: 0.02785920615196228
Epoch: 13 Batch: 1300
Training Loss: 0.02807135802048903
Epoch: 13 Batch: 1350
Training Loss: 0.02620052695274353
Epoch: 13 Batch: 1400
Training Loss: 0.025799224781138556
Epoch: 13 Batch: 1450
Training Loss: 0.024458547768921687
Epoch: 13 Batch: 1500
Training Loss: 0.023620573500792184
Epoch: 13 Batch: 1550
Training Loss: 0.022514647591498592
Epoch: 13 Batch: 1600
Training Loss: 0.021866162326186895
Epoch: 13 Batch: 1650
Training Loss: 0.0210740615382339
Epoch: 13 Batch: 1700
Training Loss: 0.02075943757505978
Epoch: 13 Batch: 1750
Training Loss: 0.020839270438466753
Epoch: 13 Batch: 1800
Training Loss: 0.01941001671883795
Epoch: 13 Batch: 1850
Training Loss: 0.01988939657404616
Epoch: 13 Batch: 1900
Training Loss: 0.018552767571650055
Epoch: 13 Batch: 1950
Training Loss: 0.01805547203773107
Epoch: 13 Batch: 2000
Training Loss: 0.01801179750263691
Epoch: 13 Batch: 2050
Training Loss: 0.017181804718040837
Epoch: 13 Batch: 2100
Training Loss: 0.016493918469973973
Epoch: 13 Batch: 2150
Training Loss: 0.016123360076615977
Epoch: 13 Batch: 2200
Training Loss: 0.01600985678759488
Epoch: 13 Batch: 2250
Training Loss: 0.015839318500624762
Epoch: 13 Batch: 2300
Training Loss: 0.015093649677608324
Epoch: 13 Batch: 2350
Training Loss: 0.015419326995281464
Epoch: 13 Batch: 2400
Training Loss: 0.014774485106269518
Epoch: 13 Batch: 2450
Training Loss: 0.015165107736782152
Epoch: 13 Batch: 2500
Training Loss: 0.014493970143795014
Epoch: 13 Batch: 2550
Training Loss: 0.013693916657391716
Epoch: 13 Batch: 2600
Training Loss: 0.013923312998734987
Epoch: 13 Batch: 2650
Training Loss: 0.012946484403790167
Epoch: 13 Batch: 2700
Training Loss: 0.012778641669838517
Epoch: 13 Batch: 2750
Training Loss: 0.012617795835841786
Epoch: 13 Batch: 2800
Training Loss: 0.013004908604281289
Epoch: 13 Batch: 2850
Training Loss: 0.012539903835246438
Epoch: 13 Batch: 2900
Training Loss: 0.01216222160849078
Epoch: 13 Batch: 2950
Training Loss: 0.011945260035789619
Epoch: 13 Batch: 3000
Training Loss: 0.011383296916882196
Epoch: 13 Batch: 3050
Training Loss: 0.011413933771555541
Epoch: 13 Batch: 3100
Training Loss: 0.011508971135462485
Epoch: 13 Batch: 3150
Training Loss: 0.011268952734886654
Epoch: 13 Batch: 3200
Training Loss: 0.011688309870660305
Epoch: 14 
 Validation Loss: 0.5438226219680574
---------------------------
Epoch: 14 Batch: 50
Training Loss: 0.72026591360569
Epoch: 14 Batch: 100
Training Loss: 0.35844558358192447
Epoch: 14 Batch: 150
Training Loss: 0.22894396245479584
Epoch: 14 Batch: 200
Training Loss: 0.17502983406186104
Epoch: 14 Batch: 250
Training Loss: 0.13467666113376617
Epoch: 14 Batch: 300
Training Loss: 0.11469292203585307
Epoch: 14 Batch: 350
Training Loss: 0.10031610693250384
Epoch: 14 Batch: 400
Training Loss: 0.08942304037511349
Epoch: 14 Batch: 450
Training Loss: 0.07895838975906372
Epoch: 14 Batch: 500
Training Loss: 0.07402554249763489
Epoch: 14 Batch: 550
Training Loss: 0.0645706925608895
Epoch: 14 Batch: 600
Training Loss: 0.05673592388629913
Epoch: 14 Batch: 650
Training Loss: 0.0545198812851539
Epoch: 14 Batch: 700
Training Loss: 0.052413855152470726
Epoch: 14 Batch: 750
Training Loss: 0.04761495792865753
Epoch: 14 Batch: 800
Training Loss: 0.04347688876092434
Epoch: 14 Batch: 850
Training Loss: 0.040245863690095786
Epoch: 14 Batch: 900
Training Loss: 0.03809756398200989
Epoch: 14 Batch: 950
Training Loss: 0.036065147801449426
Epoch: 14 Batch: 1000
Training Loss: 0.035171214938163754
Epoch: 14 Batch: 1050
Training Loss: 0.03382613900161925
Epoch: 14 Batch: 1100
Training Loss: 0.03256544449112632
Epoch: 14 Batch: 1150
Training Loss: 0.032410024197205255
Epoch: 14 Batch: 1200
Training Loss: 0.02956543820599715
Epoch: 14 Batch: 1250
Training Loss: 0.029675924634933473
Epoch: 14 Batch: 1300
Training Loss: 0.026600215274554032
Epoch: 14 Batch: 1350
Training Loss: 0.02691682360790394
Epoch: 14 Batch: 1400
Training Loss: 0.02413538747600147
Epoch: 14 Batch: 1450
Training Loss: 0.02438494826185292
Epoch: 14 Batch: 1500
Training Loss: 0.022693578282992045
Epoch: 14 Batch: 1550
Training Loss: 0.02211267234817628
Epoch: 14 Batch: 1600
Training Loss: 0.02166802478954196
Epoch: 14 Batch: 1650
Training Loss: 0.021419390801227452
Epoch: 14 Batch: 1700
Training Loss: 0.021230907177223878
Epoch: 14 Batch: 1750
Training Loss: 0.019114081144332887
Epoch: 14 Batch: 1800
Training Loss: 0.019590371665027407
Epoch: 14 Batch: 1850
Training Loss: 0.019469498814763248
Epoch: 14 Batch: 1900
Training Loss: 0.018577776174796255
Epoch: 14 Batch: 1950
Training Loss: 0.017751655899561368
Epoch: 14 Batch: 2000
Training Loss: 0.01798557744920254
Epoch: 14 Batch: 2050
Training Loss: 0.016954283539841814
Epoch: 14 Batch: 2100
Training Loss: 0.017271351998760585
Epoch: 14 Batch: 2150
Training Loss: 0.01597942242788714
Epoch: 14 Batch: 2200
Training Loss: 0.016846653331409802
Epoch: 14 Batch: 2250
Training Loss: 0.01564482413397895
Epoch: 14 Batch: 2300
Training Loss: 0.015281915936781012
Epoch: 14 Batch: 2350
Training Loss: 0.014569901646451748
Epoch: 14 Batch: 2400
Training Loss: 0.014681269812087217
Epoch: 14 Batch: 2450
Training Loss: 0.01424335063720236
Epoch: 14 Batch: 2500
Training Loss: 0.013535661745071412
Epoch: 14 Batch: 2550
Training Loss: 0.013209747496773215
Epoch: 14 Batch: 2600
Training Loss: 0.01367429562486135
Epoch: 14 Batch: 2650
Training Loss: 0.013403507077469016
Epoch: 14 Batch: 2700
Training Loss: 0.013196751409106785
Epoch: 14 Batch: 2750
Training Loss: 0.012611672466451472
Epoch: 14 Batch: 2800
Training Loss: 0.012028815980468477
Epoch: 14 Batch: 2850
Training Loss: 0.012311559654118722
Epoch: 14 Batch: 2900
Training Loss: 0.01181334589062066
Epoch: 14 Batch: 2950
Training Loss: 0.01174394186270439
Epoch: 14 Batch: 3000
Training Loss: 0.012343605399131775
Epoch: 14 Batch: 3050
Training Loss: 0.011930686440624174
Epoch: 14 Batch: 3100
Training Loss: 0.011246978957806865
Epoch: 14 Batch: 3150
Training Loss: 0.010858417881859674
Epoch: 14 Batch: 3200
Training Loss: 0.011040793433785438
Epoch: 15 
 Validation Loss: 0.540043733186192
---------------------------
Epoch: 15 Batch: 50
Training Loss: 0.715688853263855
Epoch: 15 Batch: 100
Training Loss: 0.34675072371959687
Epoch: 15 Batch: 150
Training Loss: 0.22348864495754242
Epoch: 15 Batch: 200
Training Loss: 0.18028000086545945
Epoch: 15 Batch: 250
Training Loss: 0.14319306898117065
Epoch: 15 Batch: 300
Training Loss: 0.11550122718016306
Epoch: 15 Batch: 350
Training Loss: 0.09937297778470175
Epoch: 15 Batch: 400
Training Loss: 0.08887873440980912
Epoch: 15 Batch: 450
Training Loss: 0.07770378053188325
Epoch: 15 Batch: 500
Training Loss: 0.07114450389146805
Epoch: 15 Batch: 550
Training Loss: 0.06318615978414362
Epoch: 15 Batch: 600
Training Loss: 0.058395253817240395
Epoch: 15 Batch: 650
Training Loss: 0.05367234473045056
Epoch: 15 Batch: 700
Training Loss: 0.0499530240893364
Epoch: 15 Batch: 750
Training Loss: 0.04752552151679992
Epoch: 15 Batch: 800
Training Loss: 0.04370633449405432
Epoch: 15 Batch: 850
Training Loss: 0.041866805237882276
Epoch: 15 Batch: 900
Training Loss: 0.03785837325784895
Epoch: 15 Batch: 950
Training Loss: 0.03855279222915047
Epoch: 15 Batch: 1000
Training Loss: 0.03418898689746857
Epoch: 15 Batch: 1050
Training Loss: 0.03558110066822597
Epoch: 15 Batch: 1100
Training Loss: 0.03275399706580422
Epoch: 15 Batch: 1150
Training Loss: 0.03175207042175791
Epoch: 15 Batch: 1200
Training Loss: 0.02875359242161115
Epoch: 15 Batch: 1250
Training Loss: 0.029789956068992613
Epoch: 15 Batch: 1300
Training Loss: 0.025560198632570412
Epoch: 15 Batch: 1350
Training Loss: 0.025772528140633195
Epoch: 15 Batch: 1400
Training Loss: 0.02420763203075954
Epoch: 15 Batch: 1450
Training Loss: 0.02456384958892033
Epoch: 15 Batch: 1500
Training Loss: 0.022302972575028737
Epoch: 15 Batch: 1550
Training Loss: 0.022673302196687267
Epoch: 15 Batch: 1600
Training Loss: 0.020809611435979605
Epoch: 15 Batch: 1650
Training Loss: 0.02158901608351505
Epoch: 15 Batch: 1700
Training Loss: 0.02094775964232052
Epoch: 15 Batch: 1750
Training Loss: 0.019383073585374014
Epoch: 15 Batch: 1800
Training Loss: 0.019331583579381306
Epoch: 15 Batch: 1850
Training Loss: 0.01847892640410243
Epoch: 15 Batch: 1900
Training Loss: 0.018907360889409718
Epoch: 15 Batch: 1950
Training Loss: 0.016881278111384464
Epoch: 15 Batch: 2000
Training Loss: 0.016615937173366548
Epoch: 15 Batch: 2050
Training Loss: 0.01772620991962712
Epoch: 15 Batch: 2100
Training Loss: 0.016416576618239993
Epoch: 15 Batch: 2150
Training Loss: 0.016263737429020018
Epoch: 15 Batch: 2200
Training Loss: 0.016258418329737404
Epoch: 15 Batch: 2250
Training Loss: 0.015290995743539598
Epoch: 15 Batch: 2300
Training Loss: 0.015081310466579768
Epoch: 15 Batch: 2350
Training Loss: 0.014682698528817359
Epoch: 15 Batch: 2400
Training Loss: 0.014254271673659483
Epoch: 15 Batch: 2450
Training Loss: 0.014234392995737036
Epoch: 15 Batch: 2500
Training Loss: 0.014348916244506836
Epoch: 15 Batch: 2550
Training Loss: 0.014896343595841351
Epoch: 15 Batch: 2600
Training Loss: 0.013004117355896877
Epoch: 15 Batch: 2650
Training Loss: 0.013307831815953524
Epoch: 15 Batch: 2700
Training Loss: 0.012764791868351125
Epoch: 15 Batch: 2750
Training Loss: 0.01265301327271895
Epoch: 15 Batch: 2800
Training Loss: 0.012110564900296075
Epoch: 15 Batch: 2850
Training Loss: 0.012255926205400836
Epoch: 15 Batch: 2900
Training Loss: 0.011844803721740328
Epoch: 15 Batch: 2950
Training Loss: 0.01183239310474719
Epoch: 15 Batch: 3000
Training Loss: 0.01185700015227
Epoch: 15 Batch: 3050
Training Loss: 0.011331465361548252
Epoch: 15 Batch: 3100
Training Loss: 0.011290352248376415
Epoch: 15 Batch: 3150
Training Loss: 0.011070986882088676
Epoch: 15 Batch: 3200
Training Loss: 0.010528105515986682
Epoch: 16 
 Validation Loss: 0.5364770104487737
---------------------------
Epoch: 16 Batch: 50
Training Loss: 0.6941389375925064
Epoch: 16 Batch: 100
Training Loss: 0.3438320705294609
Epoch: 16 Batch: 150
Training Loss: 0.2380703632036845
Epoch: 16 Batch: 200
Training Loss: 0.17452765837311746
Epoch: 16 Batch: 250
Training Loss: 0.13304195511341094
Epoch: 16 Batch: 300
Training Loss: 0.11058281143506368
Epoch: 16 Batch: 350
Training Loss: 0.10038411821637835
Epoch: 16 Batch: 400
Training Loss: 0.08953335598111152
Epoch: 16 Batch: 450
Training Loss: 0.0753773899210824
Epoch: 16 Batch: 500
Training Loss: 0.07209206914901733
Epoch: 16 Batch: 550
Training Loss: 0.062490943453528666
Epoch: 16 Batch: 600
Training Loss: 0.057199240426222486
Epoch: 16 Batch: 650
Training Loss: 0.05226574196265294
Epoch: 16 Batch: 700
Training Loss: 0.050142114843641006
Epoch: 16 Batch: 750
Training Loss: 0.046598200877507524
Epoch: 16 Batch: 800
Training Loss: 0.04317876994609833
Epoch: 16 Batch: 850
Training Loss: 0.04028582629035501
Epoch: 16 Batch: 900
Training Loss: 0.03891715168952942
Epoch: 16 Batch: 950
Training Loss: 0.03808523250253577
Epoch: 16 Batch: 1000
Training Loss: 0.03402773079276085
Epoch: 16 Batch: 1050
Training Loss: 0.03229236721992493
Epoch: 16 Batch: 1100
Training Loss: 0.030509401641108772
Epoch: 16 Batch: 1150
Training Loss: 0.029961851057798965
Epoch: 16 Batch: 1200
Training Loss: 0.030034317175547283
Epoch: 16 Batch: 1250
Training Loss: 0.027452047324180603
Epoch: 16 Batch: 1300
Training Loss: 0.02649492891935202
Epoch: 16 Batch: 1350
Training Loss: 0.026159006202662434
Epoch: 16 Batch: 1400
Training Loss: 0.025420987542186465
Epoch: 16 Batch: 1450
Training Loss: 0.02414042563273989
Epoch: 16 Batch: 1500
Training Loss: 0.024729655464490255
Epoch: 16 Batch: 1550
Training Loss: 0.0219276987929498
Epoch: 16 Batch: 1600
Training Loss: 0.022113228999078274
Epoch: 16 Batch: 1650
Training Loss: 0.020568912878180995
Epoch: 16 Batch: 1700
Training Loss: 0.020170744166654698
Epoch: 16 Batch: 1750
Training Loss: 0.019834880437169758
Epoch: 16 Batch: 1800
Training Loss: 0.01882085104783376
Epoch: 16 Batch: 1850
Training Loss: 0.01869199361350085
Epoch: 16 Batch: 1900
Training Loss: 0.018165916659330067
Epoch: 16 Batch: 1950
Training Loss: 0.01843977845632113
Epoch: 16 Batch: 2000
Training Loss: 0.018374267101287842
Epoch: 16 Batch: 2050
Training Loss: 0.016962869429006808
Epoch: 16 Batch: 2100
Training Loss: 0.017470794873578208
Epoch: 16 Batch: 2150
Training Loss: 0.015761849145556606
Epoch: 16 Batch: 2200
Training Loss: 0.01597913454879414
Epoch: 16 Batch: 2250
Training Loss: 0.01565837201807234
Epoch: 16 Batch: 2300
Training Loss: 0.015400817614534627
Epoch: 16 Batch: 2350
Training Loss: 0.015026025252139315
Epoch: 16 Batch: 2400
Training Loss: 0.014307778514921664
Epoch: 16 Batch: 2450
Training Loss: 0.014253922348119775
Epoch: 16 Batch: 2500
Training Loss: 0.014080631148815154
Epoch: 16 Batch: 2550
Training Loss: 0.013395071403653014
Epoch: 16 Batch: 2600
Training Loss: 0.01369194413606937
Epoch: 16 Batch: 2650
Training Loss: 0.013449450382646526
Epoch: 16 Batch: 2700
Training Loss: 0.01257605133233247
Epoch: 16 Batch: 2750
Training Loss: 0.012427920840003274
Epoch: 16 Batch: 2800
Training Loss: 0.012879382190959795
Epoch: 16 Batch: 2850
Training Loss: 0.012445725767236007
Epoch: 16 Batch: 2900
Training Loss: 0.01186561680045621
Epoch: 16 Batch: 2950
Training Loss: 0.011504652944661803
Epoch: 16 Batch: 3000
Training Loss: 0.01168956176439921
Epoch: 16 Batch: 3050
Training Loss: 0.011575471729528709
Epoch: 16 Batch: 3100
Training Loss: 0.011323782542059499
Epoch: 16 Batch: 3150
Training Loss: 0.011038541216698904
Epoch: 16 Batch: 3200
Training Loss: 0.011047001387923956
Epoch: 17 
 Validation Loss: 0.5332604143354628
---------------------------
Epoch: 17 Batch: 50
Training Loss: 0.7083665710687638
Epoch: 17 Batch: 100
Training Loss: 0.3450424808263779
Epoch: 17 Batch: 150
Training Loss: 0.22961257815361022
Epoch: 17 Batch: 200
Training Loss: 0.17926966041326522
Epoch: 17 Batch: 250
Training Loss: 0.14071631467342377
Epoch: 17 Batch: 300
Training Loss: 0.11474603801965713
Epoch: 17 Batch: 350
Training Loss: 0.09757171298776354
Epoch: 17 Batch: 400
Training Loss: 0.08742980092763901
Epoch: 17 Batch: 450
Training Loss: 0.07827059533860949
Epoch: 17 Batch: 500
Training Loss: 0.06898018044233323
Epoch: 17 Batch: 550
Training Loss: 0.06186290193687786
Epoch: 17 Batch: 600
Training Loss: 0.05710842197140058
Epoch: 17 Batch: 650
Training Loss: 0.05298588418043577
Epoch: 17 Batch: 700
Training Loss: 0.049269071987697054
Epoch: 17 Batch: 750
Training Loss: 0.044996299624443056
Epoch: 17 Batch: 800
Training Loss: 0.042128605991601946
Epoch: 17 Batch: 850
Training Loss: 0.03989297176108641
Epoch: 17 Batch: 900
Training Loss: 0.03901309238539802
Epoch: 17 Batch: 950
Training Loss: 0.03716998909649096
Epoch: 17 Batch: 1000
Training Loss: 0.0341704469025135
Epoch: 17 Batch: 1050
Training Loss: 0.031058430189178105
Epoch: 17 Batch: 1100
Training Loss: 0.03117955825545571
Epoch: 17 Batch: 1150
Training Loss: 0.03087546923886175
Epoch: 17 Batch: 1200
Training Loss: 0.027859903822342554
Epoch: 17 Batch: 1250
Training Loss: 0.028090461325645445
Epoch: 17 Batch: 1300
Training Loss: 0.02609737162406628
Epoch: 17 Batch: 1350
Training Loss: 0.024946555186201026
Epoch: 17 Batch: 1400
Training Loss: 0.025166605349097933
Epoch: 17 Batch: 1450
Training Loss: 0.023564064646589344
Epoch: 17 Batch: 1500
Training Loss: 0.023000032126903533
Epoch: 17 Batch: 1550
Training Loss: 0.02156056680986958
Epoch: 17 Batch: 1600
Training Loss: 0.021156631093472242
Epoch: 17 Batch: 1650
Training Loss: 0.020998006029562515
Epoch: 17 Batch: 1700
Training Loss: 0.020123850913608774
Epoch: 17 Batch: 1750
Training Loss: 0.019898362738745554
Epoch: 17 Batch: 1800
Training Loss: 0.01984597600168652
Epoch: 17 Batch: 1850
Training Loss: 0.019657969780870387
Epoch: 17 Batch: 1900
Training Loss: 0.01804902500227878
Epoch: 17 Batch: 1950
Training Loss: 0.017847070082640037
Epoch: 17 Batch: 2000
Training Loss: 0.01779777458310127
Epoch: 17 Batch: 2050
Training Loss: 0.01731608915619734
Epoch: 17 Batch: 2100
Training Loss: 0.016590803960959116
Epoch: 17 Batch: 2150
Training Loss: 0.015940516105918
Epoch: 17 Batch: 2200
Training Loss: 0.015501944869756699
Epoch: 17 Batch: 2250
Training Loss: 0.015668398168351916
Epoch: 17 Batch: 2300
Training Loss: 0.014720734668814617
Epoch: 17 Batch: 2350
Training Loss: 0.014892939557420446
Epoch: 17 Batch: 2400
Training Loss: 0.014037470320860545
Epoch: 17 Batch: 2450
Training Loss: 0.013872959516486342
Epoch: 17 Batch: 2500
Training Loss: 0.01435733141899109
Epoch: 17 Batch: 2550
Training Loss: 0.014318351663795172
Epoch: 17 Batch: 2600
Training Loss: 0.013066239173595721
Epoch: 17 Batch: 2650
Training Loss: 0.013335640914035293
Epoch: 17 Batch: 2700
Training Loss: 0.012645094648555472
Epoch: 17 Batch: 2750
Training Loss: 0.012883392095565795
Epoch: 17 Batch: 2800
Training Loss: 0.012370215037039348
Epoch: 17 Batch: 2850
Training Loss: 0.012248830178327728
Epoch: 17 Batch: 2900
Training Loss: 0.011500162759731555
Epoch: 17 Batch: 2950
Training Loss: 0.011055602805089142
Epoch: 17 Batch: 3000
Training Loss: 0.011115749845902126
Epoch: 17 Batch: 3050
Training Loss: 0.011409041041233501
Epoch: 17 Batch: 3100
Training Loss: 0.011717089135800638
Epoch: 17 Batch: 3150
Training Loss: 0.011002128389146593
Epoch: 17 Batch: 3200
Training Loss: 0.01075547161512077
Epoch: 18 
 Validation Loss: 0.5305018011066649
---------------------------
Epoch: 18 Batch: 50
Training Loss: 0.7292557901144028
Epoch: 18 Batch: 100
Training Loss: 0.3485340604186058
Epoch: 18 Batch: 150
Training Loss: 0.23707091252009074
Epoch: 18 Batch: 200
Training Loss: 0.17680897876620294
Epoch: 18 Batch: 250
Training Loss: 0.13414659035205842
Epoch: 18 Batch: 300
Training Loss: 0.11540917247533798
Epoch: 18 Batch: 350
Training Loss: 0.09341934893812452
Epoch: 18 Batch: 400
Training Loss: 0.08911706559360028
Epoch: 18 Batch: 450
Training Loss: 0.07253152032693228
Epoch: 18 Batch: 500
Training Loss: 0.07147029638290406
Epoch: 18 Batch: 550
Training Loss: 0.06333102556792172
Epoch: 18 Batch: 600
Training Loss: 0.05729642743865649
Epoch: 18 Batch: 650
Training Loss: 0.05449764650601607
Epoch: 18 Batch: 700
Training Loss: 0.048178412999425616
Epoch: 18 Batch: 750
Training Loss: 0.0441883252064387
Epoch: 18 Batch: 800
Training Loss: 0.04242748521268368
Epoch: 18 Batch: 850
Training Loss: 0.04140035306706148
Epoch: 18 Batch: 900
Training Loss: 0.03839380125204722
Epoch: 18 Batch: 950
Training Loss: 0.03640202694817593
Epoch: 18 Batch: 1000
Training Loss: 0.033984857231378555
Epoch: 18 Batch: 1050
Training Loss: 0.03309794579233442
Epoch: 18 Batch: 1100
Training Loss: 0.03134875473651019
Epoch: 18 Batch: 1150
Training Loss: 0.030598920039508655
Epoch: 18 Batch: 1200
Training Loss: 0.02912935088078181
Epoch: 18 Batch: 1250
Training Loss: 0.027300013184547424
Epoch: 18 Batch: 1300
Training Loss: 0.028083388323967274
Epoch: 18 Batch: 1350
Training Loss: 0.026562841181401852
Epoch: 18 Batch: 1400
Training Loss: 0.022222257320369993
Epoch: 18 Batch: 1450
Training Loss: 0.023411094258571494
Epoch: 18 Batch: 1500
Training Loss: 0.02362392318248749
Epoch: 18 Batch: 1550
Training Loss: 0.021913721042294654
Epoch: 18 Batch: 1600
Training Loss: 0.020955852437764405
Epoch: 18 Batch: 1650
Training Loss: 0.021329093467105518
Epoch: 18 Batch: 1700
Training Loss: 0.02029074605773477
Epoch: 18 Batch: 1750
Training Loss: 0.01979696161406381
Epoch: 18 Batch: 1800
Training Loss: 0.020171263714631398
Epoch: 18 Batch: 1850
Training Loss: 0.018493541672423078
Epoch: 18 Batch: 1900
Training Loss: 0.018339481087107406
Epoch: 18 Batch: 1950
Training Loss: 0.01765675106109717
Epoch: 18 Batch: 2000
Training Loss: 0.01748017239570618
Epoch: 18 Batch: 2050
Training Loss: 0.016003865352491053
Epoch: 18 Batch: 2100
Training Loss: 0.0168554853115763
Epoch: 18 Batch: 2150
Training Loss: 0.016043343239052352
Epoch: 18 Batch: 2200
Training Loss: 0.015442073101347143
Epoch: 18 Batch: 2250
Training Loss: 0.0149926389058431
Epoch: 18 Batch: 2300
Training Loss: 0.015603011405986288
Epoch: 18 Batch: 2350
Training Loss: 0.014839300082084981
Epoch: 18 Batch: 2400
Training Loss: 0.013538233861327171
Epoch: 18 Batch: 2450
Training Loss: 0.013375444740665203
Epoch: 18 Batch: 2500
Training Loss: 0.013622750115394593
Epoch: 18 Batch: 2550
Training Loss: 0.013103010444080129
Epoch: 18 Batch: 2600
Training Loss: 0.013288673689732185
Epoch: 18 Batch: 2650
Training Loss: 0.013666729589678206
Epoch: 18 Batch: 2700
Training Loss: 0.01286591973569658
Epoch: 18 Batch: 2750
Training Loss: 0.012433134458281777
Epoch: 18 Batch: 2800
Training Loss: 0.012429696832384382
Epoch: 18 Batch: 2850
Training Loss: 0.012268855916826348
Epoch: 18 Batch: 2900
Training Loss: 0.011624808537548987
Epoch: 18 Batch: 2950
Training Loss: 0.011567825826547914
Epoch: 18 Batch: 3000
Training Loss: 0.011376116851965587
Epoch: 18 Batch: 3050
Training Loss: 0.011073976608573413
Epoch: 18 Batch: 3100
Training Loss: 0.011236662191729392
Epoch: 18 Batch: 3150
Training Loss: 0.010968790508451915
Epoch: 18 Batch: 3200
Training Loss: 0.010636468594893813
Epoch: 19 
 Validation Loss: 0.5281514545281728
---------------------------
Epoch: 19 Batch: 50
Training Loss: 0.6653423571586609
Epoch: 19 Batch: 100
Training Loss: 0.3429290708899498
Epoch: 19 Batch: 150
Training Loss: 0.23300608019034066
Epoch: 19 Batch: 200
Training Loss: 0.17435281738638878
Epoch: 19 Batch: 250
Training Loss: 0.13552340829372406
Epoch: 19 Batch: 300
Training Loss: 0.11708081881205241
Epoch: 19 Batch: 350
Training Loss: 0.09420049233095987
Epoch: 19 Batch: 400
Training Loss: 0.08627206981182098
Epoch: 19 Batch: 450
Training Loss: 0.0769950352774726
Epoch: 19 Batch: 500
Training Loss: 0.06912220031023025
Epoch: 19 Batch: 550
Training Loss: 0.06555532753467559
Epoch: 19 Batch: 600
Training Loss: 0.056990815053383506
Epoch: 19 Batch: 650
Training Loss: 0.05223707208266625
Epoch: 19 Batch: 700
Training Loss: 0.05023926006896155
Epoch: 19 Batch: 750
Training Loss: 0.047330692847569786
Epoch: 19 Batch: 800
Training Loss: 0.042483197785913945
Epoch: 19 Batch: 850
Training Loss: 0.0397056462133632
Epoch: 19 Batch: 900
Training Loss: 0.03870871427986357
Epoch: 19 Batch: 950
Training Loss: 0.0366954275181419
Epoch: 19 Batch: 1000
Training Loss: 0.03363478383421898
Epoch: 19 Batch: 1050
Training Loss: 0.03276935634158906
Epoch: 19 Batch: 1100
Training Loss: 0.030960607691244646
Epoch: 19 Batch: 1150
Training Loss: 0.0288281359102415
Epoch: 19 Batch: 1200
Training Loss: 0.0286806121468544
Epoch: 19 Batch: 1250
Training Loss: 0.027130463695526123
Epoch: 19 Batch: 1300
Training Loss: 0.02620467130954449
Epoch: 19 Batch: 1350
Training Loss: 0.02548071755303277
Epoch: 19 Batch: 1400
Training Loss: 0.024530637924160277
Epoch: 19 Batch: 1450
Training Loss: 0.023697183789878055
Epoch: 19 Batch: 1500
Training Loss: 0.023036525865395865
Epoch: 19 Batch: 1550
Training Loss: 0.022916005292246418
Epoch: 19 Batch: 1600
Training Loss: 0.021968269739300012
Epoch: 19 Batch: 1650
Training Loss: 0.020964151656988894
Epoch: 19 Batch: 1700
Training Loss: 0.019930089151158053
Epoch: 19 Batch: 1750
Training Loss: 0.0200851913690567
Epoch: 19 Batch: 1800
Training Loss: 0.019440486944384047
Epoch: 19 Batch: 1850
Training Loss: 0.018167473061664684
Epoch: 19 Batch: 1900
Training Loss: 0.018145922892972042
Epoch: 19 Batch: 1950
Training Loss: 0.017461028083776815
Epoch: 19 Batch: 2000
Training Loss: 0.01762434037029743
Epoch: 19 Batch: 2050
Training Loss: 0.016678149583863048
Epoch: 19 Batch: 2100
Training Loss: 0.016056745407127197
Epoch: 19 Batch: 2150
Training Loss: 0.015980362282242887
Epoch: 19 Batch: 2200
Training Loss: 0.01515124041925777
Epoch: 19 Batch: 2250
Training Loss: 0.015364873819881015
Epoch: 19 Batch: 2300
Training Loss: 0.015375138171341108
Epoch: 19 Batch: 2350
Training Loss: 0.01444835438373241
Epoch: 19 Batch: 2400
Training Loss: 0.014184329062700271
Epoch: 19 Batch: 2450
Training Loss: 0.014041199367873522
Epoch: 19 Batch: 2500
Training Loss: 0.013468572688102722
Epoch: 19 Batch: 2550
Training Loss: 0.013774966059946546
Epoch: 19 Batch: 2600
Training Loss: 0.013348176605426348
Epoch: 19 Batch: 2650
Training Loss: 0.012516202375573932
Epoch: 19 Batch: 2700
Training Loss: 0.012850253582000733
Epoch: 19 Batch: 2750
Training Loss: 0.012513841997493396
Epoch: 19 Batch: 2800
Training Loss: 0.0127243934571743
Epoch: 19 Batch: 2850
Training Loss: 0.01177522387420922
Epoch: 19 Batch: 2900
Training Loss: 0.011929524643667814
Epoch: 19 Batch: 2950
Training Loss: 0.011703215362661976
Epoch: 19 Batch: 3000
Training Loss: 0.01107885072628657
Epoch: 19 Batch: 3050
Training Loss: 0.010715796810681702
Epoch: 19 Batch: 3100
Training Loss: 0.011021821114324754
Epoch: 19 Batch: 3150
Training Loss: 0.011033603917984735
Epoch: 19 Batch: 3200
Training Loss: 0.010800925688818097
Epoch: 20 
 Validation Loss: 0.5258302201827367
---------------------------
Epoch: 20 Batch: 50
Training Loss: 0.6968598937988282
Epoch: 20 Batch: 100
Training Loss: 0.32849606543779375
Epoch: 20 Batch: 150
Training Loss: 0.2377757469813029
Epoch: 20 Batch: 200
Training Loss: 0.16610195726156235
Epoch: 20 Batch: 250
Training Loss: 0.13688561975955962
Epoch: 20 Batch: 300
Training Loss: 0.1106109744310379
Epoch: 20 Batch: 350
Training Loss: 0.09682963575635638
Epoch: 20 Batch: 400
Training Loss: 0.08176730073988438
Epoch: 20 Batch: 450
Training Loss: 0.07223752439022064
Epoch: 20 Batch: 500
Training Loss: 0.0688419081568718
Epoch: 20 Batch: 550
Training Loss: 0.06137815529649908
Epoch: 20 Batch: 600
Training Loss: 0.05523461942871412
Epoch: 20 Batch: 650
Training Loss: 0.0535202217560548
Epoch: 20 Batch: 700
Training Loss: 0.04715821449245725
Epoch: 20 Batch: 750
Training Loss: 0.04795203200976054
Epoch: 20 Batch: 800
Training Loss: 0.042510030791163445
Epoch: 20 Batch: 850
Training Loss: 0.039363786858670854
Epoch: 20 Batch: 900
Training Loss: 0.03843456324603822
Epoch: 20 Batch: 950
Training Loss: 0.036371496790333795
Epoch: 20 Batch: 1000
Training Loss: 0.03423618197441101
Epoch: 20 Batch: 1050
Training Loss: 0.03288767661367144
Epoch: 20 Batch: 1100
Training Loss: 0.03105130379850214
Epoch: 20 Batch: 1150
Training Loss: 0.03093059487964796
Epoch: 20 Batch: 1200
Training Loss: 0.028400939653317133
Epoch: 20 Batch: 1250
Training Loss: 0.028553503131866456
Epoch: 20 Batch: 1300
Training Loss: 0.026569374066132766
Epoch: 20 Batch: 1350
Training Loss: 0.027359201267913535
Epoch: 20 Batch: 1400
Training Loss: 0.023173165257487978
Epoch: 20 Batch: 1450
Training Loss: 0.02359152824714266
Epoch: 20 Batch: 1500
Training Loss: 0.022437742233276367
Epoch: 20 Batch: 1550
Training Loss: 0.0216106443635879
Epoch: 20 Batch: 1600
Training Loss: 0.020972643177956343
Epoch: 20 Batch: 1650
Training Loss: 0.02010495126247406
Epoch: 20 Batch: 1700
Training Loss: 0.020553038050146664
Epoch: 20 Batch: 1750
Training Loss: 0.01902184593677521
Epoch: 20 Batch: 1800
Training Loss: 0.019627083308166926
Epoch: 20 Batch: 1850
Training Loss: 0.017677603686178052
Epoch: 20 Batch: 1900
Training Loss: 0.01773475121510656
Epoch: 20 Batch: 1950
Training Loss: 0.017894173692434262
Epoch: 20 Batch: 2000
Training Loss: 0.017883974969387054
Epoch: 20 Batch: 2050
Training Loss: 0.016420262862996356
Epoch: 20 Batch: 2100
Training Loss: 0.016665734535171874
Epoch: 20 Batch: 2150
Training Loss: 0.0162852048458055
Epoch: 20 Batch: 2200
Training Loss: 0.014768462032079696
Epoch: 20 Batch: 2250
Training Loss: 0.014483710408210754
Epoch: 20 Batch: 2300
Training Loss: 0.014788257777690888
Epoch: 20 Batch: 2350
Training Loss: 0.01451949177904332
Epoch: 20 Batch: 2400
Training Loss: 0.014224762506783008
Epoch: 20 Batch: 2450
Training Loss: 0.013385177765573774
Epoch: 20 Batch: 2500
Training Loss: 0.014168208956718446
Epoch: 20 Batch: 2550
Training Loss: 0.013082033652885287
Epoch: 20 Batch: 2600
Training Loss: 0.01351900297861833
Epoch: 20 Batch: 2650
Training Loss: 0.01246260460817589
Epoch: 20 Batch: 2700
Training Loss: 0.012019810047414568
Epoch: 20 Batch: 2750
Training Loss: 0.012897730816494335
Epoch: 20 Batch: 2800
Training Loss: 0.011821673086711339
Epoch: 20 Batch: 2850
Training Loss: 0.012731671500624271
Epoch: 20 Batch: 2900
Training Loss: 0.012043785463119376
Epoch: 20 Batch: 2950
Training Loss: 0.01125019884715646
Epoch: 20 Batch: 3000
Training Loss: 0.011518255968888601
Epoch: 20 Batch: 3050
Training Loss: 0.010952678508445866
Epoch: 20 Batch: 3100
Training Loss: 0.011057363315936057
Epoch: 20 Batch: 3150
Training Loss: 0.01063292350087847
Epoch: 20 Batch: 3200
Training Loss: 0.010813810992985964
Epoch: 21 
 Validation Loss: 0.5235389047198825
---------------------------
Epoch: 21 Batch: 50
Training Loss: 0.6885047990083695
Epoch: 21 Batch: 100
Training Loss: 0.33221096664667127
Epoch: 21 Batch: 150
Training Loss: 0.22065296808878582
Epoch: 21 Batch: 200
Training Loss: 0.179593146443367
Epoch: 21 Batch: 250
Training Loss: 0.13249801576137543
Epoch: 21 Batch: 300
Training Loss: 0.11508199681838353
Epoch: 21 Batch: 350
Training Loss: 0.10256069370678493
Epoch: 21 Batch: 400
Training Loss: 0.0862751291692257
Epoch: 21 Batch: 450
Training Loss: 0.07472655216852823
Epoch: 21 Batch: 500
Training Loss: 0.0677884396314621
Epoch: 21 Batch: 550
Training Loss: 0.06391722803766077
Epoch: 21 Batch: 600
Training Loss: 0.054971456428368884
Epoch: 21 Batch: 650
Training Loss: 0.05419354768899771
Epoch: 21 Batch: 700
Training Loss: 0.04631084276097162
Epoch: 21 Batch: 750
Training Loss: 0.04547908993562062
Epoch: 21 Batch: 800
Training Loss: 0.04417859263718128
Epoch: 21 Batch: 850
Training Loss: 0.0373628248537288
Epoch: 21 Batch: 900
Training Loss: 0.03874403244919247
Epoch: 21 Batch: 950
Training Loss: 0.03715402882350118
Epoch: 21 Batch: 1000
Training Loss: 0.03358388802409172
Epoch: 21 Batch: 1050
Training Loss: 0.03234473288059234
Epoch: 21 Batch: 1100
Training Loss: 0.030141682570630855
Epoch: 21 Batch: 1150
Training Loss: 0.027982067102971284
Epoch: 21 Batch: 1200
Training Loss: 0.02777081069846948
Epoch: 21 Batch: 1250
Training Loss: 0.026513597702980042
Epoch: 21 Batch: 1300
Training Loss: 0.02662736014677928
Epoch: 21 Batch: 1350
Training Loss: 0.025595246906633732
Epoch: 21 Batch: 1400
Training Loss: 0.02406660673873765
Epoch: 21 Batch: 1450
Training Loss: 0.022595486373736942
Epoch: 21 Batch: 1500
Training Loss: 0.022044839859008788
Epoch: 21 Batch: 1550
Training Loss: 0.02121741085283218
Epoch: 21 Batch: 1600
Training Loss: 0.02184421841055155
Epoch: 21 Batch: 1650
Training Loss: 0.02125379558765527
Epoch: 21 Batch: 1700
Training Loss: 0.020121281322310954
Epoch: 21 Batch: 1750
Training Loss: 0.019703133395739963
Epoch: 21 Batch: 1800
Training Loss: 0.01872021938363711
Epoch: 21 Batch: 1850
Training Loss: 0.017843817424129795
Epoch: 21 Batch: 1900
Training Loss: 0.017612325674609134
Epoch: 21 Batch: 1950
Training Loss: 0.017863390201177352
Epoch: 21 Batch: 2000
Training Loss: 0.017244272008538244
Epoch: 21 Batch: 2050
Training Loss: 0.01669538810485747
Epoch: 21 Batch: 2100
Training Loss: 0.01602953668151583
Epoch: 21 Batch: 2150
Training Loss: 0.01576290488243103
Epoch: 21 Batch: 2200
Training Loss: 0.01474982341582125
Epoch: 21 Batch: 2250
Training Loss: 0.015133223573366801
Epoch: 21 Batch: 2300
Training Loss: 0.015181191887544548
Epoch: 21 Batch: 2350
Training Loss: 0.01444314126004564
Epoch: 21 Batch: 2400
Training Loss: 0.014372891932725906
Epoch: 21 Batch: 2450
Training Loss: 0.01320096379640151
Epoch: 21 Batch: 2500
Training Loss: 0.013037465190887451
Epoch: 21 Batch: 2550
Training Loss: 0.013381743781706866
Epoch: 21 Batch: 2600
Training Loss: 0.01246860241660705
Epoch: 21 Batch: 2650
Training Loss: 0.012823501218040035
Epoch: 21 Batch: 2700
Training Loss: 0.013327567974726359
Epoch: 21 Batch: 2750
Training Loss: 0.012354442011226307
Epoch: 21 Batch: 2800
Training Loss: 0.012056568882295064
Epoch: 21 Batch: 2850
Training Loss: 0.012374299921487508
Epoch: 21 Batch: 2900
Training Loss: 0.011572643312914618
Epoch: 21 Batch: 2950
Training Loss: 0.011439908096345805
Epoch: 21 Batch: 3000
Training Loss: 0.01108195028702418
Epoch: 21 Batch: 3050
Training Loss: 0.0110040204837674
Epoch: 21 Batch: 3100
Training Loss: 0.010963506737063007
Epoch: 21 Batch: 3150
Training Loss: 0.010681787189983187
Epoch: 21 Batch: 3200
Training Loss: 0.010166582809761167
Epoch: 22 
 Validation Loss: 0.5216776953803168
---------------------------
Epoch: 22 Batch: 50
Training Loss: 0.6875125235319137
Epoch: 22 Batch: 100
Training Loss: 0.33743676245212556
Epoch: 22 Batch: 150
Training Loss: 0.22452248990535736
Epoch: 22 Batch: 200
Training Loss: 0.171930772960186
Epoch: 22 Batch: 250
Training Loss: 0.12869412219524384
Epoch: 22 Batch: 300
Training Loss: 0.11558509488900502
Epoch: 22 Batch: 350
Training Loss: 0.09921861188752311
Epoch: 22 Batch: 400
Training Loss: 0.08905006058514119
Epoch: 22 Batch: 450
Training Loss: 0.07510191122690836
Epoch: 22 Batch: 500
Training Loss: 0.06635347998142242
Epoch: 22 Batch: 550
Training Loss: 0.06221407798203555
Epoch: 22 Batch: 600
Training Loss: 0.056180229286352794
Epoch: 22 Batch: 650
Training Loss: 0.05116417971941141
Epoch: 22 Batch: 700
Training Loss: 0.0496572978581701
Epoch: 22 Batch: 750
Training Loss: 0.045386854807535806
Epoch: 22 Batch: 800
Training Loss: 0.04363314524292946
Epoch: 22 Batch: 850
Training Loss: 0.040818128550753875
Epoch: 22 Batch: 900
Training Loss: 0.03793379667732451
Epoch: 22 Batch: 950
Training Loss: 0.03722067588254025
Epoch: 22 Batch: 1000
Training Loss: 0.034170559644699096
Epoch: 22 Batch: 1050
Training Loss: 0.032106045683225
Epoch: 22 Batch: 1100
Training Loss: 0.030604740354147824
Epoch: 22 Batch: 1150
Training Loss: 0.028451994476111037
Epoch: 22 Batch: 1200
Training Loss: 0.02867161976794402
Epoch: 22 Batch: 1250
Training Loss: 0.02718093988895416
Epoch: 22 Batch: 1300
Training Loss: 0.02694740231220539
Epoch: 22 Batch: 1350
Training Loss: 0.02415600472026401
Epoch: 22 Batch: 1400
Training Loss: 0.02450943942580904
Epoch: 22 Batch: 1450
Training Loss: 0.02310285224996764
Epoch: 22 Batch: 1500
Training Loss: 0.02405109401543935
Epoch: 22 Batch: 1550
Training Loss: 0.02022043110862855
Epoch: 22 Batch: 1600
Training Loss: 0.020177600849419833
Epoch: 22 Batch: 1650
Training Loss: 0.020636062694318367
Epoch: 22 Batch: 1700
Training Loss: 0.019613826029440937
Epoch: 22 Batch: 1750
Training Loss: 0.018649379662105014
Epoch: 22 Batch: 1800
Training Loss: 0.018201580593983333
Epoch: 22 Batch: 1850
Training Loss: 0.018086435585408598
Epoch: 22 Batch: 1900
Training Loss: 0.017484756770886872
Epoch: 22 Batch: 1950
Training Loss: 0.017164713404117488
Epoch: 22 Batch: 2000
Training Loss: 0.016765886828303336
Epoch: 22 Batch: 2050
Training Loss: 0.016509616287743174
Epoch: 22 Batch: 2100
Training Loss: 0.01608454404842286
Epoch: 22 Batch: 2150
Training Loss: 0.016063959376756535
Epoch: 22 Batch: 2200
Training Loss: 0.015414623550393365
Epoch: 22 Batch: 2250
Training Loss: 0.014839201304647658
Epoch: 22 Batch: 2300
Training Loss: 0.015003824246966321
Epoch: 22 Batch: 2350
Training Loss: 0.014527175046027975
Epoch: 22 Batch: 2400
Training Loss: 0.013627846539020539
Epoch: 22 Batch: 2450
Training Loss: 0.014303001895242808
Epoch: 22 Batch: 2500
Training Loss: 0.013197104167938232
Epoch: 22 Batch: 2550
Training Loss: 0.01313669507410012
Epoch: 22 Batch: 2600
Training Loss: 0.01314882511129746
Epoch: 22 Batch: 2650
Training Loss: 0.012868950816820253
Epoch: 22 Batch: 2700
Training Loss: 0.012246425096635465
Epoch: 22 Batch: 2750
Training Loss: 0.012579998720775951
Epoch: 22 Batch: 2800
Training Loss: 0.011152608564921788
Epoch: 22 Batch: 2850
Training Loss: 0.012143048978688424
Epoch: 22 Batch: 2900
Training Loss: 0.011360633013577297
Epoch: 22 Batch: 2950
Training Loss: 0.011631986822112132
Epoch: 22 Batch: 3000
Training Loss: 0.011279844800631206
Epoch: 22 Batch: 3050
Training Loss: 0.011306266813981728
Epoch: 22 Batch: 3100
Training Loss: 0.010740605169726957
Epoch: 22 Batch: 3150
Training Loss: 0.01061105984543997
Epoch: 22 Batch: 3200
Training Loss: 0.009942810544744133
Epoch: 23 
 Validation Loss: 0.5193888922532399
---------------------------
Epoch: 23 Batch: 50
Training Loss: 0.6642986941337585
Epoch: 23 Batch: 100
Training Loss: 0.3483370640873909
Epoch: 23 Batch: 150
Training Loss: 0.23276319126288095
Epoch: 23 Batch: 200
Training Loss: 0.1708101849257946
Epoch: 23 Batch: 250
Training Loss: 0.13628818368911744
Epoch: 23 Batch: 300
Training Loss: 0.11289742191632589
Epoch: 23 Batch: 350
Training Loss: 0.10105501941272191
Epoch: 23 Batch: 400
Training Loss: 0.08217787936329841
Epoch: 23 Batch: 450
Training Loss: 0.07643918646706475
Epoch: 23 Batch: 500
Training Loss: 0.0672217385172844
Epoch: 23 Batch: 550
Training Loss: 0.06275583104653792
Epoch: 23 Batch: 600
Training Loss: 0.05409247135122617
Epoch: 23 Batch: 650
Training Loss: 0.05071908116340637
Epoch: 23 Batch: 700
Training Loss: 0.0463950971194676
Epoch: 23 Batch: 750
Training Loss: 0.0457370183467865
Epoch: 23 Batch: 800
Training Loss: 0.0419559158757329
Epoch: 23 Batch: 850
Training Loss: 0.03938882641932544
Epoch: 23 Batch: 900
Training Loss: 0.03845152129729589
Epoch: 23 Batch: 950
Training Loss: 0.03500105424931175
Epoch: 23 Batch: 1000
Training Loss: 0.033744725823402405
Epoch: 23 Batch: 1050
Training Loss: 0.03115260674839928
Epoch: 23 Batch: 1100
Training Loss: 0.030876495431769978
Epoch: 23 Batch: 1150
Training Loss: 0.031414906667626426
Epoch: 23 Batch: 1200
Training Loss: 0.02852727380891641
Epoch: 23 Batch: 1250
Training Loss: 0.02661842453479767
Epoch: 23 Batch: 1300
Training Loss: 0.02401487627854714
Epoch: 23 Batch: 1350
Training Loss: 0.024737576422867953
Epoch: 23 Batch: 1400
Training Loss: 0.02315143278666905
Epoch: 23 Batch: 1450
Training Loss: 0.02344681417119914
Epoch: 23 Batch: 1500
Training Loss: 0.02242789779106776
Epoch: 23 Batch: 1550
Training Loss: 0.022392500158279174
Epoch: 23 Batch: 1600
Training Loss: 0.020572601072490215
Epoch: 23 Batch: 1650
Training Loss: 0.019892841649777962
Epoch: 23 Batch: 1700
Training Loss: 0.019687280987992006
Epoch: 23 Batch: 1750
Training Loss: 0.019793079699788774
Epoch: 23 Batch: 1800
Training Loss: 0.01901295464899805
Epoch: 23 Batch: 1850
Training Loss: 0.01801152024720166
Epoch: 23 Batch: 1900
Training Loss: 0.01848805816550004
Epoch: 23 Batch: 1950
Training Loss: 0.01681020438671112
Epoch: 23 Batch: 2000
Training Loss: 0.017021109223365784
Epoch: 23 Batch: 2050
Training Loss: 0.01629603708662638
Epoch: 23 Batch: 2100
Training Loss: 0.015887991388638813
Epoch: 23 Batch: 2150
Training Loss: 0.015732950776122336
Epoch: 23 Batch: 2200
Training Loss: 0.01534827320413156
Epoch: 23 Batch: 2250
Training Loss: 0.014233511103524102
Epoch: 23 Batch: 2300
Training Loss: 0.014393607157727946
Epoch: 23 Batch: 2350
Training Loss: 0.0149876757758729
Epoch: 23 Batch: 2400
Training Loss: 0.014645539733270804
Epoch: 23 Batch: 2450
Training Loss: 0.014055850493664644
Epoch: 23 Batch: 2500
Training Loss: 0.013615667414665222
Epoch: 23 Batch: 2550
Training Loss: 0.013128378753568612
Epoch: 23 Batch: 2600
Training Loss: 0.012883484203081864
Epoch: 23 Batch: 2650
Training Loss: 0.013100322507462412
Epoch: 23 Batch: 2700
Training Loss: 0.012450891810434836
Epoch: 23 Batch: 2750
Training Loss: 0.012373550144108859
Epoch: 23 Batch: 2800
Training Loss: 0.011815317113484654
Epoch: 23 Batch: 2850
Training Loss: 0.012093212060761034
Epoch: 23 Batch: 2900
Training Loss: 0.012111317710629826
Epoch: 23 Batch: 2950
Training Loss: 0.01175364016476324
Epoch: 23 Batch: 3000
Training Loss: 0.011699982712666193
Epoch: 23 Batch: 3050
Training Loss: 0.01132214789507819
Epoch: 23 Batch: 3100
Training Loss: 0.01073646366596222
Epoch: 23 Batch: 3150
Training Loss: 0.010325452562362428
Epoch: 23 Batch: 3200
Training Loss: 0.01036903808824718
Epoch: 24 
 Validation Loss: 0.5176551269160377
---------------------------
Epoch: 24 Batch: 50
Training Loss: 0.705843750834465
Epoch: 24 Batch: 100
Training Loss: 0.32683391839265824
Epoch: 24 Batch: 150
Training Loss: 0.22818092962106068
Epoch: 24 Batch: 200
Training Loss: 0.17118778571486473
Epoch: 24 Batch: 250
Training Loss: 0.13467969381809233
Epoch: 24 Batch: 300
Training Loss: 0.11319421499967575
Epoch: 24 Batch: 350
Training Loss: 0.09574299199240548
Epoch: 24 Batch: 400
Training Loss: 0.08270585700869561
Epoch: 24 Batch: 450
Training Loss: 0.07509811335139804
Epoch: 24 Batch: 500
Training Loss: 0.07128128814697265
Epoch: 24 Batch: 550
Training Loss: 0.06088165266947313
Epoch: 24 Batch: 600
Training Loss: 0.056014314045508704
Epoch: 24 Batch: 650
Training Loss: 0.05294426670441261
Epoch: 24 Batch: 700
Training Loss: 0.05003284735339029
Epoch: 24 Batch: 750
Training Loss: 0.0456867614587148
Epoch: 24 Batch: 800
Training Loss: 0.04479301482439041
Epoch: 24 Batch: 850
Training Loss: 0.03885577093152439
Epoch: 24 Batch: 900
Training Loss: 0.036224993103080326
Epoch: 24 Batch: 950
Training Loss: 0.03518660799453133
Epoch: 24 Batch: 1000
Training Loss: 0.03373432937264442
Epoch: 24 Batch: 1050
Training Loss: 0.031196710155123757
Epoch: 24 Batch: 1100
Training Loss: 0.0312342016805302
Epoch: 24 Batch: 1150
Training Loss: 0.029916450329448865
Epoch: 24 Batch: 1200
Training Loss: 0.0281849700709184
Epoch: 24 Batch: 1250
Training Loss: 0.02753015127182007
Epoch: 24 Batch: 1300
Training Loss: 0.025305308676682986
Epoch: 24 Batch: 1350
Training Loss: 0.024932363563113742
Epoch: 24 Batch: 1400
Training Loss: 0.024067878872156143
Epoch: 24 Batch: 1450
Training Loss: 0.024025617118539483
Epoch: 24 Batch: 1500
Training Loss: 0.022626827657222747
Epoch: 24 Batch: 1550
Training Loss: 0.02055607549605831
Epoch: 24 Batch: 1600
Training Loss: 0.02079089203849435
Epoch: 24 Batch: 1650
Training Loss: 0.020407937783183474
Epoch: 24 Batch: 1700
Training Loss: 0.019335900913266573
Epoch: 24 Batch: 1750
Training Loss: 0.01850215574673244
Epoch: 24 Batch: 1800
Training Loss: 0.018821317238940132
Epoch: 24 Batch: 1850
Training Loss: 0.017429909448365907
Epoch: 24 Batch: 1900
Training Loss: 0.018442799292112653
Epoch: 24 Batch: 1950
Training Loss: 0.017507364123295514
Epoch: 24 Batch: 2000
Training Loss: 0.016594374373555182
Epoch: 24 Batch: 2050
Training Loss: 0.01605268328655057
Epoch: 24 Batch: 2100
Training Loss: 0.015593325779551552
Epoch: 24 Batch: 2150
Training Loss: 0.015124364467554314
Epoch: 24 Batch: 2200
Training Loss: 0.014655505947091363
Epoch: 24 Batch: 2250
Training Loss: 0.01481735708978441
Epoch: 24 Batch: 2300
Training Loss: 0.014855222624281178
Epoch: 24 Batch: 2350
Training Loss: 0.014337075530214512
Epoch: 24 Batch: 2400
Training Loss: 0.014282167752583822
Epoch: 24 Batch: 2450
Training Loss: 0.014079942873546055
Epoch: 24 Batch: 2500
Training Loss: 0.013530543673038483
Epoch: 24 Batch: 2550
Training Loss: 0.013369835580096525
Epoch: 24 Batch: 2600
Training Loss: 0.013098324124629681
Epoch: 24 Batch: 2650
Training Loss: 0.012924845837197213
Epoch: 24 Batch: 2700
Training Loss: 0.012234703799088796
Epoch: 24 Batch: 2750
Training Loss: 0.011898585536263205
Epoch: 24 Batch: 2800
Training Loss: 0.01269083596765995
Epoch: 24 Batch: 2850
Training Loss: 0.011717614291007058
Epoch: 24 Batch: 2900
Training Loss: 0.011785627716574175
Epoch: 24 Batch: 2950
Training Loss: 0.011453114000417418
Epoch: 24 Batch: 3000
Training Loss: 0.011274455497662227
Epoch: 24 Batch: 3050
Training Loss: 0.010964600707663865
Epoch: 24 Batch: 3100
Training Loss: 0.01099552386230038
Epoch: 24 Batch: 3150
Training Loss: 0.010609618595668249
Epoch: 24 Batch: 3200
Training Loss: 0.009917858382686972
Epoch: 25 
 Validation Loss: 0.5160754369364844
---------------------------
Epoch: 25 Batch: 50
Training Loss: 0.7006068468093872
Epoch: 25 Batch: 100
Training Loss: 0.32611425876617434
Epoch: 25 Batch: 150
Training Loss: 0.21643922487894693
Epoch: 25 Batch: 200
Training Loss: 0.17289102435112
Epoch: 25 Batch: 250
Training Loss: 0.13784973239898682
Epoch: 25 Batch: 300
Training Loss: 0.1095643604795138
Epoch: 25 Batch: 350
Training Loss: 0.09301960323538098
Epoch: 25 Batch: 400
Training Loss: 0.08418065316975117
Epoch: 25 Batch: 450
Training Loss: 0.07176857935057745
Epoch: 25 Batch: 500
Training Loss: 0.06543586230278015
Epoch: 25 Batch: 550
Training Loss: 0.060524148507551716
Epoch: 25 Batch: 600
Training Loss: 0.056879395494858424
Epoch: 25 Batch: 650
Training Loss: 0.05196811460531675
Epoch: 25 Batch: 700
Training Loss: 0.04743662280695779
Epoch: 25 Batch: 750
Training Loss: 0.042624713579813636
Epoch: 25 Batch: 800
Training Loss: 0.0423259311541915
Epoch: 25 Batch: 850
Training Loss: 0.03953544150380527
Epoch: 25 Batch: 900
Training Loss: 0.0363622948858473
Epoch: 25 Batch: 950
Training Loss: 0.034666943550109866
Epoch: 25 Batch: 1000
Training Loss: 0.032521333187818524
Epoch: 25 Batch: 1050
Training Loss: 0.03120354272070385
Epoch: 25 Batch: 1100
Training Loss: 0.030664724545045333
Epoch: 25 Batch: 1150
Training Loss: 0.029071911780730538
Epoch: 25 Batch: 1200
Training Loss: 0.029090647250413895
Epoch: 25 Batch: 1250
Training Loss: 0.02644830174446106
Epoch: 25 Batch: 1300
Training Loss: 0.027037905340011305
Epoch: 25 Batch: 1350
Training Loss: 0.02350742726414292
Epoch: 25 Batch: 1400
Training Loss: 0.023747694811650684
Epoch: 25 Batch: 1450
Training Loss: 0.02359260053470217
Epoch: 25 Batch: 1500
Training Loss: 0.021880527396996818
Epoch: 25 Batch: 1550
Training Loss: 0.021563464768471256
Epoch: 25 Batch: 1600
Training Loss: 0.01989770319312811
Epoch: 25 Batch: 1650
Training Loss: 0.020377899715394686
Epoch: 25 Batch: 1700
Training Loss: 0.02025856968234567
Epoch: 25 Batch: 1750
Training Loss: 0.01898480132647923
Epoch: 25 Batch: 1800
Training Loss: 0.019036411460902954
Epoch: 25 Batch: 1850
Training Loss: 0.01777496687463812
Epoch: 25 Batch: 1900
Training Loss: 0.016766381530385267
Epoch: 25 Batch: 1950
Training Loss: 0.01750928832934453
Epoch: 25 Batch: 2000
Training Loss: 0.016293357372283934
Epoch: 25 Batch: 2050
Training Loss: 0.01578289328551874
Epoch: 25 Batch: 2100
Training Loss: 0.016157398124535878
Epoch: 25 Batch: 2150
Training Loss: 0.015891094970148664
Epoch: 25 Batch: 2200
Training Loss: 0.014854250794107264
Epoch: 25 Batch: 2250
Training Loss: 0.014399497032165528
Epoch: 25 Batch: 2300
Training Loss: 0.015074019315450087
Epoch: 25 Batch: 2350
Training Loss: 0.01478864238617268
Epoch: 25 Batch: 2400
Training Loss: 0.013611781088014445
Epoch: 25 Batch: 2450
Training Loss: 0.013191648819008653
Epoch: 25 Batch: 2500
Training Loss: 0.013274303424358367
Epoch: 25 Batch: 2550
Training Loss: 0.013918051976783603
Epoch: 25 Batch: 2600
Training Loss: 0.012950158726710539
Epoch: 25 Batch: 2650
Training Loss: 0.012557813311522862
Epoch: 25 Batch: 2700
Training Loss: 0.011847362639727415
Epoch: 25 Batch: 2750
Training Loss: 0.012226081826470114
Epoch: 25 Batch: 2800
Training Loss: 0.01212557988507407
Epoch: 25 Batch: 2850
Training Loss: 0.012231315311632658
Epoch: 25 Batch: 2900
Training Loss: 0.011135591474072686
Epoch: 25 Batch: 2950
Training Loss: 0.011215069496025474
Epoch: 25 Batch: 3000
Training Loss: 0.011073553810516994
Epoch: 25 Batch: 3050
Training Loss: 0.011326020643359325
Epoch: 25 Batch: 3100
Training Loss: 0.011260808648601656
Epoch: 25 Batch: 3150
Training Loss: 0.010614942094636343
Epoch: 25 Batch: 3200
Training Loss: 0.010586096169427038
Epoch: 26 
 Validation Loss: 0.5146924191051059
---------------------------
Epoch: 26 Batch: 50
Training Loss: 0.6837639844417572
Epoch: 26 Batch: 100
Training Loss: 0.32237601429224017
Epoch: 26 Batch: 150
Training Loss: 0.22708624422550203
Epoch: 26 Batch: 200
Training Loss: 0.16911001041531562
Epoch: 26 Batch: 250
Training Loss: 0.1337443791627884
Epoch: 26 Batch: 300
Training Loss: 0.10873991638422012
Epoch: 26 Batch: 350
Training Loss: 0.09636146937097823
Epoch: 26 Batch: 400
Training Loss: 0.08412309736013412
Epoch: 26 Batch: 450
Training Loss: 0.07447534011469947
Epoch: 26 Batch: 500
Training Loss: 0.06940803813934326
Epoch: 26 Batch: 550
Training Loss: 0.060640544457869094
Epoch: 26 Batch: 600
Training Loss: 0.05580380688110987
Epoch: 26 Batch: 650
Training Loss: 0.05100527084790743
Epoch: 26 Batch: 700
Training Loss: 0.048022511090551105
Epoch: 26 Batch: 750
Training Loss: 0.04291878751913707
Epoch: 26 Batch: 800
Training Loss: 0.04101249758154154
Epoch: 26 Batch: 850
Training Loss: 0.03894689405665678
Epoch: 26 Batch: 900
Training Loss: 0.03610789163245095
Epoch: 26 Batch: 950
Training Loss: 0.03531120017955178
Epoch: 26 Batch: 1000
Training Loss: 0.03283678591251373
Epoch: 26 Batch: 1050
Training Loss: 0.03174561639626821
Epoch: 26 Batch: 1100
Training Loss: 0.03002924171361056
Epoch: 26 Batch: 1150
Training Loss: 0.028075811577879865
Epoch: 26 Batch: 1200
Training Loss: 0.02893414410452048
Epoch: 26 Batch: 1250
Training Loss: 0.02665481994152069
Epoch: 26 Batch: 1300
Training Loss: 0.0255581914461576
Epoch: 26 Batch: 1350
Training Loss: 0.023167159491115145
Epoch: 26 Batch: 1400
Training Loss: 0.023329719922372275
Epoch: 26 Batch: 1450
Training Loss: 0.021632800205000517
Epoch: 26 Batch: 1500
Training Loss: 0.02232030032078425
Epoch: 26 Batch: 1550
Training Loss: 0.02164188288873242
Epoch: 26 Batch: 1600
Training Loss: 0.020613326337188483
Epoch: 26 Batch: 1650
Training Loss: 0.019932658834890885
Epoch: 26 Batch: 1700
Training Loss: 0.019008544052348416
Epoch: 26 Batch: 1750
Training Loss: 0.019574857234954834
Epoch: 26 Batch: 1800
Training Loss: 0.017913616051276525
Epoch: 26 Batch: 1850
Training Loss: 0.01777188152880282
Epoch: 26 Batch: 1900
Training Loss: 0.017826904921155226
Epoch: 26 Batch: 1950
Training Loss: 0.017786347575676747
Epoch: 26 Batch: 2000
Training Loss: 0.01679778836667538
Epoch: 26 Batch: 2050
Training Loss: 0.016307476860720937
Epoch: 26 Batch: 2100
Training Loss: 0.01620163571266901
Epoch: 26 Batch: 2150
Training Loss: 0.015821597007818
Epoch: 26 Batch: 2200
Training Loss: 0.014948503388599916
Epoch: 26 Batch: 2250
Training Loss: 0.015429070512453714
Epoch: 26 Batch: 2300
Training Loss: 0.015016652669595636
Epoch: 26 Batch: 2350
Training Loss: 0.01411139892770889
Epoch: 26 Batch: 2400
Training Loss: 0.013577007614076137
Epoch: 26 Batch: 2450
Training Loss: 0.01361050757826591
Epoch: 26 Batch: 2500
Training Loss: 0.013188230240345001
Epoch: 26 Batch: 2550
Training Loss: 0.012835720076280482
Epoch: 26 Batch: 2600
Training Loss: 0.013337544661301833
Epoch: 26 Batch: 2650
Training Loss: 0.012867874311950972
Epoch: 26 Batch: 2700
Training Loss: 0.012233554776068087
Epoch: 26 Batch: 2750
Training Loss: 0.012088495113632896
Epoch: 26 Batch: 2800
Training Loss: 0.011634416793073926
Epoch: 26 Batch: 2850
Training Loss: 0.01164450138284449
Epoch: 26 Batch: 2900
Training Loss: 0.011492760520556877
Epoch: 26 Batch: 2950
Training Loss: 0.011633249258590958
Epoch: 26 Batch: 3000
Training Loss: 0.010969761321942011
Epoch: 26 Batch: 3050
Training Loss: 0.011025148630142211
Epoch: 26 Batch: 3100
Training Loss: 0.010826664211288575
Epoch: 26 Batch: 3150
Training Loss: 0.010644181635644701
Epoch: 26 Batch: 3200
Training Loss: 0.010383920017629862
Epoch: 27 
 Validation Loss: 0.5130209667815102
---------------------------
Epoch: 27 Batch: 50
Training Loss: 0.701341512799263
Epoch: 27 Batch: 100
Training Loss: 0.33320595651865004
Epoch: 27 Batch: 150
Training Loss: 0.22777858674526213
Epoch: 27 Batch: 200
Training Loss: 0.16850802630186082
Epoch: 27 Batch: 250
Training Loss: 0.13239665961265565
Epoch: 27 Batch: 300
Training Loss: 0.11020986765623092
Epoch: 27 Batch: 350
Training Loss: 0.09617713894162859
Epoch: 27 Batch: 400
Training Loss: 0.08249900072813034
Epoch: 27 Batch: 450
Training Loss: 0.07220780200428432
Epoch: 27 Batch: 500
Training Loss: 0.0650899812579155
Epoch: 27 Batch: 550
Training Loss: 0.05870569256218997
Epoch: 27 Batch: 600
Training Loss: 0.05293837527434031
Epoch: 27 Batch: 650
Training Loss: 0.05199789652457604
Epoch: 27 Batch: 700
Training Loss: 0.04959088870457241
Epoch: 27 Batch: 750
Training Loss: 0.04514499131838481
Epoch: 27 Batch: 800
Training Loss: 0.04006524045020342
Epoch: 27 Batch: 850
Training Loss: 0.038862371620009925
Epoch: 27 Batch: 900
Training Loss: 0.036558739840984344
Epoch: 27 Batch: 950
Training Loss: 0.036298804690963346
Epoch: 27 Batch: 1000
Training Loss: 0.03423334464430809
Epoch: 27 Batch: 1050
Training Loss: 0.0322508343344643
Epoch: 27 Batch: 1100
Training Loss: 0.029820676879449323
Epoch: 27 Batch: 1150
Training Loss: 0.029229595298352447
Epoch: 27 Batch: 1200
Training Loss: 0.027162781953811645
Epoch: 27 Batch: 1250
Training Loss: 0.026590479803085328
Epoch: 27 Batch: 1300
Training Loss: 0.025584438007611496
Epoch: 27 Batch: 1350
Training Loss: 0.025405790077315436
Epoch: 27 Batch: 1400
Training Loss: 0.022361250519752504
Epoch: 27 Batch: 1450
Training Loss: 0.022540380708102523
Epoch: 27 Batch: 1500
Training Loss: 0.022696670611699422
Epoch: 27 Batch: 1550
Training Loss: 0.02127525912177178
Epoch: 27 Batch: 1600
Training Loss: 0.021018611267209053
Epoch: 27 Batch: 1650
Training Loss: 0.021549772638263125
Epoch: 27 Batch: 1700
Training Loss: 0.0204693085305831
Epoch: 27 Batch: 1750
Training Loss: 0.01890923207146781
Epoch: 27 Batch: 1800
Training Loss: 0.018767791108952627
Epoch: 27 Batch: 1850
Training Loss: 0.01748156510494851
Epoch: 27 Batch: 1900
Training Loss: 0.01691993755729575
Epoch: 27 Batch: 1950
Training Loss: 0.016650494734446207
Epoch: 27 Batch: 2000
Training Loss: 0.017452379792928695
Epoch: 27 Batch: 2050
Training Loss: 0.015496513669083759
Epoch: 27 Batch: 2100
Training Loss: 0.01563710175809406
Epoch: 27 Batch: 2150
Training Loss: 0.016279543028321377
Epoch: 27 Batch: 2200
Training Loss: 0.015490449111570012
Epoch: 27 Batch: 2250
Training Loss: 0.014900405486424764
Epoch: 27 Batch: 2300
Training Loss: 0.014417077691658684
Epoch: 27 Batch: 2350
Training Loss: 0.014372115249329425
Epoch: 27 Batch: 2400
Training Loss: 0.01341786383340756
Epoch: 27 Batch: 2450
Training Loss: 0.013782367122416594
Epoch: 27 Batch: 2500
Training Loss: 0.01336527556180954
Epoch: 27 Batch: 2550
Training Loss: 0.012956001571580476
Epoch: 27 Batch: 2600
Training Loss: 0.012778537250482119
Epoch: 27 Batch: 2650
Training Loss: 0.012924905261903439
Epoch: 27 Batch: 2700
Training Loss: 0.012225283560929475
Epoch: 27 Batch: 2750
Training Loss: 0.011578284632075917
Epoch: 27 Batch: 2800
Training Loss: 0.011675509416631289
Epoch: 27 Batch: 2850
Training Loss: 0.012630946521173443
Epoch: 27 Batch: 2900
Training Loss: 0.01128907095769356
Epoch: 27 Batch: 2950
Training Loss: 0.011241068243980407
Epoch: 27 Batch: 3000
Training Loss: 0.01101005545258522
Epoch: 27 Batch: 3050
Training Loss: 0.010934706388926897
Epoch: 27 Batch: 3100
Training Loss: 0.01065034741355527
Epoch: 27 Batch: 3150
Training Loss: 0.010869657964933487
Epoch: 27 Batch: 3200
Training Loss: 0.010524887144565582
Epoch: 28 
 Validation Loss: 0.5118374473518795
---------------------------
Epoch: 28 Batch: 50
Training Loss: 0.6668972963094711
Epoch: 28 Batch: 100
Training Loss: 0.3199367988109589
Epoch: 28 Batch: 150
Training Loss: 0.21593126396338144
Epoch: 28 Batch: 200
Training Loss: 0.1648709298670292
Epoch: 28 Batch: 250
Training Loss: 0.1401787761449814
Epoch: 28 Batch: 300
Training Loss: 0.10801536252101263
Epoch: 28 Batch: 350
Training Loss: 0.09807626358100346
Epoch: 28 Batch: 400
Training Loss: 0.08330952621996403
Epoch: 28 Batch: 450
Training Loss: 0.07266458895471362
Epoch: 28 Batch: 500
Training Loss: 0.06568423736095429
Epoch: 28 Batch: 550
Training Loss: 0.061900212981484154
Epoch: 28 Batch: 600
Training Loss: 0.055406133433183034
Epoch: 28 Batch: 650
Training Loss: 0.049998973195369424
Epoch: 28 Batch: 700
Training Loss: 0.047802248724869316
Epoch: 28 Batch: 750
Training Loss: 0.04404985622564952
Epoch: 28 Batch: 800
Training Loss: 0.0419081611931324
Epoch: 28 Batch: 850
Training Loss: 0.03844132342759301
Epoch: 28 Batch: 900
Training Loss: 0.0363024534450637
Epoch: 28 Batch: 950
Training Loss: 0.03411566430016568
Epoch: 28 Batch: 1000
Training Loss: 0.034085826098918914
Epoch: 28 Batch: 1050
Training Loss: 0.032204285860061646
Epoch: 28 Batch: 1100
Training Loss: 0.031801016845486384
Epoch: 28 Batch: 1150
Training Loss: 0.02884688341099283
Epoch: 28 Batch: 1200
Training Loss: 0.02718234933912754
Epoch: 28 Batch: 1250
Training Loss: 0.026565676712989806
Epoch: 28 Batch: 1300
Training Loss: 0.02651676826752149
Epoch: 28 Batch: 1350
Training Loss: 0.023140467648152953
Epoch: 28 Batch: 1400
Training Loss: 0.02419676987188203
Epoch: 28 Batch: 1450
Training Loss: 0.023722948267542083
Epoch: 28 Batch: 1500
Training Loss: 0.021406439503033958
Epoch: 28 Batch: 1550
Training Loss: 0.021693979578633463
Epoch: 28 Batch: 1600
Training Loss: 0.020648246705532072
Epoch: 28 Batch: 1650
Training Loss: 0.020199249477097482
Epoch: 28 Batch: 1700
Training Loss: 0.02021829878582674
Epoch: 28 Batch: 1750
Training Loss: 0.01796752483504159
Epoch: 28 Batch: 1800
Training Loss: 0.01809765953156683
Epoch: 28 Batch: 1850
Training Loss: 0.01788025923677393
Epoch: 28 Batch: 1900
Training Loss: 0.017536997685306952
Epoch: 28 Batch: 1950
Training Loss: 0.017882501421830593
Epoch: 28 Batch: 2000
Training Loss: 0.01633485262095928
Epoch: 28 Batch: 2050
Training Loss: 0.016066116327192726
Epoch: 28 Batch: 2100
Training Loss: 0.015455024341742198
Epoch: 28 Batch: 2150
Training Loss: 0.016000423195750214
Epoch: 28 Batch: 2200
Training Loss: 0.015661764551292767
Epoch: 28 Batch: 2250
Training Loss: 0.014609508885277641
Epoch: 28 Batch: 2300
Training Loss: 0.014166739194289497
Epoch: 28 Batch: 2350
Training Loss: 0.01442024974112815
Epoch: 28 Batch: 2400
Training Loss: 0.013980677165091038
Epoch: 28 Batch: 2450
Training Loss: 0.013959418997472646
Epoch: 28 Batch: 2500
Training Loss: 0.013124436485767365
Epoch: 28 Batch: 2550
Training Loss: 0.0132565108350679
Epoch: 28 Batch: 2600
Training Loss: 0.012265138041514616
Epoch: 28 Batch: 2650
Training Loss: 0.012322964612043128
Epoch: 28 Batch: 2700
Training Loss: 0.01211172706551022
Epoch: 28 Batch: 2750
Training Loss: 0.011918691938573664
Epoch: 28 Batch: 2800
Training Loss: 0.01149187042244843
Epoch: 28 Batch: 2850
Training Loss: 0.011955821869666117
Epoch: 28 Batch: 2900
Training Loss: 0.01110474396368553
Epoch: 28 Batch: 2950
Training Loss: 0.011606695641905574
Epoch: 28 Batch: 3000
Training Loss: 0.011203715483347575
Epoch: 28 Batch: 3050
Training Loss: 0.01102465021805685
Epoch: 28 Batch: 3100
Training Loss: 0.011122690208496586
Epoch: 28 Batch: 3150
Training Loss: 0.010752561660039992
Epoch: 28 Batch: 3200
Training Loss: 0.010307348184287548
Epoch: 29 
 Validation Loss: 0.5104157014025582
---------------------------
Epoch: 29 Batch: 50
Training Loss: 0.6430925834178924
Epoch: 29 Batch: 100
Training Loss: 0.3313197311758995
Epoch: 29 Batch: 150
Training Loss: 0.23174664775530499
Epoch: 29 Batch: 200
Training Loss: 0.1620631508529186
Epoch: 29 Batch: 250
Training Loss: 0.13476819062232973
Epoch: 29 Batch: 300
Training Loss: 0.11267480661471685
Epoch: 29 Batch: 350
Training Loss: 0.09121247070176261
Epoch: 29 Batch: 400
Training Loss: 0.08132520705461502
Epoch: 29 Batch: 450
Training Loss: 0.07329386678006913
Epoch: 29 Batch: 500
Training Loss: 0.06589495307207108
Epoch: 29 Batch: 550
Training Loss: 0.05763497764414007
Epoch: 29 Batch: 600
Training Loss: 0.05738360707958539
Epoch: 29 Batch: 650
Training Loss: 0.05071907112231622
Epoch: 29 Batch: 700
Training Loss: 0.04700803748198918
Epoch: 29 Batch: 750
Training Loss: 0.045266116698582964
Epoch: 29 Batch: 800
Training Loss: 0.04277967605739832
Epoch: 29 Batch: 850
Training Loss: 0.03907073087552015
Epoch: 29 Batch: 900
Training Loss: 0.035514919029341806
Epoch: 29 Batch: 950
Training Loss: 0.03533156956496992
Epoch: 29 Batch: 1000
Training Loss: 0.033136814057826994
Epoch: 29 Batch: 1050
Training Loss: 0.030739068133490425
Epoch: 29 Batch: 1100
Training Loss: 0.030748834339055148
Epoch: 29 Batch: 1150
Training Loss: 0.028060742072437122
Epoch: 29 Batch: 1200
Training Loss: 0.027675557459394137
Epoch: 29 Batch: 1250
Training Loss: 0.027107109665870665
Epoch: 29 Batch: 1300
Training Loss: 0.025690493125181933
Epoch: 29 Batch: 1350
Training Loss: 0.02484763209466581
Epoch: 29 Batch: 1400
Training Loss: 0.02499449604323932
Epoch: 29 Batch: 1450
Training Loss: 0.0240575012872959
Epoch: 29 Batch: 1500
Training Loss: 0.0213008531332016
Epoch: 29 Batch: 1550
Training Loss: 0.021590059130422532
Epoch: 29 Batch: 1600
Training Loss: 0.020646541099995374
Epoch: 29 Batch: 1650
Training Loss: 0.020009109341736997
Epoch: 29 Batch: 1700
Training Loss: 0.01947353867923512
Epoch: 29 Batch: 1750
Training Loss: 0.0187254593031747
Epoch: 29 Batch: 1800
Training Loss: 0.01863298099901941
Epoch: 29 Batch: 1850
Training Loss: 0.018134793735839226
Epoch: 29 Batch: 1900
Training Loss: 0.016980656761872142
Epoch: 29 Batch: 1950
Training Loss: 0.018131771423877813
Epoch: 29 Batch: 2000
Training Loss: 0.015750570118427277
Epoch: 29 Batch: 2050
Training Loss: 0.01584950830878281
Epoch: 29 Batch: 2100
Training Loss: 0.01569667445761817
Epoch: 29 Batch: 2150
Training Loss: 0.016389072939406994
Epoch: 29 Batch: 2200
Training Loss: 0.015268788459626112
Epoch: 29 Batch: 2250
Training Loss: 0.01408154088921017
Epoch: 29 Batch: 2300
Training Loss: 0.013672124432480854
Epoch: 29 Batch: 2350
Training Loss: 0.014318769992666042
Epoch: 29 Batch: 2400
Training Loss: 0.014160960192481677
Epoch: 29 Batch: 2450
Training Loss: 0.013849861609692477
Epoch: 29 Batch: 2500
Training Loss: 0.013070695602893829
Epoch: 29 Batch: 2550
Training Loss: 0.012718427052684859
Epoch: 29 Batch: 2600
Training Loss: 0.012790647859756763
Epoch: 29 Batch: 2650
Training Loss: 0.012286244293428818
Epoch: 29 Batch: 2700
Training Loss: 0.012229625935907717
Epoch: 29 Batch: 2750
Training Loss: 0.01212879817052321
Epoch: 29 Batch: 2800
Training Loss: 0.011963561241115842
Epoch: 29 Batch: 2850
Training Loss: 0.011687021119552747
Epoch: 29 Batch: 2900
Training Loss: 0.011132828213017563
Epoch: 29 Batch: 2950
Training Loss: 0.0113101736973908
Epoch: 29 Batch: 3000
Training Loss: 0.01140290825565656
Epoch: 29 Batch: 3050
Training Loss: 0.01097174346446991
Epoch: 29 Batch: 3100
Training Loss: 0.010213429235642956
Epoch: 29 Batch: 3150
Training Loss: 0.00994794439701807
Epoch: 29 Batch: 3200
Training Loss: 0.010153784118592739
Epoch: 30 
 Validation Loss: 0.5101352744632297
---------------------------
Epoch: 30 Batch: 50
Training Loss: 0.6644705653190612
Epoch: 30 Batch: 100
Training Loss: 0.3366515669226646
Epoch: 30 Batch: 150
Training Loss: 0.230391592780749
Epoch: 30 Batch: 200
Training Loss: 0.16797315895557405
Epoch: 30 Batch: 250
Training Loss: 0.13575324368476868
Epoch: 30 Batch: 300
Training Loss: 0.1151373091340065
Epoch: 30 Batch: 350
Training Loss: 0.0924561368567603
Epoch: 30 Batch: 400
Training Loss: 0.08224696084856987
Epoch: 30 Batch: 450
Training Loss: 0.07238095323244731
Epoch: 30 Batch: 500
Training Loss: 0.06619897890090942
Epoch: 30 Batch: 550
Training Loss: 0.05886103261600841
Epoch: 30 Batch: 600
Training Loss: 0.055762887150049206
Epoch: 30 Batch: 650
Training Loss: 0.04855116293980525
Epoch: 30 Batch: 700
Training Loss: 0.046507028426442824
Epoch: 30 Batch: 750
Training Loss: 0.044569041689236956
Epoch: 30 Batch: 800
Training Loss: 0.03871823452413082
Epoch: 30 Batch: 850
Training Loss: 0.03877020152176128
Epoch: 30 Batch: 900
Training Loss: 0.0361302458246549
Epoch: 30 Batch: 950
Training Loss: 0.0345252524865301
Epoch: 30 Batch: 1000
Training Loss: 0.034036910653114316
Epoch: 30 Batch: 1050
Training Loss: 0.0321171479281925
Epoch: 30 Batch: 1100
Training Loss: 0.03137602995742451
Epoch: 30 Batch: 1150
Training Loss: 0.028453117842259613
Epoch: 30 Batch: 1200
Training Loss: 0.02788476544121901
Epoch: 30 Batch: 1250
Training Loss: 0.026400637292861937
Epoch: 30 Batch: 1300
Training Loss: 0.024832736070339496
Epoch: 30 Batch: 1350
Training Loss: 0.02395323830622214
Epoch: 30 Batch: 1400
Training Loss: 0.02321804112621716
Epoch: 30 Batch: 1450
Training Loss: 0.023191971696656325
Epoch: 30 Batch: 1500
Training Loss: 0.02259184952576955
Epoch: 30 Batch: 1550
Training Loss: 0.020959774947935536
Epoch: 30 Batch: 1600
Training Loss: 0.021337524000555278
Epoch: 30 Batch: 1650
Training Loss: 0.02059783550825986
Epoch: 30 Batch: 1700
Training Loss: 0.01921857861911549
Epoch: 30 Batch: 1750
Training Loss: 0.018410214935030256
Epoch: 30 Batch: 1800
Training Loss: 0.018088850445217557
Epoch: 30 Batch: 1850
Training Loss: 0.017554177319681324
Epoch: 30 Batch: 1900
Training Loss: 0.017462181135227805
Epoch: 30 Batch: 1950
Training Loss: 0.017315574487050376
Epoch: 30 Batch: 2000
Training Loss: 0.01606150308251381
Epoch: 30 Batch: 2050
Training Loss: 0.01588896031786756
Epoch: 30 Batch: 2100
Training Loss: 0.015856638139202482
Epoch: 30 Batch: 2150
Training Loss: 0.014743578226067299
Epoch: 30 Batch: 2200
Training Loss: 0.014371159781109203
Epoch: 30 Batch: 2250
Training Loss: 0.014630144410663182
Epoch: 30 Batch: 2300
Training Loss: 0.014378423729668494
Epoch: 30 Batch: 2350
Training Loss: 0.01351543147513207
Epoch: 30 Batch: 2400
Training Loss: 0.01320549476891756
Epoch: 30 Batch: 2450
Training Loss: 0.013590265561123284
Epoch: 30 Batch: 2500
Training Loss: 0.01264425150156021
Epoch: 30 Batch: 2550
Training Loss: 0.013364360133806864
Epoch: 30 Batch: 2600
Training Loss: 0.01298835107913384
Epoch: 30 Batch: 2650
Training Loss: 0.012690875237842776
Epoch: 30 Batch: 2700
Training Loss: 0.012217939529154035
Epoch: 30 Batch: 2750
Training Loss: 0.011484867789528586
Epoch: 30 Batch: 2800
Training Loss: 0.011940954050847462
Epoch: 30 Batch: 2850
Training Loss: 0.011846735372877958
Epoch: 30 Batch: 2900
Training Loss: 0.011198619676047359
Epoch: 30 Batch: 2950
Training Loss: 0.011583025101887977
Epoch: 30 Batch: 3000
Training Loss: 0.011152842660744984
Epoch: 30 Batch: 3050
Training Loss: 0.010872424958182162
Epoch: 30 Batch: 3100
Training Loss: 0.010372499016023452
Epoch: 30 Batch: 3150
Training Loss: 0.010620724729129246
Epoch: 30 Batch: 3200
Training Loss: 0.010413837125524879
Epoch: 31 
 Validation Loss: 0.5082054399781757
---------------------------
Epoch: 31 Batch: 50
Training Loss: 0.6864012324810028
Epoch: 31 Batch: 100
Training Loss: 0.319588905274868
Epoch: 31 Batch: 150
Training Loss: 0.20881203571955362
Epoch: 31 Batch: 200
Training Loss: 0.16840421229600908
Epoch: 31 Batch: 250
Training Loss: 0.1395794150829315
Epoch: 31 Batch: 300
Training Loss: 0.11050473014513651
Epoch: 31 Batch: 350
Training Loss: 0.09319951117038727
Epoch: 31 Batch: 400
Training Loss: 0.08245005942881108
Epoch: 31 Batch: 450
Training Loss: 0.07337171090973749
Epoch: 31 Batch: 500
Training Loss: 0.06797235745191574
Epoch: 31 Batch: 550
Training Loss: 0.05915196841413325
Epoch: 31 Batch: 600
Training Loss: 0.05436035230755806
Epoch: 31 Batch: 650
Training Loss: 0.052841510910254255
Epoch: 31 Batch: 700
Training Loss: 0.04559196791478566
Epoch: 31 Batch: 750
Training Loss: 0.04423334753513336
Epoch: 31 Batch: 800
Training Loss: 0.04097555924206972
Epoch: 31 Batch: 850
Training Loss: 0.03920191214365118
Epoch: 31 Batch: 900
Training Loss: 0.03763231337070465
Epoch: 31 Batch: 950
Training Loss: 0.034993993106641266
Epoch: 31 Batch: 1000
Training Loss: 0.031269042134284976
Epoch: 31 Batch: 1050
Training Loss: 0.03139766292912619
Epoch: 31 Batch: 1100
Training Loss: 0.029828296655958347
Epoch: 31 Batch: 1150
Training Loss: 0.028143799460452534
Epoch: 31 Batch: 1200
Training Loss: 0.02742369512716929
Epoch: 31 Batch: 1250
Training Loss: 0.027289847230911254
Epoch: 31 Batch: 1300
Training Loss: 0.024222978949546815
Epoch: 31 Batch: 1350
Training Loss: 0.02388010651976974
Epoch: 31 Batch: 1400
Training Loss: 0.02442471661737987
Epoch: 31 Batch: 1450
Training Loss: 0.022752513515538184
Epoch: 31 Batch: 1500
Training Loss: 0.022621054867903393
Epoch: 31 Batch: 1550
Training Loss: 0.022489153762017527
Epoch: 31 Batch: 1600
Training Loss: 0.020294528305530548
Epoch: 31 Batch: 1650
Training Loss: 0.02017286530046752
Epoch: 31 Batch: 1700
Training Loss: 0.019632806585115545
Epoch: 31 Batch: 1750
Training Loss: 0.018305832351957048
Epoch: 31 Batch: 1800
Training Loss: 0.01774488314986229
Epoch: 31 Batch: 1850
Training Loss: 0.017608369846601742
Epoch: 31 Batch: 1900
Training Loss: 0.01706899051603518
Epoch: 31 Batch: 1950
Training Loss: 0.01678870386038071
Epoch: 31 Batch: 2000
Training Loss: 0.01572116443514824
Epoch: 31 Batch: 2050
Training Loss: 0.01576629343556195
Epoch: 31 Batch: 2100
Training Loss: 0.01601831507115137
Epoch: 31 Batch: 2150
Training Loss: 0.015899663914081662
Epoch: 31 Batch: 2200
Training Loss: 0.015517258048057557
Epoch: 31 Batch: 2250
Training Loss: 0.014221310416857402
Epoch: 31 Batch: 2300
Training Loss: 0.014652625011361163
Epoch: 31 Batch: 2350
Training Loss: 0.01378859871245445
Epoch: 31 Batch: 2400
Training Loss: 0.013290944149096807
Epoch: 31 Batch: 2450
Training Loss: 0.013171839422109175
Epoch: 31 Batch: 2500
Training Loss: 0.0129972860455513
Epoch: 31 Batch: 2550
Training Loss: 0.013531918864624173
Epoch: 31 Batch: 2600
Training Loss: 0.01287072072808559
Epoch: 31 Batch: 2650
Training Loss: 0.012679799255335106
Epoch: 31 Batch: 2700
Training Loss: 0.012166601022084554
Epoch: 31 Batch: 2750
Training Loss: 0.011764385700225831
Epoch: 31 Batch: 2800
Training Loss: 0.0115818361725126
Epoch: 31 Batch: 2850
Training Loss: 0.01137992216829668
Epoch: 31 Batch: 2900
Training Loss: 0.01093573662741431
Epoch: 31 Batch: 2950
Training Loss: 0.010937467318470196
Epoch: 31 Batch: 3000
Training Loss: 0.010734958469867707
Epoch: 31 Batch: 3050
Training Loss: 0.01106440577350679
Epoch: 31 Batch: 3100
Training Loss: 0.010278849121062986
Epoch: 31 Batch: 3150
Training Loss: 0.010238077356701805
Epoch: 31 Batch: 3200
Training Loss: 0.010315045733004808
Epoch: 32 
 Validation Loss: 0.5070929874976476
---------------------------
Epoch: 32 Batch: 50
Training Loss: 0.6404497343301773
Epoch: 32 Batch: 100
Training Loss: 0.34065519481897355
Epoch: 32 Batch: 150
Training Loss: 0.21893143137296042
Epoch: 32 Batch: 200
Training Loss: 0.16218691408634187
Epoch: 32 Batch: 250
Training Loss: 0.13630956411361694
Epoch: 32 Batch: 300
Training Loss: 0.1066342634956042
Epoch: 32 Batch: 350
Training Loss: 0.0925389050585883
Epoch: 32 Batch: 400
Training Loss: 0.08339331038296223
Epoch: 32 Batch: 450
Training Loss: 0.0735642972919676
Epoch: 32 Batch: 500
Training Loss: 0.06721483212709427
Epoch: 32 Batch: 550
Training Loss: 0.058625056743621826
Epoch: 32 Batch: 600
Training Loss: 0.054574158092339835
Epoch: 32 Batch: 650
Training Loss: 0.050085215935340294
Epoch: 32 Batch: 700
Training Loss: 0.04809001705476216
Epoch: 32 Batch: 750
Training Loss: 0.0452066969871521
Epoch: 32 Batch: 800
Training Loss: 0.042432645484805104
Epoch: 32 Batch: 850
Training Loss: 0.03962014065069311
Epoch: 32 Batch: 900
Training Loss: 0.03637442204687331
Epoch: 32 Batch: 950
Training Loss: 0.03691110482341365
Epoch: 32 Batch: 1000
Training Loss: 0.03413270211219788
Epoch: 32 Batch: 1050
Training Loss: 0.032047013356572104
Epoch: 32 Batch: 1100
Training Loss: 0.029608388976617293
Epoch: 32 Batch: 1150
Training Loss: 0.026842649682708407
Epoch: 32 Batch: 1200
Training Loss: 0.027764536440372467
Epoch: 32 Batch: 1250
Training Loss: 0.026826087546348572
Epoch: 32 Batch: 1300
Training Loss: 0.025168221249030186
Epoch: 32 Batch: 1350
Training Loss: 0.02349873372802028
Epoch: 32 Batch: 1400
Training Loss: 0.02248723905001368
Epoch: 32 Batch: 1450
Training Loss: 0.022605291070609258
Epoch: 32 Batch: 1500
Training Loss: 0.02246869033575058
Epoch: 32 Batch: 1550
Training Loss: 0.020568752500318713
Epoch: 32 Batch: 1600
Training Loss: 0.01988571126013994
Epoch: 32 Batch: 1650
Training Loss: 0.019513516805388712
Epoch: 32 Batch: 1700
Training Loss: 0.018593450013329002
Epoch: 32 Batch: 1750
Training Loss: 0.018795305524553572
Epoch: 32 Batch: 1800
Training Loss: 0.01884097749988238
Epoch: 32 Batch: 1850
Training Loss: 0.01763162113524772
Epoch: 32 Batch: 1900
Training Loss: 0.017903695702552794
Epoch: 32 Batch: 1950
Training Loss: 0.016030968137276478
Epoch: 32 Batch: 2000
Training Loss: 0.017753525778651236
Epoch: 32 Batch: 2050
Training Loss: 0.015216952635020745
Epoch: 32 Batch: 2100
Training Loss: 0.015570913510663168
Epoch: 32 Batch: 2150
Training Loss: 0.01574635279733081
Epoch: 32 Batch: 2200
Training Loss: 0.015194317332722924
Epoch: 32 Batch: 2250
Training Loss: 0.014682456758287218
Epoch: 32 Batch: 2300
Training Loss: 0.014285166380198106
Epoch: 32 Batch: 2350
Training Loss: 0.014382001656167051
Epoch: 32 Batch: 2400
Training Loss: 0.014353642066319783
Epoch: 32 Batch: 2450
Training Loss: 0.013136651041556377
Epoch: 32 Batch: 2500
Training Loss: 0.012768627715110779
Epoch: 32 Batch: 2550
Training Loss: 0.012992491967537824
Epoch: 32 Batch: 2600
Training Loss: 0.012971795900509907
Epoch: 32 Batch: 2650
Training Loss: 0.012552856139416965
Epoch: 32 Batch: 2700
Training Loss: 0.012114013510721701
Epoch: 32 Batch: 2750
Training Loss: 0.012102064165202054
Epoch: 32 Batch: 2800
Training Loss: 0.01200107743697507
Epoch: 32 Batch: 2850
Training Loss: 0.011751599416398166
Epoch: 32 Batch: 2900
Training Loss: 0.01157854327867771
Epoch: 32 Batch: 2950
Training Loss: 0.011148392568200322
Epoch: 32 Batch: 3000
Training Loss: 0.010760420342286428
Epoch: 32 Batch: 3050
Training Loss: 0.01004451865055522
Epoch: 32 Batch: 3100
Training Loss: 0.010502056633272479
Epoch: 32 Batch: 3150
Training Loss: 0.010823506381776598
Epoch: 32 Batch: 3200
Training Loss: 0.010057299518957735
Epoch: 33 
 Validation Loss: 0.5058765143156052
---------------------------
Epoch: 33 Batch: 50
Training Loss: 0.6725583875179291
Epoch: 33 Batch: 100
Training Loss: 0.33896606594324113
Epoch: 33 Batch: 150
Training Loss: 0.21964285651842752
Epoch: 33 Batch: 200
Training Loss: 0.1670038504898548
Epoch: 33 Batch: 250
Training Loss: 0.13138714921474456
Epoch: 33 Batch: 300
Training Loss: 0.11305272360642751
Epoch: 33 Batch: 350
Training Loss: 0.09812530832631247
Epoch: 33 Batch: 400
Training Loss: 0.08191853120923043
Epoch: 33 Batch: 450
Training Loss: 0.07000650359524621
Epoch: 33 Batch: 500
Training Loss: 0.06751635694503784
Epoch: 33 Batch: 550
Training Loss: 0.061378590247847815
Epoch: 33 Batch: 600
Training Loss: 0.05340865115324656
Epoch: 33 Batch: 650
Training Loss: 0.0486995576436703
Epoch: 33 Batch: 700
Training Loss: 0.04688605129718781
Epoch: 33 Batch: 750
Training Loss: 0.04472863785425822
Epoch: 33 Batch: 800
Training Loss: 0.041408994905650615
Epoch: 33 Batch: 850
Training Loss: 0.03787749868981979
Epoch: 33 Batch: 900
Training Loss: 0.03608368009328842
Epoch: 33 Batch: 950
Training Loss: 0.03469360295094942
Epoch: 33 Batch: 1000
Training Loss: 0.03246951332688332
Epoch: 33 Batch: 1050
Training Loss: 0.03238406859693073
Epoch: 33 Batch: 1100
Training Loss: 0.02954545476219871
Epoch: 33 Batch: 1150
Training Loss: 0.029719788613526716
Epoch: 33 Batch: 1200
Training Loss: 0.027406390979886053
Epoch: 33 Batch: 1250
Training Loss: 0.025905336165428162
Epoch: 33 Batch: 1300
Training Loss: 0.025635132789611815
Epoch: 33 Batch: 1350
Training Loss: 0.024328290621439617
Epoch: 33 Batch: 1400
Training Loss: 0.022771701110260827
Epoch: 33 Batch: 1450
Training Loss: 0.022495249201511514
Epoch: 33 Batch: 1500
Training Loss: 0.02262837407986323
Epoch: 33 Batch: 1550
Training Loss: 0.02076351931018214
Epoch: 33 Batch: 1600
Training Loss: 0.020445003733038904
Epoch: 33 Batch: 1650
Training Loss: 0.01904081819635449
Epoch: 33 Batch: 1700
Training Loss: 0.018440126075464136
Epoch: 33 Batch: 1750
Training Loss: 0.01890058127471379
Epoch: 33 Batch: 1800
Training Loss: 0.018678686685032315
Epoch: 33 Batch: 1850
Training Loss: 0.01787290044732996
Epoch: 33 Batch: 1900
Training Loss: 0.01662483540020491
Epoch: 33 Batch: 1950
Training Loss: 0.016700653632481893
Epoch: 33 Batch: 2000
Training Loss: 0.015708882227540015
Epoch: 33 Batch: 2050
Training Loss: 0.016370560410546093
Epoch: 33 Batch: 2100
Training Loss: 0.015713303614230383
Epoch: 33 Batch: 2150
Training Loss: 0.01548492941745492
Epoch: 33 Batch: 2200
Training Loss: 0.014078834354877471
Epoch: 33 Batch: 2250
Training Loss: 0.014437441613939074
Epoch: 33 Batch: 2300
Training Loss: 0.014025053485580113
Epoch: 33 Batch: 2350
Training Loss: 0.013627342962204142
Epoch: 33 Batch: 2400
Training Loss: 0.013175982708732287
Epoch: 33 Batch: 2450
Training Loss: 0.0135932956179794
Epoch: 33 Batch: 2500
Training Loss: 0.012713779652118683
Epoch: 33 Batch: 2550
Training Loss: 0.013328580178466498
Epoch: 33 Batch: 2600
Training Loss: 0.013186419846919867
Epoch: 33 Batch: 2650
Training Loss: 0.011940898276724905
Epoch: 33 Batch: 2700
Training Loss: 0.011496616359110232
Epoch: 33 Batch: 2750
Training Loss: 0.01274490901556882
Epoch: 33 Batch: 2800
Training Loss: 0.01148339442908764
Epoch: 33 Batch: 2850
Training Loss: 0.012039177867404202
Epoch: 33 Batch: 2900
Training Loss: 0.011302160774839335
Epoch: 33 Batch: 2950
Training Loss: 0.010971710914272372
Epoch: 33 Batch: 3000
Training Loss: 0.010762802402178447
Epoch: 33 Batch: 3050
Training Loss: 0.011398184465580299
Epoch: 33 Batch: 3100
Training Loss: 0.010629651056182
Epoch: 33 Batch: 3150
Training Loss: 0.010513291727928888
Epoch: 33 Batch: 3200
Training Loss: 0.010058196587488055
Epoch: 34 
 Validation Loss: 0.505107236901919
---------------------------
Epoch: 34 Batch: 50
Training Loss: 0.664262375831604
Epoch: 34 Batch: 100
Training Loss: 0.3298261272907257
Epoch: 34 Batch: 150
Training Loss: 0.21256534139315286
Epoch: 34 Batch: 200
Training Loss: 0.16393592089414596
Epoch: 34 Batch: 250
Training Loss: 0.1313100029230118
Epoch: 34 Batch: 300
Training Loss: 0.11228573560714722
Epoch: 34 Batch: 350
Training Loss: 0.09723792799881527
Epoch: 34 Batch: 400
Training Loss: 0.08151666849851608
Epoch: 34 Batch: 450
Training Loss: 0.0709157415231069
Epoch: 34 Batch: 500
Training Loss: 0.06433249408006668
Epoch: 34 Batch: 550
Training Loss: 0.0607823060317473
Epoch: 34 Batch: 600
Training Loss: 0.05381605183084806
Epoch: 34 Batch: 650
Training Loss: 0.051505052126370944
Epoch: 34 Batch: 700
Training Loss: 0.046337563821247645
Epoch: 34 Batch: 750
Training Loss: 0.04298101564248403
Epoch: 34 Batch: 800
Training Loss: 0.0401104985922575
Epoch: 34 Batch: 850
Training Loss: 0.0379976929285947
Epoch: 34 Batch: 900
Training Loss: 0.03617116434706582
Epoch: 34 Batch: 950
Training Loss: 0.03439637196691413
Epoch: 34 Batch: 1000
Training Loss: 0.034161364406347275
Epoch: 34 Batch: 1050
Training Loss: 0.031131353066081092
Epoch: 34 Batch: 1100
Training Loss: 0.030093611424619502
Epoch: 34 Batch: 1150
Training Loss: 0.027994621966196145
Epoch: 34 Batch: 1200
Training Loss: 0.028864865278204282
Epoch: 34 Batch: 1250
Training Loss: 0.025840198397636415
Epoch: 34 Batch: 1300
Training Loss: 0.024619598044798924
Epoch: 34 Batch: 1350
Training Loss: 0.02339845030396073
Epoch: 34 Batch: 1400
Training Loss: 0.02410196432045528
Epoch: 34 Batch: 1450
Training Loss: 0.021876683337935087
Epoch: 34 Batch: 1500
Training Loss: 0.021304663757483164
Epoch: 34 Batch: 1550
Training Loss: 0.02055079963899428
Epoch: 34 Batch: 1600
Training Loss: 0.02065907448530197
Epoch: 34 Batch: 1650
Training Loss: 0.01942344544511853
Epoch: 34 Batch: 1700
Training Loss: 0.019976408762090347
Epoch: 34 Batch: 1750
Training Loss: 0.01889557773726327
Epoch: 34 Batch: 1800
Training Loss: 0.018935177557998232
Epoch: 34 Batch: 1850
Training Loss: 0.018215699147533727
Epoch: 34 Batch: 1900
Training Loss: 0.017776603400707244
Epoch: 34 Batch: 1950
Training Loss: 0.0165858599008658
Epoch: 34 Batch: 2000
Training Loss: 0.01574827191233635
Epoch: 34 Batch: 2050
Training Loss: 0.015528910421743626
Epoch: 34 Batch: 2100
Training Loss: 0.016060026770546323
Epoch: 34 Batch: 2150
Training Loss: 0.014492208361625672
Epoch: 34 Batch: 2200
Training Loss: 0.015186694630167702
Epoch: 34 Batch: 2250
Training Loss: 0.014923730611801148
Epoch: 34 Batch: 2300
Training Loss: 0.013863081634044648
Epoch: 34 Batch: 2350
Training Loss: 0.013976581122012848
Epoch: 34 Batch: 2400
Training Loss: 0.013764795201520125
Epoch: 34 Batch: 2450
Training Loss: 0.012990219507898602
Epoch: 34 Batch: 2500
Training Loss: 0.013158130598068238
Epoch: 34 Batch: 2550
Training Loss: 0.012842013812532612
Epoch: 34 Batch: 2600
Training Loss: 0.013059428196686965
Epoch: 34 Batch: 2650
Training Loss: 0.012368229301470631
Epoch: 34 Batch: 2700
Training Loss: 0.012616615792115529
Epoch: 34 Batch: 2750
Training Loss: 0.011975076198577881
Epoch: 34 Batch: 2800
Training Loss: 0.01120681410389287
Epoch: 34 Batch: 2850
Training Loss: 0.011747194246241922
Epoch: 34 Batch: 2900
Training Loss: 0.010595027085008292
Epoch: 34 Batch: 2950
Training Loss: 0.010336027014053475
Epoch: 34 Batch: 3000
Training Loss: 0.010829924196004868
Epoch: 34 Batch: 3050
Training Loss: 0.010550490715464607
Epoch: 34 Batch: 3100
Training Loss: 0.01082092696620572
Epoch: 34 Batch: 3150
Training Loss: 0.010476108401540726
Epoch: 34 Batch: 3200
Training Loss: 0.010303605338558555
Epoch: 35 
 Validation Loss: 0.5039505690336228
---------------------------
Epoch: 35 Batch: 50
Training Loss: 0.691551656126976
Epoch: 35 Batch: 100
Training Loss: 0.3244598037004471
Epoch: 35 Batch: 150
Training Loss: 0.21294203599294026
Epoch: 35 Batch: 200
Training Loss: 0.16283581778407097
Epoch: 35 Batch: 250
Training Loss: 0.13196296739578248
Epoch: 35 Batch: 300
Training Loss: 0.11388137608766556
Epoch: 35 Batch: 350
Training Loss: 0.09578490946974073
Epoch: 35 Batch: 400
Training Loss: 0.08066338121891022
Epoch: 35 Batch: 450
Training Loss: 0.07801133102840847
Epoch: 35 Batch: 500
Training Loss: 0.06724467641115188
Epoch: 35 Batch: 550
Training Loss: 0.06098492855375463
Epoch: 35 Batch: 600
Training Loss: 0.054274211128552755
Epoch: 35 Batch: 650
Training Loss: 0.052590113373903125
Epoch: 35 Batch: 700
Training Loss: 0.046193254845482964
Epoch: 35 Batch: 750
Training Loss: 0.04285740292072296
Epoch: 35 Batch: 800
Training Loss: 0.03933846138417721
Epoch: 35 Batch: 850
Training Loss: 0.03811013060457566
Epoch: 35 Batch: 900
Training Loss: 0.035209764970673456
Epoch: 35 Batch: 950
Training Loss: 0.03598153402930812
Epoch: 35 Batch: 1000
Training Loss: 0.03341583174467087
Epoch: 35 Batch: 1050
Training Loss: 0.031059220575150988
Epoch: 35 Batch: 1100
Training Loss: 0.030345929942347787
Epoch: 35 Batch: 1150
Training Loss: 0.02794474386650583
Epoch: 35 Batch: 1200
Training Loss: 0.02775199495255947
Epoch: 35 Batch: 1250
Training Loss: 0.026022293543815612
Epoch: 35 Batch: 1300
Training Loss: 0.024103131936146664
Epoch: 35 Batch: 1350
Training Loss: 0.024768035500137894
Epoch: 35 Batch: 1400
Training Loss: 0.022431780908788952
Epoch: 35 Batch: 1450
Training Loss: 0.022420590458245113
Epoch: 35 Batch: 1500
Training Loss: 0.02100767664114634
Epoch: 35 Batch: 1550
Training Loss: 0.021074543499177503
Epoch: 35 Batch: 1600
Training Loss: 0.02064427822828293
Epoch: 35 Batch: 1650
Training Loss: 0.019699229272929105
Epoch: 35 Batch: 1700
Training Loss: 0.019576394610545216
Epoch: 35 Batch: 1750
Training Loss: 0.018971320935658047
Epoch: 35 Batch: 1800
Training Loss: 0.01788716799683041
Epoch: 35 Batch: 1850
Training Loss: 0.018510861799523638
Epoch: 35 Batch: 1900
Training Loss: 0.017117830781560196
Epoch: 35 Batch: 1950
Training Loss: 0.016276818345754576
Epoch: 35 Batch: 2000
Training Loss: 0.017077662453055382
Epoch: 35 Batch: 2050
Training Loss: 0.015799628859613
Epoch: 35 Batch: 2100
Training Loss: 0.01507413658357802
Epoch: 35 Batch: 2150
Training Loss: 0.015045396283615467
Epoch: 35 Batch: 2200
Training Loss: 0.015577000162818214
Epoch: 35 Batch: 2250
Training Loss: 0.015075949827829997
Epoch: 35 Batch: 2300
Training Loss: 0.014050908982753754
Epoch: 35 Batch: 2350
Training Loss: 0.01380926233656863
Epoch: 35 Batch: 2400
Training Loss: 0.013766868176559607
Epoch: 35 Batch: 2450
Training Loss: 0.012949970997109705
Epoch: 35 Batch: 2500
Training Loss: 0.013049556565284729
Epoch: 35 Batch: 2550
Training Loss: 0.012952166199684144
Epoch: 35 Batch: 2600
Training Loss: 0.012332574220804068
Epoch: 35 Batch: 2650
Training Loss: 0.01229048121650264
Epoch: 35 Batch: 2700
Training Loss: 0.012366368847864646
Epoch: 35 Batch: 2750
Training Loss: 0.01121811417016116
Epoch: 35 Batch: 2800
Training Loss: 0.011042488057698523
Epoch: 35 Batch: 2850
Training Loss: 0.011209478535150226
Epoch: 35 Batch: 2900
Training Loss: 0.011631612705773321
Epoch: 35 Batch: 2950
Training Loss: 0.011440466005923385
Epoch: 35 Batch: 3000
Training Loss: 0.010611395517985026
Epoch: 35 Batch: 3050
Training Loss: 0.010530356735479637
Epoch: 35 Batch: 3100
Training Loss: 0.010462332973557134
Epoch: 35 Batch: 3150
Training Loss: 0.01017416531131381
Epoch: 35 Batch: 3200
Training Loss: 0.009965651826933026
Epoch: 36 
 Validation Loss: 0.503160191906823
---------------------------
Epoch: 36 Batch: 50
Training Loss: 0.6755300629138946
Epoch: 36 Batch: 100
Training Loss: 0.33708086907863616
Epoch: 36 Batch: 150
Training Loss: 0.2232522447903951
Epoch: 36 Batch: 200
Training Loss: 0.15664389729499817
Epoch: 36 Batch: 250
Training Loss: 0.13632875657081603
Epoch: 36 Batch: 300
Training Loss: 0.11419805824756622
Epoch: 36 Batch: 350
Training Loss: 0.0937654857976096
Epoch: 36 Batch: 400
Training Loss: 0.08182915277779103
Epoch: 36 Batch: 450
Training Loss: 0.07402336577574412
Epoch: 36 Batch: 500
Training Loss: 0.06437086659669876
Epoch: 36 Batch: 550
Training Loss: 0.061676625284281646
Epoch: 36 Batch: 600
Training Loss: 0.052884641687075296
Epoch: 36 Batch: 650
Training Loss: 0.04983274079286135
Epoch: 36 Batch: 700
Training Loss: 0.04512150666543416
Epoch: 36 Batch: 750
Training Loss: 0.04396144763628642
Epoch: 36 Batch: 800
Training Loss: 0.04126277096569538
Epoch: 36 Batch: 850
Training Loss: 0.03924687361015993
Epoch: 36 Batch: 900
Training Loss: 0.035603102213806574
Epoch: 36 Batch: 950
Training Loss: 0.03579098748533349
Epoch: 36 Batch: 1000
Training Loss: 0.03254646277427673
Epoch: 36 Batch: 1050
Training Loss: 0.03074970273744492
Epoch: 36 Batch: 1100
Training Loss: 0.02896212417971004
Epoch: 36 Batch: 1150
Training Loss: 0.029990563418554224
Epoch: 36 Batch: 1200
Training Loss: 0.027125744496782622
Epoch: 36 Batch: 1250
Training Loss: 0.02588004620075226
Epoch: 36 Batch: 1300
Training Loss: 0.0260157157595341
Epoch: 36 Batch: 1350
Training Loss: 0.024731754329469467
Epoch: 36 Batch: 1400
Training Loss: 0.024085393377712795
Epoch: 36 Batch: 1450
Training Loss: 0.02183197187966314
Epoch: 36 Batch: 1500
Training Loss: 0.02202867251634598
Epoch: 36 Batch: 1550
Training Loss: 0.020843397321239594
Epoch: 36 Batch: 1600
Training Loss: 0.020940805841237305
Epoch: 36 Batch: 1650
Training Loss: 0.020639470400232257
Epoch: 36 Batch: 1700
Training Loss: 0.018227980873164008
Epoch: 36 Batch: 1750
Training Loss: 0.01894586033480508
Epoch: 36 Batch: 1800
Training Loss: 0.01773055957423316
Epoch: 36 Batch: 1850
Training Loss: 0.017464460231162406
Epoch: 36 Batch: 1900
Training Loss: 0.01625283464005119
Epoch: 36 Batch: 1950
Training Loss: 0.016002260645230613
Epoch: 36 Batch: 2000
Training Loss: 0.016021389096975328
Epoch: 36 Batch: 2050
Training Loss: 0.015909688298295185
Epoch: 36 Batch: 2100
Training Loss: 0.01593966542255311
Epoch: 36 Batch: 2150
Training Loss: 0.015028475190317908
Epoch: 36 Batch: 2200
Training Loss: 0.014882785921747034
Epoch: 36 Batch: 2250
Training Loss: 0.014420157419310676
Epoch: 36 Batch: 2300
Training Loss: 0.01420762405447338
Epoch: 36 Batch: 2350
Training Loss: 0.013594203642074098
Epoch: 36 Batch: 2400
Training Loss: 0.013719970298310121
Epoch: 36 Batch: 2450
Training Loss: 0.01270722962155634
Epoch: 36 Batch: 2500
Training Loss: 0.012350726354122162
Epoch: 36 Batch: 2550
Training Loss: 0.01291405879983715
Epoch: 36 Batch: 2600
Training Loss: 0.012007400210087116
Epoch: 36 Batch: 2650
Training Loss: 0.012957295044413153
Epoch: 36 Batch: 2700
Training Loss: 0.011614587152445758
Epoch: 36 Batch: 2750
Training Loss: 0.0119777915911241
Epoch: 36 Batch: 2800
Training Loss: 0.011484230554529599
Epoch: 36 Batch: 2850
Training Loss: 0.011653145070661577
Epoch: 36 Batch: 2900
Training Loss: 0.011525525450706483
Epoch: 36 Batch: 2950
Training Loss: 0.011147221296520557
Epoch: 36 Batch: 3000
Training Loss: 0.01073063267270724
Epoch: 36 Batch: 3050
Training Loss: 0.010706696969563844
Epoch: 36 Batch: 3100
Training Loss: 0.0102985490522077
Epoch: 36 Batch: 3150
Training Loss: 0.01018294410100059
Epoch: 36 Batch: 3200
Training Loss: 0.010387743012979627
Epoch: 37 
 Validation Loss: 0.5020636879735523
---------------------------
Epoch: 37 Batch: 50
Training Loss: 0.6432267487049103
Epoch: 37 Batch: 100
Training Loss: 0.33905345588922503
Epoch: 37 Batch: 150
Training Loss: 0.21301169872283934
Epoch: 37 Batch: 200
Training Loss: 0.1735361835360527
Epoch: 37 Batch: 250
Training Loss: 0.13600104403495789
Epoch: 37 Batch: 300
Training Loss: 0.11027687768141428
Epoch: 37 Batch: 350
Training Loss: 0.08919814714363643
Epoch: 37 Batch: 400
Training Loss: 0.082418440207839
Epoch: 37 Batch: 450
Training Loss: 0.07312497913837433
Epoch: 37 Batch: 500
Training Loss: 0.0660381675362587
Epoch: 37 Batch: 550
Training Loss: 0.05872246498411352
Epoch: 37 Batch: 600
Training Loss: 0.05277930567661921
Epoch: 37 Batch: 650
Training Loss: 0.05084637788625864
Epoch: 37 Batch: 700
Training Loss: 0.04761438970054899
Epoch: 37 Batch: 750
Training Loss: 0.042338266253471375
Epoch: 37 Batch: 800
Training Loss: 0.040471771508455275
Epoch: 37 Batch: 850
Training Loss: 0.039314262691666095
Epoch: 37 Batch: 900
Training Loss: 0.035778948764006294
Epoch: 37 Batch: 950
Training Loss: 0.03382129229997334
Epoch: 37 Batch: 1000
Training Loss: 0.03145461392402649
Epoch: 37 Batch: 1050
Training Loss: 0.03021182982694535
Epoch: 37 Batch: 1100
Training Loss: 0.028862904689528724
Epoch: 37 Batch: 1150
Training Loss: 0.02810585952323416
Epoch: 37 Batch: 1200
Training Loss: 0.028415570283929506
Epoch: 37 Batch: 1250
Training Loss: 0.02631764199733734
Epoch: 37 Batch: 1300
Training Loss: 0.024638185867896446
Epoch: 37 Batch: 1350
Training Loss: 0.024456737836201985
Epoch: 37 Batch: 1400
Training Loss: 0.022783596856253488
Epoch: 37 Batch: 1450
Training Loss: 0.022611771883635685
Epoch: 37 Batch: 1500
Training Loss: 0.02130402918656667
Epoch: 37 Batch: 1550
Training Loss: 0.020328588908718477
Epoch: 37 Batch: 1600
Training Loss: 0.020701998602598904
Epoch: 37 Batch: 1650
Training Loss: 0.019777189475117307
Epoch: 37 Batch: 1700
Training Loss: 0.018851194504429312
Epoch: 37 Batch: 1750
Training Loss: 0.018261843034199306
Epoch: 37 Batch: 1800
Training Loss: 0.017359811928537156
Epoch: 37 Batch: 1850
Training Loss: 0.017878967829652734
Epoch: 37 Batch: 1900
Training Loss: 0.017564451851342852
Epoch: 37 Batch: 1950
Training Loss: 0.0173578924093491
Epoch: 37 Batch: 2000
Training Loss: 0.01637037931382656
Epoch: 37 Batch: 2050
Training Loss: 0.01621661516224466
Epoch: 37 Batch: 2100
Training Loss: 0.015624138116836548
Epoch: 37 Batch: 2150
Training Loss: 0.01510405539080154
Epoch: 37 Batch: 2200
Training Loss: 0.014772422909736634
Epoch: 37 Batch: 2250
Training Loss: 0.014716576947106255
Epoch: 37 Batch: 2300
Training Loss: 0.014140918786111085
Epoch: 37 Batch: 2350
Training Loss: 0.014088584628510982
Epoch: 37 Batch: 2400
Training Loss: 0.014032110422849655
Epoch: 37 Batch: 2450
Training Loss: 0.013812129144765893
Epoch: 37 Batch: 2500
Training Loss: 0.013050875186920167
Epoch: 37 Batch: 2550
Training Loss: 0.012558605180067175
Epoch: 37 Batch: 2600
Training Loss: 0.01277716774206895
Epoch: 37 Batch: 2650
Training Loss: 0.012447271650692203
Epoch: 37 Batch: 2700
Training Loss: 0.012402474527005797
Epoch: 37 Batch: 2750
Training Loss: 0.012127864035693083
Epoch: 37 Batch: 2800
Training Loss: 0.011728828123637607
Epoch: 37 Batch: 2850
Training Loss: 0.011133726416972646
Epoch: 37 Batch: 2900
Training Loss: 0.010966654019109134
Epoch: 37 Batch: 2950
Training Loss: 0.01128471519987462
Epoch: 37 Batch: 3000
Training Loss: 0.010597292761007945
Epoch: 37 Batch: 3050
Training Loss: 0.010989433184998934
Epoch: 37 Batch: 3100
Training Loss: 0.010681822607594151
Epoch: 37 Batch: 3150
Training Loss: 0.010452420191159323
Epoch: 37 Batch: 3200
Training Loss: 0.0105573386605829
Epoch: 38 
 Validation Loss: 0.5016838918129604
---------------------------
Epoch: 38 Batch: 50
Training Loss: 0.6434439951181412
Epoch: 38 Batch: 100
Training Loss: 0.31405275493860246
Epoch: 38 Batch: 150
Training Loss: 0.2244789973894755
Epoch: 38 Batch: 200
Training Loss: 0.16277389198541642
Epoch: 38 Batch: 250
Training Loss: 0.13311975729465483
Epoch: 38 Batch: 300
Training Loss: 0.10821590125560761
Epoch: 38 Batch: 350
Training Loss: 0.09508082176957812
Epoch: 38 Batch: 400
Training Loss: 0.08320177100598812
Epoch: 38 Batch: 450
Training Loss: 0.07268958257304298
Epoch: 38 Batch: 500
Training Loss: 0.06367851829528809
Epoch: 38 Batch: 550
Training Loss: 0.058083069703795696
Epoch: 38 Batch: 600
Training Loss: 0.05768427262703578
Epoch: 38 Batch: 650
Training Loss: 0.048347359758156995
Epoch: 38 Batch: 700
Training Loss: 0.04636820371661867
Epoch: 38 Batch: 750
Training Loss: 0.045287474354108175
Epoch: 38 Batch: 800
Training Loss: 0.040567932091653346
Epoch: 38 Batch: 850
Training Loss: 0.03910322697723613
Epoch: 38 Batch: 900
Training Loss: 0.03599140743414561
Epoch: 38 Batch: 950
Training Loss: 0.03576406507115615
Epoch: 38 Batch: 1000
Training Loss: 0.03135657051205635
Epoch: 38 Batch: 1050
Training Loss: 0.03312611506098793
Epoch: 38 Batch: 1100
Training Loss: 0.03115969647060741
Epoch: 38 Batch: 1150
Training Loss: 0.028808844426403874
Epoch: 38 Batch: 1200
Training Loss: 0.027882289613286655
Epoch: 38 Batch: 1250
Training Loss: 0.025132436084747314
Epoch: 38 Batch: 1300
Training Loss: 0.02464922941648043
Epoch: 38 Batch: 1350
Training Loss: 0.02382399881327594
Epoch: 38 Batch: 1400
Training Loss: 0.024175920145852226
Epoch: 38 Batch: 1450
Training Loss: 0.02278432114370938
Epoch: 38 Batch: 1500
Training Loss: 0.02073232356707255
Epoch: 38 Batch: 1550
Training Loss: 0.02129474801401938
Epoch: 38 Batch: 1600
Training Loss: 0.02036152536049485
Epoch: 38 Batch: 1650
Training Loss: 0.01884778663967595
Epoch: 38 Batch: 1700
Training Loss: 0.01885914181961733
Epoch: 38 Batch: 1750
Training Loss: 0.018296469926834105
Epoch: 38 Batch: 1800
Training Loss: 0.017587362312608296
Epoch: 38 Batch: 1850
Training Loss: 0.01783991973142366
Epoch: 38 Batch: 1900
Training Loss: 0.01764370863374911
Epoch: 38 Batch: 1950
Training Loss: 0.01681391540246132
Epoch: 38 Batch: 2000
Training Loss: 0.015688557043671607
Epoch: 38 Batch: 2050
Training Loss: 0.01607068319146226
Epoch: 38 Batch: 2100
Training Loss: 0.0157077266063009
Epoch: 38 Batch: 2150
Training Loss: 0.01455950494422469
Epoch: 38 Batch: 2200
Training Loss: 0.014609382139010862
Epoch: 38 Batch: 2250
Training Loss: 0.014317449861102635
Epoch: 38 Batch: 2300
Training Loss: 0.014229632395765055
Epoch: 38 Batch: 2350
Training Loss: 0.013985658985503177
Epoch: 38 Batch: 2400
Training Loss: 0.013662403747439385
Epoch: 38 Batch: 2450
Training Loss: 0.012844588780889706
Epoch: 38 Batch: 2500
Training Loss: 0.01332100212574005
Epoch: 38 Batch: 2550
Training Loss: 0.01267330433808121
Epoch: 38 Batch: 2600
Training Loss: 0.011993722869799687
Epoch: 38 Batch: 2650
Training Loss: 0.012342501878738404
Epoch: 38 Batch: 2700
Training Loss: 0.011842699371002338
Epoch: 38 Batch: 2750
Training Loss: 0.010988140723922036
Epoch: 38 Batch: 2800
Training Loss: 0.01182323917746544
Epoch: 38 Batch: 2850
Training Loss: 0.011372063902386449
Epoch: 38 Batch: 2900
Training Loss: 0.011258428528391082
Epoch: 38 Batch: 2950
Training Loss: 0.010816150600627317
Epoch: 38 Batch: 3000
Training Loss: 0.010538013031085333
Epoch: 38 Batch: 3050
Training Loss: 0.010557034611701965
Epoch: 38 Batch: 3100
Training Loss: 0.010330735358499711
Epoch: 38 Batch: 3150
Training Loss: 0.01057542503826202
Epoch: 38 Batch: 3200
Training Loss: 0.01043587270192802
Epoch: 39 
 Validation Loss: 0.5003637260860867
---------------------------
Epoch: 39 Batch: 50
Training Loss: 0.642632104754448
Epoch: 39 Batch: 100
Training Loss: 0.3176064455509186
Epoch: 39 Batch: 150
Training Loss: 0.2203632136185964
Epoch: 39 Batch: 200
Training Loss: 0.15988599255681038
Epoch: 39 Batch: 250
Training Loss: 0.12625078058242797
Epoch: 39 Batch: 300
Training Loss: 0.11422221382459005
Epoch: 39 Batch: 350
Training Loss: 0.09341945350170136
Epoch: 39 Batch: 400
Training Loss: 0.078839720338583
Epoch: 39 Batch: 450
Training Loss: 0.0707918671766917
Epoch: 39 Batch: 500
Training Loss: 0.06527015256881714
Epoch: 39 Batch: 550
Training Loss: 0.05613385092128407
Epoch: 39 Batch: 600
Training Loss: 0.04998496626814206
Epoch: 39 Batch: 650
Training Loss: 0.051435993222089915
Epoch: 39 Batch: 700
Training Loss: 0.04797678500413895
Epoch: 39 Batch: 750
Training Loss: 0.043624414761861165
Epoch: 39 Batch: 800
Training Loss: 0.04092856295406819
Epoch: 39 Batch: 850
Training Loss: 0.03676108258612016
Epoch: 39 Batch: 900
Training Loss: 0.038020650380187565
Epoch: 39 Batch: 950
Training Loss: 0.0344757396296451
Epoch: 39 Batch: 1000
Training Loss: 0.03289826169610024
Epoch: 39 Batch: 1050
Training Loss: 0.030892729106403533
Epoch: 39 Batch: 1100
Training Loss: 0.028892990919676693
Epoch: 39 Batch: 1150
Training Loss: 0.029760013585505278
Epoch: 39 Batch: 1200
Training Loss: 0.028332558249433835
Epoch: 39 Batch: 1250
Training Loss: 0.026503395891189576
Epoch: 39 Batch: 1300
Training Loss: 0.023945043820601242
Epoch: 39 Batch: 1350
Training Loss: 0.02336139491310826
Epoch: 39 Batch: 1400
Training Loss: 0.023951039952891214
Epoch: 39 Batch: 1450
Training Loss: 0.022259032068581416
Epoch: 39 Batch: 1500
Training Loss: 0.02252571670214335
Epoch: 39 Batch: 1550
Training Loss: 0.020763271566360226
Epoch: 39 Batch: 1600
Training Loss: 0.020100225452333687
Epoch: 39 Batch: 1650
Training Loss: 0.019350883147933266
Epoch: 39 Batch: 1700
Training Loss: 0.020111154125017277
Epoch: 39 Batch: 1750
Training Loss: 0.019699375578335353
Epoch: 39 Batch: 1800
Training Loss: 0.018259781383805806
Epoch: 39 Batch: 1850
Training Loss: 0.017604015131254452
Epoch: 39 Batch: 1900
Training Loss: 0.017148378961964656
Epoch: 39 Batch: 1950
Training Loss: 0.016694049040476482
Epoch: 39 Batch: 2000
Training Loss: 0.015990206494927408
Epoch: 39 Batch: 2050
Training Loss: 0.015966059013110837
Epoch: 39 Batch: 2100
Training Loss: 0.015021178992021652
Epoch: 39 Batch: 2150
Training Loss: 0.01553269742533218
Epoch: 39 Batch: 2200
Training Loss: 0.014785292636264455
Epoch: 39 Batch: 2250
Training Loss: 0.014731202668613858
Epoch: 39 Batch: 2300
Training Loss: 0.014468384151873381
Epoch: 39 Batch: 2350
Training Loss: 0.013868009029550755
Epoch: 39 Batch: 2400
Training Loss: 0.013497266943256061
Epoch: 39 Batch: 2450
Training Loss: 0.013484094009107474
Epoch: 39 Batch: 2500
Training Loss: 0.013278970444202422
Epoch: 39 Batch: 2550
Training Loss: 0.013094992707757389
Epoch: 39 Batch: 2600
Training Loss: 0.011914729063327497
Epoch: 39 Batch: 2650
Training Loss: 0.011903063171314744
Epoch: 39 Batch: 2700
Training Loss: 0.01197759715495286
Epoch: 39 Batch: 2750
Training Loss: 0.012366681055589156
Epoch: 39 Batch: 2800
Training Loss: 0.011594908790928976
Epoch: 39 Batch: 2850
Training Loss: 0.011512002997231065
Epoch: 39 Batch: 2900
Training Loss: 0.01128884096597803
Epoch: 39 Batch: 2950
Training Loss: 0.010652130852311344
Epoch: 39 Batch: 3000
Training Loss: 0.010590880384047827
Epoch: 39 Batch: 3050
Training Loss: 0.01047374508419975
Epoch: 39 Batch: 3100
Training Loss: 0.010055093053848514
Epoch: 39 Batch: 3150
Training Loss: 0.010370424096546475
Epoch: 39 Batch: 3200
Training Loss: 0.010303079998120666
Epoch: 40 
 Validation Loss: 0.4998667230208715
---------------------------
Epoch: 40 Batch: 50
Training Loss: 0.6643986451625824
Epoch: 40 Batch: 100
Training Loss: 0.3349868530035019
Epoch: 40 Batch: 150
Training Loss: 0.2268888020515442
Epoch: 40 Batch: 200
Training Loss: 0.1576266574859619
Epoch: 40 Batch: 250
Training Loss: 0.1227448434829712
Epoch: 40 Batch: 300
Training Loss: 0.10578986088434855
Epoch: 40 Batch: 350
Training Loss: 0.09319183077130999
Epoch: 40 Batch: 400
Training Loss: 0.08356366373598575
Epoch: 40 Batch: 450
Training Loss: 0.07187664396233029
Epoch: 40 Batch: 500
Training Loss: 0.0659725268483162
Epoch: 40 Batch: 550
Training Loss: 0.059756492755629796
Epoch: 40 Batch: 600
Training Loss: 0.05572342495123545
Epoch: 40 Batch: 650
Training Loss: 0.05067761774246509
Epoch: 40 Batch: 700
Training Loss: 0.04590357708079474
Epoch: 40 Batch: 750
Training Loss: 0.042892920811971026
Epoch: 40 Batch: 800
Training Loss: 0.040609613507986066
Epoch: 40 Batch: 850
Training Loss: 0.03721145517685834
Epoch: 40 Batch: 900
Training Loss: 0.036859567364056905
Epoch: 40 Batch: 950
Training Loss: 0.03352379965154748
Epoch: 40 Batch: 1000
Training Loss: 0.03497109910845757
Epoch: 40 Batch: 1050
Training Loss: 0.030982373782566616
Epoch: 40 Batch: 1100
Training Loss: 0.029009917893192985
Epoch: 40 Batch: 1150
Training Loss: 0.028489512205123902
Epoch: 40 Batch: 1200
Training Loss: 0.028519212876756985
Epoch: 40 Batch: 1250
Training Loss: 0.026040144419670105
Epoch: 40 Batch: 1300
Training Loss: 0.024522088582699116
Epoch: 40 Batch: 1350
Training Loss: 0.023488673214559202
Epoch: 40 Batch: 1400
Training Loss: 0.023543366704668316
Epoch: 40 Batch: 1450
Training Loss: 0.02270384765904525
Epoch: 40 Batch: 1500
Training Loss: 0.022338405191898347
Epoch: 40 Batch: 1550
Training Loss: 0.020944979171599112
Epoch: 40 Batch: 1600
Training Loss: 0.021124267373234033
Epoch: 40 Batch: 1650
Training Loss: 0.01973729032458681
Epoch: 40 Batch: 1700
Training Loss: 0.018628612283398124
Epoch: 40 Batch: 1750
Training Loss: 0.018186698079109193
Epoch: 40 Batch: 1800
Training Loss: 0.01891845964723163
Epoch: 40 Batch: 1850
Training Loss: 0.017749961904577307
Epoch: 40 Batch: 1900
Training Loss: 0.016782969330486498
Epoch: 40 Batch: 1950
Training Loss: 0.01636992384225894
Epoch: 40 Batch: 2000
Training Loss: 0.016561009123921396
Epoch: 40 Batch: 2050
Training Loss: 0.016289588925315113
Epoch: 40 Batch: 2100
Training Loss: 0.01534825149036589
Epoch: 40 Batch: 2150
Training Loss: 0.015080142132071562
Epoch: 40 Batch: 2200
Training Loss: 0.014739967760714618
Epoch: 40 Batch: 2250
Training Loss: 0.013262033859888712
Epoch: 40 Batch: 2300
Training Loss: 0.014042981616828753
Epoch: 40 Batch: 2350
Training Loss: 0.013738710296914933
Epoch: 40 Batch: 2400
Training Loss: 0.01336776149769624
Epoch: 40 Batch: 2450
Training Loss: 0.012866992135437167
Epoch: 40 Batch: 2500
Training Loss: 0.013354530060291291
Epoch: 40 Batch: 2550
Training Loss: 0.0136079226288141
Epoch: 40 Batch: 2600
Training Loss: 0.01260208133321542
Epoch: 40 Batch: 2650
Training Loss: 0.012652078945681733
Epoch: 40 Batch: 2700
Training Loss: 0.011770993449069836
Epoch: 40 Batch: 2750
Training Loss: 0.011257423227483576
Epoch: 40 Batch: 2800
Training Loss: 0.011532322700534548
Epoch: 40 Batch: 2850
Training Loss: 0.011412258085451628
Epoch: 40 Batch: 2900
Training Loss: 0.011081049740314484
Epoch: 40 Batch: 2950
Training Loss: 0.010518284217785981
Epoch: 40 Batch: 3000
Training Loss: 0.010436864962180456
Epoch: 40 Batch: 3050
Training Loss: 0.010940059345276629
Epoch: 40 Batch: 3100
Training Loss: 0.010068025233284119
Epoch: 40 Batch: 3150
Training Loss: 0.010246095345133826
Epoch: 40 Batch: 3200
Training Loss: 0.009624275863170623
Epoch: 41 
 Validation Loss: 0.4995744016435411
---------------------------
Epoch: 41 Batch: 50
Training Loss: 0.6577349871397018
Epoch: 41 Batch: 100
Training Loss: 0.32943688809871674
Epoch: 41 Batch: 150
Training Loss: 0.2134056309858958
Epoch: 41 Batch: 200
Training Loss: 0.16565852463245392
Epoch: 41 Batch: 250
Training Loss: 0.13024527108669282
Epoch: 41 Batch: 300
Training Loss: 0.10486449937025706
Epoch: 41 Batch: 350
Training Loss: 0.09038689255714416
Epoch: 41 Batch: 400
Training Loss: 0.07866073451936245
Epoch: 41 Batch: 450
Training Loss: 0.07369985507594215
Epoch: 41 Batch: 500
Training Loss: 0.063805330991745
Epoch: 41 Batch: 550
Training Loss: 0.06057557447390123
Epoch: 41 Batch: 600
Training Loss: 0.054022457152605056
Epoch: 41 Batch: 650
Training Loss: 0.05064096455390637
Epoch: 41 Batch: 700
Training Loss: 0.04526523969003132
Epoch: 41 Batch: 750
Training Loss: 0.041565181334813434
Epoch: 41 Batch: 800
Training Loss: 0.03949050758033991
Epoch: 41 Batch: 850
Training Loss: 0.03922532667131985
Epoch: 41 Batch: 900
Training Loss: 0.03748857210079829
Epoch: 41 Batch: 950
Training Loss: 0.03435683187685515
Epoch: 41 Batch: 1000
Training Loss: 0.03283462134003639
Epoch: 41 Batch: 1050
Training Loss: 0.0309591277440389
Epoch: 41 Batch: 1100
Training Loss: 0.029547661624171516
Epoch: 41 Batch: 1150
Training Loss: 0.02822832807250645
Epoch: 41 Batch: 1200
Training Loss: 0.026474289521574976
Epoch: 41 Batch: 1250
Training Loss: 0.025197625184059144
Epoch: 41 Batch: 1300
Training Loss: 0.026129663449067336
Epoch: 41 Batch: 1350
Training Loss: 0.02249540556360174
Epoch: 41 Batch: 1400
Training Loss: 0.023653416548456464
Epoch: 41 Batch: 1450
Training Loss: 0.02272946522153657
Epoch: 41 Batch: 1500
Training Loss: 0.02073143337170283
Epoch: 41 Batch: 1550
Training Loss: 0.020110956814981276
Epoch: 41 Batch: 1600
Training Loss: 0.019774494115263223
Epoch: 41 Batch: 1650
Training Loss: 0.01953096044786049
Epoch: 41 Batch: 1700
Training Loss: 0.018974160587086397
Epoch: 41 Batch: 1750
Training Loss: 0.01891445732116699
Epoch: 41 Batch: 1800
Training Loss: 0.018389089256525038
Epoch: 41 Batch: 1850
Training Loss: 0.017412792492557217
Epoch: 41 Batch: 1900
Training Loss: 0.017187211278237795
Epoch: 41 Batch: 1950
Training Loss: 0.016289880688373858
Epoch: 41 Batch: 2000
Training Loss: 0.01552179577946663
Epoch: 41 Batch: 2050
Training Loss: 0.01508812798232567
Epoch: 41 Batch: 2100
Training Loss: 0.015539756828830355
Epoch: 41 Batch: 2150
Training Loss: 0.014500022724617359
Epoch: 41 Batch: 2200
Training Loss: 0.01448852769353173
Epoch: 41 Batch: 2250
Training Loss: 0.014159140388170878
Epoch: 41 Batch: 2300
Training Loss: 0.013813127110833706
Epoch: 41 Batch: 2350
Training Loss: 0.013857988182534562
Epoch: 41 Batch: 2400
Training Loss: 0.01347447739293178
Epoch: 41 Batch: 2450
Training Loss: 0.013228823530430697
Epoch: 41 Batch: 2500
Training Loss: 0.013277595591545104
Epoch: 41 Batch: 2550
Training Loss: 0.012626143600426468
Epoch: 41 Batch: 2600
Training Loss: 0.012488780571864201
Epoch: 41 Batch: 2650
Training Loss: 0.011913667647343761
Epoch: 41 Batch: 2700
Training Loss: 0.012092514413374441
Epoch: 41 Batch: 2750
Training Loss: 0.012318262533708052
Epoch: 41 Batch: 2800
Training Loss: 0.01174164174922875
Epoch: 41 Batch: 2850
Training Loss: 0.012237659096717835
Epoch: 41 Batch: 2900
Training Loss: 0.011407370803685023
Epoch: 41 Batch: 2950
Training Loss: 0.011091955354658223
Epoch: 41 Batch: 3000
Training Loss: 0.010813540031512579
Epoch: 41 Batch: 3050
Training Loss: 0.010773146709457773
Epoch: 41 Batch: 3100
Training Loss: 0.010912591017061664
Epoch: 41 Batch: 3150
Training Loss: 0.010262510975201925
Epoch: 41 Batch: 3200
Training Loss: 0.0100778432469815
Epoch: 42 
 Validation Loss: 0.49834077292018464
---------------------------
Epoch: 42 Batch: 50
Training Loss: 0.7118008577823639
Epoch: 42 Batch: 100
Training Loss: 0.3232917499542236
Epoch: 42 Batch: 150
Training Loss: 0.2092335840066274
Epoch: 42 Batch: 200
Training Loss: 0.1626006631553173
Epoch: 42 Batch: 250
Training Loss: 0.12852615880966187
Epoch: 42 Batch: 300
Training Loss: 0.10192539383967718
Epoch: 42 Batch: 350
Training Loss: 0.09324873200484685
Epoch: 42 Batch: 400
Training Loss: 0.0797565783560276
Epoch: 42 Batch: 450
Training Loss: 0.07271474023660024
Epoch: 42 Batch: 500
Training Loss: 0.06431200563907623
Epoch: 42 Batch: 550
Training Loss: 0.057180313251235265
Epoch: 42 Batch: 600
Training Loss: 0.05312993879119555
Epoch: 42 Batch: 650
Training Loss: 0.051785458509738626
Epoch: 42 Batch: 700
Training Loss: 0.04843950514282499
Epoch: 42 Batch: 750
Training Loss: 0.04219121404488881
Epoch: 42 Batch: 800
Training Loss: 0.04023090913891792
Epoch: 42 Batch: 850
Training Loss: 0.03672597650219413
Epoch: 42 Batch: 900
Training Loss: 0.03689853847026825
Epoch: 42 Batch: 950
Training Loss: 0.033499024573125334
Epoch: 42 Batch: 1000
Training Loss: 0.032180489569902423
Epoch: 42 Batch: 1050
Training Loss: 0.02929606758412861
Epoch: 42 Batch: 1100
Training Loss: 0.029619301340796732
Epoch: 42 Batch: 1150
Training Loss: 0.028059297463168268
Epoch: 42 Batch: 1200
Training Loss: 0.026408082072933516
Epoch: 42 Batch: 1250
Training Loss: 0.024958650398254394
Epoch: 42 Batch: 1300
Training Loss: 0.02503286829361549
Epoch: 42 Batch: 1350
Training Loss: 0.023444172784134194
Epoch: 42 Batch: 1400
Training Loss: 0.023505084386893683
Epoch: 42 Batch: 1450
Training Loss: 0.021333866324918023
Epoch: 42 Batch: 1500
Training Loss: 0.02185722315311432
Epoch: 42 Batch: 1550
Training Loss: 0.021634661959063623
Epoch: 42 Batch: 1600
Training Loss: 0.020459221079945564
Epoch: 42 Batch: 1650
Training Loss: 0.018823548392816022
Epoch: 42 Batch: 1700
Training Loss: 0.02028594276484321
Epoch: 42 Batch: 1750
Training Loss: 0.017937201040131707
Epoch: 42 Batch: 1800
Training Loss: 0.01927070283227497
Epoch: 42 Batch: 1850
Training Loss: 0.017087951914684193
Epoch: 42 Batch: 1900
Training Loss: 0.016952875334965554
Epoch: 42 Batch: 1950
Training Loss: 0.01577223886282016
Epoch: 42 Batch: 2000
Training Loss: 0.01603362475335598
Epoch: 42 Batch: 2050
Training Loss: 0.01570782680337022
Epoch: 42 Batch: 2100
Training Loss: 0.0156388283485458
Epoch: 42 Batch: 2150
Training Loss: 0.01502316760462384
Epoch: 42 Batch: 2200
Training Loss: 0.014439284273169258
Epoch: 42 Batch: 2250
Training Loss: 0.014322670896848043
Epoch: 42 Batch: 2300
Training Loss: 0.013971052739931189
Epoch: 42 Batch: 2350
Training Loss: 0.013867064651022566
Epoch: 42 Batch: 2400
Training Loss: 0.013428196447590987
Epoch: 42 Batch: 2450
Training Loss: 0.012726301149446137
Epoch: 42 Batch: 2500
Training Loss: 0.012695274996757508
Epoch: 42 Batch: 2550
Training Loss: 0.013160189004505381
Epoch: 42 Batch: 2600
Training Loss: 0.012943546760540742
Epoch: 42 Batch: 2650
Training Loss: 0.01178205169596762
Epoch: 42 Batch: 2700
Training Loss: 0.012215295577490771
Epoch: 42 Batch: 2750
Training Loss: 0.011978729638186369
Epoch: 42 Batch: 2800
Training Loss: 0.011761270548616138
Epoch: 42 Batch: 2850
Training Loss: 0.011906877881602237
Epoch: 42 Batch: 2900
Training Loss: 0.011333264616029015
Epoch: 42 Batch: 2950
Training Loss: 0.01120028261410988
Epoch: 42 Batch: 3000
Training Loss: 0.011317187786102295
Epoch: 42 Batch: 3050
Training Loss: 0.010457009594948566
Epoch: 42 Batch: 3100
Training Loss: 0.010518694193132462
Epoch: 42 Batch: 3150
Training Loss: 0.01031965554706634
Epoch: 42 Batch: 3200
Training Loss: 0.010362603310495615
Epoch: 43 
 Validation Loss: 0.4984828283389409
---------------------------
Epoch: 43 Batch: 50
Training Loss: 0.6579646182060241
Epoch: 43 Batch: 100
Training Loss: 0.32139078646898267
Epoch: 43 Batch: 150
Training Loss: 0.2168341487646103
Epoch: 43 Batch: 200
Training Loss: 0.1550823213160038
Epoch: 43 Batch: 250
Training Loss: 0.134544478058815
Epoch: 43 Batch: 300
Training Loss: 0.1068369201819102
Epoch: 43 Batch: 350
Training Loss: 0.09096808459077563
Epoch: 43 Batch: 400
Training Loss: 0.08155311688780785
Epoch: 43 Batch: 450
Training Loss: 0.07019712640179528
Epoch: 43 Batch: 500
Training Loss: 0.06384001970291138
Epoch: 43 Batch: 550
Training Loss: 0.061027408567341894
Epoch: 43 Batch: 600
Training Loss: 0.053063922276099525
Epoch: 43 Batch: 650
Training Loss: 0.04856607551758106
Epoch: 43 Batch: 700
Training Loss: 0.0462644185764449
Epoch: 43 Batch: 750
Training Loss: 0.041996806780497234
Epoch: 43 Batch: 800
Training Loss: 0.04028134930878877
Epoch: 43 Batch: 850
Training Loss: 0.03592087349470924
Epoch: 43 Batch: 900
Training Loss: 0.034669643011358046
Epoch: 43 Batch: 950
Training Loss: 0.034254843812239796
Epoch: 43 Batch: 1000
Training Loss: 0.031110433995723726
Epoch: 43 Batch: 1050
Training Loss: 0.029757061827750432
Epoch: 43 Batch: 1100
Training Loss: 0.029435580779205668
Epoch: 43 Batch: 1150
Training Loss: 0.02662514090538025
Epoch: 43 Batch: 1200
Training Loss: 0.026458743487795193
Epoch: 43 Batch: 1250
Training Loss: 0.025840636491775513
Epoch: 43 Batch: 1300
Training Loss: 0.024509720389659588
Epoch: 43 Batch: 1350
Training Loss: 0.024349513627864697
Epoch: 43 Batch: 1400
Training Loss: 0.0229534717968532
Epoch: 43 Batch: 1450
Training Loss: 0.022826439923253554
Epoch: 43 Batch: 1500
Training Loss: 0.02209330318371455
Epoch: 43 Batch: 1550
Training Loss: 0.020496917270844982
Epoch: 43 Batch: 1600
Training Loss: 0.020690536741167307
Epoch: 43 Batch: 1650
Training Loss: 0.01981858795339411
Epoch: 43 Batch: 1700
Training Loss: 0.018059054350151735
Epoch: 43 Batch: 1750
Training Loss: 0.018153144751276287
Epoch: 43 Batch: 1800
Training Loss: 0.016732229077153736
Epoch: 43 Batch: 1850
Training Loss: 0.01751673208700644
Epoch: 43 Batch: 1900
Training Loss: 0.017327064652191967
Epoch: 43 Batch: 1950
Training Loss: 0.016842041932619536
Epoch: 43 Batch: 2000
Training Loss: 0.01655134418606758
Epoch: 43 Batch: 2050
Training Loss: 0.016584337746224753
Epoch: 43 Batch: 2100
Training Loss: 0.015620337213788714
Epoch: 43 Batch: 2150
Training Loss: 0.01488282770611519
Epoch: 43 Batch: 2200
Training Loss: 0.015578978061676026
Epoch: 43 Batch: 2250
Training Loss: 0.014551531526777479
Epoch: 43 Batch: 2300
Training Loss: 0.01374896070231562
Epoch: 43 Batch: 2350
Training Loss: 0.014154155406546085
Epoch: 43 Batch: 2400
Training Loss: 0.013188087940216064
Epoch: 43 Batch: 2450
Training Loss: 0.01336297907391373
Epoch: 43 Batch: 2500
Training Loss: 0.01315322562456131
Epoch: 43 Batch: 2550
Training Loss: 0.01284623593676324
Epoch: 43 Batch: 2600
Training Loss: 0.012224104862946731
Epoch: 43 Batch: 2650
Training Loss: 0.011757148110641623
Epoch: 43 Batch: 2700
Training Loss: 0.011794699022063503
Epoch: 43 Batch: 2750
Training Loss: 0.011619974060492082
Epoch: 43 Batch: 2800
Training Loss: 0.011855617154921803
Epoch: 43 Batch: 2850
Training Loss: 0.011214024853288082
Epoch: 43 Batch: 2900
Training Loss: 0.011343528463922697
Epoch: 43 Batch: 2950
Training Loss: 0.011057807287927402
Epoch: 43 Batch: 3000
Training Loss: 0.010715390215317408
Epoch: 43 Batch: 3050
Training Loss: 0.011140888665543228
Epoch: 43 Batch: 3100
Training Loss: 0.010357130592869175
Epoch: 43 Batch: 3150
Training Loss: 0.010622527239814637
Epoch: 43 Batch: 3200
Training Loss: 0.010018163621425628
Epoch: 44 
 Validation Loss: 0.49721341795391505
---------------------------
Epoch: 44 Batch: 50
Training Loss: 0.6283959549665451
Epoch: 44 Batch: 100
Training Loss: 0.3176166048645973
Epoch: 44 Batch: 150
Training Loss: 0.21405441502730052
Epoch: 44 Batch: 200
Training Loss: 0.15921568483114243
Epoch: 44 Batch: 250
Training Loss: 0.1262687985897064
Epoch: 44 Batch: 300
Training Loss: 0.11025624146064122
Epoch: 44 Batch: 350
Training Loss: 0.0907766865832465
Epoch: 44 Batch: 400
Training Loss: 0.0783976037055254
Epoch: 44 Batch: 450
Training Loss: 0.07334552453623877
Epoch: 44 Batch: 500
Training Loss: 0.06642924547195435
Epoch: 44 Batch: 550
Training Loss: 0.05716059256683696
Epoch: 44 Batch: 600
Training Loss: 0.05323492124676704
Epoch: 44 Batch: 650
Training Loss: 0.05001371952203604
Epoch: 44 Batch: 700
Training Loss: 0.048430541966642655
Epoch: 44 Batch: 750
Training Loss: 0.041866440018018085
Epoch: 44 Batch: 800
Training Loss: 0.03987189631909132
Epoch: 44 Batch: 850
Training Loss: 0.03755511073505177
Epoch: 44 Batch: 900
Training Loss: 0.03726810220215056
Epoch: 44 Batch: 950
Training Loss: 0.033373690184793976
Epoch: 44 Batch: 1000
Training Loss: 0.031012635082006454
Epoch: 44 Batch: 1050
Training Loss: 0.030985832356271288
Epoch: 44 Batch: 1100
Training Loss: 0.029742319583892823
Epoch: 44 Batch: 1150
Training Loss: 0.02737573960553045
Epoch: 44 Batch: 1200
Training Loss: 0.025764252220590908
Epoch: 44 Batch: 1250
Training Loss: 0.0259794447183609
Epoch: 44 Batch: 1300
Training Loss: 0.024309162130722633
Epoch: 44 Batch: 1350
Training Loss: 0.02471922159194946
Epoch: 44 Batch: 1400
Training Loss: 0.02438165009021759
Epoch: 44 Batch: 1450
Training Loss: 0.02190019428730011
Epoch: 44 Batch: 1500
Training Loss: 0.023533046901226044
Epoch: 44 Batch: 1550
Training Loss: 0.021052526197125835
Epoch: 44 Batch: 1600
Training Loss: 0.02042064145207405
Epoch: 44 Batch: 1650
Training Loss: 0.01958278339920622
Epoch: 44 Batch: 1700
Training Loss: 0.018749591410160065
Epoch: 44 Batch: 1750
Training Loss: 0.017754944375583104
Epoch: 44 Batch: 1800
Training Loss: 0.018568364994393453
Epoch: 44 Batch: 1850
Training Loss: 0.016748491300118937
Epoch: 44 Batch: 1900
Training Loss: 0.015976382039095224
Epoch: 44 Batch: 1950
Training Loss: 0.017037556385382627
Epoch: 44 Batch: 2000
Training Loss: 0.015535252690315247
Epoch: 44 Batch: 2050
Training Loss: 0.015655384965059235
Epoch: 44 Batch: 2100
Training Loss: 0.014847010374069214
Epoch: 44 Batch: 2150
Training Loss: 0.0150868103531904
Epoch: 44 Batch: 2200
Training Loss: 0.01428025711666454
Epoch: 44 Batch: 2250
Training Loss: 0.014513959248860677
Epoch: 44 Batch: 2300
Training Loss: 0.013998165946939718
Epoch: 44 Batch: 2350
Training Loss: 0.013631042533732474
Epoch: 44 Batch: 2400
Training Loss: 0.01284180031468471
Epoch: 44 Batch: 2450
Training Loss: 0.012807783229010446
Epoch: 44 Batch: 2500
Training Loss: 0.012894194900989532
Epoch: 44 Batch: 2550
Training Loss: 0.012876678845461677
Epoch: 44 Batch: 2600
Training Loss: 0.012576076606145271
Epoch: 44 Batch: 2650
Training Loss: 0.012095414951162519
Epoch: 44 Batch: 2700
Training Loss: 0.011481857774434267
Epoch: 44 Batch: 2750
Training Loss: 0.011703477675264532
Epoch: 44 Batch: 2800
Training Loss: 0.011523930707148144
Epoch: 44 Batch: 2850
Training Loss: 0.011741518420085572
Epoch: 44 Batch: 2900
Training Loss: 0.011251819072098568
Epoch: 44 Batch: 2950
Training Loss: 0.010954998711408195
Epoch: 44 Batch: 3000
Training Loss: 0.010959985971450805
Epoch: 44 Batch: 3050
Training Loss: 0.010204647515640885
Epoch: 44 Batch: 3100
Training Loss: 0.010293977539385518
Epoch: 44 Batch: 3150
Training Loss: 0.010425258354535179
Epoch: 44 Batch: 3200
Training Loss: 0.010062870904803276
Epoch: 45 
 Validation Loss: 0.4963539504342609
---------------------------
Epoch: 45 Batch: 50
Training Loss: 0.6515128403902054
Epoch: 45 Batch: 100
Training Loss: 0.33006318151950836
Epoch: 45 Batch: 150
Training Loss: 0.20622501055399578
Epoch: 45 Batch: 200
Training Loss: 0.16327195569872857
Epoch: 45 Batch: 250
Training Loss: 0.13139200520515443
Epoch: 45 Batch: 300
Training Loss: 0.11103841831286748
Epoch: 45 Batch: 350
Training Loss: 0.09165099910327366
Epoch: 45 Batch: 400
Training Loss: 0.08057166308164597
Epoch: 45 Batch: 450
Training Loss: 0.07139150745338864
Epoch: 45 Batch: 500
Training Loss: 0.0654711052775383
Epoch: 45 Batch: 550
Training Loss: 0.05900343699888749
Epoch: 45 Batch: 600
Training Loss: 0.052731625785430274
Epoch: 45 Batch: 650
Training Loss: 0.04834608114682711
Epoch: 45 Batch: 700
Training Loss: 0.04479636234896524
Epoch: 45 Batch: 750
Training Loss: 0.041600056767463686
Epoch: 45 Batch: 800
Training Loss: 0.04178266856819391
Epoch: 45 Batch: 850
Training Loss: 0.03703860570402706
Epoch: 45 Batch: 900
Training Loss: 0.03604355255762736
Epoch: 45 Batch: 950
Training Loss: 0.0339328071945592
Epoch: 45 Batch: 1000
Training Loss: 0.030729410737752913
Epoch: 45 Batch: 1050
Training Loss: 0.030167423429943267
Epoch: 45 Batch: 1100
Training Loss: 0.02943713377822529
Epoch: 45 Batch: 1150
Training Loss: 0.0276390099266301
Epoch: 45 Batch: 1200
Training Loss: 0.02712898882726828
Epoch: 45 Batch: 1250
Training Loss: 0.025118763613700867
Epoch: 45 Batch: 1300
Training Loss: 0.02481998574275237
Epoch: 45 Batch: 1350
Training Loss: 0.023727206698170415
Epoch: 45 Batch: 1400
Training Loss: 0.022512464097567967
Epoch: 45 Batch: 1450
Training Loss: 0.022789084726366503
Epoch: 45 Batch: 1500
Training Loss: 0.021626207331816357
Epoch: 45 Batch: 1550
Training Loss: 0.02161941320665421
Epoch: 45 Batch: 1600
Training Loss: 0.020124364756047727
Epoch: 45 Batch: 1650
Training Loss: 0.019488931496938068
Epoch: 45 Batch: 1700
Training Loss: 0.018504803952048808
Epoch: 45 Batch: 1750
Training Loss: 0.018083620599337987
Epoch: 45 Batch: 1800
Training Loss: 0.017840980854299332
Epoch: 45 Batch: 1850
Training Loss: 0.016451231837272644
Epoch: 45 Batch: 1900
Training Loss: 0.01727698493944971
Epoch: 45 Batch: 1950
Training Loss: 0.016022103551106576
Epoch: 45 Batch: 2000
Training Loss: 0.016393701285123824
Epoch: 45 Batch: 2050
Training Loss: 0.015747606768840697
Epoch: 45 Batch: 2100
Training Loss: 0.015554634857745398
Epoch: 45 Batch: 2150
Training Loss: 0.015259348819422168
Epoch: 45 Batch: 2200
Training Loss: 0.014352153620936654
Epoch: 45 Batch: 2250
Training Loss: 0.014617803745799594
Epoch: 45 Batch: 2300
Training Loss: 0.013959077402301456
Epoch: 45 Batch: 2350
Training Loss: 0.01358607696725967
Epoch: 45 Batch: 2400
Training Loss: 0.013767452066143354
Epoch: 45 Batch: 2450
Training Loss: 0.012703343252746427
Epoch: 45 Batch: 2500
Training Loss: 0.012687493312358857
Epoch: 45 Batch: 2550
Training Loss: 0.012355354463352876
Epoch: 45 Batch: 2600
Training Loss: 0.011975034784812193
Epoch: 45 Batch: 2650
Training Loss: 0.011981602108703469
Epoch: 45 Batch: 2700
Training Loss: 0.012337228170147648
Epoch: 45 Batch: 2750
Training Loss: 0.01163494715907357
Epoch: 45 Batch: 2800
Training Loss: 0.011727931552699635
Epoch: 45 Batch: 2850
Training Loss: 0.011193696187253584
Epoch: 45 Batch: 2900
Training Loss: 0.0112224598485848
Epoch: 45 Batch: 2950
Training Loss: 0.010697420896109889
Epoch: 45 Batch: 3000
Training Loss: 0.010835709820191065
Epoch: 45 Batch: 3050
Training Loss: 0.010234123702909126
Epoch: 45 Batch: 3100
Training Loss: 0.010675749999861563
Epoch: 45 Batch: 3150
Training Loss: 0.010438167405506921
Epoch: 45 Batch: 3200
Training Loss: 0.009867515545338393
Epoch: 46 
 Validation Loss: 0.49586064252588485
---------------------------
Epoch: 46 Batch: 50
Training Loss: 0.6556781631708145
Epoch: 46 Batch: 100
Training Loss: 0.32452662199735643
Epoch: 46 Batch: 150
Training Loss: 0.20865129550298056
Epoch: 46 Batch: 200
Training Loss: 0.16143883451819419
Epoch: 46 Batch: 250
Training Loss: 0.12510352158546448
Epoch: 46 Batch: 300
Training Loss: 0.10799189219872157
Epoch: 46 Batch: 350
Training Loss: 0.08901631883212498
Epoch: 46 Batch: 400
Training Loss: 0.07826723679900169
Epoch: 46 Batch: 450
Training Loss: 0.06827344589763218
Epoch: 46 Batch: 500
Training Loss: 0.06420741629600525
Epoch: 46 Batch: 550
Training Loss: 0.05741946063258431
Epoch: 46 Batch: 600
Training Loss: 0.05752740730841954
Epoch: 46 Batch: 650
Training Loss: 0.04803045185712668
Epoch: 46 Batch: 700
Training Loss: 0.04524186491966248
Epoch: 46 Batch: 750
Training Loss: 0.04124575467904409
Epoch: 46 Batch: 800
Training Loss: 0.041653691828250884
Epoch: 46 Batch: 850
Training Loss: 0.03691662879551158
Epoch: 46 Batch: 900
Training Loss: 0.034583600097232395
Epoch: 46 Batch: 950
Training Loss: 0.033118927478790285
Epoch: 46 Batch: 1000
Training Loss: 0.03212760716676712
Epoch: 46 Batch: 1050
Training Loss: 0.0291986022960572
Epoch: 46 Batch: 1100
Training Loss: 0.030407442125407132
Epoch: 46 Batch: 1150
Training Loss: 0.02783481890740602
Epoch: 46 Batch: 1200
Training Loss: 0.026385860939820607
Epoch: 46 Batch: 1250
Training Loss: 0.024641239404678345
Epoch: 46 Batch: 1300
Training Loss: 0.023941018925263333
Epoch: 46 Batch: 1350
Training Loss: 0.024491212213480915
Epoch: 46 Batch: 1400
Training Loss: 0.022556254587003163
Epoch: 46 Batch: 1450
Training Loss: 0.02220620938416185
Epoch: 46 Batch: 1500
Training Loss: 0.021920104106267293
Epoch: 46 Batch: 1550
Training Loss: 0.02050732485709652
Epoch: 46 Batch: 1600
Training Loss: 0.018519993238151074
Epoch: 46 Batch: 1650
Training Loss: 0.019628735961336078
Epoch: 46 Batch: 1700
Training Loss: 0.019398426308351403
Epoch: 46 Batch: 1750
Training Loss: 0.018413677079336985
Epoch: 46 Batch: 1800
Training Loss: 0.01824028627740012
Epoch: 46 Batch: 1850
Training Loss: 0.016994401152069506
Epoch: 46 Batch: 1900
Training Loss: 0.01743562770517249
Epoch: 46 Batch: 1950
Training Loss: 0.01669506915104695
Epoch: 46 Batch: 2000
Training Loss: 0.015875644862651826
Epoch: 46 Batch: 2050
Training Loss: 0.01546258906038796
Epoch: 46 Batch: 2100
Training Loss: 0.016208365162213644
Epoch: 46 Batch: 2150
Training Loss: 0.014308108673539273
Epoch: 46 Batch: 2200
Training Loss: 0.014606975994326851
Epoch: 46 Batch: 2250
Training Loss: 0.013944087915950351
Epoch: 46 Batch: 2300
Training Loss: 0.014691374522188435
Epoch: 46 Batch: 2350
Training Loss: 0.013577433127038022
Epoch: 46 Batch: 2400
Training Loss: 0.013991493607560794
Epoch: 46 Batch: 2450
Training Loss: 0.013096852497178681
Epoch: 46 Batch: 2500
Training Loss: 0.012163373482227326
Epoch: 46 Batch: 2550
Training Loss: 0.01265477340595395
Epoch: 46 Batch: 2600
Training Loss: 0.01255653297671905
Epoch: 46 Batch: 2650
Training Loss: 0.012559481499330052
Epoch: 46 Batch: 2700
Training Loss: 0.012146071626080408
Epoch: 46 Batch: 2750
Training Loss: 0.012190199212594465
Epoch: 46 Batch: 2800
Training Loss: 0.011614592224359512
Epoch: 46 Batch: 2850
Training Loss: 0.011293546003207826
Epoch: 46 Batch: 2900
Training Loss: 0.011037463967142435
Epoch: 46 Batch: 2950
Training Loss: 0.01053801823470552
Epoch: 46 Batch: 3000
Training Loss: 0.01063239378730456
Epoch: 46 Batch: 3050
Training Loss: 0.010846948789768532
Epoch: 46 Batch: 3100
Training Loss: 0.010047619352417608
Epoch: 46 Batch: 3150
Training Loss: 0.010388693166157556
Epoch: 46 Batch: 3200
Training Loss: 0.01014323951676488
Epoch: 47 
 Validation Loss: 0.495324797100491
---------------------------
Epoch: 47 Batch: 50
Training Loss: 0.6726053410768509
Epoch: 47 Batch: 100
Training Loss: 0.31192031264305115
Epoch: 47 Batch: 150
Training Loss: 0.2093977032105128
Epoch: 47 Batch: 200
Training Loss: 0.16568951189517975
Epoch: 47 Batch: 250
Training Loss: 0.13133485066890715
Epoch: 47 Batch: 300
Training Loss: 0.10482787599166234
Epoch: 47 Batch: 350
Training Loss: 0.09371191433497837
Epoch: 47 Batch: 400
Training Loss: 0.07992824159562588
Epoch: 47 Batch: 450
Training Loss: 0.07040617280536228
Epoch: 47 Batch: 500
Training Loss: 0.06388992738723755
Epoch: 47 Batch: 550
Training Loss: 0.05724396732720462
Epoch: 47 Batch: 600
Training Loss: 0.05499881267547607
Epoch: 47 Batch: 650
Training Loss: 0.04924912127164694
Epoch: 47 Batch: 700
Training Loss: 0.046658562847546166
Epoch: 47 Batch: 750
Training Loss: 0.04141010789076487
Epoch: 47 Batch: 800
Training Loss: 0.03941961277276278
Epoch: 47 Batch: 850
Training Loss: 0.03714943272225997
Epoch: 47 Batch: 900
Training Loss: 0.034935442242357465
Epoch: 47 Batch: 950
Training Loss: 0.034684920060007195
Epoch: 47 Batch: 1000
Training Loss: 0.03244151759147644
Epoch: 47 Batch: 1050
Training Loss: 0.0319764727921713
Epoch: 47 Batch: 1100
Training Loss: 0.028748582276430998
Epoch: 47 Batch: 1150
Training Loss: 0.028132667515588843
Epoch: 47 Batch: 1200
Training Loss: 0.026935865506529807
Epoch: 47 Batch: 1250
Training Loss: 0.025272434782981874
Epoch: 47 Batch: 1300
Training Loss: 0.02439125038110293
Epoch: 47 Batch: 1350
Training Loss: 0.02373760199105298
Epoch: 47 Batch: 1400
Training Loss: 0.02251766909446035
Epoch: 47 Batch: 1450
Training Loss: 0.021644633515127774
Epoch: 47 Batch: 1500
Training Loss: 0.020273893912633262
Epoch: 47 Batch: 1550
Training Loss: 0.020981711456852573
Epoch: 47 Batch: 1600
Training Loss: 0.020322445034980773
Epoch: 47 Batch: 1650
Training Loss: 0.019524058916352012
Epoch: 47 Batch: 1700
Training Loss: 0.019025127274148606
Epoch: 47 Batch: 1750
Training Loss: 0.018057231766836983
Epoch: 47 Batch: 1800
Training Loss: 0.018384317010641096
Epoch: 47 Batch: 1850
Training Loss: 0.017123299273284705
Epoch: 47 Batch: 1900
Training Loss: 0.016904472740072955
Epoch: 47 Batch: 1950
Training Loss: 0.015977632067142387
Epoch: 47 Batch: 2000
Training Loss: 0.01560012586414814
Epoch: 47 Batch: 2050
Training Loss: 0.01574168250328157
Epoch: 47 Batch: 2100
Training Loss: 0.015823406406811305
Epoch: 47 Batch: 2150
Training Loss: 0.014606222233106923
Epoch: 47 Batch: 2200
Training Loss: 0.014368973956866697
Epoch: 47 Batch: 2250
Training Loss: 0.01475169743431939
Epoch: 47 Batch: 2300
Training Loss: 0.013892152257587598
Epoch: 47 Batch: 2350
Training Loss: 0.013560401607067027
Epoch: 47 Batch: 2400
Training Loss: 0.014114913977682591
Epoch: 47 Batch: 2450
Training Loss: 0.01355116704288794
Epoch: 47 Batch: 2500
Training Loss: 0.013394145321846008
Epoch: 47 Batch: 2550
Training Loss: 0.012740437365045735
Epoch: 47 Batch: 2600
Training Loss: 0.012782712406837024
Epoch: 47 Batch: 2650
Training Loss: 0.012359781332735745
Epoch: 47 Batch: 2700
Training Loss: 0.01152506818373998
Epoch: 47 Batch: 2750
Training Loss: 0.011610872095281428
Epoch: 47 Batch: 2800
Training Loss: 0.0114149438057627
Epoch: 47 Batch: 2850
Training Loss: 0.01135492553836421
Epoch: 47 Batch: 2900
Training Loss: 0.010943742219744057
Epoch: 47 Batch: 2950
Training Loss: 0.011081195443363513
Epoch: 47 Batch: 3000
Training Loss: 0.010199402352174123
Epoch: 47 Batch: 3050
Training Loss: 0.010426528375656878
Epoch: 47 Batch: 3100
Training Loss: 0.010852072546558995
Epoch: 47 Batch: 3150
Training Loss: 0.01060436675472865
Epoch: 47 Batch: 3200
Training Loss: 0.00980949953198433
Epoch: 48 
 Validation Loss: 0.4948885374599033
---------------------------
Epoch: 48 Batch: 50
Training Loss: 0.6685861420631408
Epoch: 48 Batch: 100
Training Loss: 0.3237812626361847
Epoch: 48 Batch: 150
Training Loss: 0.20982633928457897
Epoch: 48 Batch: 200
Training Loss: 0.1683520983159542
Epoch: 48 Batch: 250
Training Loss: 0.13257493829727174
Epoch: 48 Batch: 300
Training Loss: 0.11078173677126567
Epoch: 48 Batch: 350
Training Loss: 0.09103047047342573
Epoch: 48 Batch: 400
Training Loss: 0.07761771284043789
Epoch: 48 Batch: 450
Training Loss: 0.06936823646227519
Epoch: 48 Batch: 500
Training Loss: 0.06483727508783341
Epoch: 48 Batch: 550
Training Loss: 0.05896194701844996
Epoch: 48 Batch: 600
Training Loss: 0.05423840835690499
Epoch: 48 Batch: 650
Training Loss: 0.04938836253606356
Epoch: 48 Batch: 700
Training Loss: 0.04316428601741791
Epoch: 48 Batch: 750
Training Loss: 0.04238104768594106
Epoch: 48 Batch: 800
Training Loss: 0.04148354556411505
Epoch: 48 Batch: 850
Training Loss: 0.03786708004334394
Epoch: 48 Batch: 900
Training Loss: 0.03439870278040568
Epoch: 48 Batch: 950
Training Loss: 0.03559980207367947
Epoch: 48 Batch: 1000
Training Loss: 0.03093874505162239
Epoch: 48 Batch: 1050
Training Loss: 0.030678309571175347
Epoch: 48 Batch: 1100
Training Loss: 0.03000799374146895
Epoch: 48 Batch: 1150
Training Loss: 0.027973222706628883
Epoch: 48 Batch: 1200
Training Loss: 0.026912244980533916
Epoch: 48 Batch: 1250
Training Loss: 0.02577152924537659
Epoch: 48 Batch: 1300
Training Loss: 0.025484861823228688
Epoch: 48 Batch: 1350
Training Loss: 0.023024298901911135
Epoch: 48 Batch: 1400
Training Loss: 0.02284946880170277
Epoch: 48 Batch: 1450
Training Loss: 0.021370977224974795
Epoch: 48 Batch: 1500
Training Loss: 0.020308665215969086
Epoch: 48 Batch: 1550
Training Loss: 0.020575493170369056
Epoch: 48 Batch: 1600
Training Loss: 0.02042452311143279
Epoch: 48 Batch: 1650
Training Loss: 0.019191696842511496
Epoch: 48 Batch: 1700
Training Loss: 0.018460326615501853
Epoch: 48 Batch: 1750
Training Loss: 0.01845646459715707
Epoch: 48 Batch: 1800
Training Loss: 0.018014413134919274
Epoch: 48 Batch: 1850
Training Loss: 0.017241512907517922
Epoch: 48 Batch: 1900
Training Loss: 0.016927817667785443
Epoch: 48 Batch: 1950
Training Loss: 0.016148068614495106
Epoch: 48 Batch: 2000
Training Loss: 0.015865906432271003
Epoch: 48 Batch: 2050
Training Loss: 0.015569730371963688
Epoch: 48 Batch: 2100
Training Loss: 0.016037597230502538
Epoch: 48 Batch: 2150
Training Loss: 0.015628456473350524
Epoch: 48 Batch: 2200
Training Loss: 0.014912478124553506
Epoch: 48 Batch: 2250
Training Loss: 0.014140161712964375
Epoch: 48 Batch: 2300
Training Loss: 0.014065090845460477
Epoch: 48 Batch: 2350
Training Loss: 0.013958727704717757
Epoch: 48 Batch: 2400
Training Loss: 0.013790513190130393
Epoch: 48 Batch: 2450
Training Loss: 0.012454834981840484
Epoch: 48 Batch: 2500
Training Loss: 0.01306643625497818
Epoch: 48 Batch: 2550
Training Loss: 0.013552748572592642
Epoch: 48 Batch: 2600
Training Loss: 0.01218312368943141
Epoch: 48 Batch: 2650
Training Loss: 0.011849066109027502
Epoch: 48 Batch: 2700
Training Loss: 0.011493436098098754
Epoch: 48 Batch: 2750
Training Loss: 0.011441362207586115
Epoch: 48 Batch: 2800
Training Loss: 0.011446375804288046
Epoch: 48 Batch: 2850
Training Loss: 0.011510316562234309
Epoch: 48 Batch: 2900
Training Loss: 0.011334215382049824
Epoch: 48 Batch: 2950
Training Loss: 0.010975643992424011
Epoch: 48 Batch: 3000
Training Loss: 0.01047209323445956
Epoch: 48 Batch: 3050
Training Loss: 0.010327906491326503
Epoch: 48 Batch: 3100
Training Loss: 0.010302465961825463
Epoch: 48 Batch: 3150
Training Loss: 0.010110715637131343
Epoch: 48 Batch: 3200
Training Loss: 0.009842356611043214
Epoch: 49 
 Validation Loss: 0.4939978513452742
---------------------------
Epoch: 49 Batch: 50
Training Loss: 0.6745555609464645
Epoch: 49 Batch: 100
Training Loss: 0.32428277105093
Epoch: 49 Batch: 150
Training Loss: 0.21726496915022533
Epoch: 49 Batch: 200
Training Loss: 0.15588475912809371
Epoch: 49 Batch: 250
Training Loss: 0.128523588180542
Epoch: 49 Batch: 300
Training Loss: 0.10555903782447179
Epoch: 49 Batch: 350
Training Loss: 0.09417168157441276
Epoch: 49 Batch: 400
Training Loss: 0.08024549573659896
Epoch: 49 Batch: 450
Training Loss: 0.07202862289216783
Epoch: 49 Batch: 500
Training Loss: 0.06226273357868194
Epoch: 49 Batch: 550
Training Loss: 0.0576408702135086
Epoch: 49 Batch: 600
Training Loss: 0.05392618636290232
Epoch: 49 Batch: 650
Training Loss: 0.050781873372884895
Epoch: 49 Batch: 700
Training Loss: 0.04571598985365459
Epoch: 49 Batch: 750
Training Loss: 0.04347069025039673
Epoch: 49 Batch: 800
Training Loss: 0.03985293872654438
Epoch: 49 Batch: 850
Training Loss: 0.036687395467477686
Epoch: 49 Batch: 900
Training Loss: 0.034748145043849946
Epoch: 49 Batch: 950
Training Loss: 0.03315162850053687
Epoch: 49 Batch: 1000
Training Loss: 0.03208666875958443
Epoch: 49 Batch: 1050
Training Loss: 0.030601365992001126
Epoch: 49 Batch: 1100
Training Loss: 0.03034679903225465
Epoch: 49 Batch: 1150
Training Loss: 0.02810495086338209
Epoch: 49 Batch: 1200
Training Loss: 0.027318798998991648
Epoch: 49 Batch: 1250
Training Loss: 0.02605380504131317
Epoch: 49 Batch: 1300
Training Loss: 0.024606726009112137
Epoch: 49 Batch: 1350
Training Loss: 0.023098034108126605
Epoch: 49 Batch: 1400
Training Loss: 0.022651798597403936
Epoch: 49 Batch: 1450
Training Loss: 0.021293514843644768
Epoch: 49 Batch: 1500
Training Loss: 0.021806795040766398
Epoch: 49 Batch: 1550
Training Loss: 0.020436282100216033
Epoch: 49 Batch: 1600
Training Loss: 0.0211432003416121
Epoch: 49 Batch: 1650
Training Loss: 0.019638528101371996
Epoch: 49 Batch: 1700
Training Loss: 0.018604083517018487
Epoch: 49 Batch: 1750
Training Loss: 0.017663544058799743
Epoch: 49 Batch: 1800
Training Loss: 0.018342061622275245
Epoch: 49 Batch: 1850
Training Loss: 0.017744424407546585
Epoch: 49 Batch: 1900
Training Loss: 0.017914194747021325
Epoch: 49 Batch: 1950
Training Loss: 0.016639892657597858
Epoch: 49 Batch: 2000
Training Loss: 0.015660281345248224
Epoch: 49 Batch: 2050
Training Loss: 0.015852477027148735
Epoch: 49 Batch: 2100
Training Loss: 0.015099840320291973
Epoch: 49 Batch: 2150
Training Loss: 0.015296961651291957
Epoch: 49 Batch: 2200
Training Loss: 0.015176306678490206
Epoch: 49 Batch: 2250
Training Loss: 0.014684562895033094
Epoch: 49 Batch: 2300
Training Loss: 0.013848068986250007
Epoch: 49 Batch: 2350
Training Loss: 0.01347650852609188
Epoch: 49 Batch: 2400
Training Loss: 0.013279324173927307
Epoch: 49 Batch: 2450
Training Loss: 0.0130354463445897
Epoch: 49 Batch: 2500
Training Loss: 0.012699887573719025
Epoch: 49 Batch: 2550
Training Loss: 0.012860812392889285
Epoch: 49 Batch: 2600
Training Loss: 0.011949599603047738
Epoch: 49 Batch: 2650
Training Loss: 0.012592762652433143
Epoch: 49 Batch: 2700
Training Loss: 0.012340815906171445
Epoch: 49 Batch: 2750
Training Loss: 0.011680648402734237
Epoch: 49 Batch: 2800
Training Loss: 0.011758664261017527
Epoch: 49 Batch: 2850
Training Loss: 0.01121821593820003
Epoch: 49 Batch: 2900
Training Loss: 0.011322133952173694
Epoch: 49 Batch: 2950
Training Loss: 0.0107066299753674
Epoch: 49 Batch: 3000
Training Loss: 0.010832218954960506
Epoch: 49 Batch: 3050
Training Loss: 0.010616830440818287
Epoch: 49 Batch: 3100
Training Loss: 0.010670067168051196
Epoch: 49 Batch: 3150
Training Loss: 0.009969080440581791
Epoch: 49 Batch: 3200
Training Loss: 0.009929219968616962
Epoch: 50 
 Validation Loss: 0.4935172196891573
---------------------------
Epoch: 50 Batch: 50
Training Loss: 0.642034434080124
Epoch: 50 Batch: 100
Training Loss: 0.3202850568294525
Epoch: 50 Batch: 150
Training Loss: 0.21504082481066386
Epoch: 50 Batch: 200
Training Loss: 0.16337139815092086
Epoch: 50 Batch: 250
Training Loss: 0.13281636106967926
Epoch: 50 Batch: 300
Training Loss: 0.10959317872921626
Epoch: 50 Batch: 350
Training Loss: 0.09110087301049914
Epoch: 50 Batch: 400
Training Loss: 0.08681641690433026
Epoch: 50 Batch: 450
Training Loss: 0.06837214648723602
Epoch: 50 Batch: 500
Training Loss: 0.06424588829278946
Epoch: 50 Batch: 550
Training Loss: 0.058140731182965366
Epoch: 50 Batch: 600
Training Loss: 0.05381582612792651
Epoch: 50 Batch: 650
Training Loss: 0.05238938716741708
Epoch: 50 Batch: 700
Training Loss: 0.04611637660435268
Epoch: 50 Batch: 750
Training Loss: 0.04347395284970602
Epoch: 50 Batch: 800
Training Loss: 0.03892452895641327
Epoch: 50 Batch: 850
Training Loss: 0.03653561606126673
Epoch: 50 Batch: 900
Training Loss: 0.036185910205046336
Epoch: 50 Batch: 950
Training Loss: 0.03338395068519994
Epoch: 50 Batch: 1000
Training Loss: 0.032453694820404055
Epoch: 50 Batch: 1050
Training Loss: 0.029896350644883656
Epoch: 50 Batch: 1100
Training Loss: 0.029744270335544238
Epoch: 50 Batch: 1150
Training Loss: 0.025761602158131808
Epoch: 50 Batch: 1200
Training Loss: 0.027052897612253823
Epoch: 50 Batch: 1250
Training Loss: 0.024998645091056825
Epoch: 50 Batch: 1300
Training Loss: 0.025487563541302313
Epoch: 50 Batch: 1350
Training Loss: 0.024560156045136627
Epoch: 50 Batch: 1400
Training Loss: 0.02391916302697999
Epoch: 50 Batch: 1450
Training Loss: 0.022519577536089668
Epoch: 50 Batch: 1500
Training Loss: 0.022245987872282665
Epoch: 50 Batch: 1550
Training Loss: 0.020323311724970417
Epoch: 50 Batch: 1600
Training Loss: 0.019535960126668216
Epoch: 50 Batch: 1650
Training Loss: 0.01935994478789243
Epoch: 50 Batch: 1700
Training Loss: 0.018819141791147343
Epoch: 50 Batch: 1750
Training Loss: 0.018454327736582076
Epoch: 50 Batch: 1800
Training Loss: 0.018394147836499744
Epoch: 50 Batch: 1850
Training Loss: 0.01742885074099979
Epoch: 50 Batch: 1900
Training Loss: 0.016166390431554694
Epoch: 50 Batch: 1950
Training Loss: 0.01611349208232684
Epoch: 50 Batch: 2000
Training Loss: 0.016441725850105286
Epoch: 50 Batch: 2050
Training Loss: 0.015065997853511717
Epoch: 50 Batch: 2100
Training Loss: 0.015943235684008825
Epoch: 50 Batch: 2150
Training Loss: 0.015127068952072498
Epoch: 50 Batch: 2200
Training Loss: 0.014643765200268138
Epoch: 50 Batch: 2250
Training Loss: 0.014536255015267266
Epoch: 50 Batch: 2300
Training Loss: 0.012977609815804855
Epoch: 50 Batch: 2350
Training Loss: 0.013532561025720962
Epoch: 50 Batch: 2400
Training Loss: 0.01329256227860848
Epoch: 50 Batch: 2450
Training Loss: 0.012375596214313896
Epoch: 50 Batch: 2500
Training Loss: 0.012805083644390106
Epoch: 50 Batch: 2550
Training Loss: 0.011798835233146069
Epoch: 50 Batch: 2600
Training Loss: 0.01212247908115387
Epoch: 50 Batch: 2650
Training Loss: 0.011965571327029535
Epoch: 50 Batch: 2700
Training Loss: 0.012538514048964888
Epoch: 50 Batch: 2750
Training Loss: 0.011505389560352672
Epoch: 50 Batch: 2800
Training Loss: 0.011586350873112678
Epoch: 50 Batch: 2850
Training Loss: 0.011363539319289358
Epoch: 50 Batch: 2900
Training Loss: 0.011175309255205351
Epoch: 50 Batch: 2950
Training Loss: 0.010768269330768262
Epoch: 50 Batch: 3000
Training Loss: 0.010559893935918808
Epoch: 50 Batch: 3050
Training Loss: 0.010770154820113885
Epoch: 50 Batch: 3100
Training Loss: 0.010290168937175504
Epoch: 50 Batch: 3150
Training Loss: 0.010235954721768697
Epoch: 50 Batch: 3200
Training Loss: 0.01000573143362999
Epoch: 51 
 Validation Loss: 0.49313629501395756
---------------------------
Epoch: 51 Batch: 50
Training Loss: 0.6779649567604065
Epoch: 51 Batch: 100
Training Loss: 0.299535827934742
Epoch: 51 Batch: 150
Training Loss: 0.21205437044302622
Epoch: 51 Batch: 200
Training Loss: 0.15575620979070665
Epoch: 51 Batch: 250
Training Loss: 0.12853141403198243
Epoch: 51 Batch: 300
Training Loss: 0.10400927513837814
Epoch: 51 Batch: 350
Training Loss: 0.09177230715751648
Epoch: 51 Batch: 400
Training Loss: 0.08147189855575561
Epoch: 51 Batch: 450
Training Loss: 0.07203580088085598
Epoch: 51 Batch: 500
Training Loss: 0.0644087181687355
Epoch: 51 Batch: 550
Training Loss: 0.055256676836447284
Epoch: 51 Batch: 600
Training Loss: 0.05386675834655762
Epoch: 51 Batch: 650
Training Loss: 0.04903062389447139
Epoch: 51 Batch: 700
Training Loss: 0.04597665254558836
Epoch: 51 Batch: 750
Training Loss: 0.04268489591280619
Epoch: 51 Batch: 800
Training Loss: 0.03870688434690237
Epoch: 51 Batch: 850
Training Loss: 0.03864899803610409
Epoch: 51 Batch: 900
Training Loss: 0.03686917742093404
Epoch: 51 Batch: 950
Training Loss: 0.03209945415195666
Epoch: 51 Batch: 1000
Training Loss: 0.030539046376943588
Epoch: 51 Batch: 1050
Training Loss: 0.03001150403703962
Epoch: 51 Batch: 1100
Training Loss: 0.029503058086742053
Epoch: 51 Batch: 1150
Training Loss: 0.02800861524498981
Epoch: 51 Batch: 1200
Training Loss: 0.027257130220532416
Epoch: 51 Batch: 1250
Training Loss: 0.02503232264518738
Epoch: 51 Batch: 1300
Training Loss: 0.02451752781867981
Epoch: 51 Batch: 1350
Training Loss: 0.023341243752726802
Epoch: 51 Batch: 1400
Training Loss: 0.022740470660584315
Epoch: 51 Batch: 1450
Training Loss: 0.022319660967793958
Epoch: 51 Batch: 1500
Training Loss: 0.021527991672356923
Epoch: 51 Batch: 1550
Training Loss: 0.02041145134356714
Epoch: 51 Batch: 1600
Training Loss: 0.020136222671717406
Epoch: 51 Batch: 1650
Training Loss: 0.019661990422191043
Epoch: 51 Batch: 1700
Training Loss: 0.019076007341637332
Epoch: 51 Batch: 1750
Training Loss: 0.019225337164742607
Epoch: 51 Batch: 1800
Training Loss: 0.017840646058321
Epoch: 51 Batch: 1850
Training Loss: 0.01828544719799145
Epoch: 51 Batch: 1900
Training Loss: 0.01632056568798266
Epoch: 51 Batch: 1950
Training Loss: 0.01605150432158739
Epoch: 51 Batch: 2000
Training Loss: 0.015741320669651032
Epoch: 51 Batch: 2050
Training Loss: 0.015802355990177246
Epoch: 51 Batch: 2100
Training Loss: 0.014705656596592495
Epoch: 51 Batch: 2150
Training Loss: 0.014431833846624508
Epoch: 51 Batch: 2200
Training Loss: 0.014230773868885908
Epoch: 51 Batch: 2250
Training Loss: 0.01434667052163018
Epoch: 51 Batch: 2300
Training Loss: 0.013650140736414039
Epoch: 51 Batch: 2350
Training Loss: 0.013391360488343745
Epoch: 51 Batch: 2400
Training Loss: 0.01372188288718462
Epoch: 51 Batch: 2450
Training Loss: 0.013199319450222716
Epoch: 51 Batch: 2500
Training Loss: 0.012879835164546967
Epoch: 51 Batch: 2550
Training Loss: 0.012641540239838992
Epoch: 51 Batch: 2600
Training Loss: 0.012720513871082893
Epoch: 51 Batch: 2650
Training Loss: 0.011933810519722273
Epoch: 51 Batch: 2700
Training Loss: 0.010898875395456949
Epoch: 51 Batch: 2750
Training Loss: 0.011523826490749012
Epoch: 51 Batch: 2800
Training Loss: 0.011921780247773442
Epoch: 51 Batch: 2850
Training Loss: 0.011207307033371507
Epoch: 51 Batch: 2900
Training Loss: 0.011411864223151371
Epoch: 51 Batch: 2950
Training Loss: 0.01102535181126352
Epoch: 51 Batch: 3000
Training Loss: 0.01070223593711853
Epoch: 51 Batch: 3050
Training Loss: 0.010500464185339505
Epoch: 51 Batch: 3100
Training Loss: 0.01019329402715929
Epoch: 51 Batch: 3150
Training Loss: 0.009886787524299017
Epoch: 51 Batch: 3200
Training Loss: 0.009974585939198732
Epoch: 52 
 Validation Loss: 0.492747007144822
---------------------------
Epoch: 52 Batch: 50
Training Loss: 0.6506126934289932
Epoch: 52 Batch: 100
Training Loss: 0.3125391274690628
Epoch: 52 Batch: 150
Training Loss: 0.21210034151871998
Epoch: 52 Batch: 200
Training Loss: 0.15404052868485452
Epoch: 52 Batch: 250
Training Loss: 0.12702222311496736
Epoch: 52 Batch: 300
Training Loss: 0.10692616035540899
Epoch: 52 Batch: 350
Training Loss: 0.09230712950229644
Epoch: 52 Batch: 400
Training Loss: 0.07853816196322441
Epoch: 52 Batch: 450
Training Loss: 0.07065226766798231
Epoch: 52 Batch: 500
Training Loss: 0.06383113414049149
Epoch: 52 Batch: 550
Training Loss: 0.05772911402312192
Epoch: 52 Batch: 600
Training Loss: 0.05453059166669846
Epoch: 52 Batch: 650
Training Loss: 0.04772027836396144
Epoch: 52 Batch: 700
Training Loss: 0.045358917202268326
Epoch: 52 Batch: 750
Training Loss: 0.042553991079330444
Epoch: 52 Batch: 800
Training Loss: 0.04050854507833719
Epoch: 52 Batch: 850
Training Loss: 0.03808008751448463
Epoch: 52 Batch: 900
Training Loss: 0.035725004242526164
Epoch: 52 Batch: 950
Training Loss: 0.03432319095260219
Epoch: 52 Batch: 1000
Training Loss: 0.031041618168354036
Epoch: 52 Batch: 1050
Training Loss: 0.030686203581946236
Epoch: 52 Batch: 1100
Training Loss: 0.028058054555546153
Epoch: 52 Batch: 1150
Training Loss: 0.02876235060069872
Epoch: 52 Batch: 1200
Training Loss: 0.027294966479142507
Epoch: 52 Batch: 1250
Training Loss: 0.025688366293907167
Epoch: 52 Batch: 1300
Training Loss: 0.023929782739052406
Epoch: 52 Batch: 1350
Training Loss: 0.023577815824084813
Epoch: 52 Batch: 1400
Training Loss: 0.021703458428382873
Epoch: 52 Batch: 1450
Training Loss: 0.02168712258338928
Epoch: 52 Batch: 1500
Training Loss: 0.022090936342875164
Epoch: 52 Batch: 1550
Training Loss: 0.02025342487519787
Epoch: 52 Batch: 1600
Training Loss: 0.019586297385394573
Epoch: 52 Batch: 1650
Training Loss: 0.018958020174142085
Epoch: 52 Batch: 1700
Training Loss: 0.017709150296800275
Epoch: 52 Batch: 1750
Training Loss: 0.01761755907535553
Epoch: 52 Batch: 1800
Training Loss: 0.017817632771200603
Epoch: 52 Batch: 1850
Training Loss: 0.017016513718141092
Epoch: 52 Batch: 1900
Training Loss: 0.016571701106272246
Epoch: 52 Batch: 1950
Training Loss: 0.01603479102635995
Epoch: 52 Batch: 2000
Training Loss: 0.01607854074239731
Epoch: 52 Batch: 2050
Training Loss: 0.015274665719125329
Epoch: 52 Batch: 2100
Training Loss: 0.015337198816594623
Epoch: 52 Batch: 2150
Training Loss: 0.014809671501780666
Epoch: 52 Batch: 2200
Training Loss: 0.014163763523101806
Epoch: 52 Batch: 2250
Training Loss: 0.013952904330359565
Epoch: 52 Batch: 2300
Training Loss: 0.013939519071060679
Epoch: 52 Batch: 2350
Training Loss: 0.014247737526893615
Epoch: 52 Batch: 2400
Training Loss: 0.012797714148958524
Epoch: 52 Batch: 2450
Training Loss: 0.013358811225209917
Epoch: 52 Batch: 2500
Training Loss: 0.013104909682273866
Epoch: 52 Batch: 2550
Training Loss: 0.013275251365175433
Epoch: 52 Batch: 2600
Training Loss: 0.011922984054455391
Epoch: 52 Batch: 2650
Training Loss: 0.012016464752971
Epoch: 52 Batch: 2700
Training Loss: 0.011805219959329675
Epoch: 52 Batch: 2750
Training Loss: 0.011879198735410518
Epoch: 52 Batch: 2800
Training Loss: 0.011227749777691705
Epoch: 52 Batch: 2850
Training Loss: 0.010940593857514232
Epoch: 52 Batch: 2900
Training Loss: 0.011070444923022697
Epoch: 52 Batch: 2950
Training Loss: 0.010991497716661226
Epoch: 52 Batch: 3000
Training Loss: 0.010657819201548895
Epoch: 52 Batch: 3050
Training Loss: 0.010139172956591747
Epoch: 52 Batch: 3100
Training Loss: 0.010462697244459583
Epoch: 52 Batch: 3150
Training Loss: 0.009898681139189099
Epoch: 52 Batch: 3200
Training Loss: 0.009873952846974135
Epoch: 53 
 Validation Loss: 0.4922505100568136
---------------------------
Epoch: 53 Batch: 50
Training Loss: 0.6554118937253952
Epoch: 53 Batch: 100
Training Loss: 0.31874433904886246
Epoch: 53 Batch: 150
Training Loss: 0.20602749168872833
Epoch: 53 Batch: 200
Training Loss: 0.1565598413348198
Epoch: 53 Batch: 250
Training Loss: 0.1287634003162384
Epoch: 53 Batch: 300
Training Loss: 0.10598327696323395
Epoch: 53 Batch: 350
Training Loss: 0.08979683901582446
Epoch: 53 Batch: 400
Training Loss: 0.08248404406011105
Epoch: 53 Batch: 450
Training Loss: 0.07076335377163358
Epoch: 53 Batch: 500
Training Loss: 0.06624411535263061
Epoch: 53 Batch: 550
Training Loss: 0.05692369851199063
Epoch: 53 Batch: 600
Training Loss: 0.05399863928556442
Epoch: 53 Batch: 650
Training Loss: 0.049655987391105064
Epoch: 53 Batch: 700
Training Loss: 0.043325256790433614
Epoch: 53 Batch: 750
Training Loss: 0.040744630932807925
Epoch: 53 Batch: 800
Training Loss: 0.03835561487823725
Epoch: 53 Batch: 850
Training Loss: 0.03691231951994055
Epoch: 53 Batch: 900
Training Loss: 0.034586580991745
Epoch: 53 Batch: 950
Training Loss: 0.03144030015719564
Epoch: 53 Batch: 1000
Training Loss: 0.03293886163830757
Epoch: 53 Batch: 1050
Training Loss: 0.031514947300865535
Epoch: 53 Batch: 1100
Training Loss: 0.028178656968203458
Epoch: 53 Batch: 1150
Training Loss: 0.027390749558158543
Epoch: 53 Batch: 1200
Training Loss: 0.026215168287356694
Epoch: 53 Batch: 1250
Training Loss: 0.026951854038238524
Epoch: 53 Batch: 1300
Training Loss: 0.024876862810208247
Epoch: 53 Batch: 1350
Training Loss: 0.02501572456624773
Epoch: 53 Batch: 1400
Training Loss: 0.022591479761259897
Epoch: 53 Batch: 1450
Training Loss: 0.022227135238976313
Epoch: 53 Batch: 1500
Training Loss: 0.021291358629862468
Epoch: 53 Batch: 1550
Training Loss: 0.021056348469949537
Epoch: 53 Batch: 1600
Training Loss: 0.019118720665574074
Epoch: 53 Batch: 1650
Training Loss: 0.020091836687290306
Epoch: 53 Batch: 1700
Training Loss: 0.01951348141712301
Epoch: 53 Batch: 1750
Training Loss: 0.018755821347236634
Epoch: 53 Batch: 1800
Training Loss: 0.018067573954661686
Epoch: 53 Batch: 1850
Training Loss: 0.017614558902946678
Epoch: 53 Batch: 1900
Training Loss: 0.01693230012529775
Epoch: 53 Batch: 1950
Training Loss: 0.016244384096218988
Epoch: 53 Batch: 2000
Training Loss: 0.016047624483704566
Epoch: 53 Batch: 2050
Training Loss: 0.01514463466842
Epoch: 53 Batch: 2100
Training Loss: 0.01518772773799442
Epoch: 53 Batch: 2150
Training Loss: 0.014667478702789128
Epoch: 53 Batch: 2200
Training Loss: 0.014903123419393193
Epoch: 53 Batch: 2250
Training Loss: 0.013837014555931091
Epoch: 53 Batch: 2300
Training Loss: 0.013208489819713261
Epoch: 53 Batch: 2350
Training Loss: 0.012928042399122359
Epoch: 53 Batch: 2400
Training Loss: 0.01331582546234131
Epoch: 53 Batch: 2450
Training Loss: 0.01274460426398686
Epoch: 53 Batch: 2500
Training Loss: 0.012858273100852967
Epoch: 53 Batch: 2550
Training Loss: 0.01312117126642489
Epoch: 53 Batch: 2600
Training Loss: 0.01205406456039502
Epoch: 53 Batch: 2650
Training Loss: 0.011994904077277994
Epoch: 53 Batch: 2700
Training Loss: 0.01132974742739289
Epoch: 53 Batch: 2750
Training Loss: 0.011443379976532676
Epoch: 53 Batch: 2800
Training Loss: 0.011575784534215927
Epoch: 53 Batch: 2850
Training Loss: 0.011109874196219862
Epoch: 53 Batch: 2900
Training Loss: 0.01097497248444064
Epoch: 53 Batch: 2950
Training Loss: 0.010338372557850207
Epoch: 53 Batch: 3000
Training Loss: 0.011043563485145569
Epoch: 53 Batch: 3050
Training Loss: 0.010235344484204151
Epoch: 53 Batch: 3100
Training Loss: 0.010743597707440777
Epoch: 53 Batch: 3150
Training Loss: 0.010277037563778105
Epoch: 53 Batch: 3200
Training Loss: 0.010248589804396033
Epoch: 54 
 Validation Loss: 0.4923440482881334
---------------------------
Epoch: 54 Batch: 50
Training Loss: 0.6428552430868149
Epoch: 54 Batch: 100
Training Loss: 0.30942375838756564
Epoch: 54 Batch: 150
Training Loss: 0.21759625852108003
Epoch: 54 Batch: 200
Training Loss: 0.15898645296692848
Epoch: 54 Batch: 250
Training Loss: 0.12450447762012481
Epoch: 54 Batch: 300
Training Loss: 0.10581253995498022
Epoch: 54 Batch: 350
Training Loss: 0.09204222023487091
Epoch: 54 Batch: 400
Training Loss: 0.07752766579389572
Epoch: 54 Batch: 450
Training Loss: 0.06821310924159156
Epoch: 54 Batch: 500
Training Loss: 0.06527719575166702
Epoch: 54 Batch: 550
Training Loss: 0.060829155770215124
Epoch: 54 Batch: 600
Training Loss: 0.05541153947512309
Epoch: 54 Batch: 650
Training Loss: 0.04802337041267982
Epoch: 54 Batch: 700
Training Loss: 0.04474846554653985
Epoch: 54 Batch: 750
Training Loss: 0.04237782895565033
Epoch: 54 Batch: 800
Training Loss: 0.04002755094319582
Epoch: 54 Batch: 850
Training Loss: 0.0370487678752226
Epoch: 54 Batch: 900
Training Loss: 0.03516753587457869
Epoch: 54 Batch: 950
Training Loss: 0.03425219614254801
Epoch: 54 Batch: 1000
Training Loss: 0.03311508384346962
Epoch: 54 Batch: 1050
Training Loss: 0.030660201282728287
Epoch: 54 Batch: 1100
Training Loss: 0.029026963033459405
Epoch: 54 Batch: 1150
Training Loss: 0.029649924869122712
Epoch: 54 Batch: 1200
Training Loss: 0.025910955866177875
Epoch: 54 Batch: 1250
Training Loss: 0.024791150832176208
Epoch: 54 Batch: 1300
Training Loss: 0.024297013145226698
Epoch: 54 Batch: 1350
Training Loss: 0.023341946844701415
Epoch: 54 Batch: 1400
Training Loss: 0.02248688186917986
Epoch: 54 Batch: 1450
Training Loss: 0.021291961608261897
Epoch: 54 Batch: 1500
Training Loss: 0.02166961169242859
Epoch: 54 Batch: 1550
Training Loss: 0.020279182368709195
Epoch: 54 Batch: 1600
Training Loss: 0.021261137463152408
Epoch: 54 Batch: 1650
Training Loss: 0.019564053289818042
Epoch: 54 Batch: 1700
Training Loss: 0.01790869488435633
Epoch: 54 Batch: 1750
Training Loss: 0.018458203434944153
Epoch: 54 Batch: 1800
Training Loss: 0.018053910517030293
Epoch: 54 Batch: 1850
Training Loss: 0.01665799814301568
Epoch: 54 Batch: 1900
Training Loss: 0.015628595195318524
Epoch: 54 Batch: 1950
Training Loss: 0.016173131465911865
Epoch: 54 Batch: 2000
Training Loss: 0.016314075008034706
Epoch: 54 Batch: 2050
Training Loss: 0.01624110979277913
Epoch: 54 Batch: 2100
Training Loss: 0.015739307758354006
Epoch: 54 Batch: 2150
Training Loss: 0.014191242719805517
Epoch: 54 Batch: 2200
Training Loss: 0.014294187467206608
Epoch: 54 Batch: 2250
Training Loss: 0.013983684049712288
Epoch: 54 Batch: 2300
Training Loss: 0.013888570329417354
Epoch: 54 Batch: 2350
Training Loss: 0.013859399379567898
Epoch: 54 Batch: 2400
Training Loss: 0.013546803978582224
Epoch: 54 Batch: 2450
Training Loss: 0.013398540141631146
Epoch: 54 Batch: 2500
Training Loss: 0.013081924164295197
Epoch: 54 Batch: 2550
Training Loss: 0.012871499669318106
Epoch: 54 Batch: 2600
Training Loss: 0.011921557142184331
Epoch: 54 Batch: 2650
Training Loss: 0.012432314843501685
Epoch: 54 Batch: 2700
Training Loss: 0.011477639630988792
Epoch: 54 Batch: 2750
Training Loss: 0.011831160187721252
Epoch: 54 Batch: 2800
Training Loss: 0.01119628909443106
Epoch: 54 Batch: 2850
Training Loss: 0.011728145281473796
Epoch: 54 Batch: 2900
Training Loss: 0.010803516558532057
Epoch: 54 Batch: 2950
Training Loss: 0.011124599919480792
Epoch: 54 Batch: 3000
Training Loss: 0.010777831037839254
Epoch: 54 Batch: 3050
Training Loss: 0.010295843091167387
Epoch: 54 Batch: 3100
Training Loss: 0.009931654401363866
Epoch: 54 Batch: 3150
Training Loss: 0.010237086888343568
Epoch: 54 Batch: 3200
Training Loss: 0.009502389105036854
Epoch: 55 
 Validation Loss: 0.4916688442230225
---------------------------
Epoch: 55 Batch: 50
Training Loss: 0.6342290169000626
Epoch: 55 Batch: 100
Training Loss: 0.3261848670244217
Epoch: 55 Batch: 150
Training Loss: 0.21499135494232177
Epoch: 55 Batch: 200
Training Loss: 0.16506603866815567
Epoch: 55 Batch: 250
Training Loss: 0.1298804808855057
Epoch: 55 Batch: 300
Training Loss: 0.1046445714433988
Epoch: 55 Batch: 350
Training Loss: 0.08919547736644745
Epoch: 55 Batch: 400
Training Loss: 0.07562001816928386
Epoch: 55 Batch: 450
Training Loss: 0.07024192723962996
Epoch: 55 Batch: 500
Training Loss: 0.06474045497179032
Epoch: 55 Batch: 550
Training Loss: 0.05865434841676192
Epoch: 55 Batch: 600
Training Loss: 0.05209419002135595
Epoch: 55 Batch: 650
Training Loss: 0.046605263306544374
Epoch: 55 Batch: 700
Training Loss: 0.04672318224396024
Epoch: 55 Batch: 750
Training Loss: 0.0437256060441335
Epoch: 55 Batch: 800
Training Loss: 0.037730825804173945
Epoch: 55 Batch: 850
Training Loss: 0.037432736614171194
Epoch: 55 Batch: 900
Training Loss: 0.03672183752059936
Epoch: 55 Batch: 950
Training Loss: 0.03579588601463719
Epoch: 55 Batch: 1000
Training Loss: 0.03111826157569885
Epoch: 55 Batch: 1050
Training Loss: 0.029962576570964995
Epoch: 55 Batch: 1100
Training Loss: 0.028175172968344254
Epoch: 55 Batch: 1150
Training Loss: 0.028236935864324155
Epoch: 55 Batch: 1200
Training Loss: 0.027003932719429333
Epoch: 55 Batch: 1250
Training Loss: 0.02577986361980438
Epoch: 55 Batch: 1300
Training Loss: 0.02513870023764097
Epoch: 55 Batch: 1350
Training Loss: 0.023436620080912556
Epoch: 55 Batch: 1400
Training Loss: 0.022217325142451696
Epoch: 55 Batch: 1450
Training Loss: 0.021770232854218318
Epoch: 55 Batch: 1500
Training Loss: 0.022154833356539408
Epoch: 55 Batch: 1550
Training Loss: 0.020527535984593052
Epoch: 55 Batch: 1600
Training Loss: 0.019069777745753528
Epoch: 55 Batch: 1650
Training Loss: 0.018791621977632695
Epoch: 55 Batch: 1700
Training Loss: 0.019134306381730474
Epoch: 55 Batch: 1750
Training Loss: 0.017451376863888333
Epoch: 55 Batch: 1800
Training Loss: 0.01687776875164774
Epoch: 55 Batch: 1850
Training Loss: 0.016829870182114678
Epoch: 55 Batch: 1900
Training Loss: 0.01798311953481875
Epoch: 55 Batch: 1950
Training Loss: 0.015700108546477097
Epoch: 55 Batch: 2000
Training Loss: 0.015363592565059661
Epoch: 55 Batch: 2050
Training Loss: 0.015814128561717707
Epoch: 55 Batch: 2100
Training Loss: 0.014910492641585215
Epoch: 55 Batch: 2150
Training Loss: 0.015148904157239337
Epoch: 55 Batch: 2200
Training Loss: 0.014663936509327454
Epoch: 55 Batch: 2250
Training Loss: 0.014573633829752604
Epoch: 55 Batch: 2300
Training Loss: 0.013706713228122048
Epoch: 55 Batch: 2350
Training Loss: 0.014159170072129431
Epoch: 55 Batch: 2400
Training Loss: 0.01290669821202755
Epoch: 55 Batch: 2450
Training Loss: 0.013199195229277318
Epoch: 55 Batch: 2500
Training Loss: 0.012703002750873566
Epoch: 55 Batch: 2550
Training Loss: 0.012229560064334495
Epoch: 55 Batch: 2600
Training Loss: 0.012553807210463744
Epoch: 55 Batch: 2650
Training Loss: 0.012311149127078506
Epoch: 55 Batch: 2700
Training Loss: 0.011661785002107973
Epoch: 55 Batch: 2750
Training Loss: 0.01186883446303281
Epoch: 55 Batch: 2800
Training Loss: 0.0114669007062912
Epoch: 55 Batch: 2850
Training Loss: 0.01057554192710341
Epoch: 55 Batch: 2900
Training Loss: 0.010596703455365937
Epoch: 55 Batch: 2950
Training Loss: 0.010840052568306358
Epoch: 55 Batch: 3000
Training Loss: 0.011107531845569611
Epoch: 55 Batch: 3050
Training Loss: 0.010558137502826628
Epoch: 55 Batch: 3100
Training Loss: 0.010376347486049898
Epoch: 55 Batch: 3150
Training Loss: 0.010439926764321705
Epoch: 55 Batch: 3200
Training Loss: 0.009530580267310143
Epoch: 56 
 Validation Loss: 0.49120049476623534
---------------------------
Epoch: 56 Batch: 50
Training Loss: 0.6693909114599228
Epoch: 56 Batch: 100
Training Loss: 0.31894914478063585
Epoch: 56 Batch: 150
Training Loss: 0.20979240516821543
Epoch: 56 Batch: 200
Training Loss: 0.15681400120258332
Epoch: 56 Batch: 250
Training Loss: 0.12628970324993133
Epoch: 56 Batch: 300
Training Loss: 0.1064828755458196
Epoch: 56 Batch: 350
Training Loss: 0.09173898313726697
Epoch: 56 Batch: 400
Training Loss: 0.07970891073346138
Epoch: 56 Batch: 450
Training Loss: 0.0706953814956877
Epoch: 56 Batch: 500
Training Loss: 0.06446750313043595
Epoch: 56 Batch: 550
Training Loss: 0.05697021083398299
Epoch: 56 Batch: 600
Training Loss: 0.052008109639088314
Epoch: 56 Batch: 650
Training Loss: 0.04791348801209376
Epoch: 56 Batch: 700
Training Loss: 0.046243411302566526
Epoch: 56 Batch: 750
Training Loss: 0.04288024663925171
Epoch: 56 Batch: 800
Training Loss: 0.0385885326936841
Epoch: 56 Batch: 850
Training Loss: 0.03762148471439586
Epoch: 56 Batch: 900
Training Loss: 0.03507719026671516
Epoch: 56 Batch: 950
Training Loss: 0.031807002958498504
Epoch: 56 Batch: 1000
Training Loss: 0.03216535648703575
Epoch: 56 Batch: 1050
Training Loss: 0.03174610645998092
Epoch: 56 Batch: 1100
Training Loss: 0.028969936208291486
Epoch: 56 Batch: 1150
Training Loss: 0.028490342523740685
Epoch: 56 Batch: 1200
Training Loss: 0.027324106444915137
Epoch: 56 Batch: 1250
Training Loss: 0.024927230429649352
Epoch: 56 Batch: 1300
Training Loss: 0.02378051478129167
Epoch: 56 Batch: 1350
Training Loss: 0.02376609287880085
Epoch: 56 Batch: 1400
Training Loss: 0.023485585621425085
Epoch: 56 Batch: 1450
Training Loss: 0.020750172939793816
Epoch: 56 Batch: 1500
Training Loss: 0.021213883399963378
Epoch: 56 Batch: 1550
Training Loss: 0.020278668115215916
Epoch: 56 Batch: 1600
Training Loss: 0.020058498438447713
Epoch: 56 Batch: 1650
Training Loss: 0.01879680951436361
Epoch: 56 Batch: 1700
Training Loss: 0.01893835830337861
Epoch: 56 Batch: 1750
Training Loss: 0.018552757161004203
Epoch: 56 Batch: 1800
Training Loss: 0.0188181967039903
Epoch: 56 Batch: 1850
Training Loss: 0.016691525240202208
Epoch: 56 Batch: 1900
Training Loss: 0.01725033877711547
Epoch: 56 Batch: 1950
Training Loss: 0.016154343363566276
Epoch: 56 Batch: 2000
Training Loss: 0.016087780386209487
Epoch: 56 Batch: 2050
Training Loss: 0.016119426547027214
Epoch: 56 Batch: 2100
Training Loss: 0.014516099918456305
Epoch: 56 Batch: 2150
Training Loss: 0.014720680949299835
Epoch: 56 Batch: 2200
Training Loss: 0.015256989300251007
Epoch: 56 Batch: 2250
Training Loss: 0.01356400896443261
Epoch: 56 Batch: 2300
Training Loss: 0.01380506685246592
Epoch: 56 Batch: 2350
Training Loss: 0.013245308386518601
Epoch: 56 Batch: 2400
Training Loss: 0.012867651830116908
Epoch: 56 Batch: 2450
Training Loss: 0.01356739421280063
Epoch: 56 Batch: 2500
Training Loss: 0.01321866638660431
Epoch: 56 Batch: 2550
Training Loss: 0.012116871721604291
Epoch: 56 Batch: 2600
Training Loss: 0.012339718983723567
Epoch: 56 Batch: 2650
Training Loss: 0.011689911754626149
Epoch: 56 Batch: 2700
Training Loss: 0.011120414899455176
Epoch: 56 Batch: 2750
Training Loss: 0.011880760485475714
Epoch: 56 Batch: 2800
Training Loss: 0.011880783587694168
Epoch: 56 Batch: 2850
Training Loss: 0.0110450286718837
Epoch: 56 Batch: 2900
Training Loss: 0.011128991153733484
Epoch: 56 Batch: 2950
Training Loss: 0.010410779754994279
Epoch: 56 Batch: 3000
Training Loss: 0.010295733392238616
Epoch: 56 Batch: 3050
Training Loss: 0.010398387146777793
Epoch: 56 Batch: 3100
Training Loss: 0.010360343119790477
Epoch: 56 Batch: 3150
Training Loss: 0.010016150209638808
Epoch: 56 Batch: 3200
Training Loss: 0.009973644334822894
Epoch: 57 
 Validation Loss: 0.4905385027329127
---------------------------
Epoch: 57 Batch: 50
Training Loss: 0.6812736654281616
Epoch: 57 Batch: 100
Training Loss: 0.30775078147649765
Epoch: 57 Batch: 150
Training Loss: 0.2065111603339513
Epoch: 57 Batch: 200
Training Loss: 0.15447816595435143
Epoch: 57 Batch: 250
Training Loss: 0.1320924128293991
Epoch: 57 Batch: 300
Training Loss: 0.10442392021417618
Epoch: 57 Batch: 350
Training Loss: 0.0899697870016098
Epoch: 57 Batch: 400
Training Loss: 0.08251871891319752
Epoch: 57 Batch: 450
Training Loss: 0.06927501685089535
Epoch: 57 Batch: 500
Training Loss: 0.06562366718053818
Epoch: 57 Batch: 550
Training Loss: 0.05769964331930334
Epoch: 57 Batch: 600
Training Loss: 0.052872539112965265
Epoch: 57 Batch: 650
Training Loss: 0.04936041543116936
Epoch: 57 Batch: 700
Training Loss: 0.046540223998682836
Epoch: 57 Batch: 750
Training Loss: 0.04218471578756968
Epoch: 57 Batch: 800
Training Loss: 0.04009767580777407
Epoch: 57 Batch: 850
Training Loss: 0.037381908297538756
Epoch: 57 Batch: 900
Training Loss: 0.03504803776741028
Epoch: 57 Batch: 950
Training Loss: 0.034019334347624525
Epoch: 57 Batch: 1000
Training Loss: 0.03259416854381561
Epoch: 57 Batch: 1050
Training Loss: 0.030994175927979605
Epoch: 57 Batch: 1100
Training Loss: 0.030463011075149884
Epoch: 57 Batch: 1150
Training Loss: 0.028032381975132487
Epoch: 57 Batch: 1200
Training Loss: 0.02623418629169464
Epoch: 57 Batch: 1250
Training Loss: 0.026841673469543456
Epoch: 57 Batch: 1300
Training Loss: 0.025751664982392238
Epoch: 57 Batch: 1350
Training Loss: 0.023895169055020368
Epoch: 57 Batch: 1400
Training Loss: 0.023218690093074527
Epoch: 57 Batch: 1450
Training Loss: 0.021117831036962312
Epoch: 57 Batch: 1500
Training Loss: 0.020765315194924673
Epoch: 57 Batch: 1550
Training Loss: 0.02016839608069389
Epoch: 57 Batch: 1600
Training Loss: 0.019983038287609816
Epoch: 57 Batch: 1650
Training Loss: 0.018931551629846746
Epoch: 57 Batch: 1700
Training Loss: 0.01935815411455491
Epoch: 57 Batch: 1750
Training Loss: 0.018149047442844936
Epoch: 57 Batch: 1800
Training Loss: 0.017324213418695663
Epoch: 57 Batch: 1850
Training Loss: 0.017748519794361012
Epoch: 57 Batch: 1900
Training Loss: 0.016838810004686053
Epoch: 57 Batch: 1950
Training Loss: 0.015883380052370902
Epoch: 57 Batch: 2000
Training Loss: 0.015380125716328621
Epoch: 57 Batch: 2050
Training Loss: 0.01507505757052724
Epoch: 57 Batch: 2100
Training Loss: 0.01479474758818036
Epoch: 57 Batch: 2150
Training Loss: 0.015390383032865303
Epoch: 57 Batch: 2200
Training Loss: 0.015279847654429348
Epoch: 57 Batch: 2250
Training Loss: 0.013892445511288112
Epoch: 57 Batch: 2300
Training Loss: 0.013888007881848708
Epoch: 57 Batch: 2350
Training Loss: 0.013353806480448297
Epoch: 57 Batch: 2400
Training Loss: 0.013113209009170533
Epoch: 57 Batch: 2450
Training Loss: 0.013313641925247349
Epoch: 57 Batch: 2500
Training Loss: 0.012800575053691863
Epoch: 57 Batch: 2550
Training Loss: 0.012483148644952213
Epoch: 57 Batch: 2600
Training Loss: 0.012186267513495224
Epoch: 57 Batch: 2650
Training Loss: 0.011739341414199685
Epoch: 57 Batch: 2700
Training Loss: 0.011991328254893975
Epoch: 57 Batch: 2750
Training Loss: 0.011910603772510181
Epoch: 57 Batch: 2800
Training Loss: 0.010765901686889784
Epoch: 57 Batch: 2850
Training Loss: 0.01133055145280403
Epoch: 57 Batch: 2900
Training Loss: 0.010614443164447259
Epoch: 57 Batch: 2950
Training Loss: 0.010705808855719486
Epoch: 57 Batch: 3000
Training Loss: 0.010706888755162557
Epoch: 57 Batch: 3050
Training Loss: 0.010056756510109198
Epoch: 57 Batch: 3100
Training Loss: 0.010008869642211544
Epoch: 57 Batch: 3150
Training Loss: 0.009822481151611086
Epoch: 57 Batch: 3200
Training Loss: 0.009268061965703964
Epoch: 58 
 Validation Loss: 0.4904290341668659
---------------------------
Epoch: 58 Batch: 50
Training Loss: 0.6275297510623932
Epoch: 58 Batch: 100
Training Loss: 0.3217261379957199
Epoch: 58 Batch: 150
Training Loss: 0.2177687805891037
Epoch: 58 Batch: 200
Training Loss: 0.16581277951598167
Epoch: 58 Batch: 250
Training Loss: 0.13081219387054444
Epoch: 58 Batch: 300
Training Loss: 0.10721504638592402
Epoch: 58 Batch: 350
Training Loss: 0.0896122009413583
Epoch: 58 Batch: 400
Training Loss: 0.07767974950373173
Epoch: 58 Batch: 450
Training Loss: 0.07111494733227623
Epoch: 58 Batch: 500
Training Loss: 0.06384136939048767
Epoch: 58 Batch: 550
Training Loss: 0.05992412317882884
Epoch: 58 Batch: 600
Training Loss: 0.052312153826157254
Epoch: 58 Batch: 650
Training Loss: 0.05021257373002859
Epoch: 58 Batch: 700
Training Loss: 0.04418185080800738
Epoch: 58 Batch: 750
Training Loss: 0.0425521414677302
Epoch: 58 Batch: 800
Training Loss: 0.04030261691659689
Epoch: 58 Batch: 850
Training Loss: 0.03815128947005553
Epoch: 58 Batch: 900
Training Loss: 0.03521564804845386
Epoch: 58 Batch: 950
Training Loss: 0.03443379521369934
Epoch: 58 Batch: 1000
Training Loss: 0.032651275366544726
Epoch: 58 Batch: 1050
Training Loss: 0.030594221296764555
Epoch: 58 Batch: 1100
Training Loss: 0.029094865836880424
Epoch: 58 Batch: 1150
Training Loss: 0.026819165634072346
Epoch: 58 Batch: 1200
Training Loss: 0.025934962530930836
Epoch: 58 Batch: 1250
Training Loss: 0.02596372857093811
Epoch: 58 Batch: 1300
Training Loss: 0.024874003965121048
Epoch: 58 Batch: 1350
Training Loss: 0.023947130618272004
Epoch: 58 Batch: 1400
Training Loss: 0.02176103372659002
Epoch: 58 Batch: 1450
Training Loss: 0.022301463936937268
Epoch: 58 Batch: 1500
Training Loss: 0.021375381628672283
Epoch: 58 Batch: 1550
Training Loss: 0.019612372767540718
Epoch: 58 Batch: 1600
Training Loss: 0.01965109072625637
Epoch: 58 Batch: 1650
Training Loss: 0.019373718626571425
Epoch: 58 Batch: 1700
Training Loss: 0.018894460236324982
Epoch: 58 Batch: 1750
Training Loss: 0.01841094081742423
Epoch: 58 Batch: 1800
Training Loss: 0.017553928163316516
Epoch: 58 Batch: 1850
Training Loss: 0.016746156505636267
Epoch: 58 Batch: 1900
Training Loss: 0.01705884376638814
Epoch: 58 Batch: 1950
Training Loss: 0.016016285908527862
Epoch: 58 Batch: 2000
Training Loss: 0.015585940644145011
Epoch: 58 Batch: 2050
Training Loss: 0.015589864980883715
Epoch: 58 Batch: 2100
Training Loss: 0.01516284589256559
Epoch: 58 Batch: 2150
Training Loss: 0.014262972099836483
Epoch: 58 Batch: 2200
Training Loss: 0.01452264746481722
Epoch: 58 Batch: 2250
Training Loss: 0.014380791107813517
Epoch: 58 Batch: 2300
Training Loss: 0.013860790703607642
Epoch: 58 Batch: 2350
Training Loss: 0.013271422994897721
Epoch: 58 Batch: 2400
Training Loss: 0.013203206571439903
Epoch: 58 Batch: 2450
Training Loss: 0.012609464526176452
Epoch: 58 Batch: 2500
Training Loss: 0.013151799392700196
Epoch: 58 Batch: 2550
Training Loss: 0.012054210737639782
Epoch: 58 Batch: 2600
Training Loss: 0.012195156583419213
Epoch: 58 Batch: 2650
Training Loss: 0.012156310745005337
Epoch: 58 Batch: 2700
Training Loss: 0.011349932860445094
Epoch: 58 Batch: 2750
Training Loss: 0.012055329398675398
Epoch: 58 Batch: 2800
Training Loss: 0.011632251792720386
Epoch: 58 Batch: 2850
Training Loss: 0.011034254965029265
Epoch: 58 Batch: 2900
Training Loss: 0.011016195478110478
Epoch: 58 Batch: 2950
Training Loss: 0.010437985587928255
Epoch: 58 Batch: 3000
Training Loss: 0.01021340083082517
Epoch: 58 Batch: 3050
Training Loss: 0.010138686377494061
Epoch: 58 Batch: 3100
Training Loss: 0.010319532973151054
Epoch: 58 Batch: 3150
Training Loss: 0.009793312407675243
Epoch: 58 Batch: 3200
Training Loss: 0.010092791356146335
Epoch: 59 
 Validation Loss: 0.4897591160403358
---------------------------
Epoch: 59 Batch: 50
Training Loss: 0.6661921781301499
Epoch: 59 Batch: 100
Training Loss: 0.3342937386035919
Epoch: 59 Batch: 150
Training Loss: 0.2098885969320933
Epoch: 59 Batch: 200
Training Loss: 0.16448221147060393
Epoch: 59 Batch: 250
Training Loss: 0.13139425814151764
Epoch: 59 Batch: 300
Training Loss: 0.0999158235390981
Epoch: 59 Batch: 350
Training Loss: 0.0928081147159849
Epoch: 59 Batch: 400
Training Loss: 0.08097388796508312
Epoch: 59 Batch: 450
Training Loss: 0.07078661726580726
Epoch: 59 Batch: 500
Training Loss: 0.061965936839580536
Epoch: 59 Batch: 550
Training Loss: 0.05731895165009932
Epoch: 59 Batch: 600
Training Loss: 0.053891121943791705
Epoch: 59 Batch: 650
Training Loss: 0.04863768802239345
Epoch: 59 Batch: 700
Training Loss: 0.04345436406987054
Epoch: 59 Batch: 750
Training Loss: 0.04391843330860138
Epoch: 59 Batch: 800
Training Loss: 0.03842133525758982
Epoch: 59 Batch: 850
Training Loss: 0.03834967150407679
Epoch: 59 Batch: 900
Training Loss: 0.03612835225131777
Epoch: 59 Batch: 950
Training Loss: 0.03261496493690892
Epoch: 59 Batch: 1000
Training Loss: 0.03177295967936516
Epoch: 59 Batch: 1050
Training Loss: 0.02992421587308248
Epoch: 59 Batch: 1100
Training Loss: 0.028684363554824482
Epoch: 59 Batch: 1150
Training Loss: 0.02780666110308274
Epoch: 59 Batch: 1200
Training Loss: 0.025646744892001153
Epoch: 59 Batch: 1250
Training Loss: 0.02441734263896942
Epoch: 59 Batch: 1300
Training Loss: 0.02481072024657176
Epoch: 59 Batch: 1350
Training Loss: 0.02359051256268113
Epoch: 59 Batch: 1400
Training Loss: 0.02290974361555917
Epoch: 59 Batch: 1450
Training Loss: 0.0223139305361386
Epoch: 59 Batch: 1500
Training Loss: 0.02162293662627538
Epoch: 59 Batch: 1550
Training Loss: 0.021588330288087168
Epoch: 59 Batch: 1600
Training Loss: 0.020072743445634842
Epoch: 59 Batch: 1650
Training Loss: 0.01917700668176015
Epoch: 59 Batch: 1700
Training Loss: 0.01841707723982194
Epoch: 59 Batch: 1750
Training Loss: 0.017018080013138908
Epoch: 59 Batch: 1800
Training Loss: 0.017160488300853306
Epoch: 59 Batch: 1850
Training Loss: 0.017210285889135826
Epoch: 59 Batch: 1900
Training Loss: 0.016537644988612123
Epoch: 59 Batch: 1950
Training Loss: 0.016082681463314936
Epoch: 59 Batch: 2000
Training Loss: 0.015239271745085716
Epoch: 59 Batch: 2050
Training Loss: 0.014729347330767934
Epoch: 59 Batch: 2100
Training Loss: 0.015276586626257214
Epoch: 59 Batch: 2150
Training Loss: 0.015577976232351258
Epoch: 59 Batch: 2200
Training Loss: 0.01505734912373803
Epoch: 59 Batch: 2250
Training Loss: 0.013958440701166789
Epoch: 59 Batch: 2300
Training Loss: 0.013527356787868167
Epoch: 59 Batch: 2350
Training Loss: 0.013558048935646706
Epoch: 59 Batch: 2400
Training Loss: 0.013379638989766438
Epoch: 59 Batch: 2450
Training Loss: 0.012996624866310431
Epoch: 59 Batch: 2500
Training Loss: 0.013063471221923828
Epoch: 59 Batch: 2550
Training Loss: 0.012926829620903614
Epoch: 59 Batch: 2600
Training Loss: 0.012262899692241962
Epoch: 59 Batch: 2650
Training Loss: 0.011879647441630094
Epoch: 59 Batch: 2700
Training Loss: 0.011727332020247424
Epoch: 59 Batch: 2750
Training Loss: 0.01153771874037656
Epoch: 59 Batch: 2800
Training Loss: 0.011770956026656287
Epoch: 59 Batch: 2850
Training Loss: 0.011089918780745122
Epoch: 59 Batch: 2900
Training Loss: 0.011568529369502232
Epoch: 59 Batch: 2950
Training Loss: 0.01080966008921801
Epoch: 59 Batch: 3000
Training Loss: 0.01018338097135226
Epoch: 59 Batch: 3050
Training Loss: 0.010714593529701232
Epoch: 59 Batch: 3100
Training Loss: 0.010092245724893385
Epoch: 59 Batch: 3150
Training Loss: 0.010534882980679708
Epoch: 59 Batch: 3200
Training Loss: 0.009649243569001556
Epoch: 60 
 Validation Loss: 0.48917479746871523
---------------------------
Epoch: 60 Batch: 50
Training Loss: 0.6507009565830231
Epoch: 60 Batch: 100
Training Loss: 0.302619149684906
Epoch: 60 Batch: 150
Training Loss: 0.20656966010729472
Epoch: 60 Batch: 200
Training Loss: 0.15624417766928672
Epoch: 60 Batch: 250
Training Loss: 0.12327306830883027
Epoch: 60 Batch: 300
Training Loss: 0.10384179174900054
Epoch: 60 Batch: 350
Training Loss: 0.08955633299691336
Epoch: 60 Batch: 400
Training Loss: 0.07970973894000054
Epoch: 60 Batch: 450
Training Loss: 0.06692148056295183
Epoch: 60 Batch: 500
Training Loss: 0.06351361119747162
Epoch: 60 Batch: 550
Training Loss: 0.05764750361442566
Epoch: 60 Batch: 600
Training Loss: 0.05278518890341123
Epoch: 60 Batch: 650
Training Loss: 0.0488853362431893
Epoch: 60 Batch: 700
Training Loss: 0.044830465359347205
Epoch: 60 Batch: 750
Training Loss: 0.04457488095760345
Epoch: 60 Batch: 800
Training Loss: 0.03944847784936428
Epoch: 60 Batch: 850
Training Loss: 0.036030652803533215
Epoch: 60 Batch: 900
Training Loss: 0.03495330754253599
Epoch: 60 Batch: 950
Training Loss: 0.034116683069028354
Epoch: 60 Batch: 1000
Training Loss: 0.03137192702293396
Epoch: 60 Batch: 1050
Training Loss: 0.030523513214928764
Epoch: 60 Batch: 1100
Training Loss: 0.029115204431793905
Epoch: 60 Batch: 1150
Training Loss: 0.027077992092008177
Epoch: 60 Batch: 1200
Training Loss: 0.02580959697564443
Epoch: 60 Batch: 1250
Training Loss: 0.025895520401000977
Epoch: 60 Batch: 1300
Training Loss: 0.024147227291877453
Epoch: 60 Batch: 1350
Training Loss: 0.024053941744345206
Epoch: 60 Batch: 1400
Training Loss: 0.02239821561745235
Epoch: 60 Batch: 1450
Training Loss: 0.02197060397986708
Epoch: 60 Batch: 1500
Training Loss: 0.021594884713490803
Epoch: 60 Batch: 1550
Training Loss: 0.02082852873110002
Epoch: 60 Batch: 1600
Training Loss: 0.020987807717174293
Epoch: 60 Batch: 1650
Training Loss: 0.018717028354153488
Epoch: 60 Batch: 1700
Training Loss: 0.019370089839486515
Epoch: 60 Batch: 1750
Training Loss: 0.018684410418782917
Epoch: 60 Batch: 1800
Training Loss: 0.01840870304240121
Epoch: 60 Batch: 1850
Training Loss: 0.01764127889195004
Epoch: 60 Batch: 1900
Training Loss: 0.017191284314582223
Epoch: 60 Batch: 1950
Training Loss: 0.016391268815749732
Epoch: 60 Batch: 2000
Training Loss: 0.01603160308301449
Epoch: 60 Batch: 2050
Training Loss: 0.014782553678605615
Epoch: 60 Batch: 2100
Training Loss: 0.01547752437137422
Epoch: 60 Batch: 2150
Training Loss: 0.014370133946108263
Epoch: 60 Batch: 2200
Training Loss: 0.014272896850650961
Epoch: 60 Batch: 2250
Training Loss: 0.01388501505057017
Epoch: 60 Batch: 2300
Training Loss: 0.01438622280307438
Epoch: 60 Batch: 2350
Training Loss: 0.013279515821883019
Epoch: 60 Batch: 2400
Training Loss: 0.013560939605037372
Epoch: 60 Batch: 2450
Training Loss: 0.013192298667771475
Epoch: 60 Batch: 2500
Training Loss: 0.012294134831428528
Epoch: 60 Batch: 2550
Training Loss: 0.01204314622224546
Epoch: 60 Batch: 2600
Training Loss: 0.01266131192445755
Epoch: 60 Batch: 2650
Training Loss: 0.012041670916215429
Epoch: 60 Batch: 2700
Training Loss: 0.011573766900433434
Epoch: 60 Batch: 2750
Training Loss: 0.011479953343218023
Epoch: 60 Batch: 2800
Training Loss: 0.011363522708415985
Epoch: 60 Batch: 2850
Training Loss: 0.011579989466750831
Epoch: 60 Batch: 2900
Training Loss: 0.011198730664006595
Epoch: 60 Batch: 2950
Training Loss: 0.010345079252275369
Epoch: 60 Batch: 3000
Training Loss: 0.0106745299299558
Epoch: 60 Batch: 3050
Training Loss: 0.010101333133509902
Epoch: 60 Batch: 3100
Training Loss: 0.010587640481610452
Epoch: 60 Batch: 3150
Training Loss: 0.010654212331014967
Epoch: 60 Batch: 3200
Training Loss: 0.010153805213049054
Epoch: 61 
 Validation Loss: 0.48905618521902294
---------------------------
Epoch: 61 Batch: 50
Training Loss: 0.6430482470989227
Epoch: 61 Batch: 100
Training Loss: 0.3200912442803383
Epoch: 61 Batch: 150
Training Loss: 0.21765740076700846
Epoch: 61 Batch: 200
Training Loss: 0.15261631205677986
Epoch: 61 Batch: 250
Training Loss: 0.13108437013626098
Epoch: 61 Batch: 300
Training Loss: 0.10854646225770315
Epoch: 61 Batch: 350
Training Loss: 0.09092567205429077
Epoch: 61 Batch: 400
Training Loss: 0.08266580156981945
Epoch: 61 Batch: 450
Training Loss: 0.07256313621997833
Epoch: 61 Batch: 500
Training Loss: 0.06282498347759247
Epoch: 61 Batch: 550
Training Loss: 0.056624706766822124
Epoch: 61 Batch: 600
Training Loss: 0.0537174650033315
Epoch: 61 Batch: 650
Training Loss: 0.04749621776434092
Epoch: 61 Batch: 700
Training Loss: 0.045378299866403855
Epoch: 61 Batch: 750
Training Loss: 0.04222612059116364
Epoch: 61 Batch: 800
Training Loss: 0.039140154235064985
Epoch: 61 Batch: 850
Training Loss: 0.0346633146089666
Epoch: 61 Batch: 900
Training Loss: 0.03421843661202325
Epoch: 61 Batch: 950
Training Loss: 0.03335313354667865
Epoch: 61 Batch: 1000
Training Loss: 0.03172843393683433
Epoch: 61 Batch: 1050
Training Loss: 0.031528474943978445
Epoch: 61 Batch: 1100
Training Loss: 0.029258796247568997
Epoch: 61 Batch: 1150
Training Loss: 0.027446272191794022
Epoch: 61 Batch: 1200
Training Loss: 0.027416524589061738
Epoch: 61 Batch: 1250
Training Loss: 0.025377744507789613
Epoch: 61 Batch: 1300
Training Loss: 0.02453248205093237
Epoch: 61 Batch: 1350
Training Loss: 0.023379563578852902
Epoch: 61 Batch: 1400
Training Loss: 0.022528384242738998
Epoch: 61 Batch: 1450
Training Loss: 0.020788588338884813
Epoch: 61 Batch: 1500
Training Loss: 0.020712808271249137
Epoch: 61 Batch: 1550
Training Loss: 0.0208606965503385
Epoch: 61 Batch: 1600
Training Loss: 0.02082661075517535
Epoch: 61 Batch: 1650
Training Loss: 0.01901118941379316
Epoch: 61 Batch: 1700
Training Loss: 0.018637582256513484
Epoch: 61 Batch: 1750
Training Loss: 0.018535029087747845
Epoch: 61 Batch: 1800
Training Loss: 0.017191459072960748
Epoch: 61 Batch: 1850
Training Loss: 0.017065374963992352
Epoch: 61 Batch: 1900
Training Loss: 0.017170142249057167
Epoch: 61 Batch: 1950
Training Loss: 0.016368969755294994
Epoch: 61 Batch: 2000
Training Loss: 0.0163142075240612
Epoch: 61 Batch: 2050
Training Loss: 0.015227295000378678
Epoch: 61 Batch: 2100
Training Loss: 0.01524377886738096
Epoch: 61 Batch: 2150
Training Loss: 0.01415669457856999
Epoch: 61 Batch: 2200
Training Loss: 0.014753202511505647
Epoch: 61 Batch: 2250
Training Loss: 0.013566346208254497
Epoch: 61 Batch: 2300
Training Loss: 0.013379307518834652
Epoch: 61 Batch: 2350
Training Loss: 0.013299619339882059
Epoch: 61 Batch: 2400
Training Loss: 0.012580059977869193
Epoch: 61 Batch: 2450
Training Loss: 0.012484708781145057
Epoch: 61 Batch: 2500
Training Loss: 0.012034834861755371
Epoch: 61 Batch: 2550
Training Loss: 0.012553616107678882
Epoch: 61 Batch: 2600
Training Loss: 0.011814695023573362
Epoch: 61 Batch: 2650
Training Loss: 0.01189961793287745
Epoch: 61 Batch: 2700
Training Loss: 0.01177087872116654
Epoch: 61 Batch: 2750
Training Loss: 0.011657584266229109
Epoch: 61 Batch: 2800
Training Loss: 0.011283044761845044
Epoch: 61 Batch: 2850
Training Loss: 0.010695815860179432
Epoch: 61 Batch: 2900
Training Loss: 0.01094192627175101
Epoch: 61 Batch: 2950
Training Loss: 0.010138100244231143
Epoch: 61 Batch: 3000
Training Loss: 0.011333845933278401
Epoch: 61 Batch: 3050
Training Loss: 0.010952696643891882
Epoch: 61 Batch: 3100
Training Loss: 0.010131551027297974
Epoch: 61 Batch: 3150
Training Loss: 0.010260860796958681
Epoch: 61 Batch: 3200
Training Loss: 0.009586870716884733
Epoch: 62 
 Validation Loss: 0.48842479785283405
---------------------------
Epoch: 62 Batch: 50
Training Loss: 0.6570342302322387
Epoch: 62 Batch: 100
Training Loss: 0.31679792612791063
Epoch: 62 Batch: 150
Training Loss: 0.21729922870794932
Epoch: 62 Batch: 200
Training Loss: 0.1550404453277588
Epoch: 62 Batch: 250
Training Loss: 0.12461331880092621
Epoch: 62 Batch: 300
Training Loss: 0.10183829853932062
Epoch: 62 Batch: 350
Training Loss: 0.09384999224117824
Epoch: 62 Batch: 400
Training Loss: 0.07893414780497551
Epoch: 62 Batch: 450
Training Loss: 0.06814642959170872
Epoch: 62 Batch: 500
Training Loss: 0.06304666525125503
Epoch: 62 Batch: 550
Training Loss: 0.05525321971286427
Epoch: 62 Batch: 600
Training Loss: 0.05422813971837362
Epoch: 62 Batch: 650
Training Loss: 0.050645918937829826
Epoch: 62 Batch: 700
Training Loss: 0.04492395975760051
Epoch: 62 Batch: 750
Training Loss: 0.04191980083783468
Epoch: 62 Batch: 800
Training Loss: 0.0404394057020545
Epoch: 62 Batch: 850
Training Loss: 0.03744735567008748
Epoch: 62 Batch: 900
Training Loss: 0.03560952802499135
Epoch: 62 Batch: 950
Training Loss: 0.03346863069032368
Epoch: 62 Batch: 1000
Training Loss: 0.03060369822382927
Epoch: 62 Batch: 1050
Training Loss: 0.03015925938174838
Epoch: 62 Batch: 1100
Training Loss: 0.030238317142833364
Epoch: 62 Batch: 1150
Training Loss: 0.02762031018733978
Epoch: 62 Batch: 1200
Training Loss: 0.026662582606077193
Epoch: 62 Batch: 1250
Training Loss: 0.024442720675468446
Epoch: 62 Batch: 1300
Training Loss: 0.02461946840469654
Epoch: 62 Batch: 1350
Training Loss: 0.02379882644723963
Epoch: 62 Batch: 1400
Training Loss: 0.02336320583309446
Epoch: 62 Batch: 1450
Training Loss: 0.022061409518636506
Epoch: 62 Batch: 1500
Training Loss: 0.02175458965698878
Epoch: 62 Batch: 1550
Training Loss: 0.02045609176158905
Epoch: 62 Batch: 1600
Training Loss: 0.01939099997282028
Epoch: 62 Batch: 1650
Training Loss: 0.018648786147435505
Epoch: 62 Batch: 1700
Training Loss: 0.018505462092511794
Epoch: 62 Batch: 1750
Training Loss: 0.01815862965583801
Epoch: 62 Batch: 1800
Training Loss: 0.016814125031232835
Epoch: 62 Batch: 1850
Training Loss: 0.016790129703444404
Epoch: 62 Batch: 1900
Training Loss: 0.01693619732794009
Epoch: 62 Batch: 1950
Training Loss: 0.016144326741878802
Epoch: 62 Batch: 2000
Training Loss: 0.015479984879493713
Epoch: 62 Batch: 2050
Training Loss: 0.015148056164020445
Epoch: 62 Batch: 2100
Training Loss: 0.01510175741854168
Epoch: 62 Batch: 2150
Training Loss: 0.01511940881263378
Epoch: 62 Batch: 2200
Training Loss: 0.014413024796680971
Epoch: 62 Batch: 2250
Training Loss: 0.014441609369383919
Epoch: 62 Batch: 2300
Training Loss: 0.013661818167437677
Epoch: 62 Batch: 2350
Training Loss: 0.01380855179847555
Epoch: 62 Batch: 2400
Training Loss: 0.012943975776433945
Epoch: 62 Batch: 2450
Training Loss: 0.01285368441318979
Epoch: 62 Batch: 2500
Training Loss: 0.012267516207695008
Epoch: 62 Batch: 2550
Training Loss: 0.01257037824275447
Epoch: 62 Batch: 2600
Training Loss: 0.012421383594091121
Epoch: 62 Batch: 2650
Training Loss: 0.011685154258080248
Epoch: 62 Batch: 2700
Training Loss: 0.011600054634941948
Epoch: 62 Batch: 2750
Training Loss: 0.01135008961504156
Epoch: 62 Batch: 2800
Training Loss: 0.011297890512006624
Epoch: 62 Batch: 2850
Training Loss: 0.01168500722500316
Epoch: 62 Batch: 2900
Training Loss: 0.01074023145026174
Epoch: 62 Batch: 2950
Training Loss: 0.010635342193862139
Epoch: 62 Batch: 3000
Training Loss: 0.010520173758268357
Epoch: 62 Batch: 3050
Training Loss: 0.01059055848199813
Epoch: 62 Batch: 3100
Training Loss: 0.01036872891649123
Epoch: 62 Batch: 3150
Training Loss: 0.010141334126866052
Epoch: 62 Batch: 3200
Training Loss: 0.009645282281562686
Epoch: 63 
 Validation Loss: 0.48797863026460014
---------------------------
Epoch: 63 Batch: 50
Training Loss: 0.6396635866165161
Epoch: 63 Batch: 100
Training Loss: 0.3475336575508118
Epoch: 63 Batch: 150
Training Loss: 0.21821586966514586
Epoch: 63 Batch: 200
Training Loss: 0.15594300165772437
Epoch: 63 Batch: 250
Training Loss: 0.12647266328334808
Epoch: 63 Batch: 300
Training Loss: 0.1033784423271815
Epoch: 63 Batch: 350
Training Loss: 0.08949321840490614
Epoch: 63 Batch: 400
Training Loss: 0.08128386519849301
Epoch: 63 Batch: 450
Training Loss: 0.06998531679312388
Epoch: 63 Batch: 500
Training Loss: 0.06139939403533935
Epoch: 63 Batch: 550
Training Loss: 0.05518067088994113
Epoch: 63 Batch: 600
Training Loss: 0.052455833852291106
Epoch: 63 Batch: 650
Training Loss: 0.04919258305659661
Epoch: 63 Batch: 700
Training Loss: 0.043863400518894195
Epoch: 63 Batch: 750
Training Loss: 0.04233342627684275
Epoch: 63 Batch: 800
Training Loss: 0.0399728723987937
Epoch: 63 Batch: 850
Training Loss: 0.03726429616703707
Epoch: 63 Batch: 900
Training Loss: 0.03542671740055084
Epoch: 63 Batch: 950
Training Loss: 0.033626907216875175
Epoch: 63 Batch: 1000
Training Loss: 0.03134244680404663
Epoch: 63 Batch: 1050
Training Loss: 0.029863667885462443
Epoch: 63 Batch: 1100
Training Loss: 0.027834998911077324
Epoch: 63 Batch: 1150
Training Loss: 0.026344705431357675
Epoch: 63 Batch: 1200
Training Loss: 0.026721385444204014
Epoch: 63 Batch: 1250
Training Loss: 0.025695913171768187
Epoch: 63 Batch: 1300
Training Loss: 0.024092356402140396
Epoch: 63 Batch: 1350
Training Loss: 0.023708684863867582
Epoch: 63 Batch: 1400
Training Loss: 0.022569017729588917
Epoch: 63 Batch: 1450
Training Loss: 0.022408065713685133
Epoch: 63 Batch: 1500
Training Loss: 0.020763371725877125
Epoch: 63 Batch: 1550
Training Loss: 0.019666223795183242
Epoch: 63 Batch: 1600
Training Loss: 0.01823971323668957
Epoch: 63 Batch: 1650
Training Loss: 0.020144722082398155
Epoch: 63 Batch: 1700
Training Loss: 0.01865825784557006
Epoch: 63 Batch: 1750
Training Loss: 0.018735536660466876
Epoch: 63 Batch: 1800
Training Loss: 0.01844884105854564
Epoch: 63 Batch: 1850
Training Loss: 0.017191269123876418
Epoch: 63 Batch: 1900
Training Loss: 0.01663252005451604
Epoch: 63 Batch: 1950
Training Loss: 0.015790449931071354
Epoch: 63 Batch: 2000
Training Loss: 0.016077445328235625
Epoch: 63 Batch: 2050
Training Loss: 0.015515511588352482
Epoch: 63 Batch: 2100
Training Loss: 0.014805725003991807
Epoch: 63 Batch: 2150
Training Loss: 0.015154724231986112
Epoch: 63 Batch: 2200
Training Loss: 0.014439603903076866
Epoch: 63 Batch: 2250
Training Loss: 0.014370316876305474
Epoch: 63 Batch: 2300
Training Loss: 0.014044919610023498
Epoch: 63 Batch: 2350
Training Loss: 0.013038639476958741
Epoch: 63 Batch: 2400
Training Loss: 0.012846560453375181
Epoch: 63 Batch: 2450
Training Loss: 0.013175442693184833
Epoch: 63 Batch: 2500
Training Loss: 0.012777900826931
Epoch: 63 Batch: 2550
Training Loss: 0.01255104353614882
Epoch: 63 Batch: 2600
Training Loss: 0.012431351404923659
Epoch: 63 Batch: 2650
Training Loss: 0.010955866575241089
Epoch: 63 Batch: 2700
Training Loss: 0.01197958043328038
Epoch: 63 Batch: 2750
Training Loss: 0.011247295065359635
Epoch: 63 Batch: 2800
Training Loss: 0.01169229914035116
Epoch: 63 Batch: 2850
Training Loss: 0.011124237127471388
Epoch: 63 Batch: 2900
Training Loss: 0.011078850277538957
Epoch: 63 Batch: 2950
Training Loss: 0.010472838363405002
Epoch: 63 Batch: 3000
Training Loss: 0.010850220640500386
Epoch: 63 Batch: 3050
Training Loss: 0.010757306755566206
Epoch: 63 Batch: 3100
Training Loss: 0.010840562370515639
Epoch: 63 Batch: 3150
Training Loss: 0.010085960787440102
Epoch: 63 Batch: 3200
Training Loss: 0.010215598298236728
Epoch: 64 
 Validation Loss: 0.4889474749565125
---------------------------
Epoch: 64 Batch: 50
Training Loss: 0.6493343657255173
Epoch: 64 Batch: 100
Training Loss: 0.3143073034286499
Epoch: 64 Batch: 150
Training Loss: 0.21824517091115317
Epoch: 64 Batch: 200
Training Loss: 0.1576802060008049
Epoch: 64 Batch: 250
Training Loss: 0.12346686434745789
Epoch: 64 Batch: 300
Training Loss: 0.10358588099479675
Epoch: 64 Batch: 350
Training Loss: 0.08613797162260328
Epoch: 64 Batch: 400
Training Loss: 0.08076471187174321
Epoch: 64 Batch: 450
Training Loss: 0.07128898170259264
Epoch: 64 Batch: 500
Training Loss: 0.06222295784950256
Epoch: 64 Batch: 550
Training Loss: 0.058009329329837454
Epoch: 64 Batch: 600
Training Loss: 0.05346537048617999
Epoch: 64 Batch: 650
Training Loss: 0.04791350080416753
Epoch: 64 Batch: 700
Training Loss: 0.046076340973377226
Epoch: 64 Batch: 750
Training Loss: 0.04072929271062215
Epoch: 64 Batch: 800
Training Loss: 0.03923468817025423
Epoch: 64 Batch: 850
Training Loss: 0.0358461192074944
Epoch: 64 Batch: 900
Training Loss: 0.036271555026372276
Epoch: 64 Batch: 950
Training Loss: 0.03227220064715335
Epoch: 64 Batch: 1000
Training Loss: 0.03217982566356659
Epoch: 64 Batch: 1050
Training Loss: 0.029399591655958266
Epoch: 64 Batch: 1100
Training Loss: 0.028342666680162602
Epoch: 64 Batch: 1150
Training Loss: 0.027084984494292218
Epoch: 64 Batch: 1200
Training Loss: 0.026143482228120168
Epoch: 64 Batch: 1250
Training Loss: 0.02551824667453766
Epoch: 64 Batch: 1300
Training Loss: 0.024872914460989146
Epoch: 64 Batch: 1350
Training Loss: 0.022687643082053573
Epoch: 64 Batch: 1400
Training Loss: 0.021917201706341333
Epoch: 64 Batch: 1450
Training Loss: 0.02258163536417073
Epoch: 64 Batch: 1500
Training Loss: 0.021762118657430014
Epoch: 64 Batch: 1550
Training Loss: 0.020474098401684914
Epoch: 64 Batch: 1600
Training Loss: 0.01968518067151308
Epoch: 64 Batch: 1650
Training Loss: 0.018719611782016175
Epoch: 64 Batch: 1700
Training Loss: 0.01853033467250712
Epoch: 64 Batch: 1750
Training Loss: 0.017664789744785855
Epoch: 64 Batch: 1800
Training Loss: 0.017070214052995046
Epoch: 64 Batch: 1850
Training Loss: 0.01746064202205555
Epoch: 64 Batch: 1900
Training Loss: 0.01593870046891664
Epoch: 64 Batch: 1950
Training Loss: 0.016904390362592843
Epoch: 64 Batch: 2000
Training Loss: 0.015649139523506165
Epoch: 64 Batch: 2050
Training Loss: 0.015172842333956462
Epoch: 64 Batch: 2100
Training Loss: 0.014901862215428125
Epoch: 64 Batch: 2150
Training Loss: 0.014500257705533228
Epoch: 64 Batch: 2200
Training Loss: 0.0139554778825153
Epoch: 64 Batch: 2250
Training Loss: 0.013909729189342922
Epoch: 64 Batch: 2300
Training Loss: 0.013653149449306986
Epoch: 64 Batch: 2350
Training Loss: 0.013558866039235541
Epoch: 64 Batch: 2400
Training Loss: 0.013167180654903254
Epoch: 64 Batch: 2450
Training Loss: 0.013183680626810813
Epoch: 64 Batch: 2500
Training Loss: 0.013292113709449768
Epoch: 64 Batch: 2550
Training Loss: 0.012974813510389888
Epoch: 64 Batch: 2600
Training Loss: 0.012230846927716182
Epoch: 64 Batch: 2650
Training Loss: 0.011973582258764303
Epoch: 64 Batch: 2700
Training Loss: 0.011605662416528773
Epoch: 64 Batch: 2750
Training Loss: 0.012268600290471858
Epoch: 64 Batch: 2800
Training Loss: 0.011042998443756785
Epoch: 64 Batch: 2850
Training Loss: 0.011229376981132909
Epoch: 64 Batch: 2900
Training Loss: 0.010642049363974867
Epoch: 64 Batch: 2950
Training Loss: 0.010975098145210136
Epoch: 64 Batch: 3000
Training Loss: 0.010881849000851313
Epoch: 64 Batch: 3050
Training Loss: 0.010379917680240068
Epoch: 64 Batch: 3100
Training Loss: 0.010556729282102276
Epoch: 64 Batch: 3150
Training Loss: 0.010156098736657036
Epoch: 64 Batch: 3200
Training Loss: 0.009867959972470999
Epoch: 65 
 Validation Loss: 0.4876536981927024
---------------------------
Epoch: 65 Batch: 50
Training Loss: 0.6510064768791198
Epoch: 65 Batch: 100
Training Loss: 0.3177300277352333
Epoch: 65 Batch: 150
Training Loss: 0.21387093067169188
Epoch: 65 Batch: 200
Training Loss: 0.16090671569108964
Epoch: 65 Batch: 250
Training Loss: 0.1238189412355423
Epoch: 65 Batch: 300
Training Loss: 0.102923976679643
Epoch: 65 Batch: 350
Training Loss: 0.0912427681684494
Epoch: 65 Batch: 400
Training Loss: 0.07772282183170319
Epoch: 65 Batch: 450
Training Loss: 0.06798404799567329
Epoch: 65 Batch: 500
Training Loss: 0.061985207259655
Epoch: 65 Batch: 550
Training Loss: 0.05807637463916432
Epoch: 65 Batch: 600
Training Loss: 0.05176667466759682
Epoch: 65 Batch: 650
Training Loss: 0.04689285140771132
Epoch: 65 Batch: 700
Training Loss: 0.044658702782222205
Epoch: 65 Batch: 750
Training Loss: 0.043421632965405785
Epoch: 65 Batch: 800
Training Loss: 0.038562051206827166
Epoch: 65 Batch: 850
Training Loss: 0.036146605680970584
Epoch: 65 Batch: 900
Training Loss: 0.0351093347536193
Epoch: 65 Batch: 950
Training Loss: 0.03390605295959272
Epoch: 65 Batch: 1000
Training Loss: 0.032715441823005675
Epoch: 65 Batch: 1050
Training Loss: 0.03103233876682463
Epoch: 65 Batch: 1100
Training Loss: 0.030398582491007718
Epoch: 65 Batch: 1150
Training Loss: 0.026997883579005365
Epoch: 65 Batch: 1200
Training Loss: 0.026647017151117326
Epoch: 65 Batch: 1250
Training Loss: 0.02484603307247162
Epoch: 65 Batch: 1300
Training Loss: 0.02452005562873987
Epoch: 65 Batch: 1350
Training Loss: 0.02322072645028432
Epoch: 65 Batch: 1400
Training Loss: 0.02243881474648203
Epoch: 65 Batch: 1450
Training Loss: 0.021100727484144013
Epoch: 65 Batch: 1500
Training Loss: 0.021601617753505706
Epoch: 65 Batch: 1550
Training Loss: 0.020220074365215918
Epoch: 65 Batch: 1600
Training Loss: 0.01947433328256011
Epoch: 65 Batch: 1650
Training Loss: 0.019884165453188347
Epoch: 65 Batch: 1700
Training Loss: 0.018706331533544205
Epoch: 65 Batch: 1750
Training Loss: 0.017860338500567844
Epoch: 65 Batch: 1800
Training Loss: 0.017677195254299376
Epoch: 65 Batch: 1850
Training Loss: 0.01658147086968293
Epoch: 65 Batch: 1900
Training Loss: 0.016415790241015586
Epoch: 65 Batch: 1950
Training Loss: 0.01590456860187726
Epoch: 65 Batch: 2000
Training Loss: 0.015507997646927833
Epoch: 65 Batch: 2050
Training Loss: 0.015225859735070206
Epoch: 65 Batch: 2100
Training Loss: 0.015425786148934138
Epoch: 65 Batch: 2150
Training Loss: 0.013891371751940527
Epoch: 65 Batch: 2200
Training Loss: 0.014470692696896466
Epoch: 65 Batch: 2250
Training Loss: 0.014004782756169638
Epoch: 65 Batch: 2300
Training Loss: 0.013659299003041308
Epoch: 65 Batch: 2350
Training Loss: 0.013563360318224481
Epoch: 65 Batch: 2400
Training Loss: 0.013559577104945977
Epoch: 65 Batch: 2450
Training Loss: 0.013352218537914509
Epoch: 65 Batch: 2500
Training Loss: 0.01232259019613266
Epoch: 65 Batch: 2550
Training Loss: 0.012328098392954059
Epoch: 65 Batch: 2600
Training Loss: 0.012491507679224014
Epoch: 65 Batch: 2650
Training Loss: 0.012244983517898704
Epoch: 65 Batch: 2700
Training Loss: 0.011608058147960239
Epoch: 65 Batch: 2750
Training Loss: 0.011741649779406461
Epoch: 65 Batch: 2800
Training Loss: 0.011662450528570584
Epoch: 65 Batch: 2850
Training Loss: 0.011246278244152403
Epoch: 65 Batch: 2900
Training Loss: 0.010945904850959778
Epoch: 65 Batch: 2950
Training Loss: 0.011040070541834427
Epoch: 65 Batch: 3000
Training Loss: 0.010429328312476477
Epoch: 65 Batch: 3050
Training Loss: 0.010364454447245989
Epoch: 65 Batch: 3100
Training Loss: 0.010146018247450551
Epoch: 65 Batch: 3150
Training Loss: 0.010436797473165725
Epoch: 65 Batch: 3200
Training Loss: 0.009853734681382776
Epoch: 66 
 Validation Loss: 0.4868524031506644
---------------------------
Epoch: 66 Batch: 50
Training Loss: 0.6549522387981415
Epoch: 66 Batch: 100
Training Loss: 0.3236749118566513
Epoch: 66 Batch: 150
Training Loss: 0.2089295760790507
Epoch: 66 Batch: 200
Training Loss: 0.15157293424010276
Epoch: 66 Batch: 250
Training Loss: 0.12720368659496306
Epoch: 66 Batch: 300
Training Loss: 0.10466722269852956
Epoch: 66 Batch: 350
Training Loss: 0.09201695731707982
Epoch: 66 Batch: 400
Training Loss: 0.07696019299328327
Epoch: 66 Batch: 450
Training Loss: 0.06946458809905583
Epoch: 66 Batch: 500
Training Loss: 0.06079270374774933
Epoch: 66 Batch: 550
Training Loss: 0.05982759399847551
Epoch: 66 Batch: 600
Training Loss: 0.05340789616107941
Epoch: 66 Batch: 650
Training Loss: 0.04653683589054988
Epoch: 66 Batch: 700
Training Loss: 0.0457694605418614
Epoch: 66 Batch: 750
Training Loss: 0.040707043846448264
Epoch: 66 Batch: 800
Training Loss: 0.041025197282433506
Epoch: 66 Batch: 850
Training Loss: 0.03719489171224482
Epoch: 66 Batch: 900
Training Loss: 0.03428234951363669
Epoch: 66 Batch: 950
Training Loss: 0.03384964309240642
Epoch: 66 Batch: 1000
Training Loss: 0.03196679198741913
Epoch: 66 Batch: 1050
Training Loss: 0.03125426014264425
Epoch: 66 Batch: 1100
Training Loss: 0.026669192530892113
Epoch: 66 Batch: 1150
Training Loss: 0.028227745761042054
Epoch: 66 Batch: 1200
Training Loss: 0.025747156490882238
Epoch: 66 Batch: 1250
Training Loss: 0.024561357927322388
Epoch: 66 Batch: 1300
Training Loss: 0.024179514142183157
Epoch: 66 Batch: 1350
Training Loss: 0.024412461011498063
Epoch: 66 Batch: 1400
Training Loss: 0.02156490370631218
Epoch: 66 Batch: 1450
Training Loss: 0.022368502904628884
Epoch: 66 Batch: 1500
Training Loss: 0.02055956635872523
Epoch: 66 Batch: 1550
Training Loss: 0.020121917801518594
Epoch: 66 Batch: 1600
Training Loss: 0.01994030363857746
Epoch: 66 Batch: 1650
Training Loss: 0.0197375150160356
Epoch: 66 Batch: 1700
Training Loss: 0.01925117000060923
Epoch: 66 Batch: 1750
Training Loss: 0.01751458919048309
Epoch: 66 Batch: 1800
Training Loss: 0.017562601764996848
Epoch: 66 Batch: 1850
Training Loss: 0.016517101461822923
Epoch: 66 Batch: 1900
Training Loss: 0.01661029400009858
Epoch: 66 Batch: 1950
Training Loss: 0.015585804123144883
Epoch: 66 Batch: 2000
Training Loss: 0.01563323576748371
Epoch: 66 Batch: 2050
Training Loss: 0.014647365663109755
Epoch: 66 Batch: 2100
Training Loss: 0.014926958240213848
Epoch: 66 Batch: 2150
Training Loss: 0.014994926106098086
Epoch: 66 Batch: 2200
Training Loss: 0.014053421034054322
Epoch: 66 Batch: 2250
Training Loss: 0.01444284431139628
Epoch: 66 Batch: 2300
Training Loss: 0.01362352914136389
Epoch: 66 Batch: 2350
Training Loss: 0.013541575606833113
Epoch: 66 Batch: 2400
Training Loss: 0.013150736006597679
Epoch: 66 Batch: 2450
Training Loss: 0.012714101732993612
Epoch: 66 Batch: 2500
Training Loss: 0.013181454980373382
Epoch: 66 Batch: 2550
Training Loss: 0.012535822601879344
Epoch: 66 Batch: 2600
Training Loss: 0.012573071592129195
Epoch: 66 Batch: 2650
Training Loss: 0.012032745829168356
Epoch: 66 Batch: 2700
Training Loss: 0.012009796323599638
Epoch: 66 Batch: 2750
Training Loss: 0.011124729676680132
Epoch: 66 Batch: 2800
Training Loss: 0.011148995384573937
Epoch: 66 Batch: 2850
Training Loss: 0.010945702362478825
Epoch: 66 Batch: 2900
Training Loss: 0.01098460405037321
Epoch: 66 Batch: 2950
Training Loss: 0.010854689812256117
Epoch: 66 Batch: 3000
Training Loss: 0.010625339517990749
Epoch: 66 Batch: 3050
Training Loss: 0.010419424928602625
Epoch: 66 Batch: 3100
Training Loss: 0.010371640757206948
Epoch: 66 Batch: 3150
Training Loss: 0.010413629951931182
Epoch: 66 Batch: 3200
Training Loss: 0.009679073067381978
Epoch: 67 
 Validation Loss: 0.4869071430630154
---------------------------
Epoch: 67 Batch: 50
Training Loss: 0.6562364023923873
Epoch: 67 Batch: 100
Training Loss: 0.30751244992017746
Epoch: 67 Batch: 150
Training Loss: 0.21458604156970978
Epoch: 67 Batch: 200
Training Loss: 0.16499742731451988
Epoch: 67 Batch: 250
Training Loss: 0.1243283270597458
Epoch: 67 Batch: 300
Training Loss: 0.1055632108449936
Epoch: 67 Batch: 350
Training Loss: 0.0922560544524874
Epoch: 67 Batch: 400
Training Loss: 0.07846293285489082
Epoch: 67 Batch: 450
Training Loss: 0.07051484968927171
Epoch: 67 Batch: 500
Training Loss: 0.06329852044582367
Epoch: 67 Batch: 550
Training Loss: 0.05733736368742856
Epoch: 67 Batch: 600
Training Loss: 0.05243031938870748
Epoch: 67 Batch: 650
Training Loss: 0.04911738689129169
Epoch: 67 Batch: 700
Training Loss: 0.044732683300971986
Epoch: 67 Batch: 750
Training Loss: 0.0418156996568044
Epoch: 67 Batch: 800
Training Loss: 0.03789912387728691
Epoch: 67 Batch: 850
Training Loss: 0.037938647375387304
Epoch: 67 Batch: 900
Training Loss: 0.03318748007218043
Epoch: 67 Batch: 950
Training Loss: 0.03421066622985037
Epoch: 67 Batch: 1000
Training Loss: 0.031631051391363144
Epoch: 67 Batch: 1050
Training Loss: 0.029709390628905524
Epoch: 67 Batch: 1100
Training Loss: 0.029474425424229015
Epoch: 67 Batch: 1150
Training Loss: 0.02742124985093656
Epoch: 67 Batch: 1200
Training Loss: 0.025580247739950816
Epoch: 67 Batch: 1250
Training Loss: 0.02550316879749298
Epoch: 67 Batch: 1300
Training Loss: 0.024780677694540756
Epoch: 67 Batch: 1350
Training Loss: 0.023929575483004253
Epoch: 67 Batch: 1400
Training Loss: 0.02258678246821676
Epoch: 67 Batch: 1450
Training Loss: 0.021349776942154456
Epoch: 67 Batch: 1500
Training Loss: 0.02123590809106827
Epoch: 67 Batch: 1550
Training Loss: 0.018814504781076984
Epoch: 67 Batch: 1600
Training Loss: 0.018772748913615942
Epoch: 67 Batch: 1650
Training Loss: 0.019701456879124498
Epoch: 67 Batch: 1700
Training Loss: 0.018302533153225393
Epoch: 67 Batch: 1750
Training Loss: 0.018241506900106157
Epoch: 67 Batch: 1800
Training Loss: 0.01726968013577991
Epoch: 67 Batch: 1850
Training Loss: 0.017292088254078016
Epoch: 67 Batch: 1900
Training Loss: 0.016600977198073737
Epoch: 67 Batch: 1950
Training Loss: 0.015950335218356204
Epoch: 67 Batch: 2000
Training Loss: 0.016090117216110228
Epoch: 67 Batch: 2050
Training Loss: 0.014675292459929861
Epoch: 67 Batch: 2100
Training Loss: 0.015354576167606172
Epoch: 67 Batch: 2150
Training Loss: 0.014170103669166566
Epoch: 67 Batch: 2200
Training Loss: 0.014109984527934681
Epoch: 67 Batch: 2250
Training Loss: 0.013848083681530423
Epoch: 67 Batch: 2300
Training Loss: 0.013533589528954547
Epoch: 67 Batch: 2350
Training Loss: 0.013624959182231984
Epoch: 67 Batch: 2400
Training Loss: 0.013475164175033569
Epoch: 67 Batch: 2450
Training Loss: 0.01312383756345632
Epoch: 67 Batch: 2500
Training Loss: 0.012610879266262054
Epoch: 67 Batch: 2550
Training Loss: 0.0120634972114189
Epoch: 67 Batch: 2600
Training Loss: 0.012451473073317454
Epoch: 67 Batch: 2650
Training Loss: 0.012255391635984745
Epoch: 67 Batch: 2700
Training Loss: 0.011896887101508952
Epoch: 67 Batch: 2750
Training Loss: 0.011244532758539373
Epoch: 67 Batch: 2800
Training Loss: 0.011370628646441868
Epoch: 67 Batch: 2850
Training Loss: 0.011127082893722936
Epoch: 67 Batch: 2900
Training Loss: 0.01109812770424218
Epoch: 67 Batch: 2950
Training Loss: 0.010592289344739107
Epoch: 67 Batch: 3000
Training Loss: 0.010356885174910227
Epoch: 67 Batch: 3050
Training Loss: 0.010113657146203714
Epoch: 67 Batch: 3100
Training Loss: 0.010650874328228735
Epoch: 67 Batch: 3150
Training Loss: 0.009622723914328075
Epoch: 67 Batch: 3200
Training Loss: 0.009404615508392453
Epoch: 68 
 Validation Loss: 0.4864427516857783
---------------------------
Epoch: 68 Batch: 50
Training Loss: 0.6717434227466583
Epoch: 68 Batch: 100
Training Loss: 0.32960390746593476
Epoch: 68 Batch: 150
Training Loss: 0.20076832632223765
Epoch: 68 Batch: 200
Training Loss: 0.15699841514229773
Epoch: 68 Batch: 250
Training Loss: 0.12365948283672333
Epoch: 68 Batch: 300
Training Loss: 0.11308588733275732
Epoch: 68 Batch: 350
Training Loss: 0.08877239993640355
Epoch: 68 Batch: 400
Training Loss: 0.08253688424825668
Epoch: 68 Batch: 450
Training Loss: 0.07214304599497054
Epoch: 68 Batch: 500
Training Loss: 0.06463828778266907
Epoch: 68 Batch: 550
Training Loss: 0.05602074688131159
Epoch: 68 Batch: 600
Training Loss: 0.05099029724796613
Epoch: 68 Batch: 650
Training Loss: 0.04910556495189667
Epoch: 68 Batch: 700
Training Loss: 0.04509026714733669
Epoch: 68 Batch: 750
Training Loss: 0.04287541695435842
Epoch: 68 Batch: 800
Training Loss: 0.039684536121785643
Epoch: 68 Batch: 850
Training Loss: 0.038507056586882644
Epoch: 68 Batch: 900
Training Loss: 0.03494360195265876
Epoch: 68 Batch: 950
Training Loss: 0.03300980141288356
Epoch: 68 Batch: 1000
Training Loss: 0.03139266830682755
Epoch: 68 Batch: 1050
Training Loss: 0.028579680181684948
Epoch: 68 Batch: 1100
Training Loss: 0.027668659714135256
Epoch: 68 Batch: 1150
Training Loss: 0.02781493894431902
Epoch: 68 Batch: 1200
Training Loss: 0.026294354572892187
Epoch: 68 Batch: 1250
Training Loss: 0.024268910956382752
Epoch: 68 Batch: 1300
Training Loss: 0.02510259293592893
Epoch: 68 Batch: 1350
Training Loss: 0.0229779037060561
Epoch: 68 Batch: 1400
Training Loss: 0.02292780571750232
Epoch: 68 Batch: 1450
Training Loss: 0.021613854067078952
Epoch: 68 Batch: 1500
Training Loss: 0.02001621377468109
Epoch: 68 Batch: 1550
Training Loss: 0.020082511478854762
Epoch: 68 Batch: 1600
Training Loss: 0.02011824879795313
Epoch: 68 Batch: 1650
Training Loss: 0.018911340923020332
Epoch: 68 Batch: 1700
Training Loss: 0.01671704656937543
Epoch: 68 Batch: 1750
Training Loss: 0.018920978341783797
Epoch: 68 Batch: 1800
Training Loss: 0.017484398268991046
Epoch: 68 Batch: 1850
Training Loss: 0.01650978275247522
Epoch: 68 Batch: 1900
Training Loss: 0.015648691230698637
Epoch: 68 Batch: 1950
Training Loss: 0.01607565265435439
Epoch: 68 Batch: 2000
Training Loss: 0.01637963292002678
Epoch: 68 Batch: 2050
Training Loss: 0.015358709111446288
Epoch: 68 Batch: 2100
Training Loss: 0.015118662601425534
Epoch: 68 Batch: 2150
Training Loss: 0.01512597379296325
Epoch: 68 Batch: 2200
Training Loss: 0.01403751558878205
Epoch: 68 Batch: 2250
Training Loss: 0.013809975862503052
Epoch: 68 Batch: 2300
Training Loss: 0.01390682780224344
Epoch: 68 Batch: 2350
Training Loss: 0.013221100860453666
Epoch: 68 Batch: 2400
Training Loss: 0.013562838373084862
Epoch: 68 Batch: 2450
Training Loss: 0.012993768344120103
Epoch: 68 Batch: 2500
Training Loss: 0.012754883635044099
Epoch: 68 Batch: 2550
Training Loss: 0.01264818854191724
Epoch: 68 Batch: 2600
Training Loss: 0.012225575435620088
Epoch: 68 Batch: 2650
Training Loss: 0.011869095822550215
Epoch: 68 Batch: 2700
Training Loss: 0.011211972402201759
Epoch: 68 Batch: 2750
Training Loss: 0.01142832963033156
Epoch: 68 Batch: 2800
Training Loss: 0.01127346920115607
Epoch: 68 Batch: 2850
Training Loss: 0.010950616859553152
Epoch: 68 Batch: 2900
Training Loss: 0.011259363930800865
Epoch: 68 Batch: 2950
Training Loss: 0.010946372868651051
Epoch: 68 Batch: 3000
Training Loss: 0.010217497587203979
Epoch: 68 Batch: 3050
Training Loss: 0.010025921991614044
Epoch: 68 Batch: 3100
Training Loss: 0.00992222963802276
Epoch: 68 Batch: 3150
Training Loss: 0.010016714760235378
Epoch: 68 Batch: 3200
Training Loss: 0.01010115741752088
Epoch: 69 
 Validation Loss: 0.48666826585928596
---------------------------
Epoch: 69 Batch: 50
Training Loss: 0.6755736213922501
Epoch: 69 Batch: 100
Training Loss: 0.32402188032865525
Epoch: 69 Batch: 150
Training Loss: 0.19899215479691823
Epoch: 69 Batch: 200
Training Loss: 0.15693643257021905
Epoch: 69 Batch: 250
Training Loss: 0.13450774478912353
Epoch: 69 Batch: 300
Training Loss: 0.10374672800302505
Epoch: 69 Batch: 350
Training Loss: 0.08797294786998204
Epoch: 69 Batch: 400
Training Loss: 0.0807504427433014
Epoch: 69 Batch: 450
Training Loss: 0.07230724228752984
Epoch: 69 Batch: 500
Training Loss: 0.06303510868549347
Epoch: 69 Batch: 550
Training Loss: 0.06004957323724573
Epoch: 69 Batch: 600
Training Loss: 0.05066003988186518
Epoch: 69 Batch: 650
Training Loss: 0.047746532284296477
Epoch: 69 Batch: 700
Training Loss: 0.04269803856100355
Epoch: 69 Batch: 750
Training Loss: 0.041918349146842955
Epoch: 69 Batch: 800
Training Loss: 0.03770853038877249
Epoch: 69 Batch: 850
Training Loss: 0.03664218131233664
Epoch: 69 Batch: 900
Training Loss: 0.03491942243443595
Epoch: 69 Batch: 950
Training Loss: 0.033055379422087416
Epoch: 69 Batch: 1000
Training Loss: 0.031288483202457425
Epoch: 69 Batch: 1050
Training Loss: 0.030146952612059456
Epoch: 69 Batch: 1100
Training Loss: 0.0295048281008547
Epoch: 69 Batch: 1150
Training Loss: 0.027585833072662352
Epoch: 69 Batch: 1200
Training Loss: 0.025110197141766547
Epoch: 69 Batch: 1250
Training Loss: 0.025938545322418214
Epoch: 69 Batch: 1300
Training Loss: 0.02538152025296138
Epoch: 69 Batch: 1350
Training Loss: 0.022804727068653812
Epoch: 69 Batch: 1400
Training Loss: 0.022213237349476132
Epoch: 69 Batch: 1450
Training Loss: 0.021436688509480707
Epoch: 69 Batch: 1500
Training Loss: 0.020219574570655822
Epoch: 69 Batch: 1550
Training Loss: 0.020408646368211315
Epoch: 69 Batch: 1600
Training Loss: 0.019380882456898688
Epoch: 69 Batch: 1650
Training Loss: 0.01948127511775855
Epoch: 69 Batch: 1700
Training Loss: 0.01859424897853066
Epoch: 69 Batch: 1750
Training Loss: 0.01803034154006413
Epoch: 69 Batch: 1800
Training Loss: 0.017573491467369928
Epoch: 69 Batch: 1850
Training Loss: 0.017240139181549485
Epoch: 69 Batch: 1900
Training Loss: 0.01563709817434612
Epoch: 69 Batch: 1950
Training Loss: 0.016411851399984117
Epoch: 69 Batch: 2000
Training Loss: 0.015810242265462876
Epoch: 69 Batch: 2050
Training Loss: 0.014656760111087706
Epoch: 69 Batch: 2100
Training Loss: 0.015675153065295447
Epoch: 69 Batch: 2150
Training Loss: 0.014465293953585071
Epoch: 69 Batch: 2200
Training Loss: 0.014427151463248513
Epoch: 69 Batch: 2250
Training Loss: 0.014407469749450683
Epoch: 69 Batch: 2300
Training Loss: 0.013654728285644365
Epoch: 69 Batch: 2350
Training Loss: 0.013539795913594834
Epoch: 69 Batch: 2400
Training Loss: 0.012434873307744662
Epoch: 69 Batch: 2450
Training Loss: 0.01268581762605784
Epoch: 69 Batch: 2500
Training Loss: 0.013157399225234985
Epoch: 69 Batch: 2550
Training Loss: 0.012499950528144836
Epoch: 69 Batch: 2600
Training Loss: 0.011892681431311827
Epoch: 69 Batch: 2650
Training Loss: 0.011723776504678545
Epoch: 69 Batch: 2700
Training Loss: 0.011905554135640463
Epoch: 69 Batch: 2750
Training Loss: 0.01138164035840468
Epoch: 69 Batch: 2800
Training Loss: 0.010863074905106
Epoch: 69 Batch: 2850
Training Loss: 0.011258891224861145
Epoch: 69 Batch: 2900
Training Loss: 0.01096946120262146
Epoch: 69 Batch: 2950
Training Loss: 0.010778808320982982
Epoch: 69 Batch: 3000
Training Loss: 0.01054537519812584
Epoch: 69 Batch: 3050
Training Loss: 0.011200387985979924
Epoch: 69 Batch: 3100
Training Loss: 0.010200582656168168
Epoch: 69 Batch: 3150
Training Loss: 0.0100221577239415
Epoch: 69 Batch: 3200
Training Loss: 0.00991758907213807
Epoch: 70 
 Validation Loss: 0.4859892759058211
---------------------------
Epoch: 70 Batch: 50
Training Loss: 0.6350157779455184
Epoch: 70 Batch: 100
Training Loss: 0.3076323339343071
Epoch: 70 Batch: 150
Training Loss: 0.21368012527624766
Epoch: 70 Batch: 200
Training Loss: 0.15877062916755677
Epoch: 70 Batch: 250
Training Loss: 0.12361206424236297
Epoch: 70 Batch: 300
Training Loss: 0.1045351446668307
Epoch: 70 Batch: 350
Training Loss: 0.08797699681350163
Epoch: 70 Batch: 400
Training Loss: 0.08025719992816448
Epoch: 70 Batch: 450
Training Loss: 0.0716198315223058
Epoch: 70 Batch: 500
Training Loss: 0.06405359679460526
Epoch: 70 Batch: 550
Training Loss: 0.05657351504672657
Epoch: 70 Batch: 600
Training Loss: 0.05201422040661176
Epoch: 70 Batch: 650
Training Loss: 0.05000886839169722
Epoch: 70 Batch: 700
Training Loss: 0.04306668549776077
Epoch: 70 Batch: 750
Training Loss: 0.043302319963773095
Epoch: 70 Batch: 800
Training Loss: 0.03763605415821075
Epoch: 70 Batch: 850
Training Loss: 0.03684296039973988
Epoch: 70 Batch: 900
Training Loss: 0.034773444566461774
Epoch: 70 Batch: 950
Training Loss: 0.033219062027178316
Epoch: 70 Batch: 1000
Training Loss: 0.031198764860630036
Epoch: 70 Batch: 1050
Training Loss: 0.030843025900068738
Epoch: 70 Batch: 1100
Training Loss: 0.02789567765864459
Epoch: 70 Batch: 1150
Training Loss: 0.028678525504858596
Epoch: 70 Batch: 1200
Training Loss: 0.026288680185874304
Epoch: 70 Batch: 1250
Training Loss: 0.02427235028743744
Epoch: 70 Batch: 1300
Training Loss: 0.023311813198603117
Epoch: 70 Batch: 1350
Training Loss: 0.022852160731951395
Epoch: 70 Batch: 1400
Training Loss: 0.02320665527667318
Epoch: 70 Batch: 1450
Training Loss: 0.022647526510830584
Epoch: 70 Batch: 1500
Training Loss: 0.020947711944580078
Epoch: 70 Batch: 1550
Training Loss: 0.019471210279772357
Epoch: 70 Batch: 1600
Training Loss: 0.01935842052102089
Epoch: 70 Batch: 1650
Training Loss: 0.019173586585304953
Epoch: 70 Batch: 1700
Training Loss: 0.01847441703081131
Epoch: 70 Batch: 1750
Training Loss: 0.01680364498070308
Epoch: 70 Batch: 1800
Training Loss: 0.01729519021179941
Epoch: 70 Batch: 1850
Training Loss: 0.017389394785906818
Epoch: 70 Batch: 1900
Training Loss: 0.016970552723658713
Epoch: 70 Batch: 1950
Training Loss: 0.016750213549687313
Epoch: 70 Batch: 2000
Training Loss: 0.015949502512812616
Epoch: 70 Batch: 2050
Training Loss: 0.015270096179915638
Epoch: 70 Batch: 2100
Training Loss: 0.014787290195624034
Epoch: 70 Batch: 2150
Training Loss: 0.014777042214260546
Epoch: 70 Batch: 2200
Training Loss: 0.014413850578394803
Epoch: 70 Batch: 2250
Training Loss: 0.014535078088442484
Epoch: 70 Batch: 2300
Training Loss: 0.01348174819479818
Epoch: 70 Batch: 2350
Training Loss: 0.01327987514911814
Epoch: 70 Batch: 2400
Training Loss: 0.01378148483733336
Epoch: 70 Batch: 2450
Training Loss: 0.012597094202528195
Epoch: 70 Batch: 2500
Training Loss: 0.012175747072696686
Epoch: 70 Batch: 2550
Training Loss: 0.012289322661418542
Epoch: 70 Batch: 2600
Training Loss: 0.011702640045147676
Epoch: 70 Batch: 2650
Training Loss: 0.012407720595035913
Epoch: 70 Batch: 2700
Training Loss: 0.011874389493906939
Epoch: 70 Batch: 2750
Training Loss: 0.011733383720571344
Epoch: 70 Batch: 2800
Training Loss: 0.010874991991690227
Epoch: 70 Batch: 2850
Training Loss: 0.011058314693601507
Epoch: 70 Batch: 2900
Training Loss: 0.010578513227660081
Epoch: 70 Batch: 2950
Training Loss: 0.010682658644045814
Epoch: 70 Batch: 3000
Training Loss: 0.010442060887813569
Epoch: 70 Batch: 3050
Training Loss: 0.010209250411049264
Epoch: 70 Batch: 3100
Training Loss: 0.010122952047855623
Epoch: 70 Batch: 3150
Training Loss: 0.010015113703788273
Epoch: 70 Batch: 3200
Training Loss: 0.009887982765212655
Epoch: 71 
 Validation Loss: 0.4857652435700099
---------------------------
Epoch: 71 Batch: 50
Training Loss: 0.6198945087194443
Epoch: 71 Batch: 100
Training Loss: 0.31040986001491544
Epoch: 71 Batch: 150
Training Loss: 0.21659942547480265
Epoch: 71 Batch: 200
Training Loss: 0.1579515567421913
Epoch: 71 Batch: 250
Training Loss: 0.1312247403860092
Epoch: 71 Batch: 300
Training Loss: 0.10768939127524693
Epoch: 71 Batch: 350
Training Loss: 0.08903767032282693
Epoch: 71 Batch: 400
Training Loss: 0.08089702241122723
Epoch: 71 Batch: 450
Training Loss: 0.0678923613495297
Epoch: 71 Batch: 500
Training Loss: 0.06604638695716858
Epoch: 71 Batch: 550
Training Loss: 0.05770435544577512
Epoch: 71 Batch: 600
Training Loss: 0.05247456764181455
Epoch: 71 Batch: 650
Training Loss: 0.04747450649738312
Epoch: 71 Batch: 700
Training Loss: 0.04684258133172989
Epoch: 71 Batch: 750
Training Loss: 0.042667255759239194
Epoch: 71 Batch: 800
Training Loss: 0.04161295462399721
Epoch: 71 Batch: 850
Training Loss: 0.037656251682954674
Epoch: 71 Batch: 900
Training Loss: 0.03430025448401769
Epoch: 71 Batch: 950
Training Loss: 0.03297027340060786
Epoch: 71 Batch: 1000
Training Loss: 0.032065131068229676
Epoch: 71 Batch: 1050
Training Loss: 0.030190136233965555
Epoch: 71 Batch: 1100
Training Loss: 0.028504253084009345
Epoch: 71 Batch: 1150
Training Loss: 0.027836571024811785
Epoch: 71 Batch: 1200
Training Loss: 0.026594689513246218
Epoch: 71 Batch: 1250
Training Loss: 0.024263806533813475
Epoch: 71 Batch: 1300
Training Loss: 0.026452808127953455
Epoch: 71 Batch: 1350
Training Loss: 0.022244301195497865
Epoch: 71 Batch: 1400
Training Loss: 0.021889043599367143
Epoch: 71 Batch: 1450
Training Loss: 0.020876655085333463
Epoch: 71 Batch: 1500
Training Loss: 0.02078525791565577
Epoch: 71 Batch: 1550
Training Loss: 0.01972621727374292
Epoch: 71 Batch: 1600
Training Loss: 0.019798775557428597
Epoch: 71 Batch: 1650
Training Loss: 0.018574040586298163
Epoch: 71 Batch: 1700
Training Loss: 0.01818302108960993
Epoch: 71 Batch: 1750
Training Loss: 0.018300288438796997
Epoch: 71 Batch: 1800
Training Loss: 0.017461189031600953
Epoch: 71 Batch: 1850
Training Loss: 0.016411241727906305
Epoch: 71 Batch: 1900
Training Loss: 0.017019545592759786
Epoch: 71 Batch: 1950
Training Loss: 0.016282515678650293
Epoch: 71 Batch: 2000
Training Loss: 0.015108828514814376
Epoch: 71 Batch: 2050
Training Loss: 0.015107836636101327
Epoch: 71 Batch: 2100
Training Loss: 0.014864708852200282
Epoch: 71 Batch: 2150
Training Loss: 0.014146123930465344
Epoch: 71 Batch: 2200
Training Loss: 0.014487917043946006
Epoch: 71 Batch: 2250
Training Loss: 0.013950028777122498
Epoch: 71 Batch: 2300
Training Loss: 0.012836225551107656
Epoch: 71 Batch: 2350
Training Loss: 0.013592663445371262
Epoch: 71 Batch: 2400
Training Loss: 0.01290025319904089
Epoch: 71 Batch: 2450
Training Loss: 0.013302324231790036
Epoch: 71 Batch: 2500
Training Loss: 0.012659695243835449
Epoch: 71 Batch: 2550
Training Loss: 0.01246999956813513
Epoch: 71 Batch: 2600
Training Loss: 0.01204477703342071
Epoch: 71 Batch: 2650
Training Loss: 0.012118139907998858
Epoch: 71 Batch: 2700
Training Loss: 0.012231321809468445
Epoch: 71 Batch: 2750
Training Loss: 0.011313359921628779
Epoch: 71 Batch: 2800
Training Loss: 0.01194701571549688
Epoch: 71 Batch: 2850
Training Loss: 0.010917414301320125
Epoch: 71 Batch: 2900
Training Loss: 0.011177379396455042
Epoch: 71 Batch: 2950
Training Loss: 0.01091978273149264
Epoch: 71 Batch: 3000
Training Loss: 0.010265229721864064
Epoch: 71 Batch: 3050
Training Loss: 0.010419195519119013
Epoch: 71 Batch: 3100
Training Loss: 0.010276062161691727
Epoch: 71 Batch: 3150
Training Loss: 0.010078194510369074
Epoch: 71 Batch: 3200
Training Loss: 0.010011750776320695
Epoch: 72 
 Validation Loss: 0.4853116863303714
---------------------------
Epoch: 72 Batch: 50
Training Loss: 0.6216394120454788
Epoch: 72 Batch: 100
Training Loss: 0.31881838202476503
Epoch: 72 Batch: 150
Training Loss: 0.2117162694533666
Epoch: 72 Batch: 200
Training Loss: 0.15836602836847305
Epoch: 72 Batch: 250
Training Loss: 0.1250494021177292
Epoch: 72 Batch: 300
Training Loss: 0.10826782355705897
Epoch: 72 Batch: 350
Training Loss: 0.08782826031957354
Epoch: 72 Batch: 400
Training Loss: 0.07846382357180119
Epoch: 72 Batch: 450
Training Loss: 0.0717394890387853
Epoch: 72 Batch: 500
Training Loss: 0.06522182780504227
Epoch: 72 Batch: 550
Training Loss: 0.05822153546593406
Epoch: 72 Batch: 600
Training Loss: 0.051750449985265734
Epoch: 72 Batch: 650
Training Loss: 0.04781866064438453
Epoch: 72 Batch: 700
Training Loss: 0.04595057977097375
Epoch: 72 Batch: 750
Training Loss: 0.043397760589917504
Epoch: 72 Batch: 800
Training Loss: 0.038494416512548923
Epoch: 72 Batch: 850
Training Loss: 0.03521103992181666
Epoch: 72 Batch: 900
Training Loss: 0.03308451169066959
Epoch: 72 Batch: 950
Training Loss: 0.03208340221329739
Epoch: 72 Batch: 1000
Training Loss: 0.03290177363157272
Epoch: 72 Batch: 1050
Training Loss: 0.02936592113404047
Epoch: 72 Batch: 1100
Training Loss: 0.02927714225920764
Epoch: 72 Batch: 1150
Training Loss: 0.02840669212133988
Epoch: 72 Batch: 1200
Training Loss: 0.02707378548880418
Epoch: 72 Batch: 1250
Training Loss: 0.025269879078865052
Epoch: 72 Batch: 1300
Training Loss: 0.024023146010362185
Epoch: 72 Batch: 1350
Training Loss: 0.021771322908224883
Epoch: 72 Batch: 1400
Training Loss: 0.022326984916414532
Epoch: 72 Batch: 1450
Training Loss: 0.02264353828183536
Epoch: 72 Batch: 1500
Training Loss: 0.021111194570859273
Epoch: 72 Batch: 1550
Training Loss: 0.020659210278141882
Epoch: 72 Batch: 1600
Training Loss: 0.01948530787602067
Epoch: 72 Batch: 1650
Training Loss: 0.018731252334334633
Epoch: 72 Batch: 1700
Training Loss: 0.017808115429737988
Epoch: 72 Batch: 1750
Training Loss: 0.018383602789470127
Epoch: 72 Batch: 1800
Training Loss: 0.01719723108741972
Epoch: 72 Batch: 1850
Training Loss: 0.01686326679345724
Epoch: 72 Batch: 1900
Training Loss: 0.016198288092487738
Epoch: 72 Batch: 1950
Training Loss: 0.016086139984619923
Epoch: 72 Batch: 2000
Training Loss: 0.01571377980709076
Epoch: 72 Batch: 2050
Training Loss: 0.015420667267427211
Epoch: 72 Batch: 2100
Training Loss: 0.015050565557820457
Epoch: 72 Batch: 2150
Training Loss: 0.014203118363092113
Epoch: 72 Batch: 2200
Training Loss: 0.014862861917777494
Epoch: 72 Batch: 2250
Training Loss: 0.013922183169258966
Epoch: 72 Batch: 2300
Training Loss: 0.013590878131596939
Epoch: 72 Batch: 2350
Training Loss: 0.012651944464825569
Epoch: 72 Batch: 2400
Training Loss: 0.013088783696293831
Epoch: 72 Batch: 2450
Training Loss: 0.012942168226047438
Epoch: 72 Batch: 2500
Training Loss: 0.01242335613965988
Epoch: 72 Batch: 2550
Training Loss: 0.011968575973136752
Epoch: 72 Batch: 2600
Training Loss: 0.01155612594806231
Epoch: 72 Batch: 2650
Training Loss: 0.011513753915732762
Epoch: 72 Batch: 2700
Training Loss: 0.011547771405290674
Epoch: 72 Batch: 2750
Training Loss: 0.011592167149890554
Epoch: 72 Batch: 2800
Training Loss: 0.011712900346943311
Epoch: 72 Batch: 2850
Training Loss: 0.010940752343127602
Epoch: 72 Batch: 2900
Training Loss: 0.010779936097819229
Epoch: 72 Batch: 2950
Training Loss: 0.010510880654141054
Epoch: 72 Batch: 3000
Training Loss: 0.010169497857491175
Epoch: 72 Batch: 3050
Training Loss: 0.010928186217292411
Epoch: 72 Batch: 3100
Training Loss: 0.010048993749003256
Epoch: 72 Batch: 3150
Training Loss: 0.009637686960280887
Epoch: 72 Batch: 3200
Training Loss: 0.009789819438010454
Epoch: 73 
 Validation Loss: 0.4851861870951123
---------------------------
Epoch: 73 Batch: 50
Training Loss: 0.6281008237600326
Epoch: 73 Batch: 100
Training Loss: 0.3181428483128548
Epoch: 73 Batch: 150
Training Loss: 0.21468015571435292
Epoch: 73 Batch: 200
Training Loss: 0.15708105653524398
Epoch: 73 Batch: 250
Training Loss: 0.1292899134159088
Epoch: 73 Batch: 300
Training Loss: 0.10522162208954493
Epoch: 73 Batch: 350
Training Loss: 0.09167656915528434
Epoch: 73 Batch: 400
Training Loss: 0.07710929743945599
Epoch: 73 Batch: 450
Training Loss: 0.0743703994486067
Epoch: 73 Batch: 500
Training Loss: 0.06387175089120865
Epoch: 73 Batch: 550
Training Loss: 0.058310261043635285
Epoch: 73 Batch: 600
Training Loss: 0.053234779884417854
Epoch: 73 Batch: 650
Training Loss: 0.04852344820132622
Epoch: 73 Batch: 700
Training Loss: 0.04363334413085665
Epoch: 73 Batch: 750
Training Loss: 0.04223137311140696
Epoch: 73 Batch: 800
Training Loss: 0.03897620439529419
Epoch: 73 Batch: 850
Training Loss: 0.03794374195968404
Epoch: 73 Batch: 900
Training Loss: 0.0338261111246215
Epoch: 73 Batch: 950
Training Loss: 0.033957392949807014
Epoch: 73 Batch: 1000
Training Loss: 0.03202122765779495
Epoch: 73 Batch: 1050
Training Loss: 0.028953256834120977
Epoch: 73 Batch: 1100
Training Loss: 0.02848193569616838
Epoch: 73 Batch: 1150
Training Loss: 0.028008424665616906
Epoch: 73 Batch: 1200
Training Loss: 0.02523041476806005
Epoch: 73 Batch: 1250
Training Loss: 0.025964442491531372
Epoch: 73 Batch: 1300
Training Loss: 0.02367646820270098
Epoch: 73 Batch: 1350
Training Loss: 0.02278168561281981
Epoch: 73 Batch: 1400
Training Loss: 0.02184812373348645
Epoch: 73 Batch: 1450
Training Loss: 0.021445132946145945
Epoch: 73 Batch: 1500
Training Loss: 0.021685484985510508
Epoch: 73 Batch: 1550
Training Loss: 0.020089601432123492
Epoch: 73 Batch: 1600
Training Loss: 0.020972560904920102
Epoch: 73 Batch: 1650
Training Loss: 0.01930790760300376
Epoch: 73 Batch: 1700
Training Loss: 0.018736046605250415
Epoch: 73 Batch: 1750
Training Loss: 0.018737631627491542
Epoch: 73 Batch: 1800
Training Loss: 0.018220038844479456
Epoch: 73 Batch: 1850
Training Loss: 0.016129250687521857
Epoch: 73 Batch: 1900
Training Loss: 0.01616008482481304
Epoch: 73 Batch: 1950
Training Loss: 0.016848368598864627
Epoch: 73 Batch: 2000
Training Loss: 0.01580707147717476
Epoch: 73 Batch: 2050
Training Loss: 0.014715961726700387
Epoch: 73 Batch: 2100
Training Loss: 0.014618333407810756
Epoch: 73 Batch: 2150
Training Loss: 0.014413785185924797
Epoch: 73 Batch: 2200
Training Loss: 0.014799748848785054
Epoch: 73 Batch: 2250
Training Loss: 0.013212777270211114
Epoch: 73 Batch: 2300
Training Loss: 0.013572545556918435
Epoch: 73 Batch: 2350
Training Loss: 0.01370329536022024
Epoch: 73 Batch: 2400
Training Loss: 0.013203832171857357
Epoch: 73 Batch: 2450
Training Loss: 0.012951885765912581
Epoch: 73 Batch: 2500
Training Loss: 0.012511527633666992
Epoch: 73 Batch: 2550
Training Loss: 0.011793853593807595
Epoch: 73 Batch: 2600
Training Loss: 0.012103564108793551
Epoch: 73 Batch: 2650
Training Loss: 0.011820699718763244
Epoch: 73 Batch: 2700
Training Loss: 0.011485112916540216
Epoch: 73 Batch: 2750
Training Loss: 0.0115457252480767
Epoch: 73 Batch: 2800
Training Loss: 0.01139823640031474
Epoch: 73 Batch: 2850
Training Loss: 0.010940404001035188
Epoch: 73 Batch: 2900
Training Loss: 0.010336253447779293
Epoch: 73 Batch: 2950
Training Loss: 0.010336892220933558
Epoch: 73 Batch: 3000
Training Loss: 0.010132514417171478
Epoch: 73 Batch: 3050
Training Loss: 0.010472976682616061
Epoch: 73 Batch: 3100
Training Loss: 0.009868316467731229
Epoch: 73 Batch: 3150
Training Loss: 0.009936335569336301
Epoch: 73 Batch: 3200
Training Loss: 0.009586043488234282
Epoch: 74 
 Validation Loss: 0.4852688882086012
---------------------------
Epoch: 74 Batch: 50
Training Loss: 0.6572245514392853
Epoch: 74 Batch: 100
Training Loss: 0.31904044687747957
Epoch: 74 Batch: 150
Training Loss: 0.21905587097009024
Epoch: 74 Batch: 200
Training Loss: 0.15222588717937469
Epoch: 74 Batch: 250
Training Loss: 0.12578535997867585
Epoch: 74 Batch: 300
Training Loss: 0.10115235795577367
Epoch: 74 Batch: 350
Training Loss: 0.08795189040047782
Epoch: 74 Batch: 400
Training Loss: 0.07823452048003673
Epoch: 74 Batch: 450
Training Loss: 0.06936765995290545
Epoch: 74 Batch: 500
Training Loss: 0.06281478887796402
Epoch: 74 Batch: 550
Training Loss: 0.057902621789412065
Epoch: 74 Batch: 600
Training Loss: 0.05406779552499453
Epoch: 74 Batch: 650
Training Loss: 0.047669076919555665
Epoch: 74 Batch: 700
Training Loss: 0.042984339467116765
Epoch: 74 Batch: 750
Training Loss: 0.04242687447865804
Epoch: 74 Batch: 800
Training Loss: 0.04103991232812405
Epoch: 74 Batch: 850
Training Loss: 0.03844559914925519
Epoch: 74 Batch: 900
Training Loss: 0.03587395446168052
Epoch: 74 Batch: 950
Training Loss: 0.03263220322759528
Epoch: 74 Batch: 1000
Training Loss: 0.03136906057596207
Epoch: 74 Batch: 1050
Training Loss: 0.029954917260578702
Epoch: 74 Batch: 1100
Training Loss: 0.02816323247822848
Epoch: 74 Batch: 1150
Training Loss: 0.02707303560298422
Epoch: 74 Batch: 1200
Training Loss: 0.026541186074415843
Epoch: 74 Batch: 1250
Training Loss: 0.024863345766067507
Epoch: 74 Batch: 1300
Training Loss: 0.023615782513068274
Epoch: 74 Batch: 1350
Training Loss: 0.02268769480563976
Epoch: 74 Batch: 1400
Training Loss: 0.023841783510787147
Epoch: 74 Batch: 1450
Training Loss: 0.021696809468598203
Epoch: 74 Batch: 1500
Training Loss: 0.021385020891825358
Epoch: 74 Batch: 1550
Training Loss: 0.020351871175150717
Epoch: 74 Batch: 1600
Training Loss: 0.01998871149495244
Epoch: 74 Batch: 1650
Training Loss: 0.018176833029949303
Epoch: 74 Batch: 1700
Training Loss: 0.0173742256620351
Epoch: 74 Batch: 1750
Training Loss: 0.01824542478152684
Epoch: 74 Batch: 1800
Training Loss: 0.01731962752011087
Epoch: 74 Batch: 1850
Training Loss: 0.0170904303885795
Epoch: 74 Batch: 1900
Training Loss: 0.01641142194208346
Epoch: 74 Batch: 1950
Training Loss: 0.01621297414486225
Epoch: 74 Batch: 2000
Training Loss: 0.01519965235888958
Epoch: 74 Batch: 2050
Training Loss: 0.015509946578886451
Epoch: 74 Batch: 2100
Training Loss: 0.01565805978718258
Epoch: 74 Batch: 2150
Training Loss: 0.014458817368329957
Epoch: 74 Batch: 2200
Training Loss: 0.014522333605722948
Epoch: 74 Batch: 2250
Training Loss: 0.014924367189407348
Epoch: 74 Batch: 2300
Training Loss: 0.01445379543563594
Epoch: 74 Batch: 2350
Training Loss: 0.013891585253654642
Epoch: 74 Batch: 2400
Training Loss: 0.01307627140233914
Epoch: 74 Batch: 2450
Training Loss: 0.012043809221715343
Epoch: 74 Batch: 2500
Training Loss: 0.011942783093452454
Epoch: 74 Batch: 2550
Training Loss: 0.013159311998124217
Epoch: 74 Batch: 2600
Training Loss: 0.01195792073240647
Epoch: 74 Batch: 2650
Training Loss: 0.011142046755214907
Epoch: 74 Batch: 2700
Training Loss: 0.011862608128123814
Epoch: 74 Batch: 2750
Training Loss: 0.010886066089976918
Epoch: 74 Batch: 2800
Training Loss: 0.011285103814942497
Epoch: 74 Batch: 2850
Training Loss: 0.010358697060953107
Epoch: 74 Batch: 2900
Training Loss: 0.010694887823071974
Epoch: 74 Batch: 2950
Training Loss: 0.010168538376436395
Epoch: 74 Batch: 3000
Training Loss: 0.010934679488341014
Epoch: 74 Batch: 3050
Training Loss: 0.010292008968650318
Epoch: 74 Batch: 3100
Training Loss: 0.010117529255728569
Epoch: 74 Batch: 3150
Training Loss: 0.010158725484969124
Epoch: 74 Batch: 3200
Training Loss: 0.010058170463889838
Epoch: 75 
 Validation Loss: 0.48465152349736956
---------------------------
Epoch: 75 Batch: 50
Training Loss: 0.621453355550766
Epoch: 75 Batch: 100
Training Loss: 0.31630133867263793
Epoch: 75 Batch: 150
Training Loss: 0.20793006738026937
Epoch: 75 Batch: 200
Training Loss: 0.15596575200557708
Epoch: 75 Batch: 250
Training Loss: 0.12981145548820497
Epoch: 75 Batch: 300
Training Loss: 0.10390053977568944
Epoch: 75 Batch: 350
Training Loss: 0.09279559884752546
Epoch: 75 Batch: 400
Training Loss: 0.07478827476501465
Epoch: 75 Batch: 450
Training Loss: 0.067980917096138
Epoch: 75 Batch: 500
Training Loss: 0.061878942787647245
Epoch: 75 Batch: 550
Training Loss: 0.05921248430555517
Epoch: 75 Batch: 600
Training Loss: 0.0524413126707077
Epoch: 75 Batch: 650
Training Loss: 0.04973963100176591
Epoch: 75 Batch: 700
Training Loss: 0.04592645334345954
Epoch: 75 Batch: 750
Training Loss: 0.04140606470902761
Epoch: 75 Batch: 800
Training Loss: 0.040087586045265196
Epoch: 75 Batch: 850
Training Loss: 0.03508647217470057
Epoch: 75 Batch: 900
Training Loss: 0.03583893862035539
Epoch: 75 Batch: 950
Training Loss: 0.03287612309581355
Epoch: 75 Batch: 1000
Training Loss: 0.03200911554694176
Epoch: 75 Batch: 1050
Training Loss: 0.028934105067026046
Epoch: 75 Batch: 1100
Training Loss: 0.028502995778213847
Epoch: 75 Batch: 1150
Training Loss: 0.028026842760003133
Epoch: 75 Batch: 1200
Training Loss: 0.02612965404987335
Epoch: 75 Batch: 1250
Training Loss: 0.026821727418899537
Epoch: 75 Batch: 1300
Training Loss: 0.023688299243266767
Epoch: 75 Batch: 1350
Training Loss: 0.023384065517672786
Epoch: 75 Batch: 1400
Training Loss: 0.0215408641738551
Epoch: 75 Batch: 1450
Training Loss: 0.021958718690378912
Epoch: 75 Batch: 1500
Training Loss: 0.020992862661679587
Epoch: 75 Batch: 1550
Training Loss: 0.01950001780063875
Epoch: 75 Batch: 1600
Training Loss: 0.01950962161645293
Epoch: 75 Batch: 1650
Training Loss: 0.018478379646937052
Epoch: 75 Batch: 1700
Training Loss: 0.019304563841399024
Epoch: 75 Batch: 1750
Training Loss: 0.017345408269337247
Epoch: 75 Batch: 1800
Training Loss: 0.018093448844220903
Epoch: 75 Batch: 1850
Training Loss: 0.01613666621414391
Epoch: 75 Batch: 1900
Training Loss: 0.0161034505147683
Epoch: 75 Batch: 1950
Training Loss: 0.015687051782241234
Epoch: 75 Batch: 2000
Training Loss: 0.01612097266316414
Epoch: 75 Batch: 2050
Training Loss: 0.015359619492437781
Epoch: 75 Batch: 2100
Training Loss: 0.015026743000461942
Epoch: 75 Batch: 2150
Training Loss: 0.014076176576836165
Epoch: 75 Batch: 2200
Training Loss: 0.014043085277080536
Epoch: 75 Batch: 2250
Training Loss: 0.014491478509373134
Epoch: 75 Batch: 2300
Training Loss: 0.013242784310942111
Epoch: 75 Batch: 2350
Training Loss: 0.013797422089475266
Epoch: 75 Batch: 2400
Training Loss: 0.012780889372030894
Epoch: 75 Batch: 2450
Training Loss: 0.013456723750854025
Epoch: 75 Batch: 2500
Training Loss: 0.012315157568454742
Epoch: 75 Batch: 2550
Training Loss: 0.013692140625972373
Epoch: 75 Batch: 2600
Training Loss: 0.01247658338684302
Epoch: 75 Batch: 2650
Training Loss: 0.011961344649206918
Epoch: 75 Batch: 2700
Training Loss: 0.01146916690799925
Epoch: 75 Batch: 2750
Training Loss: 0.011550871794874018
Epoch: 75 Batch: 2800
Training Loss: 0.010957049291048732
Epoch: 75 Batch: 2850
Training Loss: 0.01118214626061289
Epoch: 75 Batch: 2900
Training Loss: 0.01050078680803036
Epoch: 75 Batch: 2950
Training Loss: 0.01053455762943979
Epoch: 75 Batch: 3000
Training Loss: 0.01043486479918162
Epoch: 75 Batch: 3050
Training Loss: 0.010740415629793386
Epoch: 75 Batch: 3100
Training Loss: 0.01012117862701416
Epoch: 75 Batch: 3150
Training Loss: 0.01031359873120747
Epoch: 75 Batch: 3200
Training Loss: 0.009992216499522328
Epoch: 76 
 Validation Loss: 0.4839974294106166
---------------------------
Epoch: 76 Batch: 50
Training Loss: 0.6481924176216125
Epoch: 76 Batch: 100
Training Loss: 0.30805521696805954
Epoch: 76 Batch: 150
Training Loss: 0.20900208711624146
Epoch: 76 Batch: 200
Training Loss: 0.1585642570257187
Epoch: 76 Batch: 250
Training Loss: 0.11771619737148285
Epoch: 76 Batch: 300
Training Loss: 0.10744556764761606
Epoch: 76 Batch: 350
Training Loss: 0.08835103520325252
Epoch: 76 Batch: 400
Training Loss: 0.07644751235842705
Epoch: 76 Batch: 450
Training Loss: 0.06949072519938151
Epoch: 76 Batch: 500
Training Loss: 0.05912064850330353
Epoch: 76 Batch: 550
Training Loss: 0.05733592624014074
Epoch: 76 Batch: 600
Training Loss: 0.05237995261947314
Epoch: 76 Batch: 650
Training Loss: 0.04684912626559918
Epoch: 76 Batch: 700
Training Loss: 0.04223778464964458
Epoch: 76 Batch: 750
Training Loss: 0.044157631397247314
Epoch: 76 Batch: 800
Training Loss: 0.03749140497297049
Epoch: 76 Batch: 850
Training Loss: 0.03843441584530999
Epoch: 76 Batch: 900
Training Loss: 0.033671846754021115
Epoch: 76 Batch: 950
Training Loss: 0.032087220926033826
Epoch: 76 Batch: 1000
Training Loss: 0.030700550734996796
Epoch: 76 Batch: 1050
Training Loss: 0.03072321366696131
Epoch: 76 Batch: 1100
Training Loss: 0.029461740932681346
Epoch: 76 Batch: 1150
Training Loss: 0.027546449925588527
Epoch: 76 Batch: 1200
Training Loss: 0.025324068690339725
Epoch: 76 Batch: 1250
Training Loss: 0.026042012476921082
Epoch: 76 Batch: 1300
Training Loss: 0.02416133784330808
Epoch: 76 Batch: 1350
Training Loss: 0.02356257332695855
Epoch: 76 Batch: 1400
Training Loss: 0.022514191908495768
Epoch: 76 Batch: 1450
Training Loss: 0.02123338886376085
Epoch: 76 Batch: 1500
Training Loss: 0.020949293037255606
Epoch: 76 Batch: 1550
Training Loss: 0.02033272679774992
Epoch: 76 Batch: 1600
Training Loss: 0.01938679622486234
Epoch: 76 Batch: 1650
Training Loss: 0.01904093128262144
Epoch: 76 Batch: 1700
Training Loss: 0.018727330477798686
Epoch: 76 Batch: 1750
Training Loss: 0.018070925065449307
Epoch: 76 Batch: 1800
Training Loss: 0.01768934069408311
Epoch: 76 Batch: 1850
Training Loss: 0.017869736922753825
Epoch: 76 Batch: 1900
Training Loss: 0.017361066874704862
Epoch: 76 Batch: 1950
Training Loss: 0.015812783776185453
Epoch: 76 Batch: 2000
Training Loss: 0.015967206403613092
Epoch: 76 Batch: 2050
Training Loss: 0.016120482421502835
Epoch: 76 Batch: 2100
Training Loss: 0.015471574876989637
Epoch: 76 Batch: 2150
Training Loss: 0.015284306642621063
Epoch: 76 Batch: 2200
Training Loss: 0.014346604171124372
Epoch: 76 Batch: 2250
Training Loss: 0.014098785294426812
Epoch: 76 Batch: 2300
Training Loss: 0.013269065696260204
Epoch: 76 Batch: 2350
Training Loss: 0.013571345336893772
Epoch: 76 Batch: 2400
Training Loss: 0.01286739115913709
Epoch: 76 Batch: 2450
Training Loss: 0.012653618369783674
Epoch: 76 Batch: 2500
Training Loss: 0.012544985115528106
Epoch: 76 Batch: 2550
Training Loss: 0.012287948049750983
Epoch: 76 Batch: 2600
Training Loss: 0.011916471880215865
Epoch: 76 Batch: 2650
Training Loss: 0.01221840407488481
Epoch: 76 Batch: 2700
Training Loss: 0.011902302282827872
Epoch: 76 Batch: 2750
Training Loss: 0.01147437351400202
Epoch: 76 Batch: 2800
Training Loss: 0.011413247915250915
Epoch: 76 Batch: 2850
Training Loss: 0.01086170091963651
Epoch: 76 Batch: 2900
Training Loss: 0.011170181570381953
Epoch: 76 Batch: 2950
Training Loss: 0.010634602530527924
Epoch: 76 Batch: 3000
Training Loss: 0.010275699645280838
Epoch: 76 Batch: 3050
Training Loss: 0.010200895989527468
Epoch: 76 Batch: 3100
Training Loss: 0.009981468771734545
Epoch: 76 Batch: 3150
Training Loss: 0.009947578717791845
Epoch: 76 Batch: 3200
Training Loss: 0.009574720738455653
Epoch: 77 
 Validation Loss: 0.48421262900034584
---------------------------
Epoch: 77 Batch: 50
Training Loss: 0.6456158781051635
Epoch: 77 Batch: 100
Training Loss: 0.32085822701454164
Epoch: 77 Batch: 150
Training Loss: 0.21567587435245514
Epoch: 77 Batch: 200
Training Loss: 0.1613778814673424
Epoch: 77 Batch: 250
Training Loss: 0.12173035037517548
Epoch: 77 Batch: 300
Training Loss: 0.104734725356102
Epoch: 77 Batch: 350
Training Loss: 0.0930244539465223
Epoch: 77 Batch: 400
Training Loss: 0.08064020209014416
Epoch: 77 Batch: 450
Training Loss: 0.07282629920376672
Epoch: 77 Batch: 500
Training Loss: 0.0604505021572113
Epoch: 77 Batch: 550
Training Loss: 0.0574200762943788
Epoch: 77 Batch: 600
Training Loss: 0.051519789199034376
Epoch: 77 Batch: 650
Training Loss: 0.04656620658360995
Epoch: 77 Batch: 700
Training Loss: 0.04619617628199713
Epoch: 77 Batch: 750
Training Loss: 0.04009063533941905
Epoch: 77 Batch: 800
Training Loss: 0.03927853263914585
Epoch: 77 Batch: 850
Training Loss: 0.03777426302433014
Epoch: 77 Batch: 900
Training Loss: 0.033377573159005906
Epoch: 77 Batch: 950
Training Loss: 0.03354637861251831
Epoch: 77 Batch: 1000
Training Loss: 0.031179865747690202
Epoch: 77 Batch: 1050
Training Loss: 0.030171447623343693
Epoch: 77 Batch: 1100
Training Loss: 0.027448465146801687
Epoch: 77 Batch: 1150
Training Loss: 0.027058526277542114
Epoch: 77 Batch: 1200
Training Loss: 0.026847888578971225
Epoch: 77 Batch: 1250
Training Loss: 0.024711301708221434
Epoch: 77 Batch: 1300
Training Loss: 0.024849010660098148
Epoch: 77 Batch: 1350
Training Loss: 0.023806059912399008
Epoch: 77 Batch: 1400
Training Loss: 0.022057075947523118
Epoch: 77 Batch: 1450
Training Loss: 0.022138450721214557
Epoch: 77 Batch: 1500
Training Loss: 0.020573922832806905
Epoch: 77 Batch: 1550
Training Loss: 0.020502888848704676
Epoch: 77 Batch: 1600
Training Loss: 0.018887303210794924
Epoch: 77 Batch: 1650
Training Loss: 0.018564404195005244
Epoch: 77 Batch: 1700
Training Loss: 0.019053803559611827
Epoch: 77 Batch: 1750
Training Loss: 0.01752265090601785
Epoch: 77 Batch: 1800
Training Loss: 0.017949137902922102
Epoch: 77 Batch: 1850
Training Loss: 0.017488838160360183
Epoch: 77 Batch: 1900
Training Loss: 0.01582915939782795
Epoch: 77 Batch: 1950
Training Loss: 0.01564429170046097
Epoch: 77 Batch: 2000
Training Loss: 0.015908582001924514
Epoch: 77 Batch: 2050
Training Loss: 0.01564915078442271
Epoch: 77 Batch: 2100
Training Loss: 0.015063338804812658
Epoch: 77 Batch: 2150
Training Loss: 0.01458038816618365
Epoch: 77 Batch: 2200
Training Loss: 0.014499366906556217
Epoch: 77 Batch: 2250
Training Loss: 0.013504231095314026
Epoch: 77 Batch: 2300
Training Loss: 0.013918695747852325
Epoch: 77 Batch: 2350
Training Loss: 0.013120212770522909
Epoch: 77 Batch: 2400
Training Loss: 0.013026216551661491
Epoch: 77 Batch: 2450
Training Loss: 0.013372712974645654
Epoch: 77 Batch: 2500
Training Loss: 0.012594125437736511
Epoch: 77 Batch: 2550
Training Loss: 0.012009037779826743
Epoch: 77 Batch: 2600
Training Loss: 0.01227331654383586
Epoch: 77 Batch: 2650
Training Loss: 0.01197726123737839
Epoch: 77 Batch: 2700
Training Loss: 0.011217890061714031
Epoch: 77 Batch: 2750
Training Loss: 0.011363621787591415
Epoch: 77 Batch: 2800
Training Loss: 0.011655056849122047
Epoch: 77 Batch: 2850
Training Loss: 0.010800290776972185
Epoch: 77 Batch: 2900
Training Loss: 0.010590708471577743
Epoch: 77 Batch: 2950
Training Loss: 0.011217253773899401
Epoch: 77 Batch: 3000
Training Loss: 0.010146874288717906
Epoch: 77 Batch: 3050
Training Loss: 0.01006819212045826
Epoch: 77 Batch: 3100
Training Loss: 0.009993024622240373
Epoch: 77 Batch: 3150
Training Loss: 0.009336199202234783
Epoch: 77 Batch: 3200
Training Loss: 0.009706482170149683
Epoch: 78 
 Validation Loss: 0.4836062947909037
---------------------------
Epoch: 78 Batch: 50
Training Loss: 0.6454722166061402
Epoch: 78 Batch: 100
Training Loss: 0.3096427124738693
Epoch: 78 Batch: 150
Training Loss: 0.20996517022450764
Epoch: 78 Batch: 200
Training Loss: 0.1549816209077835
Epoch: 78 Batch: 250
Training Loss: 0.12732026171684266
Epoch: 78 Batch: 300
Training Loss: 0.10380700508753458
Epoch: 78 Batch: 350
Training Loss: 0.088516161271504
Epoch: 78 Batch: 400
Training Loss: 0.07493388168513775
Epoch: 78 Batch: 450
Training Loss: 0.0700069495704439
Epoch: 78 Batch: 500
Training Loss: 0.06427218580245972
Epoch: 78 Batch: 550
Training Loss: 0.055885906652970746
Epoch: 78 Batch: 600
Training Loss: 0.05027236809333165
Epoch: 78 Batch: 650
Training Loss: 0.04873298122332646
Epoch: 78 Batch: 700
Training Loss: 0.04356389548097338
Epoch: 78 Batch: 750
Training Loss: 0.03822560572624206
Epoch: 78 Batch: 800
Training Loss: 0.03951487824320793
Epoch: 78 Batch: 850
Training Loss: 0.03748343709637137
Epoch: 78 Batch: 900
Training Loss: 0.03615970018837187
Epoch: 78 Batch: 950
Training Loss: 0.03241190750347941
Epoch: 78 Batch: 1000
Training Loss: 0.030793298065662385
Epoch: 78 Batch: 1050
Training Loss: 0.02924128336565835
Epoch: 78 Batch: 1100
Training Loss: 0.028437906043096022
Epoch: 78 Batch: 1150
Training Loss: 0.02884928205738897
Epoch: 78 Batch: 1200
Training Loss: 0.02477020805080732
Epoch: 78 Batch: 1250
Training Loss: 0.026231950354576112
Epoch: 78 Batch: 1300
Training Loss: 0.02470014207638227
Epoch: 78 Batch: 1350
Training Loss: 0.02308991955386268
Epoch: 78 Batch: 1400
Training Loss: 0.023572323258434023
Epoch: 78 Batch: 1450
Training Loss: 0.021410815037530045
Epoch: 78 Batch: 1500
Training Loss: 0.020682418485482534
Epoch: 78 Batch: 1550
Training Loss: 0.020549094619289523
Epoch: 78 Batch: 1600
Training Loss: 0.019838849026709796
Epoch: 78 Batch: 1650
Training Loss: 0.019805546782233497
Epoch: 78 Batch: 1700
Training Loss: 0.0183029603081591
Epoch: 78 Batch: 1750
Training Loss: 0.017690993632589067
Epoch: 78 Batch: 1800
Training Loss: 0.017175160994132358
Epoch: 78 Batch: 1850
Training Loss: 0.0166249591917605
Epoch: 78 Batch: 1900
Training Loss: 0.01685796072608546
Epoch: 78 Batch: 1950
Training Loss: 0.015985324887128977
Epoch: 78 Batch: 2000
Training Loss: 0.01593918354809284
Epoch: 78 Batch: 2050
Training Loss: 0.015198301571171458
Epoch: 78 Batch: 2100
Training Loss: 0.015205571367627098
Epoch: 78 Batch: 2150
Training Loss: 0.014274035026860792
Epoch: 78 Batch: 2200
Training Loss: 0.013982163762504404
Epoch: 78 Batch: 2250
Training Loss: 0.01400981096426646
Epoch: 78 Batch: 2300
Training Loss: 0.013678648549577465
Epoch: 78 Batch: 2350
Training Loss: 0.012947937326228365
Epoch: 78 Batch: 2400
Training Loss: 0.012767922841012478
Epoch: 78 Batch: 2450
Training Loss: 0.012802161002645687
Epoch: 78 Batch: 2500
Training Loss: 0.012107133400440217
Epoch: 78 Batch: 2550
Training Loss: 0.011836335495406506
Epoch: 78 Batch: 2600
Training Loss: 0.011561427586353742
Epoch: 78 Batch: 2650
Training Loss: 0.011795715194828105
Epoch: 78 Batch: 2700
Training Loss: 0.012054407530360751
Epoch: 78 Batch: 2750
Training Loss: 0.011987329472195019
Epoch: 78 Batch: 2800
Training Loss: 0.01111222199031285
Epoch: 78 Batch: 2850
Training Loss: 0.010806050007803398
Epoch: 78 Batch: 2900
Training Loss: 0.010710265153440935
Epoch: 78 Batch: 2950
Training Loss: 0.010632539856231818
Epoch: 78 Batch: 3000
Training Loss: 0.01045986165603002
Epoch: 78 Batch: 3050
Training Loss: 0.010632165562911113
Epoch: 78 Batch: 3100
Training Loss: 0.010656008499283944
Epoch: 78 Batch: 3150
Training Loss: 0.010125183113037594
Epoch: 78 Batch: 3200
Training Loss: 0.009535265667364002
Epoch: 79 
 Validation Loss: 0.4834644685188929
---------------------------
Epoch: 79 Batch: 50
Training Loss: 0.6428699457645416
Epoch: 79 Batch: 100
Training Loss: 0.30181360691785813
Epoch: 79 Batch: 150
Training Loss: 0.21508955975373586
Epoch: 79 Batch: 200
Training Loss: 0.1557699418067932
Epoch: 79 Batch: 250
Training Loss: 0.12763241851329804
Epoch: 79 Batch: 300
Training Loss: 0.10244052678346634
Epoch: 79 Batch: 350
Training Loss: 0.09115190889154162
Epoch: 79 Batch: 400
Training Loss: 0.0754952871054411
Epoch: 79 Batch: 450
Training Loss: 0.06970403611660003
Epoch: 79 Batch: 500
Training Loss: 0.0636702578663826
Epoch: 79 Batch: 550
Training Loss: 0.05607720212502913
Epoch: 79 Batch: 600
Training Loss: 0.053505772203207014
Epoch: 79 Batch: 650
Training Loss: 0.04904861317231105
Epoch: 79 Batch: 700
Training Loss: 0.04796521442277091
Epoch: 79 Batch: 750
Training Loss: 0.04373474721113841
Epoch: 79 Batch: 800
Training Loss: 0.03797496844083071
Epoch: 79 Batch: 850
Training Loss: 0.036604764777071336
Epoch: 79 Batch: 900
Training Loss: 0.03420125196377436
Epoch: 79 Batch: 950
Training Loss: 0.03297118121071866
Epoch: 79 Batch: 1000
Training Loss: 0.03125960662961006
Epoch: 79 Batch: 1050
Training Loss: 0.030470143528211684
Epoch: 79 Batch: 1100
Training Loss: 0.027793486633084036
Epoch: 79 Batch: 1150
Training Loss: 0.027139243302137954
Epoch: 79 Batch: 1200
Training Loss: 0.025495800003409386
Epoch: 79 Batch: 1250
Training Loss: 0.025389109992980958
Epoch: 79 Batch: 1300
Training Loss: 0.023256465815580808
Epoch: 79 Batch: 1350
Training Loss: 0.02450272074452153
Epoch: 79 Batch: 1400
Training Loss: 0.02235022070152419
Epoch: 79 Batch: 1450
Training Loss: 0.02221872095404
Epoch: 79 Batch: 1500
Training Loss: 0.02081446846326192
Epoch: 79 Batch: 1550
Training Loss: 0.02027478560324638
Epoch: 79 Batch: 1600
Training Loss: 0.01970221435651183
Epoch: 79 Batch: 1650
Training Loss: 0.017262480746616016
Epoch: 79 Batch: 1700
Training Loss: 0.01778822916395524
Epoch: 79 Batch: 1750
Training Loss: 0.017380093438284738
Epoch: 79 Batch: 1800
Training Loss: 0.017718891683552
Epoch: 79 Batch: 1850
Training Loss: 0.016565611443004093
Epoch: 79 Batch: 1900
Training Loss: 0.016415457411816246
Epoch: 79 Batch: 1950
Training Loss: 0.0163795908444967
Epoch: 79 Batch: 2000
Training Loss: 0.015363333940505982
Epoch: 79 Batch: 2050
Training Loss: 0.0160143452301258
Epoch: 79 Batch: 2100
Training Loss: 0.014838534380708422
Epoch: 79 Batch: 2150
Training Loss: 0.014111874353053958
Epoch: 79 Batch: 2200
Training Loss: 0.01454027461734685
Epoch: 79 Batch: 2250
Training Loss: 0.013861756814850702
Epoch: 79 Batch: 2300
Training Loss: 0.013412688737330229
Epoch: 79 Batch: 2350
Training Loss: 0.01384847639723027
Epoch: 79 Batch: 2400
Training Loss: 0.013326411756376425
Epoch: 79 Batch: 2450
Training Loss: 0.013139886576302197
Epoch: 79 Batch: 2500
Training Loss: 0.012672848093509674
Epoch: 79 Batch: 2550
Training Loss: 0.01294782349876329
Epoch: 79 Batch: 2600
Training Loss: 0.012396296388827837
Epoch: 79 Batch: 2650
Training Loss: 0.011778386730068134
Epoch: 79 Batch: 2700
Training Loss: 0.011915051837762198
Epoch: 79 Batch: 2750
Training Loss: 0.011631854815916581
Epoch: 79 Batch: 2800
Training Loss: 0.011428863097514425
Epoch: 79 Batch: 2850
Training Loss: 0.01089486719223491
Epoch: 79 Batch: 2900
Training Loss: 0.010991468686482002
Epoch: 79 Batch: 2950
Training Loss: 0.010393729765536422
Epoch: 79 Batch: 3000
Training Loss: 0.01028152338663737
Epoch: 79 Batch: 3050
Training Loss: 0.010128480410966716
Epoch: 79 Batch: 3100
Training Loss: 0.009812279391673303
Epoch: 79 Batch: 3150
Training Loss: 0.009723121599545555
Epoch: 79 Batch: 3200
Training Loss: 0.009775572307407856
Epoch: 80 
 Validation Loss: 0.48326653871271347
---------------------------
Epoch: 80 Batch: 50
Training Loss: 0.602423574924469
Epoch: 80 Batch: 100
Training Loss: 0.30864024221897124
Epoch: 80 Batch: 150
Training Loss: 0.20786518275737761
Epoch: 80 Batch: 200
Training Loss: 0.16118326395750046
Epoch: 80 Batch: 250
Training Loss: 0.12736645829677581
Epoch: 80 Batch: 300
Training Loss: 0.10772103170553843
Epoch: 80 Batch: 350
Training Loss: 0.08736590036324092
Epoch: 80 Batch: 400
Training Loss: 0.07757697008550167
Epoch: 80 Batch: 450
Training Loss: 0.07164494865470462
Epoch: 80 Batch: 500
Training Loss: 0.06253492295742034
Epoch: 80 Batch: 550
Training Loss: 0.05710647425868295
Epoch: 80 Batch: 600
Training Loss: 0.05285804813106855
Epoch: 80 Batch: 650
Training Loss: 0.048056048384079564
Epoch: 80 Batch: 700
Training Loss: 0.045150796217577796
Epoch: 80 Batch: 750
Training Loss: 0.041674711465835575
Epoch: 80 Batch: 800
Training Loss: 0.039980048462748524
Epoch: 80 Batch: 850
Training Loss: 0.03765243635458105
Epoch: 80 Batch: 900
Training Loss: 0.03374225404527452
Epoch: 80 Batch: 950
Training Loss: 0.03273058656014894
Epoch: 80 Batch: 1000
Training Loss: 0.03083013391494751
Epoch: 80 Batch: 1050
Training Loss: 0.02937513144243331
Epoch: 80 Batch: 1100
Training Loss: 0.028382495452057233
Epoch: 80 Batch: 1150
Training Loss: 0.026572085774463156
Epoch: 80 Batch: 1200
Training Loss: 0.026453195214271544
Epoch: 80 Batch: 1250
Training Loss: 0.024879446387290956
Epoch: 80 Batch: 1300
Training Loss: 0.02373296863757647
Epoch: 80 Batch: 1350
Training Loss: 0.023048183189498053
Epoch: 80 Batch: 1400
Training Loss: 0.022664709367922375
Epoch: 80 Batch: 1450
Training Loss: 0.023125917911529542
Epoch: 80 Batch: 1500
Training Loss: 0.021291914284229278
Epoch: 80 Batch: 1550
Training Loss: 0.020472642606304537
Epoch: 80 Batch: 1600
Training Loss: 0.019653346706181764
Epoch: 80 Batch: 1650
Training Loss: 0.019705226240736066
Epoch: 80 Batch: 1700
Training Loss: 0.018974458925864277
Epoch: 80 Batch: 1750
Training Loss: 0.01708914223739079
Epoch: 80 Batch: 1800
Training Loss: 0.016425299561685987
Epoch: 80 Batch: 1850
Training Loss: 0.017258114975851935
Epoch: 80 Batch: 1900
Training Loss: 0.016740286742386066
Epoch: 80 Batch: 1950
Training Loss: 0.016653393094356244
Epoch: 80 Batch: 2000
Training Loss: 0.015332625344395638
Epoch: 80 Batch: 2050
Training Loss: 0.015953253696604473
Epoch: 80 Batch: 2100
Training Loss: 0.015238159511770521
Epoch: 80 Batch: 2150
Training Loss: 0.014267506641010905
Epoch: 80 Batch: 2200
Training Loss: 0.014391086386008696
Epoch: 80 Batch: 2250
Training Loss: 0.013682661506864759
Epoch: 80 Batch: 2300
Training Loss: 0.014027951489324154
Epoch: 80 Batch: 2350
Training Loss: 0.01345015584154332
Epoch: 80 Batch: 2400
Training Loss: 0.012836483716964721
Epoch: 80 Batch: 2450
Training Loss: 0.012564057805100266
Epoch: 80 Batch: 2500
Training Loss: 0.012004608392715454
Epoch: 80 Batch: 2550
Training Loss: 0.01208392528926625
Epoch: 80 Batch: 2600
Training Loss: 0.01128165500668379
Epoch: 80 Batch: 2650
Training Loss: 0.01144272839123348
Epoch: 80 Batch: 2700
Training Loss: 0.011254341425719084
Epoch: 80 Batch: 2750
Training Loss: 0.011870905778624794
Epoch: 80 Batch: 2800
Training Loss: 0.010571017297250884
Epoch: 80 Batch: 2850
Training Loss: 0.011396731974785789
Epoch: 80 Batch: 2900
Training Loss: 0.01061481130534205
Epoch: 80 Batch: 2950
Training Loss: 0.010972894207905915
Epoch: 80 Batch: 3000
Training Loss: 0.010077674289544423
Epoch: 80 Batch: 3050
Training Loss: 0.010060342804330295
Epoch: 80 Batch: 3100
Training Loss: 0.009848950514870305
Epoch: 80 Batch: 3150
Training Loss: 0.010007396860728188
Epoch: 80 Batch: 3200
Training Loss: 0.009754422325640917
Epoch: 81 
 Validation Loss: 0.4831424620416429
---------------------------
Epoch: 81 Batch: 50
Training Loss: 0.64188152551651
Epoch: 81 Batch: 100
Training Loss: 0.29794873148202894
Epoch: 81 Batch: 150
Training Loss: 0.21099607427914938
Epoch: 81 Batch: 200
Training Loss: 0.15474558040499686
Epoch: 81 Batch: 250
Training Loss: 0.125052303314209
Epoch: 81 Batch: 300
Training Loss: 0.10706979582707087
Epoch: 81 Batch: 350
Training Loss: 0.09037399853978838
Epoch: 81 Batch: 400
Training Loss: 0.08101513378322124
Epoch: 81 Batch: 450
Training Loss: 0.07073096646202935
Epoch: 81 Batch: 500
Training Loss: 0.06191980826854706
Epoch: 81 Batch: 550
Training Loss: 0.05533144744959745
Epoch: 81 Batch: 600
Training Loss: 0.052628367642561596
Epoch: 81 Batch: 650
Training Loss: 0.04727719513269571
Epoch: 81 Batch: 700
Training Loss: 0.043976931955133164
Epoch: 81 Batch: 750
Training Loss: 0.04152193288008372
Epoch: 81 Batch: 800
Training Loss: 0.04001703593879938
Epoch: 81 Batch: 850
Training Loss: 0.036611313083592584
Epoch: 81 Batch: 900
Training Loss: 0.034152962995900045
Epoch: 81 Batch: 950
Training Loss: 0.033575405478477476
Epoch: 81 Batch: 1000
Training Loss: 0.03138148635625839
Epoch: 81 Batch: 1050
Training Loss: 0.03154930103392828
Epoch: 81 Batch: 1100
Training Loss: 0.02831544106656855
Epoch: 81 Batch: 1150
Training Loss: 0.026499489830887835
Epoch: 81 Batch: 1200
Training Loss: 0.026512124116222065
Epoch: 81 Batch: 1250
Training Loss: 0.02358654010295868
Epoch: 81 Batch: 1300
Training Loss: 0.024161919813889723
Epoch: 81 Batch: 1350
Training Loss: 0.022716994528417234
Epoch: 81 Batch: 1400
Training Loss: 0.0231646714253085
Epoch: 81 Batch: 1450
Training Loss: 0.022224134535625065
Epoch: 81 Batch: 1500
Training Loss: 0.021386633773644765
Epoch: 81 Batch: 1550
Training Loss: 0.020864608960766945
Epoch: 81 Batch: 1600
Training Loss: 0.02015974460169673
Epoch: 81 Batch: 1650
Training Loss: 0.01826017663334355
Epoch: 81 Batch: 1700
Training Loss: 0.018815250414259292
Epoch: 81 Batch: 1750
Training Loss: 0.018262307047843932
Epoch: 81 Batch: 1800
Training Loss: 0.018190601882007388
Epoch: 81 Batch: 1850
Training Loss: 0.01669560804560378
Epoch: 81 Batch: 1900
Training Loss: 0.016753378667329486
Epoch: 81 Batch: 1950
Training Loss: 0.01541243117589217
Epoch: 81 Batch: 2000
Training Loss: 0.014894953921437264
Epoch: 81 Batch: 2050
Training Loss: 0.015027715752764447
Epoch: 81 Batch: 2100
Training Loss: 0.014304847036089216
Epoch: 81 Batch: 2150
Training Loss: 0.014808539651161017
Epoch: 81 Batch: 2200
Training Loss: 0.013836948018182407
Epoch: 81 Batch: 2250
Training Loss: 0.01381910643312666
Epoch: 81 Batch: 2300
Training Loss: 0.014155552763005962
Epoch: 81 Batch: 2350
Training Loss: 0.01318351758287308
Epoch: 81 Batch: 2400
Training Loss: 0.012853742192188898
Epoch: 81 Batch: 2450
Training Loss: 0.013203998743271342
Epoch: 81 Batch: 2500
Training Loss: 0.013161541306972504
Epoch: 81 Batch: 2550
Training Loss: 0.01159735098773358
Epoch: 81 Batch: 2600
Training Loss: 0.01205339043186261
Epoch: 81 Batch: 2650
Training Loss: 0.011768553099542294
Epoch: 81 Batch: 2700
Training Loss: 0.011362394237959827
Epoch: 81 Batch: 2750
Training Loss: 0.010932733394882896
Epoch: 81 Batch: 2800
Training Loss: 0.011211799244795527
Epoch: 81 Batch: 2850
Training Loss: 0.011455934915626258
Epoch: 81 Batch: 2900
Training Loss: 0.01098664687625293
Epoch: 81 Batch: 2950
Training Loss: 0.010761241074335777
Epoch: 81 Batch: 3000
Training Loss: 0.010518390119075776
Epoch: 81 Batch: 3050
Training Loss: 0.01007261782396035
Epoch: 81 Batch: 3100
Training Loss: 0.009606775285736207
Epoch: 81 Batch: 3150
Training Loss: 0.009729712293261573
Epoch: 81 Batch: 3200
Training Loss: 0.010187313612550496
Epoch: 82 
 Validation Loss: 0.4827983078029421
---------------------------
Epoch: 82 Batch: 50
Training Loss: 0.6374289244413376
Epoch: 82 Batch: 100
Training Loss: 0.3140108397603035
Epoch: 82 Batch: 150
Training Loss: 0.2033626800775528
Epoch: 82 Batch: 200
Training Loss: 0.15466676473617555
Epoch: 82 Batch: 250
Training Loss: 0.12389124810695648
Epoch: 82 Batch: 300
Training Loss: 0.10449730545282364
Epoch: 82 Batch: 350
Training Loss: 0.09089753585202354
Epoch: 82 Batch: 400
Training Loss: 0.07648873984813691
Epoch: 82 Batch: 450
Training Loss: 0.06840735541449652
Epoch: 82 Batch: 500
Training Loss: 0.06482797414064408
Epoch: 82 Batch: 550
Training Loss: 0.05866161628202959
Epoch: 82 Batch: 600
Training Loss: 0.050269275307655334
Epoch: 82 Batch: 650
Training Loss: 0.0472378912797341
Epoch: 82 Batch: 700
Training Loss: 0.043770569009440284
Epoch: 82 Batch: 750
Training Loss: 0.042463912566502886
Epoch: 82 Batch: 800
Training Loss: 0.0399032972753048
Epoch: 82 Batch: 850
Training Loss: 0.03736669084605049
Epoch: 82 Batch: 900
Training Loss: 0.03387901422050264
Epoch: 82 Batch: 950
Training Loss: 0.033062842739255804
Epoch: 82 Batch: 1000
Training Loss: 0.03189318537712097
Epoch: 82 Batch: 1050
Training Loss: 0.029252583441280185
Epoch: 82 Batch: 1100
Training Loss: 0.02842803743752566
Epoch: 82 Batch: 1150
Training Loss: 0.026277339354805324
Epoch: 82 Batch: 1200
Training Loss: 0.026146880264083544
Epoch: 82 Batch: 1250
Training Loss: 0.025657230806350708
Epoch: 82 Batch: 1300
Training Loss: 0.02345366991483248
Epoch: 82 Batch: 1350
Training Loss: 0.021906436284383136
Epoch: 82 Batch: 1400
Training Loss: 0.02153236585003989
Epoch: 82 Batch: 1450
Training Loss: 0.02154754237882022
Epoch: 82 Batch: 1500
Training Loss: 0.02172775191068649
Epoch: 82 Batch: 1550
Training Loss: 0.020233979609704786
Epoch: 82 Batch: 1600
Training Loss: 0.01996823577210307
Epoch: 82 Batch: 1650
Training Loss: 0.01909704074715123
Epoch: 82 Batch: 1700
Training Loss: 0.018921710778685177
Epoch: 82 Batch: 1750
Training Loss: 0.0177430419921875
Epoch: 82 Batch: 1800
Training Loss: 0.017926257799069086
Epoch: 82 Batch: 1850
Training Loss: 0.016567162259204966
Epoch: 82 Batch: 1900
Training Loss: 0.01700953838072325
Epoch: 82 Batch: 1950
Training Loss: 0.016395823191373775
Epoch: 82 Batch: 2000
Training Loss: 0.015568807959556579
Epoch: 82 Batch: 2050
Training Loss: 0.015072630905523533
Epoch: 82 Batch: 2100
Training Loss: 0.014672518656367347
Epoch: 82 Batch: 2150
Training Loss: 0.01487934761269148
Epoch: 82 Batch: 2200
Training Loss: 0.014672893231565303
Epoch: 82 Batch: 2250
Training Loss: 0.013423853463596768
Epoch: 82 Batch: 2300
Training Loss: 0.013410155941610751
Epoch: 82 Batch: 2350
Training Loss: 0.012691319673619372
Epoch: 82 Batch: 2400
Training Loss: 0.012651511318981647
Epoch: 82 Batch: 2450
Training Loss: 0.012875983605579453
Epoch: 82 Batch: 2500
Training Loss: 0.012306239974498749
Epoch: 82 Batch: 2550
Training Loss: 0.012041003072963042
Epoch: 82 Batch: 2600
Training Loss: 0.012478888745491322
Epoch: 82 Batch: 2650
Training Loss: 0.011909725148722811
Epoch: 82 Batch: 2700
Training Loss: 0.011436352244129887
Epoch: 82 Batch: 2750
Training Loss: 0.01104144875569777
Epoch: 82 Batch: 2800
Training Loss: 0.010987235616360391
Epoch: 82 Batch: 2850
Training Loss: 0.010748129779832406
Epoch: 82 Batch: 2900
Training Loss: 0.010974968527925425
Epoch: 82 Batch: 2950
Training Loss: 0.010960867303912922
Epoch: 82 Batch: 3000
Training Loss: 0.010399629433949788
Epoch: 82 Batch: 3050
Training Loss: 0.010552447066932428
Epoch: 82 Batch: 3100
Training Loss: 0.010274131384588056
Epoch: 82 Batch: 3150
Training Loss: 0.010163970220656623
Epoch: 82 Batch: 3200
Training Loss: 0.010128721306100487
Epoch: 83 
 Validation Loss: 0.4823433382643594
---------------------------
Epoch: 83 Batch: 50
Training Loss: 0.6205019158124924
Epoch: 83 Batch: 100
Training Loss: 0.28864952266216276
Epoch: 83 Batch: 150
Training Loss: 0.20528918226559956
Epoch: 83 Batch: 200
Training Loss: 0.15620020925998687
Epoch: 83 Batch: 250
Training Loss: 0.1228203263282776
Epoch: 83 Batch: 300
Training Loss: 0.09909411936998368
Epoch: 83 Batch: 350
Training Loss: 0.091155686548778
Epoch: 83 Batch: 400
Training Loss: 0.0795041773468256
Epoch: 83 Batch: 450
Training Loss: 0.06871261305279201
Epoch: 83 Batch: 500
Training Loss: 0.06307430702447892
Epoch: 83 Batch: 550
Training Loss: 0.054146877093748616
Epoch: 83 Batch: 600
Training Loss: 0.05121390983462334
Epoch: 83 Batch: 650
Training Loss: 0.047510038339174714
Epoch: 83 Batch: 700
Training Loss: 0.04564370955739702
Epoch: 83 Batch: 750
Training Loss: 0.042713776588439945
Epoch: 83 Batch: 800
Training Loss: 0.03992450069636107
Epoch: 83 Batch: 850
Training Loss: 0.036117007171406465
Epoch: 83 Batch: 900
Training Loss: 0.03378056688441171
Epoch: 83 Batch: 950
Training Loss: 0.03376965074162734
Epoch: 83 Batch: 1000
Training Loss: 0.03238403245806694
Epoch: 83 Batch: 1050
Training Loss: 0.03109508065950303
Epoch: 83 Batch: 1100
Training Loss: 0.028041247671300715
Epoch: 83 Batch: 1150
Training Loss: 0.02724748873192331
Epoch: 83 Batch: 1200
Training Loss: 0.02597336083650589
Epoch: 83 Batch: 1250
Training Loss: 0.024235129714012146
Epoch: 83 Batch: 1300
Training Loss: 0.022951612564233632
Epoch: 83 Batch: 1350
Training Loss: 0.02264840633780868
Epoch: 83 Batch: 1400
Training Loss: 0.022017812877893447
Epoch: 83 Batch: 1450
Training Loss: 0.02146636944392632
Epoch: 83 Batch: 1500
Training Loss: 0.021476010262966156
Epoch: 83 Batch: 1550
Training Loss: 0.020036754627381602
Epoch: 83 Batch: 1600
Training Loss: 0.021191023588180542
Epoch: 83 Batch: 1650
Training Loss: 0.018342793836738123
Epoch: 83 Batch: 1700
Training Loss: 0.018365184366703032
Epoch: 83 Batch: 1750
Training Loss: 0.018027362380708966
Epoch: 83 Batch: 1800
Training Loss: 0.017549207309881847
Epoch: 83 Batch: 1850
Training Loss: 0.017972797139270885
Epoch: 83 Batch: 1900
Training Loss: 0.01719288862065265
Epoch: 83 Batch: 1950
Training Loss: 0.015590039491653442
Epoch: 83 Batch: 2000
Training Loss: 0.01561369414627552
Epoch: 83 Batch: 2050
Training Loss: 0.01580719812614162
Epoch: 83 Batch: 2100
Training Loss: 0.014035921848955609
Epoch: 83 Batch: 2150
Training Loss: 0.014641127461610838
Epoch: 83 Batch: 2200
Training Loss: 0.014649981382218274
Epoch: 83 Batch: 2250
Training Loss: 0.013907449973954095
Epoch: 83 Batch: 2300
Training Loss: 0.013808865871118462
Epoch: 83 Batch: 2350
Training Loss: 0.013205932695814904
Epoch: 83 Batch: 2400
Training Loss: 0.013024429169793924
Epoch: 83 Batch: 2450
Training Loss: 0.01257348294160804
Epoch: 83 Batch: 2500
Training Loss: 0.012503112733364106
Epoch: 83 Batch: 2550
Training Loss: 0.012152935067812601
Epoch: 83 Batch: 2600
Training Loss: 0.012689159971017104
Epoch: 83 Batch: 2650
Training Loss: 0.011850006344183437
Epoch: 83 Batch: 2700
Training Loss: 0.011607859830061594
Epoch: 83 Batch: 2750
Training Loss: 0.011372220093553716
Epoch: 83 Batch: 2800
Training Loss: 0.010251434136714255
Epoch: 83 Batch: 2850
Training Loss: 0.010950247760404621
Epoch: 83 Batch: 2900
Training Loss: 0.010680479438140475
Epoch: 83 Batch: 2950
Training Loss: 0.010828597404189028
Epoch: 83 Batch: 3000
Training Loss: 0.010193422039349875
Epoch: 83 Batch: 3050
Training Loss: 0.010760891945635686
Epoch: 83 Batch: 3100
Training Loss: 0.00995128289345772
Epoch: 83 Batch: 3150
Training Loss: 0.009865395407828074
Epoch: 83 Batch: 3200
Training Loss: 0.010291942739859223
Epoch: 84 
 Validation Loss: 0.4821757823228836
---------------------------
Epoch: 84 Batch: 50
Training Loss: 0.6197810417413712
Epoch: 84 Batch: 100
Training Loss: 0.3050102481245995
Epoch: 84 Batch: 150
Training Loss: 0.2115416403611501
Epoch: 84 Batch: 200
Training Loss: 0.1579652425646782
Epoch: 84 Batch: 250
Training Loss: 0.12440210711956025
Epoch: 84 Batch: 300
Training Loss: 0.10115218977133433
Epoch: 84 Batch: 350
Training Loss: 0.08785144252436501
Epoch: 84 Batch: 400
Training Loss: 0.07569713674485684
Epoch: 84 Batch: 450
Training Loss: 0.0719239291217592
Epoch: 84 Batch: 500
Training Loss: 0.06452438479661941
Epoch: 84 Batch: 550
Training Loss: 0.05610207714817741
Epoch: 84 Batch: 600
Training Loss: 0.051375802954037986
Epoch: 84 Batch: 650
Training Loss: 0.046998505087999194
Epoch: 84 Batch: 700
Training Loss: 0.045310027386460984
Epoch: 84 Batch: 750
Training Loss: 0.04261747244993846
Epoch: 84 Batch: 800
Training Loss: 0.04083354033529758
Epoch: 84 Batch: 850
Training Loss: 0.03683408120099236
Epoch: 84 Batch: 900
Training Loss: 0.03468599614169863
Epoch: 84 Batch: 950
Training Loss: 0.03227917514349285
Epoch: 84 Batch: 1000
Training Loss: 0.029751010030508042
Epoch: 84 Batch: 1050
Training Loss: 0.03186372453258151
Epoch: 84 Batch: 1100
Training Loss: 0.02773599294098941
Epoch: 84 Batch: 1150
Training Loss: 0.02765031311822974
Epoch: 84 Batch: 1200
Training Loss: 0.025881695747375488
Epoch: 84 Batch: 1250
Training Loss: 0.025836942315101624
Epoch: 84 Batch: 1300
Training Loss: 0.02441333573598128
Epoch: 84 Batch: 1350
Training Loss: 0.023680656132874664
Epoch: 84 Batch: 1400
Training Loss: 0.02297230209623064
Epoch: 84 Batch: 1450
Training Loss: 0.02141507362497264
Epoch: 84 Batch: 1500
Training Loss: 0.02095738943417867
Epoch: 84 Batch: 1550
Training Loss: 0.019379690847089213
Epoch: 84 Batch: 1600
Training Loss: 0.019234287030994893
Epoch: 84 Batch: 1650
Training Loss: 0.018198372956478234
Epoch: 84 Batch: 1700
Training Loss: 0.01657454679994022
Epoch: 84 Batch: 1750
Training Loss: 0.01730560030255999
Epoch: 84 Batch: 1800
Training Loss: 0.017049260404374864
Epoch: 84 Batch: 1850
Training Loss: 0.01737108457732845
Epoch: 84 Batch: 1900
Training Loss: 0.015970826901887593
Epoch: 84 Batch: 1950
Training Loss: 0.016627828173148326
Epoch: 84 Batch: 2000
Training Loss: 0.015731300726532935
Epoch: 84 Batch: 2050
Training Loss: 0.015456293283439265
Epoch: 84 Batch: 2100
Training Loss: 0.014478793953146253
Epoch: 84 Batch: 2150
Training Loss: 0.014958988164746483
Epoch: 84 Batch: 2200
Training Loss: 0.01418601549484513
Epoch: 84 Batch: 2250
Training Loss: 0.013687076210975648
Epoch: 84 Batch: 2300
Training Loss: 0.013936680620131285
Epoch: 84 Batch: 2350
Training Loss: 0.013402290356920121
Epoch: 84 Batch: 2400
Training Loss: 0.012972638085484505
Epoch: 84 Batch: 2450
Training Loss: 0.01306656641619546
Epoch: 84 Batch: 2500
Training Loss: 0.012849384999275208
Epoch: 84 Batch: 2550
Training Loss: 0.012270443731663274
Epoch: 84 Batch: 2600
Training Loss: 0.011741052109461565
Epoch: 84 Batch: 2650
Training Loss: 0.011586063054372679
Epoch: 84 Batch: 2700
Training Loss: 0.01151400790170387
Epoch: 84 Batch: 2750
Training Loss: 0.011371949390931562
Epoch: 84 Batch: 2800
Training Loss: 0.011822150094168527
Epoch: 84 Batch: 2850
Training Loss: 0.010929148134432342
Epoch: 84 Batch: 2900
Training Loss: 0.011010486822703789
Epoch: 84 Batch: 2950
Training Loss: 0.01067387771808495
Epoch: 84 Batch: 3000
Training Loss: 0.010461297323306402
Epoch: 84 Batch: 3050
Training Loss: 0.010317121050396904
Epoch: 84 Batch: 3100
Training Loss: 0.010248987847758878
Epoch: 84 Batch: 3150
Training Loss: 0.010157786844268678
Epoch: 84 Batch: 3200
Training Loss: 0.009823131319135427
Epoch: 85 
 Validation Loss: 0.4817240291171604
---------------------------
Epoch: 85 Batch: 50
Training Loss: 0.5858377897739411
Epoch: 85 Batch: 100
Training Loss: 0.3114815226197243
Epoch: 85 Batch: 150
Training Loss: 0.20070490896701812
Epoch: 85 Batch: 200
Training Loss: 0.1620635497570038
Epoch: 85 Batch: 250
Training Loss: 0.12202956795692443
Epoch: 85 Batch: 300
Training Loss: 0.1057969311873118
Epoch: 85 Batch: 350
Training Loss: 0.0933347977059228
Epoch: 85 Batch: 400
Training Loss: 0.07981732808053493
Epoch: 85 Batch: 450
Training Loss: 0.0720079411400689
Epoch: 85 Batch: 500
Training Loss: 0.061369617879390714
Epoch: 85 Batch: 550
Training Loss: 0.055401600382544776
Epoch: 85 Batch: 600
Training Loss: 0.04987822771072388
Epoch: 85 Batch: 650
Training Loss: 0.047406714879549464
Epoch: 85 Batch: 700
Training Loss: 0.04286784197602953
Epoch: 85 Batch: 750
Training Loss: 0.04253291567166646
Epoch: 85 Batch: 800
Training Loss: 0.03741519998759031
Epoch: 85 Batch: 850
Training Loss: 0.03586269708240734
Epoch: 85 Batch: 900
Training Loss: 0.034816352427005766
Epoch: 85 Batch: 950
Training Loss: 0.0323930068078794
Epoch: 85 Batch: 1000
Training Loss: 0.03004051887989044
Epoch: 85 Batch: 1050
Training Loss: 0.030310393429937816
Epoch: 85 Batch: 1100
Training Loss: 0.027639120762998407
Epoch: 85 Batch: 1150
Training Loss: 0.027845100527224335
Epoch: 85 Batch: 1200
Training Loss: 0.026128468538324037
Epoch: 85 Batch: 1250
Training Loss: 0.02427916705608368
Epoch: 85 Batch: 1300
Training Loss: 0.023629736258433415
Epoch: 85 Batch: 1350
Training Loss: 0.023483577105734083
Epoch: 85 Batch: 1400
Training Loss: 0.021612734028271265
Epoch: 85 Batch: 1450
Training Loss: 0.021523578043641717
Epoch: 85 Batch: 1500
Training Loss: 0.020386551837126413
Epoch: 85 Batch: 1550
Training Loss: 0.020614589433516226
Epoch: 85 Batch: 1600
Training Loss: 0.020025822538882493
Epoch: 85 Batch: 1650
Training Loss: 0.018667349020640055
Epoch: 85 Batch: 1700
Training Loss: 0.018612335148979635
Epoch: 85 Batch: 1750
Training Loss: 0.018997083698000225
Epoch: 85 Batch: 1800
Training Loss: 0.017600921392440797
Epoch: 85 Batch: 1850
Training Loss: 0.016812337185885456
Epoch: 85 Batch: 1900
Training Loss: 0.017101304389928518
Epoch: 85 Batch: 1950
Training Loss: 0.015796505824113502
Epoch: 85 Batch: 2000
Training Loss: 0.01504873138666153
Epoch: 85 Batch: 2050
Training Loss: 0.014792980886087185
Epoch: 85 Batch: 2100
Training Loss: 0.01464986095825831
Epoch: 85 Batch: 2150
Training Loss: 0.014518370864003203
Epoch: 85 Batch: 2200
Training Loss: 0.013667636432430961
Epoch: 85 Batch: 2250
Training Loss: 0.01371073465877109
Epoch: 85 Batch: 2300
Training Loss: 0.013687814007634701
Epoch: 85 Batch: 2350
Training Loss: 0.013555091340491113
Epoch: 85 Batch: 2400
Training Loss: 0.013248717710375786
Epoch: 85 Batch: 2450
Training Loss: 0.012680909864756526
Epoch: 85 Batch: 2500
Training Loss: 0.012307961940765381
Epoch: 85 Batch: 2550
Training Loss: 0.0120217411775215
Epoch: 85 Batch: 2600
Training Loss: 0.012274353011296346
Epoch: 85 Batch: 2650
Training Loss: 0.012175751890776292
Epoch: 85 Batch: 2700
Training Loss: 0.01160443709956275
Epoch: 85 Batch: 2750
Training Loss: 0.011279077128930526
Epoch: 85 Batch: 2800
Training Loss: 0.01086701627288546
Epoch: 85 Batch: 2850
Training Loss: 0.011664145264709205
Epoch: 85 Batch: 2900
Training Loss: 0.010973684335577077
Epoch: 85 Batch: 2950
Training Loss: 0.010691458966772436
Epoch: 85 Batch: 3000
Training Loss: 0.00987313179175059
Epoch: 85 Batch: 3050
Training Loss: 0.010101816663976575
Epoch: 85 Batch: 3100
Training Loss: 0.010246976863953376
Epoch: 85 Batch: 3150
Training Loss: 0.010121023494099813
Epoch: 85 Batch: 3200
Training Loss: 0.009860785650089384
Epoch: 86 
 Validation Loss: 0.4829258855846193
---------------------------
Epoch: 86 Batch: 50
Training Loss: 0.6267029827833176
Epoch: 86 Batch: 100
Training Loss: 0.2989621728658676
Epoch: 86 Batch: 150
Training Loss: 0.21098809003829955
Epoch: 86 Batch: 200
Training Loss: 0.1538619141280651
Epoch: 86 Batch: 250
Training Loss: 0.12163848757743835
Epoch: 86 Batch: 300
Training Loss: 0.10542622536420822
Epoch: 86 Batch: 350
Training Loss: 0.09572370554719653
Epoch: 86 Batch: 400
Training Loss: 0.07904416099190711
Epoch: 86 Batch: 450
Training Loss: 0.06790087633662753
Epoch: 86 Batch: 500
Training Loss: 0.06164345920085907
Epoch: 86 Batch: 550
Training Loss: 0.0585923445224762
Epoch: 86 Batch: 600
Training Loss: 0.050210519433021544
Epoch: 86 Batch: 650
Training Loss: 0.048801016440758335
Epoch: 86 Batch: 700
Training Loss: 0.04570214569568634
Epoch: 86 Batch: 750
Training Loss: 0.04146115585168203
Epoch: 86 Batch: 800
Training Loss: 0.03999324545264244
Epoch: 86 Batch: 850
Training Loss: 0.03548021951142479
Epoch: 86 Batch: 900
Training Loss: 0.03425585150718689
Epoch: 86 Batch: 950
Training Loss: 0.03150095127130809
Epoch: 86 Batch: 1000
Training Loss: 0.03134100544452667
Epoch: 86 Batch: 1050
Training Loss: 0.030064520353362673
Epoch: 86 Batch: 1100
Training Loss: 0.027784517867998643
Epoch: 86 Batch: 1150
Training Loss: 0.027492340051609537
Epoch: 86 Batch: 1200
Training Loss: 0.02813626264532407
Epoch: 86 Batch: 1250
Training Loss: 0.025744485592842103
Epoch: 86 Batch: 1300
Training Loss: 0.023294605108407827
Epoch: 86 Batch: 1350
Training Loss: 0.023441042061205262
Epoch: 86 Batch: 1400
Training Loss: 0.02211529844573566
Epoch: 86 Batch: 1450
Training Loss: 0.02085841339209984
Epoch: 86 Batch: 1500
Training Loss: 0.020754961669445036
Epoch: 86 Batch: 1550
Training Loss: 0.020096490863830814
Epoch: 86 Batch: 1600
Training Loss: 0.019558473639190197
Epoch: 86 Batch: 1650
Training Loss: 0.019379955150864343
Epoch: 86 Batch: 1700
Training Loss: 0.017823101474958306
Epoch: 86 Batch: 1750
Training Loss: 0.017280152423041206
Epoch: 86 Batch: 1800
Training Loss: 0.016166487253374523
Epoch: 86 Batch: 1850
Training Loss: 0.016825712435954324
Epoch: 86 Batch: 1900
Training Loss: 0.016028895425169092
Epoch: 86 Batch: 1950
Training Loss: 0.015596453211246393
Epoch: 86 Batch: 2000
Training Loss: 0.01475898677110672
Epoch: 86 Batch: 2050
Training Loss: 0.015928495686228682
Epoch: 86 Batch: 2100
Training Loss: 0.014845066794327327
Epoch: 86 Batch: 2150
Training Loss: 0.014998410330262295
Epoch: 86 Batch: 2200
Training Loss: 0.014329553354870189
Epoch: 86 Batch: 2250
Training Loss: 0.013323014696439107
Epoch: 86 Batch: 2300
Training Loss: 0.013161527955013772
Epoch: 86 Batch: 2350
Training Loss: 0.012715270836302575
Epoch: 86 Batch: 2400
Training Loss: 0.013050996176898479
Epoch: 86 Batch: 2450
Training Loss: 0.012619289853134933
Epoch: 86 Batch: 2500
Training Loss: 0.012047470378875732
Epoch: 86 Batch: 2550
Training Loss: 0.012328437952434315
Epoch: 86 Batch: 2600
Training Loss: 0.011522188553443322
Epoch: 86 Batch: 2650
Training Loss: 0.011482537737432515
Epoch: 86 Batch: 2700
Training Loss: 0.011733816862106323
Epoch: 86 Batch: 2750
Training Loss: 0.011697608893567865
Epoch: 86 Batch: 2800
Training Loss: 0.010990983002952167
Epoch: 86 Batch: 2850
Training Loss: 0.01092282425938991
Epoch: 86 Batch: 2900
Training Loss: 0.010985273297490744
Epoch: 86 Batch: 2950
Training Loss: 0.01070486708212707
Epoch: 86 Batch: 3000
Training Loss: 0.010512751420338948
Epoch: 86 Batch: 3050
Training Loss: 0.010549342730006234
Epoch: 86 Batch: 3100
Training Loss: 0.009909258052226037
Epoch: 86 Batch: 3150
Training Loss: 0.00977092204585908
Epoch: 86 Batch: 3200
Training Loss: 0.009536094162613154
Epoch: 87 
 Validation Loss: 0.4816353936990102
---------------------------
Epoch: 87 Batch: 50
Training Loss: 0.628952619433403
Epoch: 87 Batch: 100
Training Loss: 0.3149140614271164
Epoch: 87 Batch: 150
Training Loss: 0.20725714763005573
Epoch: 87 Batch: 200
Training Loss: 0.1608676789700985
Epoch: 87 Batch: 250
Training Loss: 0.1275569956302643
Epoch: 87 Batch: 300
Training Loss: 0.10982673525810242
Epoch: 87 Batch: 350
Training Loss: 0.08518223966870989
Epoch: 87 Batch: 400
Training Loss: 0.07919571138918399
Epoch: 87 Batch: 450
Training Loss: 0.06891257756286197
Epoch: 87 Batch: 500
Training Loss: 0.06271401107311249
Epoch: 87 Batch: 550
Training Loss: 0.056589843847534876
Epoch: 87 Batch: 600
Training Loss: 0.05398548801740011
Epoch: 87 Batch: 650
Training Loss: 0.04764601941292103
Epoch: 87 Batch: 700
Training Loss: 0.04633069068193436
Epoch: 87 Batch: 750
Training Loss: 0.04134457012017568
Epoch: 87 Batch: 800
Training Loss: 0.04113537512719631
Epoch: 87 Batch: 850
Training Loss: 0.035980698887039635
Epoch: 87 Batch: 900
Training Loss: 0.03529734922779931
Epoch: 87 Batch: 950
Training Loss: 0.03248523298062776
Epoch: 87 Batch: 1000
Training Loss: 0.031911934465169906
Epoch: 87 Batch: 1050
Training Loss: 0.02977449195725577
Epoch: 87 Batch: 1100
Training Loss: 0.02968983403661034
Epoch: 87 Batch: 1150
Training Loss: 0.02660469236581222
Epoch: 87 Batch: 1200
Training Loss: 0.025645316764712333
Epoch: 87 Batch: 1250
Training Loss: 0.024901000905036927
Epoch: 87 Batch: 1300
Training Loss: 0.02344591606121797
Epoch: 87 Batch: 1350
Training Loss: 0.022653594855908996
Epoch: 87 Batch: 1400
Training Loss: 0.0226213290648801
Epoch: 87 Batch: 1450
Training Loss: 0.021467216775335113
Epoch: 87 Batch: 1500
Training Loss: 0.020407986323038738
Epoch: 87 Batch: 1550
Training Loss: 0.020199803748438435
Epoch: 87 Batch: 1600
Training Loss: 0.019602730739861726
Epoch: 87 Batch: 1650
Training Loss: 0.018983052842544788
Epoch: 87 Batch: 1700
Training Loss: 0.01817333614124971
Epoch: 87 Batch: 1750
Training Loss: 0.017609842113086156
Epoch: 87 Batch: 1800
Training Loss: 0.018116724093755088
Epoch: 87 Batch: 1850
Training Loss: 0.01720975221814336
Epoch: 87 Batch: 1900
Training Loss: 0.015996031180808418
Epoch: 87 Batch: 1950
Training Loss: 0.015111789183738905
Epoch: 87 Batch: 2000
Training Loss: 0.015391079992055893
Epoch: 87 Batch: 2050
Training Loss: 0.015940271703208365
Epoch: 87 Batch: 2100
Training Loss: 0.014561465269043332
Epoch: 87 Batch: 2150
Training Loss: 0.015041127301925836
Epoch: 87 Batch: 2200
Training Loss: 0.014154940030791542
Epoch: 87 Batch: 2250
Training Loss: 0.013877915342648824
Epoch: 87 Batch: 2300
Training Loss: 0.013384462037812109
Epoch: 87 Batch: 2350
Training Loss: 0.013129652599070934
Epoch: 87 Batch: 2400
Training Loss: 0.01304671990374724
Epoch: 87 Batch: 2450
Training Loss: 0.012709744584803678
Epoch: 87 Batch: 2500
Training Loss: 0.012406181824207305
Epoch: 87 Batch: 2550
Training Loss: 0.011783178027938395
Epoch: 87 Batch: 2600
Training Loss: 0.011897831307007715
Epoch: 87 Batch: 2650
Training Loss: 0.011461874775166782
Epoch: 87 Batch: 2700
Training Loss: 0.011551489289160128
Epoch: 87 Batch: 2750
Training Loss: 0.01133953006701036
Epoch: 87 Batch: 2800
Training Loss: 0.010692549337233816
Epoch: 87 Batch: 2850
Training Loss: 0.010543747042354784
Epoch: 87 Batch: 2900
Training Loss: 0.01037476449177183
Epoch: 87 Batch: 2950
Training Loss: 0.010515117837210833
Epoch: 87 Batch: 3000
Training Loss: 0.010652048955361048
Epoch: 87 Batch: 3050
Training Loss: 0.009841934217781317
Epoch: 87 Batch: 3100
Training Loss: 0.010069367635634638
Epoch: 87 Batch: 3150
Training Loss: 0.009811278004494925
Epoch: 87 Batch: 3200
Training Loss: 0.009784043654799462
Epoch: 88 
 Validation Loss: 0.4810255961285697
---------------------------
Epoch: 88 Batch: 50
Training Loss: 0.6139618337154389
Epoch: 88 Batch: 100
Training Loss: 0.29976967453956604
Epoch: 88 Batch: 150
Training Loss: 0.2030928675333659
Epoch: 88 Batch: 200
Training Loss: 0.15086766690015793
Epoch: 88 Batch: 250
Training Loss: 0.1205624463558197
Epoch: 88 Batch: 300
Training Loss: 0.10631528466939927
Epoch: 88 Batch: 350
Training Loss: 0.08534875180040087
Epoch: 88 Batch: 400
Training Loss: 0.0781735423207283
Epoch: 88 Batch: 450
Training Loss: 0.07079456335968441
Epoch: 88 Batch: 500
Training Loss: 0.05957955926656723
Epoch: 88 Batch: 550
Training Loss: 0.057082868218421935
Epoch: 88 Batch: 600
Training Loss: 0.04987793818116188
Epoch: 88 Batch: 650
Training Loss: 0.05069066831698785
Epoch: 88 Batch: 700
Training Loss: 0.04417021278824125
Epoch: 88 Batch: 750
Training Loss: 0.04081854327519735
Epoch: 88 Batch: 800
Training Loss: 0.039007566794753076
Epoch: 88 Batch: 850
Training Loss: 0.03674594938755035
Epoch: 88 Batch: 900
Training Loss: 0.035619475344816846
Epoch: 88 Batch: 950
Training Loss: 0.03373517808161284
Epoch: 88 Batch: 1000
Training Loss: 0.030757028073072432
Epoch: 88 Batch: 1050
Training Loss: 0.030209467269125437
Epoch: 88 Batch: 1100
Training Loss: 0.0286165960539471
Epoch: 88 Batch: 1150
Training Loss: 0.027053715664407483
Epoch: 88 Batch: 1200
Training Loss: 0.02722498695055644
Epoch: 88 Batch: 1250
Training Loss: 0.025099554777145386
Epoch: 88 Batch: 1300
Training Loss: 0.024017407504411843
Epoch: 88 Batch: 1350
Training Loss: 0.024411666172522085
Epoch: 88 Batch: 1400
Training Loss: 0.022422585572515214
Epoch: 88 Batch: 1450
Training Loss: 0.02315010925819134
Epoch: 88 Batch: 1500
Training Loss: 0.021084569454193115
Epoch: 88 Batch: 1550
Training Loss: 0.02052163875872089
Epoch: 88 Batch: 1600
Training Loss: 0.019929009471088648
Epoch: 88 Batch: 1650
Training Loss: 0.018305946389834085
Epoch: 88 Batch: 1700
Training Loss: 0.01804706806645674
Epoch: 88 Batch: 1750
Training Loss: 0.018255983710289
Epoch: 88 Batch: 1800
Training Loss: 0.017373743140035206
Epoch: 88 Batch: 1850
Training Loss: 0.01668773712338628
Epoch: 88 Batch: 1900
Training Loss: 0.016907188359059785
Epoch: 88 Batch: 1950
Training Loss: 0.0160846246053011
Epoch: 88 Batch: 2000
Training Loss: 0.016521305322647093
Epoch: 88 Batch: 2050
Training Loss: 0.015437848248132845
Epoch: 88 Batch: 2100
Training Loss: 0.015131006879465921
Epoch: 88 Batch: 2150
Training Loss: 0.014520060517067133
Epoch: 88 Batch: 2200
Training Loss: 0.014412666315382178
Epoch: 88 Batch: 2250
Training Loss: 0.01429962846967909
Epoch: 88 Batch: 2300
Training Loss: 0.01406120055395624
Epoch: 88 Batch: 2350
Training Loss: 0.013639907190140257
Epoch: 88 Batch: 2400
Training Loss: 0.012946261912584306
Epoch: 88 Batch: 2450
Training Loss: 0.013033026298698114
Epoch: 88 Batch: 2500
Training Loss: 0.012446863567829132
Epoch: 88 Batch: 2550
Training Loss: 0.01232283872716567
Epoch: 88 Batch: 2600
Training Loss: 0.012040486897413548
Epoch: 88 Batch: 2650
Training Loss: 0.01179461419582367
Epoch: 88 Batch: 2700
Training Loss: 0.011462800613156071
Epoch: 88 Batch: 2750
Training Loss: 0.011084351203658364
Epoch: 88 Batch: 2800
Training Loss: 0.01083480427307742
Epoch: 88 Batch: 2850
Training Loss: 0.010783060663624814
Epoch: 88 Batch: 2900
Training Loss: 0.010599095790550626
Epoch: 88 Batch: 2950
Training Loss: 0.01082175565978228
Epoch: 88 Batch: 3000
Training Loss: 0.010746274252732595
Epoch: 88 Batch: 3050
Training Loss: 0.010131980004857799
Epoch: 88 Batch: 3100
Training Loss: 0.010137693122509988
Epoch: 88 Batch: 3150
Training Loss: 0.009473867946200901
Epoch: 88 Batch: 3200
Training Loss: 0.009696686482056975
Epoch: 89 
 Validation Loss: 0.4816273209121492
---------------------------
Epoch: 89 Batch: 50
Training Loss: 0.635533994436264
Epoch: 89 Batch: 100
Training Loss: 0.312938771545887
Epoch: 89 Batch: 150
Training Loss: 0.20601145724455516
Epoch: 89 Batch: 200
Training Loss: 0.15832732871174812
Epoch: 89 Batch: 250
Training Loss: 0.12646578133106232
Epoch: 89 Batch: 300
Training Loss: 0.10610269010066986
Epoch: 89 Batch: 350
Training Loss: 0.08659868180751801
Epoch: 89 Batch: 400
Training Loss: 0.07665640100836754
Epoch: 89 Batch: 450
Training Loss: 0.06727138174904718
Epoch: 89 Batch: 500
Training Loss: 0.06397196316719055
Epoch: 89 Batch: 550
Training Loss: 0.05795740626075051
Epoch: 89 Batch: 600
Training Loss: 0.052180355538924535
Epoch: 89 Batch: 650
Training Loss: 0.049226008561941294
Epoch: 89 Batch: 700
Training Loss: 0.04287832038743156
Epoch: 89 Batch: 750
Training Loss: 0.040723419109980265
Epoch: 89 Batch: 800
Training Loss: 0.037484259344637394
Epoch: 89 Batch: 850
Training Loss: 0.0358043875764398
Epoch: 89 Batch: 900
Training Loss: 0.035208396911621094
Epoch: 89 Batch: 950
Training Loss: 0.0317668816917821
Epoch: 89 Batch: 1000
Training Loss: 0.031124750047922135
Epoch: 89 Batch: 1050
Training Loss: 0.03011294941107432
Epoch: 89 Batch: 1100
Training Loss: 0.029335086643695833
Epoch: 89 Batch: 1150
Training Loss: 0.027305448418078214
Epoch: 89 Batch: 1200
Training Loss: 0.026567995722095172
Epoch: 89 Batch: 1250
Training Loss: 0.02524216396808624
Epoch: 89 Batch: 1300
Training Loss: 0.023678996035685906
Epoch: 89 Batch: 1350
Training Loss: 0.022648481572115863
Epoch: 89 Batch: 1400
Training Loss: 0.022265682007585254
Epoch: 89 Batch: 1450
Training Loss: 0.021043888042713035
Epoch: 89 Batch: 1500
Training Loss: 0.020957192997137705
Epoch: 89 Batch: 1550
Training Loss: 0.019612972332585243
Epoch: 89 Batch: 1600
Training Loss: 0.01850112983956933
Epoch: 89 Batch: 1650
Training Loss: 0.01926063201644204
Epoch: 89 Batch: 1700
Training Loss: 0.018621813023791595
Epoch: 89 Batch: 1750
Training Loss: 0.01772285178729466
Epoch: 89 Batch: 1800
Training Loss: 0.01710763124956025
Epoch: 89 Batch: 1850
Training Loss: 0.016811595865198085
Epoch: 89 Batch: 1900
Training Loss: 0.015926940284277262
Epoch: 89 Batch: 1950
Training Loss: 0.016265184558354892
Epoch: 89 Batch: 2000
Training Loss: 0.015383111923933029
Epoch: 89 Batch: 2050
Training Loss: 0.0153093538633207
Epoch: 89 Batch: 2100
Training Loss: 0.015091947544188727
Epoch: 89 Batch: 2150
Training Loss: 0.014224271067353182
Epoch: 89 Batch: 2200
Training Loss: 0.014035697687755932
Epoch: 89 Batch: 2250
Training Loss: 0.013568388356102837
Epoch: 89 Batch: 2300
Training Loss: 0.013343956172466278
Epoch: 89 Batch: 2350
Training Loss: 0.01335208867458587
Epoch: 89 Batch: 2400
Training Loss: 0.01323552742600441
Epoch: 89 Batch: 2450
Training Loss: 0.012135088954653058
Epoch: 89 Batch: 2500
Training Loss: 0.012531682217121124
Epoch: 89 Batch: 2550
Training Loss: 0.012267700129864263
Epoch: 89 Batch: 2600
Training Loss: 0.011997784261520093
Epoch: 89 Batch: 2650
Training Loss: 0.01206379474333997
Epoch: 89 Batch: 2700
Training Loss: 0.012219903590502562
Epoch: 89 Batch: 2750
Training Loss: 0.011325870860706676
Epoch: 89 Batch: 2800
Training Loss: 0.010992122516036033
Epoch: 89 Batch: 2850
Training Loss: 0.010959207792031137
Epoch: 89 Batch: 2900
Training Loss: 0.010646285928528884
Epoch: 89 Batch: 2950
Training Loss: 0.01072291663137533
Epoch: 89 Batch: 3000
Training Loss: 0.0105266375541687
Epoch: 89 Batch: 3050
Training Loss: 0.010267841415327104
Epoch: 89 Batch: 3100
Training Loss: 0.010043430857120022
Epoch: 89 Batch: 3150
Training Loss: 0.010026950126602535
Epoch: 89 Batch: 3200
Training Loss: 0.009862906178459524
Epoch: 90 
 Validation Loss: 0.48121071060498555
---------------------------
Epoch: 90 Batch: 50
Training Loss: 0.6729849267005921
Epoch: 90 Batch: 100
Training Loss: 0.31701102286577226
Epoch: 90 Batch: 150
Training Loss: 0.20349083681901295
Epoch: 90 Batch: 200
Training Loss: 0.15524590879678726
Epoch: 90 Batch: 250
Training Loss: 0.13013339996337892
Epoch: 90 Batch: 300
Training Loss: 0.10335019946098328
Epoch: 90 Batch: 350
Training Loss: 0.08772174664906093
Epoch: 90 Batch: 400
Training Loss: 0.0744925507158041
Epoch: 90 Batch: 450
Training Loss: 0.06951176199648115
Epoch: 90 Batch: 500
Training Loss: 0.06278061884641647
Epoch: 90 Batch: 550
Training Loss: 0.05328060171820901
Epoch: 90 Batch: 600
Training Loss: 0.054193153977394104
Epoch: 90 Batch: 650
Training Loss: 0.04695330729851356
Epoch: 90 Batch: 700
Training Loss: 0.044133459116731374
Epoch: 90 Batch: 750
Training Loss: 0.04069173749287923
Epoch: 90 Batch: 800
Training Loss: 0.039213752038776875
Epoch: 90 Batch: 850
Training Loss: 0.037985749384936165
Epoch: 90 Batch: 900
Training Loss: 0.03607861849996779
Epoch: 90 Batch: 950
Training Loss: 0.0314781507692839
Epoch: 90 Batch: 1000
Training Loss: 0.031118179172277452
Epoch: 90 Batch: 1050
Training Loss: 0.028592312790098644
Epoch: 90 Batch: 1100
Training Loss: 0.029130701178854163
Epoch: 90 Batch: 1150
Training Loss: 0.02692686026510985
Epoch: 90 Batch: 1200
Training Loss: 0.025812566702564557
Epoch: 90 Batch: 1250
Training Loss: 0.024366144180297852
Epoch: 90 Batch: 1300
Training Loss: 0.02454433127091481
Epoch: 90 Batch: 1350
Training Loss: 0.02191727892116264
Epoch: 90 Batch: 1400
Training Loss: 0.021637620542730603
Epoch: 90 Batch: 1450
Training Loss: 0.022002913623020568
Epoch: 90 Batch: 1500
Training Loss: 0.020423290193080902
Epoch: 90 Batch: 1550
Training Loss: 0.02015648588057487
Epoch: 90 Batch: 1600
Training Loss: 0.01950222663581371
Epoch: 90 Batch: 1650
Training Loss: 0.01940477658401836
Epoch: 90 Batch: 1700
Training Loss: 0.019053189263624305
Epoch: 90 Batch: 1750
Training Loss: 0.01722117441041129
Epoch: 90 Batch: 1800
Training Loss: 0.016991931746403375
Epoch: 90 Batch: 1850
Training Loss: 0.01705881495733519
Epoch: 90 Batch: 1900
Training Loss: 0.01611317669090472
Epoch: 90 Batch: 1950
Training Loss: 0.016835549565461967
Epoch: 90 Batch: 2000
Training Loss: 0.015349564597010612
Epoch: 90 Batch: 2050
Training Loss: 0.014660746280739947
Epoch: 90 Batch: 2100
Training Loss: 0.014701362479300727
Epoch: 90 Batch: 2150
Training Loss: 0.014394241876380389
Epoch: 90 Batch: 2200
Training Loss: 0.013835307589986108
Epoch: 90 Batch: 2250
Training Loss: 0.014145203166537815
Epoch: 90 Batch: 2300
Training Loss: 0.01335841311060864
Epoch: 90 Batch: 2350
Training Loss: 0.012967209777933485
Epoch: 90 Batch: 2400
Training Loss: 0.013393230525155862
Epoch: 90 Batch: 2450
Training Loss: 0.013101564670095638
Epoch: 90 Batch: 2500
Training Loss: 0.012479638385772705
Epoch: 90 Batch: 2550
Training Loss: 0.011935687848165923
Epoch: 90 Batch: 2600
Training Loss: 0.012026935987747633
Epoch: 90 Batch: 2650
Training Loss: 0.01203786810614028
Epoch: 90 Batch: 2700
Training Loss: 0.011742763287491268
Epoch: 90 Batch: 2750
Training Loss: 0.011534509279511192
Epoch: 90 Batch: 2800
Training Loss: 0.011171071838055338
Epoch: 90 Batch: 2850
Training Loss: 0.010815414650398389
Epoch: 90 Batch: 2900
Training Loss: 0.010612303964022933
Epoch: 90 Batch: 2950
Training Loss: 0.010006013391381604
Epoch: 90 Batch: 3000
Training Loss: 0.010300477464993794
Epoch: 90 Batch: 3050
Training Loss: 0.0108091495467014
Epoch: 90 Batch: 3100
Training Loss: 0.010020029573671279
Epoch: 90 Batch: 3150
Training Loss: 0.009799878777019561
Epoch: 90 Batch: 3200
Training Loss: 0.009597429381683469
Epoch: 91 
 Validation Loss: 0.48086245159308116
---------------------------
Epoch: 91 Batch: 50
Training Loss: 0.6268442070484161
Epoch: 91 Batch: 100
Training Loss: 0.3065338459610939
Epoch: 91 Batch: 150
Training Loss: 0.21140214920043945
Epoch: 91 Batch: 200
Training Loss: 0.15296737357974052
Epoch: 91 Batch: 250
Training Loss: 0.12574354660511017
Epoch: 91 Batch: 300
Training Loss: 0.1007367875178655
Epoch: 91 Batch: 350
Training Loss: 0.0869385096005031
Epoch: 91 Batch: 400
Training Loss: 0.07173066511750222
Epoch: 91 Batch: 450
Training Loss: 0.0663478453291787
Epoch: 91 Batch: 500
Training Loss: 0.06303917598724365
Epoch: 91 Batch: 550
Training Loss: 0.05782457915219394
Epoch: 91 Batch: 600
Training Loss: 0.0521114245057106
Epoch: 91 Batch: 650
Training Loss: 0.04884583661189446
Epoch: 91 Batch: 700
Training Loss: 0.04643407195806503
Epoch: 91 Batch: 750
Training Loss: 0.0406285229921341
Epoch: 91 Batch: 800
Training Loss: 0.04047246694564819
Epoch: 91 Batch: 850
Training Loss: 0.036655804549946505
Epoch: 91 Batch: 900
Training Loss: 0.03492354936069912
Epoch: 91 Batch: 950
Training Loss: 0.031488131002375956
Epoch: 91 Batch: 1000
Training Loss: 0.030781543284654617
Epoch: 91 Batch: 1050
Training Loss: 0.029537053278514316
Epoch: 91 Batch: 1100
Training Loss: 0.027160833748904142
Epoch: 91 Batch: 1150
Training Loss: 0.026658920904864436
Epoch: 91 Batch: 1200
Training Loss: 0.0263408096631368
Epoch: 91 Batch: 1250
Training Loss: 0.02475107991695404
Epoch: 91 Batch: 1300
Training Loss: 0.023553083837032317
Epoch: 91 Batch: 1350
Training Loss: 0.023520728636670995
Epoch: 91 Batch: 1400
Training Loss: 0.020399787511144366
Epoch: 91 Batch: 1450
Training Loss: 0.02168413602072617
Epoch: 91 Batch: 1500
Training Loss: 0.020946462551752726
Epoch: 91 Batch: 1550
Training Loss: 0.019832761191552684
Epoch: 91 Batch: 1600
Training Loss: 0.0189531422406435
Epoch: 91 Batch: 1650
Training Loss: 0.01958528883529432
Epoch: 91 Batch: 1700
Training Loss: 0.018285529140163872
Epoch: 91 Batch: 1750
Training Loss: 0.01744514572620392
Epoch: 91 Batch: 1800
Training Loss: 0.017430414060751596
Epoch: 91 Batch: 1850
Training Loss: 0.016293442104313823
Epoch: 91 Batch: 1900
Training Loss: 0.016484155027489913
Epoch: 91 Batch: 1950
Training Loss: 0.016146522836807446
Epoch: 91 Batch: 2000
Training Loss: 0.01503981150686741
Epoch: 91 Batch: 2050
Training Loss: 0.01491171995314156
Epoch: 91 Batch: 2100
Training Loss: 0.015039121536981491
Epoch: 91 Batch: 2150
Training Loss: 0.014441454341245252
Epoch: 91 Batch: 2200
Training Loss: 0.015049082772298293
Epoch: 91 Batch: 2250
Training Loss: 0.013652982433636983
Epoch: 91 Batch: 2300
Training Loss: 0.013292091113069783
Epoch: 91 Batch: 2350
Training Loss: 0.012698716914400141
Epoch: 91 Batch: 2400
Training Loss: 0.013064142925043902
Epoch: 91 Batch: 2450
Training Loss: 0.012930546378602787
Epoch: 91 Batch: 2500
Training Loss: 0.012077478992938996
Epoch: 91 Batch: 2550
Training Loss: 0.011491957295174693
Epoch: 91 Batch: 2600
Training Loss: 0.01173767219369228
Epoch: 91 Batch: 2650
Training Loss: 0.01128989891061243
Epoch: 91 Batch: 2700
Training Loss: 0.011839137894135935
Epoch: 91 Batch: 2750
Training Loss: 0.012078415979038585
Epoch: 91 Batch: 2800
Training Loss: 0.011995746280465807
Epoch: 91 Batch: 2850
Training Loss: 0.010520834514969274
Epoch: 91 Batch: 2900
Training Loss: 0.010475346702953865
Epoch: 91 Batch: 2950
Training Loss: 0.01020929379988525
Epoch: 91 Batch: 3000
Training Loss: 0.010128613779942194
Epoch: 91 Batch: 3050
Training Loss: 0.010395608847258521
Epoch: 91 Batch: 3100
Training Loss: 0.010537732695379564
Epoch: 91 Batch: 3150
Training Loss: 0.009631181576895336
Epoch: 91 Batch: 3200
Training Loss: 0.009379907278344035
Epoch: 92 
 Validation Loss: 0.48109644452730815
---------------------------
Epoch: 92 Batch: 50
Training Loss: 0.635551563501358
Epoch: 92 Batch: 100
Training Loss: 0.3194899958372116
Epoch: 92 Batch: 150
Training Loss: 0.2041252722342809
Epoch: 92 Batch: 200
Training Loss: 0.1581289504468441
Epoch: 92 Batch: 250
Training Loss: 0.118890380859375
Epoch: 92 Batch: 300
Training Loss: 0.1048861895998319
Epoch: 92 Batch: 350
Training Loss: 0.08969483920506069
Epoch: 92 Batch: 400
Training Loss: 0.08135437063872814
Epoch: 92 Batch: 450
Training Loss: 0.06828000671333737
Epoch: 92 Batch: 500
Training Loss: 0.06212380796670914
Epoch: 92 Batch: 550
Training Loss: 0.057444688244299456
Epoch: 92 Batch: 600
Training Loss: 0.05066562339663506
Epoch: 92 Batch: 650
Training Loss: 0.050923148668729344
Epoch: 92 Batch: 700
Training Loss: 0.04503050736018589
Epoch: 92 Batch: 750
Training Loss: 0.04173295529683431
Epoch: 92 Batch: 800
Training Loss: 0.0388419596105814
Epoch: 92 Batch: 850
Training Loss: 0.03724104134475484
Epoch: 92 Batch: 900
Training Loss: 0.03477509942319658
Epoch: 92 Batch: 950
Training Loss: 0.03245508422977046
Epoch: 92 Batch: 1000
Training Loss: 0.031698676347732546
Epoch: 92 Batch: 1050
Training Loss: 0.029269312762078784
Epoch: 92 Batch: 1100
Training Loss: 0.028384293724190105
Epoch: 92 Batch: 1150
Training Loss: 0.027719157623208087
Epoch: 92 Batch: 1200
Training Loss: 0.025932659010092417
Epoch: 92 Batch: 1250
Training Loss: 0.02432679908275604
Epoch: 92 Batch: 1300
Training Loss: 0.023345529093192173
Epoch: 92 Batch: 1350
Training Loss: 0.023734901679886712
Epoch: 92 Batch: 1400
Training Loss: 0.02201710594551904
Epoch: 92 Batch: 1450
Training Loss: 0.02098985176661919
Epoch: 92 Batch: 1500
Training Loss: 0.020388144711653393
Epoch: 92 Batch: 1550
Training Loss: 0.01978621457853625
Epoch: 92 Batch: 1600
Training Loss: 0.020018857941031455
Epoch: 92 Batch: 1650
Training Loss: 0.018601801467664315
Epoch: 92 Batch: 1700
Training Loss: 0.01879980159156463
Epoch: 92 Batch: 1750
Training Loss: 0.0176254700422287
Epoch: 92 Batch: 1800
Training Loss: 0.017300231092505985
Epoch: 92 Batch: 1850
Training Loss: 0.017179189018301063
Epoch: 92 Batch: 1900
Training Loss: 0.016591192076080723
Epoch: 92 Batch: 1950
Training Loss: 0.01654334728534405
Epoch: 92 Batch: 2000
Training Loss: 0.01560844849050045
Epoch: 92 Batch: 2050
Training Loss: 0.01495355545020685
Epoch: 92 Batch: 2100
Training Loss: 0.014952739619073414
Epoch: 92 Batch: 2150
Training Loss: 0.013887443916742192
Epoch: 92 Batch: 2200
Training Loss: 0.013855866708538748
Epoch: 92 Batch: 2250
Training Loss: 0.0138411822186576
Epoch: 92 Batch: 2300
Training Loss: 0.013898364538731783
Epoch: 92 Batch: 2350
Training Loss: 0.013344332005115266
Epoch: 92 Batch: 2400
Training Loss: 0.013084754583736261
Epoch: 92 Batch: 2450
Training Loss: 0.012594794898617024
Epoch: 92 Batch: 2500
Training Loss: 0.011771744310855866
Epoch: 92 Batch: 2550
Training Loss: 0.012157478694822274
Epoch: 92 Batch: 2600
Training Loss: 0.01216160679092774
Epoch: 92 Batch: 2650
Training Loss: 0.011641953452578131
Epoch: 92 Batch: 2700
Training Loss: 0.011493660476472642
Epoch: 92 Batch: 2750
Training Loss: 0.011103379921479658
Epoch: 92 Batch: 2800
Training Loss: 0.010604829522115843
Epoch: 92 Batch: 2850
Training Loss: 0.01093867262204488
Epoch: 92 Batch: 2900
Training Loss: 0.009833703626846445
Epoch: 92 Batch: 2950
Training Loss: 0.010488704311645638
Epoch: 92 Batch: 3000
Training Loss: 0.0100620805422465
Epoch: 92 Batch: 3050
Training Loss: 0.010432812385871761
Epoch: 92 Batch: 3100
Training Loss: 0.010096074073545394
Epoch: 92 Batch: 3150
Training Loss: 0.009834778564316886
Epoch: 92 Batch: 3200
Training Loss: 0.009764517024159432
Epoch: 93 
 Validation Loss: 0.4804828315973282
---------------------------
Epoch: 93 Batch: 50
Training Loss: 0.6319018191099167
Epoch: 93 Batch: 100
Training Loss: 0.2965335538983345
Epoch: 93 Batch: 150
Training Loss: 0.20626598437627156
Epoch: 93 Batch: 200
Training Loss: 0.1512344405055046
Epoch: 93 Batch: 250
Training Loss: 0.1266909931898117
Epoch: 93 Batch: 300
Training Loss: 0.10656509379545848
Epoch: 93 Batch: 350
Training Loss: 0.08522360784666878
Epoch: 93 Batch: 400
Training Loss: 0.07903524793684483
Epoch: 93 Batch: 450
Training Loss: 0.06726700531111823
Epoch: 93 Batch: 500
Training Loss: 0.06303051787614822
Epoch: 93 Batch: 550
Training Loss: 0.05849519334056161
Epoch: 93 Batch: 600
Training Loss: 0.051403370648622514
Epoch: 93 Batch: 650
Training Loss: 0.046810247026957
Epoch: 93 Batch: 700
Training Loss: 0.042921294655118666
Epoch: 93 Batch: 750
Training Loss: 0.04041567301750183
Epoch: 93 Batch: 800
Training Loss: 0.03715037818998099
Epoch: 93 Batch: 850
Training Loss: 0.03603861447642831
Epoch: 93 Batch: 900
Training Loss: 0.03453818294737074
Epoch: 93 Batch: 950
Training Loss: 0.03304022362357692
Epoch: 93 Batch: 1000
Training Loss: 0.03145785212516785
Epoch: 93 Batch: 1050
Training Loss: 0.029265022022383555
Epoch: 93 Batch: 1100
Training Loss: 0.027936042059551587
Epoch: 93 Batch: 1150
Training Loss: 0.026133260571438333
Epoch: 93 Batch: 1200
Training Loss: 0.025661547084649405
Epoch: 93 Batch: 1250
Training Loss: 0.02368458354473114
Epoch: 93 Batch: 1300
Training Loss: 0.024104662400025588
Epoch: 93 Batch: 1350
Training Loss: 0.024138325695638303
Epoch: 93 Batch: 1400
Training Loss: 0.02184313327074051
Epoch: 93 Batch: 1450
Training Loss: 0.021786627481723653
Epoch: 93 Batch: 1500
Training Loss: 0.01977560019493103
Epoch: 93 Batch: 1550
Training Loss: 0.019635985147568487
Epoch: 93 Batch: 1600
Training Loss: 0.018761678114533425
Epoch: 93 Batch: 1650
Training Loss: 0.019193803270657856
Epoch: 93 Batch: 1700
Training Loss: 0.017147114855401657
Epoch: 93 Batch: 1750
Training Loss: 0.018071884053094046
Epoch: 93 Batch: 1800
Training Loss: 0.01730635141332944
Epoch: 93 Batch: 1850
Training Loss: 0.0162022520561476
Epoch: 93 Batch: 1900
Training Loss: 0.015813993623382167
Epoch: 93 Batch: 1950
Training Loss: 0.01539498636355767
Epoch: 93 Batch: 2000
Training Loss: 0.015576693207025528
Epoch: 93 Batch: 2050
Training Loss: 0.016307411557290614
Epoch: 93 Batch: 2100
Training Loss: 0.015019152590206692
Epoch: 93 Batch: 2150
Training Loss: 0.01498521278070849
Epoch: 93 Batch: 2200
Training Loss: 0.01388236169110645
Epoch: 93 Batch: 2250
Training Loss: 0.013288859062724643
Epoch: 93 Batch: 2300
Training Loss: 0.014019001813038535
Epoch: 93 Batch: 2350
Training Loss: 0.013970802527792911
Epoch: 93 Batch: 2400
Training Loss: 0.012919887167712052
Epoch: 93 Batch: 2450
Training Loss: 0.012687097483751726
Epoch: 93 Batch: 2500
Training Loss: 0.012776166498661042
Epoch: 93 Batch: 2550
Training Loss: 0.012993609052078397
Epoch: 93 Batch: 2600
Training Loss: 0.011804196800176914
Epoch: 93 Batch: 2650
Training Loss: 0.012013540245452018
Epoch: 93 Batch: 2700
Training Loss: 0.011321975820594363
Epoch: 93 Batch: 2750
Training Loss: 0.011622547127983787
Epoch: 93 Batch: 2800
Training Loss: 0.01130927491400923
Epoch: 93 Batch: 2850
Training Loss: 0.011160786476051598
Epoch: 93 Batch: 2900
Training Loss: 0.010776110219544378
Epoch: 93 Batch: 2950
Training Loss: 0.010756090535955914
Epoch: 93 Batch: 3000
Training Loss: 0.010198462436596552
Epoch: 93 Batch: 3050
Training Loss: 0.00995341187617818
Epoch: 93 Batch: 3100
Training Loss: 0.009691019452387286
Epoch: 93 Batch: 3150
Training Loss: 0.009833191794062418
Epoch: 93 Batch: 3200
Training Loss: 0.009748633997514845
Epoch: 94 
 Validation Loss: 0.480658252702819
---------------------------
Epoch: 94 Batch: 50
Training Loss: 0.6199365150928497
Epoch: 94 Batch: 100
Training Loss: 0.30974565833806994
Epoch: 94 Batch: 150
Training Loss: 0.20177530825138093
Epoch: 94 Batch: 200
Training Loss: 0.16054183542728423
Epoch: 94 Batch: 250
Training Loss: 0.12333961915969849
Epoch: 94 Batch: 300
Training Loss: 0.10468507379293442
Epoch: 94 Batch: 350
Training Loss: 0.09340103447437287
Epoch: 94 Batch: 400
Training Loss: 0.076838888078928
Epoch: 94 Batch: 450
Training Loss: 0.06982548342810736
Epoch: 94 Batch: 500
Training Loss: 0.06352757179737091
Epoch: 94 Batch: 550
Training Loss: 0.05675608309832486
Epoch: 94 Batch: 600
Training Loss: 0.05229163800676664
Epoch: 94 Batch: 650
Training Loss: 0.04700218507876763
Epoch: 94 Batch: 700
Training Loss: 0.0435108425787517
Epoch: 94 Batch: 750
Training Loss: 0.04114004397392273
Epoch: 94 Batch: 800
Training Loss: 0.03963847894221544
Epoch: 94 Batch: 850
Training Loss: 0.036402929425239566
Epoch: 94 Batch: 900
Training Loss: 0.03537277059422599
Epoch: 94 Batch: 950
Training Loss: 0.03282383049789228
Epoch: 94 Batch: 1000
Training Loss: 0.031540267199277876
Epoch: 94 Batch: 1050
Training Loss: 0.03052943996020726
Epoch: 94 Batch: 1100
Training Loss: 0.028750094419175927
Epoch: 94 Batch: 1150
Training Loss: 0.026396086241887964
Epoch: 94 Batch: 1200
Training Loss: 0.026345387324690817
Epoch: 94 Batch: 1250
Training Loss: 0.02455424792766571
Epoch: 94 Batch: 1300
Training Loss: 0.023844454403106983
Epoch: 94 Batch: 1350
Training Loss: 0.02276048854545311
Epoch: 94 Batch: 1400
Training Loss: 0.02246996562395777
Epoch: 94 Batch: 1450
Training Loss: 0.022051649401927817
Epoch: 94 Batch: 1500
Training Loss: 0.02124331663052241
Epoch: 94 Batch: 1550
Training Loss: 0.02009930460683761
Epoch: 94 Batch: 1600
Training Loss: 0.020466093681752683
Epoch: 94 Batch: 1650
Training Loss: 0.01864811153122873
Epoch: 94 Batch: 1700
Training Loss: 0.018114448031958413
Epoch: 94 Batch: 1750
Training Loss: 0.017918254630906242
Epoch: 94 Batch: 1800
Training Loss: 0.017438130643632678
Epoch: 94 Batch: 1850
Training Loss: 0.01699534185834833
Epoch: 94 Batch: 1900
Training Loss: 0.016150289742570174
Epoch: 94 Batch: 1950
Training Loss: 0.016643514067698748
Epoch: 94 Batch: 2000
Training Loss: 0.01627418228983879
Epoch: 94 Batch: 2050
Training Loss: 0.014737123919696343
Epoch: 94 Batch: 2100
Training Loss: 0.01452256550391515
Epoch: 94 Batch: 2150
Training Loss: 0.014445917301399764
Epoch: 94 Batch: 2200
Training Loss: 0.01367390288548036
Epoch: 94 Batch: 2250
Training Loss: 0.01427024535338084
Epoch: 94 Batch: 2300
Training Loss: 0.013507421716399815
Epoch: 94 Batch: 2350
Training Loss: 0.012714511460446297
Epoch: 94 Batch: 2400
Training Loss: 0.012191033226748308
Epoch: 94 Batch: 2450
Training Loss: 0.012475210038983092
Epoch: 94 Batch: 2500
Training Loss: 0.012144870233535767
Epoch: 94 Batch: 2550
Training Loss: 0.011977900675698823
Epoch: 94 Batch: 2600
Training Loss: 0.011478169480195413
Epoch: 94 Batch: 2650
Training Loss: 0.011438446022429556
Epoch: 94 Batch: 2700
Training Loss: 0.011027150264492741
Epoch: 94 Batch: 2750
Training Loss: 0.01119296173615889
Epoch: 94 Batch: 2800
Training Loss: 0.010989298022219113
Epoch: 94 Batch: 2850
Training Loss: 0.011603189905484517
Epoch: 94 Batch: 2900
Training Loss: 0.010683984314573222
Epoch: 94 Batch: 2950
Training Loss: 0.010335050528332338
Epoch: 94 Batch: 3000
Training Loss: 0.010244884093602498
Epoch: 94 Batch: 3050
Training Loss: 0.010526619954187362
Epoch: 94 Batch: 3100
Training Loss: 0.010630238085023818
Epoch: 94 Batch: 3150
Training Loss: 0.010226902658977206
Epoch: 94 Batch: 3200
Training Loss: 0.009593729684129358
Epoch: 95 
 Validation Loss: 0.48029026521576773
---------------------------
Epoch: 95 Batch: 50
Training Loss: 0.6590918922424316
Epoch: 95 Batch: 100
Training Loss: 0.3240038278698921
Epoch: 95 Batch: 150
Training Loss: 0.20234519302845
Epoch: 95 Batch: 200
Training Loss: 0.15352219626307487
Epoch: 95 Batch: 250
Training Loss: 0.12513052248954773
Epoch: 95 Batch: 300
Training Loss: 0.10171996474266053
Epoch: 95 Batch: 350
Training Loss: 0.0892998217684882
Epoch: 95 Batch: 400
Training Loss: 0.07990161381661892
Epoch: 95 Batch: 450
Training Loss: 0.06868901696470049
Epoch: 95 Batch: 500
Training Loss: 0.06329102319478988
Epoch: 95 Batch: 550
Training Loss: 0.054801843924955886
Epoch: 95 Batch: 600
Training Loss: 0.05129388471444448
Epoch: 95 Batch: 650
Training Loss: 0.04913399976033431
Epoch: 95 Batch: 700
Training Loss: 0.04290505013295583
Epoch: 95 Batch: 750
Training Loss: 0.041160624305407206
Epoch: 95 Batch: 800
Training Loss: 0.03795547716319561
Epoch: 95 Batch: 850
Training Loss: 0.038570013011203094
Epoch: 95 Batch: 900
Training Loss: 0.03562586045927472
Epoch: 95 Batch: 950
Training Loss: 0.033003118728336535
Epoch: 95 Batch: 1000
Training Loss: 0.030503914505243302
Epoch: 95 Batch: 1050
Training Loss: 0.030650644444283986
Epoch: 95 Batch: 1100
Training Loss: 0.02649628929116509
Epoch: 95 Batch: 1150
Training Loss: 0.027581521220829176
Epoch: 95 Batch: 1200
Training Loss: 0.025453521957000097
Epoch: 95 Batch: 1250
Training Loss: 0.024597920989990234
Epoch: 95 Batch: 1300
Training Loss: 0.022677667484833644
Epoch: 95 Batch: 1350
Training Loss: 0.022919052066626373
Epoch: 95 Batch: 1400
Training Loss: 0.021630087069102695
Epoch: 95 Batch: 1450
Training Loss: 0.021460889002372478
Epoch: 95 Batch: 1500
Training Loss: 0.01916307614247004
Epoch: 95 Batch: 1550
Training Loss: 0.02047071356927195
Epoch: 95 Batch: 1600
Training Loss: 0.02010848041623831
Epoch: 95 Batch: 1650
Training Loss: 0.018773098028067387
Epoch: 95 Batch: 1700
Training Loss: 0.01818995211054297
Epoch: 95 Batch: 1750
Training Loss: 0.018147861889430454
Epoch: 95 Batch: 1800
Training Loss: 0.01721076356040107
Epoch: 95 Batch: 1850
Training Loss: 0.01697791444288718
Epoch: 95 Batch: 1900
Training Loss: 0.01611046629516702
Epoch: 95 Batch: 1950
Training Loss: 0.01724466742613377
Epoch: 95 Batch: 2000
Training Loss: 0.01493807679414749
Epoch: 95 Batch: 2050
Training Loss: 0.01458858649905135
Epoch: 95 Batch: 2100
Training Loss: 0.015075412193934122
Epoch: 95 Batch: 2150
Training Loss: 0.014820338487625121
Epoch: 95 Batch: 2200
Training Loss: 0.01465227712284435
Epoch: 95 Batch: 2250
Training Loss: 0.01336939149432712
Epoch: 95 Batch: 2300
Training Loss: 0.013849398739959882
Epoch: 95 Batch: 2350
Training Loss: 0.013027861473408151
Epoch: 95 Batch: 2400
Training Loss: 0.012494765197237332
Epoch: 95 Batch: 2450
Training Loss: 0.012940387859636424
Epoch: 95 Batch: 2500
Training Loss: 0.012638430631160736
Epoch: 95 Batch: 2550
Training Loss: 0.01242097795009613
Epoch: 95 Batch: 2600
Training Loss: 0.012161225286813883
Epoch: 95 Batch: 2650
Training Loss: 0.011813126138921054
Epoch: 95 Batch: 2700
Training Loss: 0.01146872599919637
Epoch: 95 Batch: 2750
Training Loss: 0.011417787844484502
Epoch: 95 Batch: 2800
Training Loss: 0.011036630954061235
Epoch: 95 Batch: 2850
Training Loss: 0.010676445113985161
Epoch: 95 Batch: 2900
Training Loss: 0.010264486793814035
Epoch: 95 Batch: 2950
Training Loss: 0.01043529679209499
Epoch: 95 Batch: 3000
Training Loss: 0.010398667057355244
Epoch: 95 Batch: 3050
Training Loss: 0.010240501585553905
Epoch: 95 Batch: 3100
Training Loss: 0.009899590871026438
Epoch: 95 Batch: 3150
Training Loss: 0.010152877247522748
Epoch: 95 Batch: 3200
Training Loss: 0.009642748441547156
Epoch: 96 
 Validation Loss: 0.48032132817639245
---------------------------
Epoch: 96 Batch: 50
Training Loss: 0.6275167030096054
Epoch: 96 Batch: 100
Training Loss: 0.32363733381032944
Epoch: 96 Batch: 150
Training Loss: 0.20072284479935965
Epoch: 96 Batch: 200
Training Loss: 0.15899815872311593
Epoch: 96 Batch: 250
Training Loss: 0.12173443710803986
Epoch: 96 Batch: 300
Training Loss: 0.09983809053897857
Epoch: 96 Batch: 350
Training Loss: 0.08808414987155369
Epoch: 96 Batch: 400
Training Loss: 0.0755867275595665
Epoch: 96 Batch: 450
Training Loss: 0.06808044208420648
Epoch: 96 Batch: 500
Training Loss: 0.06437913835048675
Epoch: 96 Batch: 550
Training Loss: 0.057512109117074446
Epoch: 96 Batch: 600
Training Loss: 0.0534302028020223
Epoch: 96 Batch: 650
Training Loss: 0.04977597548411443
Epoch: 96 Batch: 700
Training Loss: 0.04393216920750482
Epoch: 96 Batch: 750
Training Loss: 0.04067413298288981
Epoch: 96 Batch: 800
Training Loss: 0.03890722408890724
Epoch: 96 Batch: 850
Training Loss: 0.03703007827786838
Epoch: 96 Batch: 900
Training Loss: 0.03425476829210917
Epoch: 96 Batch: 950
Training Loss: 0.03239734847294657
Epoch: 96 Batch: 1000
Training Loss: 0.03289505439996719
Epoch: 96 Batch: 1050
Training Loss: 0.029428321179889496
Epoch: 96 Batch: 1100
Training Loss: 0.028309138146313754
Epoch: 96 Batch: 1150
Training Loss: 0.028624044268027594
Epoch: 96 Batch: 1200
Training Loss: 0.02587224173049132
Epoch: 96 Batch: 1250
Training Loss: 0.02401854627132416
Epoch: 96 Batch: 1300
Training Loss: 0.023432185993744775
Epoch: 96 Batch: 1350
Training Loss: 0.02346854446110902
Epoch: 96 Batch: 1400
Training Loss: 0.022124194204807282
Epoch: 96 Batch: 1450
Training Loss: 0.021740743702855603
Epoch: 96 Batch: 1500
Training Loss: 0.02065714681148529
Epoch: 96 Batch: 1550
Training Loss: 0.019475652767765906
Epoch: 96 Batch: 1600
Training Loss: 0.018814329337328673
Epoch: 96 Batch: 1650
Training Loss: 0.018989645477497215
Epoch: 96 Batch: 1700
Training Loss: 0.018069291167399464
Epoch: 96 Batch: 1750
Training Loss: 0.018286533917699542
Epoch: 96 Batch: 1800
Training Loss: 0.016906392789549297
Epoch: 96 Batch: 1850
Training Loss: 0.016775796574515264
Epoch: 96 Batch: 1900
Training Loss: 0.017175248845627433
Epoch: 96 Batch: 1950
Training Loss: 0.016201037871531952
Epoch: 96 Batch: 2000
Training Loss: 0.0158878732919693
Epoch: 96 Batch: 2050
Training Loss: 0.015526821889528414
Epoch: 96 Batch: 2100
Training Loss: 0.014918009440104166
Epoch: 96 Batch: 2150
Training Loss: 0.014113697700722273
Epoch: 96 Batch: 2200
Training Loss: 0.0147565519945188
Epoch: 96 Batch: 2250
Training Loss: 0.013263748301400079
Epoch: 96 Batch: 2300
Training Loss: 0.013205790895482768
Epoch: 96 Batch: 2350
Training Loss: 0.013023315845651829
Epoch: 96 Batch: 2400
Training Loss: 0.012550499401986598
Epoch: 96 Batch: 2450
Training Loss: 0.013326458188952232
Epoch: 96 Batch: 2500
Training Loss: 0.01213139934539795
Epoch: 96 Batch: 2550
Training Loss: 0.012220128868140426
Epoch: 96 Batch: 2600
Training Loss: 0.011598919836374429
Epoch: 96 Batch: 2650
Training Loss: 0.011891163812493378
Epoch: 96 Batch: 2700
Training Loss: 0.01126580931522228
Epoch: 96 Batch: 2750
Training Loss: 0.011322264497930353
Epoch: 96 Batch: 2800
Training Loss: 0.01089710065296718
Epoch: 96 Batch: 2850
Training Loss: 0.010932405423699764
Epoch: 96 Batch: 2900
Training Loss: 0.010573785346129846
Epoch: 96 Batch: 2950
Training Loss: 0.010221582449088662
Epoch: 96 Batch: 3000
Training Loss: 0.010675118486086528
Epoch: 96 Batch: 3050
Training Loss: 0.010256253383198723
Epoch: 96 Batch: 3100
Training Loss: 0.009758088271464072
Epoch: 96 Batch: 3150
Training Loss: 0.009515418088625348
Epoch: 96 Batch: 3200
Training Loss: 0.009439970897510648
Epoch: 97 
 Validation Loss: 0.48003600935141244
---------------------------
Epoch: 97 Batch: 50
Training Loss: 0.6449866950511932
Epoch: 97 Batch: 100
Training Loss: 0.29790713608264924
Epoch: 97 Batch: 150
Training Loss: 0.20675099929173787
Epoch: 97 Batch: 200
Training Loss: 0.1569612853229046
Epoch: 97 Batch: 250
Training Loss: 0.11991532433032989
Epoch: 97 Batch: 300
Training Loss: 0.10522972126801809
Epoch: 97 Batch: 350
Training Loss: 0.08743144205638341
Epoch: 97 Batch: 400
Training Loss: 0.07867994524538517
Epoch: 97 Batch: 450
Training Loss: 0.07047345254156324
Epoch: 97 Batch: 500
Training Loss: 0.06283873951435089
Epoch: 97 Batch: 550
Training Loss: 0.054341335892677305
Epoch: 97 Batch: 600
Training Loss: 0.050150489260752995
Epoch: 97 Batch: 650
Training Loss: 0.04643543174633613
Epoch: 97 Batch: 700
Training Loss: 0.04184627835239683
Epoch: 97 Batch: 750
Training Loss: 0.04290614791711171
Epoch: 97 Batch: 800
Training Loss: 0.03861079640686512
Epoch: 97 Batch: 850
Training Loss: 0.03635863665272208
Epoch: 97 Batch: 900
Training Loss: 0.032694581581486595
Epoch: 97 Batch: 950
Training Loss: 0.034389258591752306
Epoch: 97 Batch: 1000
Training Loss: 0.030713575661182404
Epoch: 97 Batch: 1050
Training Loss: 0.029932606504077003
Epoch: 97 Batch: 1100
Training Loss: 0.027505640197883954
Epoch: 97 Batch: 1150
Training Loss: 0.027961220844932223
Epoch: 97 Batch: 1200
Training Loss: 0.026343313331405323
Epoch: 97 Batch: 1250
Training Loss: 0.02456721017360687
Epoch: 97 Batch: 1300
Training Loss: 0.02381282499203315
Epoch: 97 Batch: 1350
Training Loss: 0.022163863093764693
Epoch: 97 Batch: 1400
Training Loss: 0.02220842114516667
Epoch: 97 Batch: 1450
Training Loss: 0.021264877730402452
Epoch: 97 Batch: 1500
Training Loss: 0.021682005385557812
Epoch: 97 Batch: 1550
Training Loss: 0.019014340446841332
Epoch: 97 Batch: 1600
Training Loss: 0.019907358177006243
Epoch: 97 Batch: 1650
Training Loss: 0.019461611798315338
Epoch: 97 Batch: 1700
Training Loss: 0.018398759698166567
Epoch: 97 Batch: 1750
Training Loss: 0.018050312059266226
Epoch: 97 Batch: 1800
Training Loss: 0.017917362766133413
Epoch: 97 Batch: 1850
Training Loss: 0.016952235602043772
Epoch: 97 Batch: 1900
Training Loss: 0.01636810618011575
Epoch: 97 Batch: 1950
Training Loss: 0.015910824369161556
Epoch: 97 Batch: 2000
Training Loss: 0.01554619960486889
Epoch: 97 Batch: 2050
Training Loss: 0.014207085283791146
Epoch: 97 Batch: 2100
Training Loss: 0.015139845340024858
Epoch: 97 Batch: 2150
Training Loss: 0.013727669452511988
Epoch: 97 Batch: 2200
Training Loss: 0.013769710876724937
Epoch: 97 Batch: 2250
Training Loss: 0.013421743896272447
Epoch: 97 Batch: 2300
Training Loss: 0.013941047671048538
Epoch: 97 Batch: 2350
Training Loss: 0.013015528904630782
Epoch: 97 Batch: 2400
Training Loss: 0.012582718258102735
Epoch: 97 Batch: 2450
Training Loss: 0.01250484336395653
Epoch: 97 Batch: 2500
Training Loss: 0.013020022332668305
Epoch: 97 Batch: 2550
Training Loss: 0.012315699060757955
Epoch: 97 Batch: 2600
Training Loss: 0.012249807807115409
Epoch: 97 Batch: 2650
Training Loss: 0.011411062535249962
Epoch: 97 Batch: 2700
Training Loss: 0.011676999496089087
Epoch: 97 Batch: 2750
Training Loss: 0.011114359736442566
Epoch: 97 Batch: 2800
Training Loss: 0.01147134359393801
Epoch: 97 Batch: 2850
Training Loss: 0.010627917647361755
Epoch: 97 Batch: 2900
Training Loss: 0.010778528904092723
Epoch: 97 Batch: 2950
Training Loss: 0.010692349530882755
Epoch: 97 Batch: 3000
Training Loss: 0.010321477542320887
Epoch: 97 Batch: 3050
Training Loss: 0.010194427967071533
Epoch: 97 Batch: 3100
Training Loss: 0.009793133312656034
Epoch: 97 Batch: 3150
Training Loss: 0.009668912045539371
Epoch: 97 Batch: 3200
Training Loss: 0.009725329149514437
Epoch: 98 
 Validation Loss: 0.47981180250644684
---------------------------
Epoch: 98 Batch: 50
Training Loss: 0.632873861193657
Epoch: 98 Batch: 100
Training Loss: 0.3024084270000458
Epoch: 98 Batch: 150
Training Loss: 0.20485760609308878
Epoch: 98 Batch: 200
Training Loss: 0.15812967121601104
Epoch: 98 Batch: 250
Training Loss: 0.1222439569234848
Epoch: 98 Batch: 300
Training Loss: 0.1016955640912056
Epoch: 98 Batch: 350
Training Loss: 0.09028039668287549
Epoch: 98 Batch: 400
Training Loss: 0.08073113620281219
Epoch: 98 Batch: 450
Training Loss: 0.07135744545194837
Epoch: 98 Batch: 500
Training Loss: 0.06099204516410828
Epoch: 98 Batch: 550
Training Loss: 0.05426624357700348
Epoch: 98 Batch: 600
Training Loss: 0.05110745017727216
Epoch: 98 Batch: 650
Training Loss: 0.04755954572787652
Epoch: 98 Batch: 700
Training Loss: 0.043611529512064796
Epoch: 98 Batch: 750
Training Loss: 0.04123647387822469
Epoch: 98 Batch: 800
Training Loss: 0.03864799022674561
Epoch: 98 Batch: 850
Training Loss: 0.03713201947071973
Epoch: 98 Batch: 900
Training Loss: 0.03368954420089722
Epoch: 98 Batch: 950
Training Loss: 0.03294827251057876
Epoch: 98 Batch: 1000
Training Loss: 0.030674887210130692
Epoch: 98 Batch: 1050
Training Loss: 0.03049865444501241
Epoch: 98 Batch: 1100
Training Loss: 0.029442928406325255
Epoch: 98 Batch: 1150
Training Loss: 0.026978593002194944
Epoch: 98 Batch: 1200
Training Loss: 0.026103318631649018
Epoch: 98 Batch: 1250
Training Loss: 0.02495442054271698
Epoch: 98 Batch: 1300
Training Loss: 0.023772453849132246
Epoch: 98 Batch: 1350
Training Loss: 0.02356636506539804
Epoch: 98 Batch: 1400
Training Loss: 0.02195374058825629
Epoch: 98 Batch: 1450
Training Loss: 0.022067919969558716
Epoch: 98 Batch: 1500
Training Loss: 0.020788371245066325
Epoch: 98 Batch: 1550
Training Loss: 0.02032529169513333
Epoch: 98 Batch: 1600
Training Loss: 0.019788557551801204
Epoch: 98 Batch: 1650
Training Loss: 0.017974027810674725
Epoch: 98 Batch: 1700
Training Loss: 0.01861964786753935
Epoch: 98 Batch: 1750
Training Loss: 0.017556533881596156
Epoch: 98 Batch: 1800
Training Loss: 0.017559714747799767
Epoch: 98 Batch: 1850
Training Loss: 0.017025489194973095
Epoch: 98 Batch: 1900
Training Loss: 0.01585030541608208
Epoch: 98 Batch: 1950
Training Loss: 0.015927985402253958
Epoch: 98 Batch: 2000
Training Loss: 0.015265622034668922
Epoch: 98 Batch: 2050
Training Loss: 0.014713070814202471
Epoch: 98 Batch: 2100
Training Loss: 0.014583993639264789
Epoch: 98 Batch: 2150
Training Loss: 0.013893596973530082
Epoch: 98 Batch: 2200
Training Loss: 0.014080782993273302
Epoch: 98 Batch: 2250
Training Loss: 0.013958895881970723
Epoch: 98 Batch: 2300
Training Loss: 0.013771391277727873
Epoch: 98 Batch: 2350
Training Loss: 0.013274639388348193
Epoch: 98 Batch: 2400
Training Loss: 0.01218083246300618
Epoch: 98 Batch: 2450
Training Loss: 0.013289193279889165
Epoch: 98 Batch: 2500
Training Loss: 0.012744633460044862
Epoch: 98 Batch: 2550
Training Loss: 0.012681502045369615
Epoch: 98 Batch: 2600
Training Loss: 0.012057864872308878
Epoch: 98 Batch: 2650
Training Loss: 0.011480337707501537
Epoch: 98 Batch: 2700
Training Loss: 0.011275761513798326
Epoch: 98 Batch: 2750
Training Loss: 0.011215537493879145
Epoch: 98 Batch: 2800
Training Loss: 0.011350566851241249
Epoch: 98 Batch: 2850
Training Loss: 0.01119520624478658
Epoch: 98 Batch: 2900
Training Loss: 0.010543657058271868
Epoch: 98 Batch: 2950
Training Loss: 0.010520084086111036
Epoch: 98 Batch: 3000
Training Loss: 0.009950075169404347
Epoch: 98 Batch: 3050
Training Loss: 0.009868519716575498
Epoch: 98 Batch: 3100
Training Loss: 0.010105898197620146
Epoch: 98 Batch: 3150
Training Loss: 0.009732471732866196
Epoch: 98 Batch: 3200
Training Loss: 0.009871805785223842
Epoch: 99 
 Validation Loss: 0.47938496039973366
---------------------------
Epoch: 99 Batch: 50
Training Loss: 0.605754685997963
Epoch: 99 Batch: 100
Training Loss: 0.3244713866710663
Epoch: 99 Batch: 150
Training Loss: 0.20474076390266419
Epoch: 99 Batch: 200
Training Loss: 0.15687225878238678
Epoch: 99 Batch: 250
Training Loss: 0.12995441043376924
Epoch: 99 Batch: 300
Training Loss: 0.0980514003833135
Epoch: 99 Batch: 350
Training Loss: 0.08365536059652055
Epoch: 99 Batch: 400
Training Loss: 0.07763636492192745
Epoch: 99 Batch: 450
Training Loss: 0.06975889133082495
Epoch: 99 Batch: 500
Training Loss: 0.0633287529349327
Epoch: 99 Batch: 550
Training Loss: 0.05765372612259605
Epoch: 99 Batch: 600
Training Loss: 0.05061454564332962
Epoch: 99 Batch: 650
Training Loss: 0.04765433971698468
Epoch: 99 Batch: 700
Training Loss: 0.04587105005979538
Epoch: 99 Batch: 750
Training Loss: 0.04235099204381307
Epoch: 99 Batch: 800
Training Loss: 0.036903986781835554
Epoch: 99 Batch: 850
Training Loss: 0.03446460201459772
Epoch: 99 Batch: 900
Training Loss: 0.033967286480797665
Epoch: 99 Batch: 950
Training Loss: 0.03192855841235111
Epoch: 99 Batch: 1000
Training Loss: 0.03209006741642952
Epoch: 99 Batch: 1050
Training Loss: 0.029490781284513927
Epoch: 99 Batch: 1100
Training Loss: 0.028153877691789106
Epoch: 99 Batch: 1150
Training Loss: 0.02536280720130257
Epoch: 99 Batch: 1200
Training Loss: 0.025012699017922083
Epoch: 99 Batch: 1250
Training Loss: 0.025405779361724855
Epoch: 99 Batch: 1300
Training Loss: 0.022513371866482956
Epoch: 99 Batch: 1350
Training Loss: 0.022853108997698185
Epoch: 99 Batch: 1400
Training Loss: 0.022459625218595777
Epoch: 99 Batch: 1450
Training Loss: 0.020677693465660358
Epoch: 99 Batch: 1500
Training Loss: 0.020448813080787658
Epoch: 99 Batch: 1550
Training Loss: 0.02018666371222465
Epoch: 99 Batch: 1600
Training Loss: 0.01876424307003617
Epoch: 99 Batch: 1650
Training Loss: 0.019451214436328773
Epoch: 99 Batch: 1700
Training Loss: 0.01812072811757817
Epoch: 99 Batch: 1750
Training Loss: 0.016636650681495665
Epoch: 99 Batch: 1800
Training Loss: 0.017268892063034905
Epoch: 99 Batch: 1850
Training Loss: 0.017263053768389935
Epoch: 99 Batch: 1900
Training Loss: 0.0167608558974768
Epoch: 99 Batch: 1950
Training Loss: 0.015297276622209793
Epoch: 99 Batch: 2000
Training Loss: 0.015524964109063148
Epoch: 99 Batch: 2050
Training Loss: 0.014575908271277823
Epoch: 99 Batch: 2100
Training Loss: 0.015147947371006013
Epoch: 99 Batch: 2150
Training Loss: 0.014341349601745606
Epoch: 99 Batch: 2200
Training Loss: 0.01383279883048751
Epoch: 99 Batch: 2250
Training Loss: 0.013702711568938361
Epoch: 99 Batch: 2300
Training Loss: 0.013853259034778761
Epoch: 99 Batch: 2350
Training Loss: 0.012926923936985909
Epoch: 99 Batch: 2400
Training Loss: 0.01266346912831068
Epoch: 99 Batch: 2450
Training Loss: 0.012618515637456154
Epoch: 99 Batch: 2500
Training Loss: 0.012349536657333375
Epoch: 99 Batch: 2550
Training Loss: 0.012564312126122268
Epoch: 99 Batch: 2600
Training Loss: 0.011915126591920852
Epoch: 99 Batch: 2650
Training Loss: 0.011487549239734433
Epoch: 99 Batch: 2700
Training Loss: 0.011168238321940104
Epoch: 99 Batch: 2750
Training Loss: 0.011302924502979626
Epoch: 99 Batch: 2800
Training Loss: 0.01157724594431264
Epoch: 99 Batch: 2850
Training Loss: 0.011373262907329358
Epoch: 99 Batch: 2900
Training Loss: 0.010758665347921437
Epoch: 99 Batch: 2950
Training Loss: 0.010571829080581664
Epoch: 99 Batch: 3000
Training Loss: 0.009974342981974284
Epoch: 99 Batch: 3050
Training Loss: 0.010283139817050245
Epoch: 99 Batch: 3100
Training Loss: 0.010416059513245859
Epoch: 99 Batch: 3150
Training Loss: 0.009994333301271712
Epoch: 99 Batch: 3200
Training Loss: 0.009454146074131132
Epoch: 100 
 Validation Loss: 0.4791887124379476
---------------------------
Epoch: 100 Batch: 50
Training Loss: 0.6106390988826752
Epoch: 100 Batch: 100
Training Loss: 0.30353268891572954
Epoch: 100 Batch: 150
Training Loss: 0.20063330312569935
Epoch: 100 Batch: 200
Training Loss: 0.15813710778951645
Epoch: 100 Batch: 250
Training Loss: 0.1264622792005539
Epoch: 100 Batch: 300
Training Loss: 0.10512376934289933
Epoch: 100 Batch: 350
Training Loss: 0.08722677860941205
Epoch: 100 Batch: 400
Training Loss: 0.0751301407814026
Epoch: 100 Batch: 450
Training Loss: 0.06986949708726671
Epoch: 100 Batch: 500
Training Loss: 0.06313869965076446
Epoch: 100 Batch: 550
Training Loss: 0.05586466122757305
Epoch: 100 Batch: 600
Training Loss: 0.050350375076135
Epoch: 100 Batch: 650
Training Loss: 0.04754622175143315
Epoch: 100 Batch: 700
Training Loss: 0.04498032054730824
Epoch: 100 Batch: 750
Training Loss: 0.04283636323610942
Epoch: 100 Batch: 800
Training Loss: 0.039703064635396
Epoch: 100 Batch: 850
Training Loss: 0.03767409503459931
Epoch: 100 Batch: 900
Training Loss: 0.03587605035967297
Epoch: 100 Batch: 950
Training Loss: 0.03469972020701358
Epoch: 100 Batch: 1000
Training Loss: 0.03166350489854813
Epoch: 100 Batch: 1050
Training Loss: 0.030054924119086494
Epoch: 100 Batch: 1100
Training Loss: 0.02787441459569064
Epoch: 100 Batch: 1150
Training Loss: 0.02798576795536539
Epoch: 100 Batch: 1200
Training Loss: 0.026096534232298532
Epoch: 100 Batch: 1250
Training Loss: 0.024538779044151308
Epoch: 100 Batch: 1300
Training Loss: 0.02424857728756391
Epoch: 100 Batch: 1350
Training Loss: 0.023080770285041245
Epoch: 100 Batch: 1400
Training Loss: 0.023308516293764115
Epoch: 100 Batch: 1450
Training Loss: 0.02121287645964787
Epoch: 100 Batch: 1500
Training Loss: 0.020266464412212373
Epoch: 100 Batch: 1550
Training Loss: 0.01953840580678755
Epoch: 100 Batch: 1600
Training Loss: 0.01848685575649142
Epoch: 100 Batch: 1650
Training Loss: 0.019292624502471
Epoch: 100 Batch: 1700
Training Loss: 0.019111555446596706
Epoch: 100 Batch: 1750
Training Loss: 0.018004902720451355
Epoch: 100 Batch: 1800
Training Loss: 0.017570681820313135
Epoch: 100 Batch: 1850
Training Loss: 0.017236952121193343
Epoch: 100 Batch: 1900
Training Loss: 0.01580505601669613
Epoch: 100 Batch: 1950
Training Loss: 0.01573122923190777
Epoch: 100 Batch: 2000
Training Loss: 0.01527122686803341
Epoch: 100 Batch: 2050
Training Loss: 0.015041854890381418
Epoch: 100 Batch: 2100
Training Loss: 0.015265702718780154
Epoch: 100 Batch: 2150
Training Loss: 0.014455918655839077
Epoch: 100 Batch: 2200
Training Loss: 0.014382891451770609
Epoch: 100 Batch: 2250
Training Loss: 0.014334896670447456
Epoch: 100 Batch: 2300
Training Loss: 0.013770962111327958
Epoch: 100 Batch: 2350
Training Loss: 0.013372522465726162
Epoch: 100 Batch: 2400
Training Loss: 0.012838735965391
Epoch: 100 Batch: 2450
Training Loss: 0.01288096339118724
Epoch: 100 Batch: 2500
Training Loss: 0.011948764789104461
Epoch: 100 Batch: 2550
Training Loss: 0.01241318542583316
Epoch: 100 Batch: 2600
Training Loss: 0.012143151622552138
Epoch: 100 Batch: 2650
Training Loss: 0.011290277737491536
Epoch: 100 Batch: 2700
Training Loss: 0.011193232083762134
Epoch: 100 Batch: 2750
Training Loss: 0.011504671270197088
Epoch: 100 Batch: 2800
Training Loss: 0.011360249402267592
Epoch: 100 Batch: 2850
Training Loss: 0.011353497191479331
Epoch: 100 Batch: 2900
Training Loss: 0.010647124812520784
Epoch: 100 Batch: 2950
Training Loss: 0.010838457073195506
Epoch: 100 Batch: 3000
Training Loss: 0.010042832255363465
Epoch: 100 Batch: 3050
Training Loss: 0.009769115780220657
Epoch: 100 Batch: 3100
Training Loss: 0.01036978465895499
Epoch: 100 Batch: 3150
Training Loss: 0.010199039152690343
Epoch: 100 Batch: 3200
Training Loss: 0.009274578569456935
Epoch: 101 
 Validation Loss: 0.47863889469040766
---------------------------
Epoch: 101 Batch: 50
Training Loss: 0.6258298206329346
Epoch: 101 Batch: 100
Training Loss: 0.31701710194349286
Epoch: 101 Batch: 150
Training Loss: 0.20869213163852693
Epoch: 101 Batch: 200
Training Loss: 0.15882836267352105
Epoch: 101 Batch: 250
Training Loss: 0.12739595818519592
Epoch: 101 Batch: 300
Training Loss: 0.10447583864132563
Epoch: 101 Batch: 350
Training Loss: 0.08857789959226335
Epoch: 101 Batch: 400
Training Loss: 0.08189440876245499
Epoch: 101 Batch: 450
Training Loss: 0.06534691115220388
Epoch: 101 Batch: 500
Training Loss: 0.0622454936504364
Epoch: 101 Batch: 550
Training Loss: 0.056938219341364775
Epoch: 101 Batch: 600
Training Loss: 0.050603428333997724
Epoch: 101 Batch: 650
Training Loss: 0.050159677679722126
Epoch: 101 Batch: 700
Training Loss: 0.04494718040738787
Epoch: 101 Batch: 750
Training Loss: 0.04125687710444133
Epoch: 101 Batch: 800
Training Loss: 0.03828130379319191
Epoch: 101 Batch: 850
Training Loss: 0.03536587413619546
Epoch: 101 Batch: 900
Training Loss: 0.0347222328848309
Epoch: 101 Batch: 950
Training Loss: 0.032589184265387684
Epoch: 101 Batch: 1000
Training Loss: 0.031199519515037535
Epoch: 101 Batch: 1050
Training Loss: 0.0286955768153781
Epoch: 101 Batch: 1100
Training Loss: 0.027617941878058695
Epoch: 101 Batch: 1150
Training Loss: 0.02636851948240529
Epoch: 101 Batch: 1200
Training Loss: 0.025424529115358988
Epoch: 101 Batch: 1250
Training Loss: 0.025662757658958434
Epoch: 101 Batch: 1300
Training Loss: 0.024403697848320006
Epoch: 101 Batch: 1350
Training Loss: 0.022404304058463484
Epoch: 101 Batch: 1400
Training Loss: 0.021458074216331755
Epoch: 101 Batch: 1450
Training Loss: 0.02227899027281794
Epoch: 101 Batch: 1500
Training Loss: 0.0208147744735082
Epoch: 101 Batch: 1550
Training Loss: 0.019191266721294772
Epoch: 101 Batch: 1600
Training Loss: 0.020405568052083253
Epoch: 101 Batch: 1650
Training Loss: 0.018538711071014406
Epoch: 101 Batch: 1700
Training Loss: 0.01790366092148949
Epoch: 101 Batch: 1750
Training Loss: 0.01668668990475791
Epoch: 101 Batch: 1800
Training Loss: 0.017599151796764797
Epoch: 101 Batch: 1850
Training Loss: 0.016050521006455293
Epoch: 101 Batch: 1900
Training Loss: 0.01657120328200491
Epoch: 101 Batch: 1950
Training Loss: 0.015558016514166807
Epoch: 101 Batch: 2000
Training Loss: 0.015487074732780456
Epoch: 101 Batch: 2050
Training Loss: 0.0152420221305475
Epoch: 101 Batch: 2100
Training Loss: 0.015290027402696155
Epoch: 101 Batch: 2150
Training Loss: 0.01422803259173105
Epoch: 101 Batch: 2200
Training Loss: 0.014609902121803978
Epoch: 101 Batch: 2250
Training Loss: 0.013853322426478068
Epoch: 101 Batch: 2300
Training Loss: 0.01317540793315224
Epoch: 101 Batch: 2350
Training Loss: 0.01289559341491537
Epoch: 101 Batch: 2400
Training Loss: 0.013417766466736793
Epoch: 101 Batch: 2450
Training Loss: 0.012790268087873653
Epoch: 101 Batch: 2500
Training Loss: 0.01225522074699402
Epoch: 101 Batch: 2550
Training Loss: 0.011901501908021814
Epoch: 101 Batch: 2600
Training Loss: 0.011857702846710498
Epoch: 101 Batch: 2650
Training Loss: 0.011575488414404526
Epoch: 101 Batch: 2700
Training Loss: 0.011688493048703228
Epoch: 101 Batch: 2750
Training Loss: 0.011060335831208662
Epoch: 101 Batch: 2800
Training Loss: 0.011697277267064368
Epoch: 101 Batch: 2850
Training Loss: 0.011247237910304153
Epoch: 101 Batch: 2900
Training Loss: 0.010396350735220416
Epoch: 101 Batch: 2950
Training Loss: 0.010212256423497604
Epoch: 101 Batch: 3000
Training Loss: 0.010050042659044266
Epoch: 101 Batch: 3050
Training Loss: 0.009893795681781456
Epoch: 101 Batch: 3100
Training Loss: 0.010086130557521698
Epoch: 101 Batch: 3150
Training Loss: 0.010146973549373566
Epoch: 101 Batch: 3200
Training Loss: 0.009845610354095697
Epoch: 102 
 Validation Loss: 0.4787587626112832
---------------------------
Epoch: 102 Batch: 50
Training Loss: 0.6240330111980438
Epoch: 102 Batch: 100
Training Loss: 0.30556846886873246
Epoch: 102 Batch: 150
Training Loss: 0.2091325988372167
Epoch: 102 Batch: 200
Training Loss: 0.15759613662958144
Epoch: 102 Batch: 250
Training Loss: 0.12203584039211274
Epoch: 102 Batch: 300
Training Loss: 0.10411014755566915
Epoch: 102 Batch: 350
Training Loss: 0.08695138939789364
Epoch: 102 Batch: 400
Training Loss: 0.07937393859028816
Epoch: 102 Batch: 450
Training Loss: 0.06657501545217302
Epoch: 102 Batch: 500
Training Loss: 0.06317608457803726
Epoch: 102 Batch: 550
Training Loss: 0.05711008667945862
Epoch: 102 Batch: 600
Training Loss: 0.05160115232070287
Epoch: 102 Batch: 650
Training Loss: 0.04745377976160783
Epoch: 102 Batch: 700
Training Loss: 0.0463423393879618
Epoch: 102 Batch: 750
Training Loss: 0.041916209975878395
Epoch: 102 Batch: 800
Training Loss: 0.03931285575032234
Epoch: 102 Batch: 850
Training Loss: 0.0370539952025694
Epoch: 102 Batch: 900
Training Loss: 0.0336164966556761
Epoch: 102 Batch: 950
Training Loss: 0.03324687596998717
Epoch: 102 Batch: 1000
Training Loss: 0.03055301249027252
Epoch: 102 Batch: 1050
Training Loss: 0.030034532036100115
Epoch: 102 Batch: 1100
Training Loss: 0.027564263425090097
Epoch: 102 Batch: 1150
Training Loss: 0.027388996741046077
Epoch: 102 Batch: 1200
Training Loss: 0.02591632569829623
Epoch: 102 Batch: 1250
Training Loss: 0.02499823136329651
Epoch: 102 Batch: 1300
Training Loss: 0.024505651730757493
Epoch: 102 Batch: 1350
Training Loss: 0.023711327402680008
Epoch: 102 Batch: 1400
Training Loss: 0.02159067764878273
Epoch: 102 Batch: 1450
Training Loss: 0.021653768502432724
Epoch: 102 Batch: 1500
Training Loss: 0.020424157698949178
Epoch: 102 Batch: 1550
Training Loss: 0.020098455221422257
Epoch: 102 Batch: 1600
Training Loss: 0.01899294266477227
Epoch: 102 Batch: 1650
Training Loss: 0.01922249613386212
Epoch: 102 Batch: 1700
Training Loss: 0.018218530188588534
Epoch: 102 Batch: 1750
Training Loss: 0.017611085823604038
Epoch: 102 Batch: 1800
Training Loss: 0.017309480359156925
Epoch: 102 Batch: 1850
Training Loss: 0.016436586766629607
Epoch: 102 Batch: 1900
Training Loss: 0.016953727634329546
Epoch: 102 Batch: 1950
Training Loss: 0.016497582029073667
Epoch: 102 Batch: 2000
Training Loss: 0.014613875165581703
Epoch: 102 Batch: 2050
Training Loss: 0.015006450501883902
Epoch: 102 Batch: 2100
Training Loss: 0.015218910064016069
Epoch: 102 Batch: 2150
Training Loss: 0.014498423088428586
Epoch: 102 Batch: 2200
Training Loss: 0.014126089784232052
Epoch: 102 Batch: 2250
Training Loss: 0.013638327519098917
Epoch: 102 Batch: 2300
Training Loss: 0.01322915687509205
Epoch: 102 Batch: 2350
Training Loss: 0.012886466789752879
Epoch: 102 Batch: 2400
Training Loss: 0.012873943249384562
Epoch: 102 Batch: 2450
Training Loss: 0.012568880392580617
Epoch: 102 Batch: 2500
Training Loss: 0.012780494832992553
Epoch: 102 Batch: 2550
Training Loss: 0.012344696720441183
Epoch: 102 Batch: 2600
Training Loss: 0.011796724532659238
Epoch: 102 Batch: 2650
Training Loss: 0.011650606123906261
Epoch: 102 Batch: 2700
Training Loss: 0.011201523398911511
Epoch: 102 Batch: 2750
Training Loss: 0.011171708583831787
Epoch: 102 Batch: 2800
Training Loss: 0.010586712594543185
Epoch: 102 Batch: 2850
Training Loss: 0.010599491512566282
Epoch: 102 Batch: 2900
Training Loss: 0.010921710452129101
Epoch: 102 Batch: 2950
Training Loss: 0.010449171308743751
Epoch: 102 Batch: 3000
Training Loss: 0.009877214243014654
Epoch: 102 Batch: 3050
Training Loss: 0.010032040310687705
Epoch: 102 Batch: 3100
Training Loss: 0.010047405977402965
Epoch: 102 Batch: 3150
Training Loss: 0.009821440104454283
Epoch: 102 Batch: 3200
Training Loss: 0.01008735978975892
Epoch: 103 
 Validation Loss: 0.4786222540669971
---------------------------
Epoch: 103 Batch: 50
Training Loss: 0.6352565562725068
Epoch: 103 Batch: 100
Training Loss: 0.31737889617681503
Epoch: 103 Batch: 150
Training Loss: 0.20889935851097108
Epoch: 103 Batch: 200
Training Loss: 0.15403284639120102
Epoch: 103 Batch: 250
Training Loss: 0.11776136088371277
Epoch: 103 Batch: 300
Training Loss: 0.1006225820382436
Epoch: 103 Batch: 350
Training Loss: 0.0894689473084041
Epoch: 103 Batch: 400
Training Loss: 0.07652428694069385
Epoch: 103 Batch: 450
Training Loss: 0.06766752938429514
Epoch: 103 Batch: 500
Training Loss: 0.062092846393585206
Epoch: 103 Batch: 550
Training Loss: 0.058125177459283306
Epoch: 103 Batch: 600
Training Loss: 0.05233754073580106
Epoch: 103 Batch: 650
Training Loss: 0.05081864467033973
Epoch: 103 Batch: 700
Training Loss: 0.044562003910541534
Epoch: 103 Batch: 750
Training Loss: 0.04097523637612661
Epoch: 103 Batch: 800
Training Loss: 0.03927954386919737
Epoch: 103 Batch: 850
Training Loss: 0.03642199775751899
Epoch: 103 Batch: 900
Training Loss: 0.033288466764820945
Epoch: 103 Batch: 950
Training Loss: 0.0323130308326922
Epoch: 103 Batch: 1000
Training Loss: 0.031665051579475405
Epoch: 103 Batch: 1050
Training Loss: 0.029989434083302815
Epoch: 103 Batch: 1100
Training Loss: 0.028323055857961826
Epoch: 103 Batch: 1150
Training Loss: 0.026945993874384008
Epoch: 103 Batch: 1200
Training Loss: 0.02474487858513991
Epoch: 103 Batch: 1250
Training Loss: 0.024499191498756408
Epoch: 103 Batch: 1300
Training Loss: 0.023219717122041263
Epoch: 103 Batch: 1350
Training Loss: 0.021992354194323223
Epoch: 103 Batch: 1400
Training Loss: 0.021297097525426318
Epoch: 103 Batch: 1450
Training Loss: 0.022955791641925943
Epoch: 103 Batch: 1500
Training Loss: 0.020156887372334797
Epoch: 103 Batch: 1550
Training Loss: 0.02094531855275554
Epoch: 103 Batch: 1600
Training Loss: 0.019550083857029676
Epoch: 103 Batch: 1650
Training Loss: 0.01858528691710848
Epoch: 103 Batch: 1700
Training Loss: 0.018872083302806406
Epoch: 103 Batch: 1750
Training Loss: 0.017806712933949063
Epoch: 103 Batch: 1800
Training Loss: 0.016930544376373292
Epoch: 103 Batch: 1850
Training Loss: 0.01651376700079119
Epoch: 103 Batch: 1900
Training Loss: 0.015740511856581034
Epoch: 103 Batch: 1950
Training Loss: 0.01537603476108649
Epoch: 103 Batch: 2000
Training Loss: 0.016306192085146904
Epoch: 103 Batch: 2050
Training Loss: 0.015054438739288144
Epoch: 103 Batch: 2100
Training Loss: 0.01423180198385602
Epoch: 103 Batch: 2150
Training Loss: 0.014146050270213636
Epoch: 103 Batch: 2200
Training Loss: 0.014333085471933538
Epoch: 103 Batch: 2250
Training Loss: 0.01391697969701555
Epoch: 103 Batch: 2300
Training Loss: 0.013052148598691691
Epoch: 103 Batch: 2350
Training Loss: 0.013134043470342109
Epoch: 103 Batch: 2400
Training Loss: 0.012851401182512443
Epoch: 103 Batch: 2450
Training Loss: 0.012194085996978138
Epoch: 103 Batch: 2500
Training Loss: 0.012453135013580321
Epoch: 103 Batch: 2550
Training Loss: 0.012414563333286959
Epoch: 103 Batch: 2600
Training Loss: 0.011756013333797454
Epoch: 103 Batch: 2650
Training Loss: 0.012006008793722909
Epoch: 103 Batch: 2700
Training Loss: 0.011293994298687687
Epoch: 103 Batch: 2750
Training Loss: 0.011367510318756104
Epoch: 103 Batch: 2800
Training Loss: 0.011251110594187465
Epoch: 103 Batch: 2850
Training Loss: 0.010734493774280214
Epoch: 103 Batch: 2900
Training Loss: 0.010588402347318056
Epoch: 103 Batch: 2950
Training Loss: 0.010158087106074317
Epoch: 103 Batch: 3000
Training Loss: 0.010213775972525279
Epoch: 103 Batch: 3050
Training Loss: 0.010422725804516527
Epoch: 103 Batch: 3100
Training Loss: 0.010604338213320701
Epoch: 103 Batch: 3150
Training Loss: 0.009581607419347006
Epoch: 103 Batch: 3200
Training Loss: 0.00980062442831695
Epoch: 104 
 Validation Loss: 0.47821197476651933
---------------------------
Epoch: 104 Batch: 50
Training Loss: 0.6286782163381577
Epoch: 104 Batch: 100
Training Loss: 0.2973582845926285
Epoch: 104 Batch: 150
Training Loss: 0.20111957689126334
Epoch: 104 Batch: 200
Training Loss: 0.1492636665701866
Epoch: 104 Batch: 250
Training Loss: 0.12735731470584868
Epoch: 104 Batch: 300
Training Loss: 0.09966819475094478
Epoch: 104 Batch: 350
Training Loss: 0.08937934952122825
Epoch: 104 Batch: 400
Training Loss: 0.07567680835723876
Epoch: 104 Batch: 450
Training Loss: 0.06876326322555543
Epoch: 104 Batch: 500
Training Loss: 0.06313260179758072
Epoch: 104 Batch: 550
Training Loss: 0.057870327125896105
Epoch: 104 Batch: 600
Training Loss: 0.049761158923308055
Epoch: 104 Batch: 650
Training Loss: 0.045129426580209
Epoch: 104 Batch: 700
Training Loss: 0.044548241879258835
Epoch: 104 Batch: 750
Training Loss: 0.03939997947216034
Epoch: 104 Batch: 800
Training Loss: 0.039006925113499166
Epoch: 104 Batch: 850
Training Loss: 0.03564813985544092
Epoch: 104 Batch: 900
Training Loss: 0.03458707624011569
Epoch: 104 Batch: 950
Training Loss: 0.03159369525156523
Epoch: 104 Batch: 1000
Training Loss: 0.03074938228726387
Epoch: 104 Batch: 1050
Training Loss: 0.02974967161814372
Epoch: 104 Batch: 1100
Training Loss: 0.027229274809360503
Epoch: 104 Batch: 1150
Training Loss: 0.026307660263517628
Epoch: 104 Batch: 1200
Training Loss: 0.026011227866013845
Epoch: 104 Batch: 1250
Training Loss: 0.026141466522216796
Epoch: 104 Batch: 1300
Training Loss: 0.024230655477597163
Epoch: 104 Batch: 1350
Training Loss: 0.02259776011661247
Epoch: 104 Batch: 1400
Training Loss: 0.022493696957826614
Epoch: 104 Batch: 1450
Training Loss: 0.021273764084125388
Epoch: 104 Batch: 1500
Training Loss: 0.020493632237116494
Epoch: 104 Batch: 1550
Training Loss: 0.020425834886489375
Epoch: 104 Batch: 1600
Training Loss: 0.019155641198158265
Epoch: 104 Batch: 1650
Training Loss: 0.0187523892250928
Epoch: 104 Batch: 1700
Training Loss: 0.019069431476733265
Epoch: 104 Batch: 1750
Training Loss: 0.018810256685529436
Epoch: 104 Batch: 1800
Training Loss: 0.01771370808283488
Epoch: 104 Batch: 1850
Training Loss: 0.01661594933754689
Epoch: 104 Batch: 1900
Training Loss: 0.01632994265932786
Epoch: 104 Batch: 1950
Training Loss: 0.01661613933551006
Epoch: 104 Batch: 2000
Training Loss: 0.01530347652733326
Epoch: 104 Batch: 2050
Training Loss: 0.015402341775777864
Epoch: 104 Batch: 2100
Training Loss: 0.01451513674997148
Epoch: 104 Batch: 2150
Training Loss: 0.014391942065815592
Epoch: 104 Batch: 2200
Training Loss: 0.013695011965253136
Epoch: 104 Batch: 2250
Training Loss: 0.013795377016067504
Epoch: 104 Batch: 2300
Training Loss: 0.012811020560886548
Epoch: 104 Batch: 2350
Training Loss: 0.013814218018917327
Epoch: 104 Batch: 2400
Training Loss: 0.012516039796173573
Epoch: 104 Batch: 2450
Training Loss: 0.012732362333609134
Epoch: 104 Batch: 2500
Training Loss: 0.012143078577518463
Epoch: 104 Batch: 2550
Training Loss: 0.012260578730527093
Epoch: 104 Batch: 2600
Training Loss: 0.012713719915885192
Epoch: 104 Batch: 2650
Training Loss: 0.01224742887155065
Epoch: 104 Batch: 2700
Training Loss: 0.011405669009244
Epoch: 104 Batch: 2750
Training Loss: 0.011757601629603993
Epoch: 104 Batch: 2800
Training Loss: 0.011180750174181802
Epoch: 104 Batch: 2850
Training Loss: 0.01046348928359517
Epoch: 104 Batch: 2900
Training Loss: 0.010672308031854958
Epoch: 104 Batch: 2950
Training Loss: 0.010781090431294198
Epoch: 104 Batch: 3000
Training Loss: 0.010016740570465723
Epoch: 104 Batch: 3050
Training Loss: 0.010103853198348499
Epoch: 104 Batch: 3100
Training Loss: 0.009655657833622348
Epoch: 104 Batch: 3150
Training Loss: 0.009916989595171006
Epoch: 104 Batch: 3200
Training Loss: 0.009729629335924982
Epoch: 105 
 Validation Loss: 0.47829519675837623
---------------------------
Epoch: 105 Batch: 50
Training Loss: 0.6507205003499985
Epoch: 105 Batch: 100
Training Loss: 0.3065462523698807
Epoch: 105 Batch: 150
Training Loss: 0.2083893585205078
Epoch: 105 Batch: 200
Training Loss: 0.15839793935418128
Epoch: 105 Batch: 250
Training Loss: 0.11883771073818207
Epoch: 105 Batch: 300
Training Loss: 0.10633015900850296
Epoch: 105 Batch: 350
Training Loss: 0.08868360647133418
Epoch: 105 Batch: 400
Training Loss: 0.08013018272817135
Epoch: 105 Batch: 450
Training Loss: 0.06872885829872555
Epoch: 105 Batch: 500
Training Loss: 0.06577033549547195
Epoch: 105 Batch: 550
Training Loss: 0.05316679949110205
Epoch: 105 Batch: 600
Training Loss: 0.05230942055583
Epoch: 105 Batch: 650
Training Loss: 0.046989721564146186
Epoch: 105 Batch: 700
Training Loss: 0.04433005051953452
Epoch: 105 Batch: 750
Training Loss: 0.04050012195110321
Epoch: 105 Batch: 800
Training Loss: 0.03881474513560534
Epoch: 105 Batch: 850
Training Loss: 0.03635308938867905
Epoch: 105 Batch: 900
Training Loss: 0.03339023477501339
Epoch: 105 Batch: 950
Training Loss: 0.03206094575555701
Epoch: 105 Batch: 1000
Training Loss: 0.031025994151830674
Epoch: 105 Batch: 1050
Training Loss: 0.030440689268566315
Epoch: 105 Batch: 1100
Training Loss: 0.02947528427297419
Epoch: 105 Batch: 1150
Training Loss: 0.02667634611544402
Epoch: 105 Batch: 1200
Training Loss: 0.025772896086176236
Epoch: 105 Batch: 1250
Training Loss: 0.025459511995315553
Epoch: 105 Batch: 1300
Training Loss: 0.023659566457454973
Epoch: 105 Batch: 1350
Training Loss: 0.024082213993425722
Epoch: 105 Batch: 1400
Training Loss: 0.02129569717815944
Epoch: 105 Batch: 1450
Training Loss: 0.020995613316009784
Epoch: 105 Batch: 1500
Training Loss: 0.02047880333662033
Epoch: 105 Batch: 1550
Training Loss: 0.020124036035230084
Epoch: 105 Batch: 1600
Training Loss: 0.019522817470133305
Epoch: 105 Batch: 1650
Training Loss: 0.019187321157166454
Epoch: 105 Batch: 1700
Training Loss: 0.018085926069932826
Epoch: 105 Batch: 1750
Training Loss: 0.018681797998292105
Epoch: 105 Batch: 1800
Training Loss: 0.017623749474684397
Epoch: 105 Batch: 1850
Training Loss: 0.01679794076326731
Epoch: 105 Batch: 1900
Training Loss: 0.016745984177840383
Epoch: 105 Batch: 1950
Training Loss: 0.015185433427492777
Epoch: 105 Batch: 2000
Training Loss: 0.015866319209337235
Epoch: 105 Batch: 2050
Training Loss: 0.015622690072873743
Epoch: 105 Batch: 2100
Training Loss: 0.014768453949973696
Epoch: 105 Batch: 2150
Training Loss: 0.014888183460679166
Epoch: 105 Batch: 2200
Training Loss: 0.013604972511529922
Epoch: 105 Batch: 2250
Training Loss: 0.013592600557539199
Epoch: 105 Batch: 2300
Training Loss: 0.013538811802864075
Epoch: 105 Batch: 2350
Training Loss: 0.013151801368023486
Epoch: 105 Batch: 2400
Training Loss: 0.013175291841228802
Epoch: 105 Batch: 2450
Training Loss: 0.01310602956888627
Epoch: 105 Batch: 2500
Training Loss: 0.012107765412330627
Epoch: 105 Batch: 2550
Training Loss: 0.011702997941596835
Epoch: 105 Batch: 2600
Training Loss: 0.011212742844453226
Epoch: 105 Batch: 2650
Training Loss: 0.01160461758667568
Epoch: 105 Batch: 2700
Training Loss: 0.01156476424800025
Epoch: 105 Batch: 2750
Training Loss: 0.011119190270250493
Epoch: 105 Batch: 2800
Training Loss: 0.010909691697784833
Epoch: 105 Batch: 2850
Training Loss: 0.010153473396050303
Epoch: 105 Batch: 2900
Training Loss: 0.010777071025864832
Epoch: 105 Batch: 2950
Training Loss: 0.010520267395649926
Epoch: 105 Batch: 3000
Training Loss: 0.010803184489409128
Epoch: 105 Batch: 3050
Training Loss: 0.009783968681194743
Epoch: 105 Batch: 3100
Training Loss: 0.009776059466023598
Epoch: 105 Batch: 3150
Training Loss: 0.00966768259093875
Epoch: 105 Batch: 3200
Training Loss: 0.0098865753877908
Epoch: 106 
 Validation Loss: 0.4782874832550685
---------------------------
Epoch: 106 Batch: 50
Training Loss: 0.6364574736356735
Epoch: 106 Batch: 100
Training Loss: 0.3120531964302063
Epoch: 106 Batch: 150
Training Loss: 0.20190227131048838
Epoch: 106 Batch: 200
Training Loss: 0.1573374418914318
Epoch: 106 Batch: 250
Training Loss: 0.12713007891178132
Epoch: 106 Batch: 300
Training Loss: 0.10578064968188604
Epoch: 106 Batch: 350
Training Loss: 0.08865872800350189
Epoch: 106 Batch: 400
Training Loss: 0.07684306532144547
Epoch: 106 Batch: 450
Training Loss: 0.06694576621055603
Epoch: 106 Batch: 500
Training Loss: 0.06190857595205307
Epoch: 106 Batch: 550
Training Loss: 0.05840564326806502
Epoch: 106 Batch: 600
Training Loss: 0.04943523019552231
Epoch: 106 Batch: 650
Training Loss: 0.046015637654524585
Epoch: 106 Batch: 700
Training Loss: 0.04227443490709577
Epoch: 106 Batch: 750
Training Loss: 0.03971808588504791
Epoch: 106 Batch: 800
Training Loss: 0.03757254578173161
Epoch: 106 Batch: 850
Training Loss: 0.03537208118859459
Epoch: 106 Batch: 900
Training Loss: 0.034213672478993734
Epoch: 106 Batch: 950
Training Loss: 0.032891178789891694
Epoch: 106 Batch: 1000
Training Loss: 0.031207483887672423
Epoch: 106 Batch: 1050
Training Loss: 0.030601512789726257
Epoch: 106 Batch: 1100
Training Loss: 0.02863350879062306
Epoch: 106 Batch: 1150
Training Loss: 0.026902022957801818
Epoch: 106 Batch: 1200
Training Loss: 0.02490112046400706
Epoch: 106 Batch: 1250
Training Loss: 0.024833101797103883
Epoch: 106 Batch: 1300
Training Loss: 0.023635714673078977
Epoch: 106 Batch: 1350
Training Loss: 0.022743342607109637
Epoch: 106 Batch: 1400
Training Loss: 0.021760660793100083
Epoch: 106 Batch: 1450
Training Loss: 0.022408386181140768
Epoch: 106 Batch: 1500
Training Loss: 0.020166019797325133
Epoch: 106 Batch: 1550
Training Loss: 0.0197653982908495
Epoch: 106 Batch: 1600
Training Loss: 0.019751467816531657
Epoch: 106 Batch: 1650
Training Loss: 0.01837356341607643
Epoch: 106 Batch: 1700
Training Loss: 0.01739983725197175
Epoch: 106 Batch: 1750
Training Loss: 0.01800594142505101
Epoch: 106 Batch: 1800
Training Loss: 0.017413248502545886
Epoch: 106 Batch: 1850
Training Loss: 0.016273719378419826
Epoch: 106 Batch: 1900
Training Loss: 0.01740301423951199
Epoch: 106 Batch: 1950
Training Loss: 0.016031440786826306
Epoch: 106 Batch: 2000
Training Loss: 0.015379609450697898
Epoch: 106 Batch: 2050
Training Loss: 0.014794915202187328
Epoch: 106 Batch: 2100
Training Loss: 0.015117051814283643
Epoch: 106 Batch: 2150
Training Loss: 0.015551788779192192
Epoch: 106 Batch: 2200
Training Loss: 0.014190486764365977
Epoch: 106 Batch: 2250
Training Loss: 0.014438471423255073
Epoch: 106 Batch: 2300
Training Loss: 0.01365328584028327
Epoch: 106 Batch: 2350
Training Loss: 0.013493944114827095
Epoch: 106 Batch: 2400
Training Loss: 0.01221701176216205
Epoch: 106 Batch: 2450
Training Loss: 0.01255696026646361
Epoch: 106 Batch: 2500
Training Loss: 0.012682693326473236
Epoch: 106 Batch: 2550
Training Loss: 0.012250832880244535
Epoch: 106 Batch: 2600
Training Loss: 0.011905146424586956
Epoch: 106 Batch: 2650
Training Loss: 0.011922870537020125
Epoch: 106 Batch: 2700
Training Loss: 0.011207374356411122
Epoch: 106 Batch: 2750
Training Loss: 0.010876766594973478
Epoch: 106 Batch: 2800
Training Loss: 0.01112241791827338
Epoch: 106 Batch: 2850
Training Loss: 0.011305429527634069
Epoch: 106 Batch: 2900
Training Loss: 0.010546527761837532
Epoch: 106 Batch: 2950
Training Loss: 0.010598793019682674
Epoch: 106 Batch: 3000
Training Loss: 0.011070220897595087
Epoch: 106 Batch: 3050
Training Loss: 0.010395095768522043
Epoch: 106 Batch: 3100
Training Loss: 0.00972460424707782
Epoch: 106 Batch: 3150
Training Loss: 0.009494085718715002
Epoch: 106 Batch: 3200
Training Loss: 0.010047765607014298
Epoch: 107 
 Validation Loss: 0.4785360674063365
---------------------------
Epoch: 107 Batch: 50
Training Loss: 0.6436674898862839
Epoch: 107 Batch: 100
Training Loss: 0.29895391315221786
Epoch: 107 Batch: 150
Training Loss: 0.2075500855843226
Epoch: 107 Batch: 200
Training Loss: 0.16040573433041572
Epoch: 107 Batch: 250
Training Loss: 0.11844151031970977
Epoch: 107 Batch: 300
Training Loss: 0.09740150769551595
Epoch: 107 Batch: 350
Training Loss: 0.08534743632589067
Epoch: 107 Batch: 400
Training Loss: 0.0794199538975954
Epoch: 107 Batch: 450
Training Loss: 0.07060305098692576
Epoch: 107 Batch: 500
Training Loss: 0.05895549815893173
Epoch: 107 Batch: 550
Training Loss: 0.057002498453313656
Epoch: 107 Batch: 600
Training Loss: 0.05229198301831881
Epoch: 107 Batch: 650
Training Loss: 0.04521208685178023
Epoch: 107 Batch: 700
Training Loss: 0.04386257814509528
Epoch: 107 Batch: 750
Training Loss: 0.04252515920003255
Epoch: 107 Batch: 800
Training Loss: 0.03979982670396567
Epoch: 107 Batch: 850
Training Loss: 0.036518579160465914
Epoch: 107 Batch: 900
Training Loss: 0.035056332349777224
Epoch: 107 Batch: 950
Training Loss: 0.03274387676464884
Epoch: 107 Batch: 1000
Training Loss: 0.030846564888954164
Epoch: 107 Batch: 1050
Training Loss: 0.029618144035339355
Epoch: 107 Batch: 1100
Training Loss: 0.028992001116275787
Epoch: 107 Batch: 1150
Training Loss: 0.027115697860717772
Epoch: 107 Batch: 1200
Training Loss: 0.02721184730529785
Epoch: 107 Batch: 1250
Training Loss: 0.02498407747745514
Epoch: 107 Batch: 1300
Training Loss: 0.024355570605167975
Epoch: 107 Batch: 1350
Training Loss: 0.023060730474966543
Epoch: 107 Batch: 1400
Training Loss: 0.02182066770536559
Epoch: 107 Batch: 1450
Training Loss: 0.021568528167132672
Epoch: 107 Batch: 1500
Training Loss: 0.02049204190572103
Epoch: 107 Batch: 1550
Training Loss: 0.0198085779336191
Epoch: 107 Batch: 1600
Training Loss: 0.01973496178165078
Epoch: 107 Batch: 1650
Training Loss: 0.018962413289330224
Epoch: 107 Batch: 1700
Training Loss: 0.01773410018752603
Epoch: 107 Batch: 1750
Training Loss: 0.01743185302189418
Epoch: 107 Batch: 1800
Training Loss: 0.016981917238897748
Epoch: 107 Batch: 1850
Training Loss: 0.016669927078324397
Epoch: 107 Batch: 1900
Training Loss: 0.015767092485176888
Epoch: 107 Batch: 1950
Training Loss: 0.015303555986820124
Epoch: 107 Batch: 2000
Training Loss: 0.015264885500073433
Epoch: 107 Batch: 2050
Training Loss: 0.014959237226625769
Epoch: 107 Batch: 2100
Training Loss: 0.014398439469791594
Epoch: 107 Batch: 2150
Training Loss: 0.014769589346508648
Epoch: 107 Batch: 2200
Training Loss: 0.014929245520721782
Epoch: 107 Batch: 2250
Training Loss: 0.013786849313312107
Epoch: 107 Batch: 2300
Training Loss: 0.013173318259094072
Epoch: 107 Batch: 2350
Training Loss: 0.013357943296432495
Epoch: 107 Batch: 2400
Training Loss: 0.01223874772588412
Epoch: 107 Batch: 2450
Training Loss: 0.01304890294464267
Epoch: 107 Batch: 2500
Training Loss: 0.012088163232803345
Epoch: 107 Batch: 2550
Training Loss: 0.01235320063198314
Epoch: 107 Batch: 2600
Training Loss: 0.011659588022873951
Epoch: 107 Batch: 2650
Training Loss: 0.011568613760876206
Epoch: 107 Batch: 2700
Training Loss: 0.011263842891763758
Epoch: 107 Batch: 2750
Training Loss: 0.011498019912026146
Epoch: 107 Batch: 2800
Training Loss: 0.011339119002223015
Epoch: 107 Batch: 2850
Training Loss: 0.01059953234697643
Epoch: 107 Batch: 2900
Training Loss: 0.010749473530670692
Epoch: 107 Batch: 2950
Training Loss: 0.01026609919838986
Epoch: 107 Batch: 3000
Training Loss: 0.010479961305856704
Epoch: 107 Batch: 3050
Training Loss: 0.009929131787331378
Epoch: 107 Batch: 3100
Training Loss: 0.00970661284462098
Epoch: 107 Batch: 3150
Training Loss: 0.010026745379917205
Epoch: 107 Batch: 3200
Training Loss: 0.009586637830361724
Epoch: 108 
 Validation Loss: 0.4782899479071299
---------------------------
Epoch: 108 Batch: 50
Training Loss: 0.6210140669345856
Epoch: 108 Batch: 100
Training Loss: 0.2964654439687729
Epoch: 108 Batch: 150
Training Loss: 0.201359876592954
Epoch: 108 Batch: 200
Training Loss: 0.1565985031425953
Epoch: 108 Batch: 250
Training Loss: 0.1226486781835556
Epoch: 108 Batch: 300
Training Loss: 0.10219678431749343
Epoch: 108 Batch: 350
Training Loss: 0.08700843938759395
Epoch: 108 Batch: 400
Training Loss: 0.07819394126534462
Epoch: 108 Batch: 450
Training Loss: 0.06853700670931075
Epoch: 108 Batch: 500
Training Loss: 0.06305164039134979
Epoch: 108 Batch: 550
Training Loss: 0.05796454532579942
Epoch: 108 Batch: 600
Training Loss: 0.051003312518199284
Epoch: 108 Batch: 650
Training Loss: 0.045047449928063615
Epoch: 108 Batch: 700
Training Loss: 0.043294175437518526
Epoch: 108 Batch: 750
Training Loss: 0.043084386308987935
Epoch: 108 Batch: 800
Training Loss: 0.03786732111126184
Epoch: 108 Batch: 850
Training Loss: 0.03710403098779566
Epoch: 108 Batch: 900
Training Loss: 0.034637736413213945
Epoch: 108 Batch: 950
Training Loss: 0.032352894513230576
Epoch: 108 Batch: 1000
Training Loss: 0.031195695817470552
Epoch: 108 Batch: 1050
Training Loss: 0.02831777703194391
Epoch: 108 Batch: 1100
Training Loss: 0.028237251232970845
Epoch: 108 Batch: 1150
Training Loss: 0.026257639579150987
Epoch: 108 Batch: 1200
Training Loss: 0.025342145562171937
Epoch: 108 Batch: 1250
Training Loss: 0.02458454008102417
Epoch: 108 Batch: 1300
Training Loss: 0.023615389076563027
Epoch: 108 Batch: 1350
Training Loss: 0.022684275949442827
Epoch: 108 Batch: 1400
Training Loss: 0.021588059812784194
Epoch: 108 Batch: 1450
Training Loss: 0.021606715444860786
Epoch: 108 Batch: 1500
Training Loss: 0.020805033961931865
Epoch: 108 Batch: 1550
Training Loss: 0.019709732301773565
Epoch: 108 Batch: 1600
Training Loss: 0.019246168322861194
Epoch: 108 Batch: 1650
Training Loss: 0.019244270649823276
Epoch: 108 Batch: 1700
Training Loss: 0.01777865485233419
Epoch: 108 Batch: 1750
Training Loss: 0.017761693034853254
Epoch: 108 Batch: 1800
Training Loss: 0.017154533449146483
Epoch: 108 Batch: 1850
Training Loss: 0.016985406714516718
Epoch: 108 Batch: 1900
Training Loss: 0.01679269839274256
Epoch: 108 Batch: 1950
Training Loss: 0.015739575517483246
Epoch: 108 Batch: 2000
Training Loss: 0.015835957184433938
Epoch: 108 Batch: 2050
Training Loss: 0.014903512306329681
Epoch: 108 Batch: 2100
Training Loss: 0.014511713541689373
Epoch: 108 Batch: 2150
Training Loss: 0.014671903197155443
Epoch: 108 Batch: 2200
Training Loss: 0.013641397045417265
Epoch: 108 Batch: 2250
Training Loss: 0.01416297968228658
Epoch: 108 Batch: 2300
Training Loss: 0.013424117370792058
Epoch: 108 Batch: 2350
Training Loss: 0.012921024586292023
Epoch: 108 Batch: 2400
Training Loss: 0.01267752164353927
Epoch: 108 Batch: 2450
Training Loss: 0.012296780177525111
Epoch: 108 Batch: 2500
Training Loss: 0.012373345363140106
Epoch: 108 Batch: 2550
Training Loss: 0.011928299139527714
Epoch: 108 Batch: 2600
Training Loss: 0.01198976110953551
Epoch: 108 Batch: 2650
Training Loss: 0.011932013349713019
Epoch: 108 Batch: 2700
Training Loss: 0.011598328131216544
Epoch: 108 Batch: 2750
Training Loss: 0.01102111547643488
Epoch: 108 Batch: 2800
Training Loss: 0.011160011057342802
Epoch: 108 Batch: 2850
Training Loss: 0.01083233560386457
Epoch: 108 Batch: 2900
Training Loss: 0.011117957779045763
Epoch: 108 Batch: 2950
Training Loss: 0.010352977938571218
Epoch: 108 Batch: 3000
Training Loss: 0.010388145436843237
Epoch: 108 Batch: 3050
Training Loss: 0.010788847217794325
Epoch: 108 Batch: 3100
Training Loss: 0.009725477532032997
Epoch: 108 Batch: 3150
Training Loss: 0.010181314765460907
Epoch: 108 Batch: 3200
Training Loss: 0.009605957912281155
Epoch: 109 
 Validation Loss: 0.47777752743826973
---------------------------
Epoch: 109 Batch: 50
Training Loss: 0.6545620161294937
Epoch: 109 Batch: 100
Training Loss: 0.304595288336277
Epoch: 109 Batch: 150
Training Loss: 0.20261136571566263
Epoch: 109 Batch: 200
Training Loss: 0.14910821110010147
Epoch: 109 Batch: 250
Training Loss: 0.1222649827003479
Epoch: 109 Batch: 300
Training Loss: 0.10250359266996384
Epoch: 109 Batch: 350
Training Loss: 0.09359052181243896
Epoch: 109 Batch: 400
Training Loss: 0.08123907342553138
Epoch: 109 Batch: 450
Training Loss: 0.07120824800597297
Epoch: 109 Batch: 500
Training Loss: 0.0632569088935852
Epoch: 109 Batch: 550
Training Loss: 0.054066758914427325
Epoch: 109 Batch: 600
Training Loss: 0.05119699239730835
Epoch: 109 Batch: 650
Training Loss: 0.04985933381777543
Epoch: 109 Batch: 700
Training Loss: 0.043193970492907935
Epoch: 109 Batch: 750
Training Loss: 0.039186931133270264
Epoch: 109 Batch: 800
Training Loss: 0.03884219110012054
Epoch: 109 Batch: 850
Training Loss: 0.03759764951818129
Epoch: 109 Batch: 900
Training Loss: 0.03365574565198686
Epoch: 109 Batch: 950
Training Loss: 0.03269164505757784
Epoch: 109 Batch: 1000
Training Loss: 0.03026506468653679
Epoch: 109 Batch: 1050
Training Loss: 0.02990545201869238
Epoch: 109 Batch: 1100
Training Loss: 0.02858610445802862
Epoch: 109 Batch: 1150
Training Loss: 0.026832557476085164
Epoch: 109 Batch: 1200
Training Loss: 0.023924899051586786
Epoch: 109 Batch: 1250
Training Loss: 0.025608198833465576
Epoch: 109 Batch: 1300
Training Loss: 0.02350683916073579
Epoch: 109 Batch: 1350
Training Loss: 0.022604631185531618
Epoch: 109 Batch: 1400
Training Loss: 0.022717204540967942
Epoch: 109 Batch: 1450
Training Loss: 0.020352622476117365
Epoch: 109 Batch: 1500
Training Loss: 0.02192604547739029
Epoch: 109 Batch: 1550
Training Loss: 0.020380906674169724
Epoch: 109 Batch: 1600
Training Loss: 0.019288397375494242
Epoch: 109 Batch: 1650
Training Loss: 0.018438212564497285
Epoch: 109 Batch: 1700
Training Loss: 0.017652216904303606
Epoch: 109 Batch: 1750
Training Loss: 0.018234917674745832
Epoch: 109 Batch: 1800
Training Loss: 0.01697242110967636
Epoch: 109 Batch: 1850
Training Loss: 0.016908699837890832
Epoch: 109 Batch: 1900
Training Loss: 0.015880175656393953
Epoch: 109 Batch: 1950
Training Loss: 0.016512525096917762
Epoch: 109 Batch: 2000
Training Loss: 0.015441717103123665
Epoch: 109 Batch: 2050
Training Loss: 0.014865192858184256
Epoch: 109 Batch: 2100
Training Loss: 0.014958213695458003
Epoch: 109 Batch: 2150
Training Loss: 0.013783573244893274
Epoch: 109 Batch: 2200
Training Loss: 0.014342112080617385
Epoch: 109 Batch: 2250
Training Loss: 0.013003698415226406
Epoch: 109 Batch: 2300
Training Loss: 0.012782092185124107
Epoch: 109 Batch: 2350
Training Loss: 0.012930207417366352
Epoch: 109 Batch: 2400
Training Loss: 0.01274133479843537
Epoch: 109 Batch: 2450
Training Loss: 0.012904077087129866
Epoch: 109 Batch: 2500
Training Loss: 0.012260483884811402
Epoch: 109 Batch: 2550
Training Loss: 0.012032669689141068
Epoch: 109 Batch: 2600
Training Loss: 0.011822321197161308
Epoch: 109 Batch: 2650
Training Loss: 0.011602220580262957
Epoch: 109 Batch: 2700
Training Loss: 0.011183917963946308
Epoch: 109 Batch: 2750
Training Loss: 0.01136019911549308
Epoch: 109 Batch: 2800
Training Loss: 0.011105776588831629
Epoch: 109 Batch: 2850
Training Loss: 0.011233940197710405
Epoch: 109 Batch: 2900
Training Loss: 0.01038718538037662
Epoch: 109 Batch: 2950
Training Loss: 0.01038513239157402
Epoch: 109 Batch: 3000
Training Loss: 0.010573243399461111
Epoch: 109 Batch: 3050
Training Loss: 0.009981219231105242
Epoch: 109 Batch: 3100
Training Loss: 0.010309085855560918
Epoch: 109 Batch: 3150
Training Loss: 0.009681774737342956
Epoch: 109 Batch: 3200
Training Loss: 0.009931122800335288
Epoch: 110 
 Validation Loss: 0.4776918997367223
---------------------------
Epoch: 110 Batch: 50
Training Loss: 0.6217463511228561
Epoch: 110 Batch: 100
Training Loss: 0.32086293905973434
Epoch: 110 Batch: 150
Training Loss: 0.20424331764380138
Epoch: 110 Batch: 200
Training Loss: 0.1610464483499527
Epoch: 110 Batch: 250
Training Loss: 0.12347036349773408
Epoch: 110 Batch: 300
Training Loss: 0.1011235965291659
Epoch: 110 Batch: 350
Training Loss: 0.08425797138895308
Epoch: 110 Batch: 400
Training Loss: 0.0739816777408123
Epoch: 110 Batch: 450
Training Loss: 0.06632786472638448
Epoch: 110 Batch: 500
Training Loss: 0.06142998880147934
Epoch: 110 Batch: 550
Training Loss: 0.05813517798076977
Epoch: 110 Batch: 600
Training Loss: 0.05306780164440473
Epoch: 110 Batch: 650
Training Loss: 0.045083461908193734
Epoch: 110 Batch: 700
Training Loss: 0.04234572355236326
Epoch: 110 Batch: 750
Training Loss: 0.040668312668800354
Epoch: 110 Batch: 800
Training Loss: 0.039883272983133794
Epoch: 110 Batch: 850
Training Loss: 0.036221273751819834
Epoch: 110 Batch: 900
Training Loss: 0.035811182823446065
Epoch: 110 Batch: 950
Training Loss: 0.03368557089253476
Epoch: 110 Batch: 1000
Training Loss: 0.03187987506389618
Epoch: 110 Batch: 1050
Training Loss: 0.03005797939641135
Epoch: 110 Batch: 1100
Training Loss: 0.02593315533616326
Epoch: 110 Batch: 1150
Training Loss: 0.027036580132401507
Epoch: 110 Batch: 1200
Training Loss: 0.02608034392197927
Epoch: 110 Batch: 1250
Training Loss: 0.023263086700439452
Epoch: 110 Batch: 1300
Training Loss: 0.023675013413796058
Epoch: 110 Batch: 1350
Training Loss: 0.024698027637269762
Epoch: 110 Batch: 1400
Training Loss: 0.022166099420615606
Epoch: 110 Batch: 1450
Training Loss: 0.022206772216435137
Epoch: 110 Batch: 1500
Training Loss: 0.020391064802805583
Epoch: 110 Batch: 1550
Training Loss: 0.020574750419585935
Epoch: 110 Batch: 1600
Training Loss: 0.018618461117148398
Epoch: 110 Batch: 1650
Training Loss: 0.018738594037113768
Epoch: 110 Batch: 1700
Training Loss: 0.017851654185968287
Epoch: 110 Batch: 1750
Training Loss: 0.017390610371317183
Epoch: 110 Batch: 1800
Training Loss: 0.017130853335062663
Epoch: 110 Batch: 1850
Training Loss: 0.017226283695246723
Epoch: 110 Batch: 1900
Training Loss: 0.01627439236954639
Epoch: 110 Batch: 1950
Training Loss: 0.016479822183266664
Epoch: 110 Batch: 2000
Training Loss: 0.015280086010694504
Epoch: 110 Batch: 2050
Training Loss: 0.015375809684032347
Epoch: 110 Batch: 2100
Training Loss: 0.014400202887398857
Epoch: 110 Batch: 2150
Training Loss: 0.014333423736483551
Epoch: 110 Batch: 2200
Training Loss: 0.014072986069050702
Epoch: 110 Batch: 2250
Training Loss: 0.013313967956437005
Epoch: 110 Batch: 2300
Training Loss: 0.013051562037156976
Epoch: 110 Batch: 2350
Training Loss: 0.013258854911682454
Epoch: 110 Batch: 2400
Training Loss: 0.01259637214243412
Epoch: 110 Batch: 2450
Training Loss: 0.012788574050883858
Epoch: 110 Batch: 2500
Training Loss: 0.012418318796157837
Epoch: 110 Batch: 2550
Training Loss: 0.011914606480037464
Epoch: 110 Batch: 2600
Training Loss: 0.011807543062246764
Epoch: 110 Batch: 2650
Training Loss: 0.011522539617880335
Epoch: 110 Batch: 2700
Training Loss: 0.011390378464151312
Epoch: 110 Batch: 2750
Training Loss: 0.011027473297986118
Epoch: 110 Batch: 2800
Training Loss: 0.010943092405796051
Epoch: 110 Batch: 2850
Training Loss: 0.011172558288825185
Epoch: 110 Batch: 2900
Training Loss: 0.011467509351927658
Epoch: 110 Batch: 2950
Training Loss: 0.010987093458741399
Epoch: 110 Batch: 3000
Training Loss: 0.010244498054186504
Epoch: 110 Batch: 3050
Training Loss: 0.010389292191286556
Epoch: 110 Batch: 3100
Training Loss: 0.010219372424387163
Epoch: 110 Batch: 3150
Training Loss: 0.00993478884772649
Epoch: 110 Batch: 3200
Training Loss: 0.00966688372194767
Epoch: 111 
 Validation Loss: 0.4776204837693108
---------------------------
Epoch: 111 Batch: 50
Training Loss: 0.6277536803483963
Epoch: 111 Batch: 100
Training Loss: 0.30229401230812075
Epoch: 111 Batch: 150
Training Loss: 0.2161300535996755
Epoch: 111 Batch: 200
Training Loss: 0.15487141534686089
Epoch: 111 Batch: 250
Training Loss: 0.1255223126411438
Epoch: 111 Batch: 300
Training Loss: 0.10239455550909042
Epoch: 111 Batch: 350
Training Loss: 0.08591769329139165
Epoch: 111 Batch: 400
Training Loss: 0.07618988513946533
Epoch: 111 Batch: 450
Training Loss: 0.06864442401462131
Epoch: 111 Batch: 500
Training Loss: 0.06158930194377899
Epoch: 111 Batch: 550
Training Loss: 0.055234858772971414
Epoch: 111 Batch: 600
Training Loss: 0.05364812860886256
Epoch: 111 Batch: 650
Training Loss: 0.04692971128683824
Epoch: 111 Batch: 700
Training Loss: 0.04321474837405341
Epoch: 111 Batch: 750
Training Loss: 0.04116073819001516
Epoch: 111 Batch: 800
Training Loss: 0.03907658901065588
Epoch: 111 Batch: 850
Training Loss: 0.0365834976645077
Epoch: 111 Batch: 900
Training Loss: 0.03632563524776035
Epoch: 111 Batch: 950
Training Loss: 0.03366748568258787
Epoch: 111 Batch: 1000
Training Loss: 0.03163385325670242
Epoch: 111 Batch: 1050
Training Loss: 0.028790146907170612
Epoch: 111 Batch: 1100
Training Loss: 0.026698436574502424
Epoch: 111 Batch: 1150
Training Loss: 0.026547200576118802
Epoch: 111 Batch: 1200
Training Loss: 0.025809619377056756
Epoch: 111 Batch: 1250
Training Loss: 0.024718250608444212
Epoch: 111 Batch: 1300
Training Loss: 0.02422765906040485
Epoch: 111 Batch: 1350
Training Loss: 0.02322314253559819
Epoch: 111 Batch: 1400
Training Loss: 0.02146392781819616
Epoch: 111 Batch: 1450
Training Loss: 0.020248241876733714
Epoch: 111 Batch: 1500
Training Loss: 0.020314183453718822
Epoch: 111 Batch: 1550
Training Loss: 0.01944026425961525
Epoch: 111 Batch: 1600
Training Loss: 0.019501361660659314
Epoch: 111 Batch: 1650
Training Loss: 0.018659858468807106
Epoch: 111 Batch: 1700
Training Loss: 0.01852383126230801
Epoch: 111 Batch: 1750
Training Loss: 0.01745891983168466
Epoch: 111 Batch: 1800
Training Loss: 0.01781018187602361
Epoch: 111 Batch: 1850
Training Loss: 0.01633057164179312
Epoch: 111 Batch: 1900
Training Loss: 0.016409468525334408
Epoch: 111 Batch: 1950
Training Loss: 0.01581629092876728
Epoch: 111 Batch: 2000
Training Loss: 0.015683856278657912
Epoch: 111 Batch: 2050
Training Loss: 0.015169399278919872
Epoch: 111 Batch: 2100
Training Loss: 0.014391122573897953
Epoch: 111 Batch: 2150
Training Loss: 0.014471371451089548
Epoch: 111 Batch: 2200
Training Loss: 0.014386564046144485
Epoch: 111 Batch: 2250
Training Loss: 0.013559650633070203
Epoch: 111 Batch: 2300
Training Loss: 0.01359746172376301
Epoch: 111 Batch: 2350
Training Loss: 0.013455636095493398
Epoch: 111 Batch: 2400
Training Loss: 0.012977125371495883
Epoch: 111 Batch: 2450
Training Loss: 0.012631314725291972
Epoch: 111 Batch: 2500
Training Loss: 0.012299047386646271
Epoch: 111 Batch: 2550
Training Loss: 0.012373086775050444
Epoch: 111 Batch: 2600
Training Loss: 0.011900330140040471
Epoch: 111 Batch: 2650
Training Loss: 0.011472532951606895
Epoch: 111 Batch: 2700
Training Loss: 0.011610984018555395
Epoch: 111 Batch: 2750
Training Loss: 0.010824321703477338
Epoch: 111 Batch: 2800
Training Loss: 0.010880430617502757
Epoch: 111 Batch: 2850
Training Loss: 0.010241888855632982
Epoch: 111 Batch: 2900
Training Loss: 0.010902253975128305
Epoch: 111 Batch: 2950
Training Loss: 0.010694225188029014
Epoch: 111 Batch: 3000
Training Loss: 0.010141963402430216
Epoch: 111 Batch: 3050
Training Loss: 0.010228180240412228
Epoch: 111 Batch: 3100
Training Loss: 0.010402741076484803
Epoch: 111 Batch: 3150
Training Loss: 0.009922371137709845
Epoch: 111 Batch: 3200
Training Loss: 0.009535090010613203
Epoch: 112 
 Validation Loss: 0.47705430421564315
---------------------------
Epoch: 112 Batch: 50
Training Loss: 0.6337208014726639
Epoch: 112 Batch: 100
Training Loss: 0.31272599548101426
Epoch: 112 Batch: 150
Training Loss: 0.21133151412010193
Epoch: 112 Batch: 200
Training Loss: 0.14876919761300086
Epoch: 112 Batch: 250
Training Loss: 0.12285700166225433
Epoch: 112 Batch: 300
Training Loss: 0.1073294496536255
Epoch: 112 Batch: 350
Training Loss: 0.08484607449599675
Epoch: 112 Batch: 400
Training Loss: 0.0786790581047535
Epoch: 112 Batch: 450
Training Loss: 0.06751601735750834
Epoch: 112 Batch: 500
Training Loss: 0.0611026719212532
Epoch: 112 Batch: 550
Training Loss: 0.05658523933453993
Epoch: 112 Batch: 600
Training Loss: 0.05108712409933408
Epoch: 112 Batch: 650
Training Loss: 0.04575214757369115
Epoch: 112 Batch: 700
Training Loss: 0.043557263229574476
Epoch: 112 Batch: 750
Training Loss: 0.043043157577514646
Epoch: 112 Batch: 800
Training Loss: 0.03718989223241806
Epoch: 112 Batch: 850
Training Loss: 0.03584813920890584
Epoch: 112 Batch: 900
Training Loss: 0.03495351718531715
Epoch: 112 Batch: 950
Training Loss: 0.033073894130556204
Epoch: 112 Batch: 1000
Training Loss: 0.0314123632311821
Epoch: 112 Batch: 1050
Training Loss: 0.030256623256774175
Epoch: 112 Batch: 1100
Training Loss: 0.027894944575699892
Epoch: 112 Batch: 1150
Training Loss: 0.026011470271193462
Epoch: 112 Batch: 1200
Training Loss: 0.024976982722679775
Epoch: 112 Batch: 1250
Training Loss: 0.024422195506095885
Epoch: 112 Batch: 1300
Training Loss: 0.02470423453129255
Epoch: 112 Batch: 1350
Training Loss: 0.02254017902745141
Epoch: 112 Batch: 1400
Training Loss: 0.021107515224388668
Epoch: 112 Batch: 1450
Training Loss: 0.020955407455049712
Epoch: 112 Batch: 1500
Training Loss: 0.02077717606226603
Epoch: 112 Batch: 1550
Training Loss: 0.019942372710474075
Epoch: 112 Batch: 1600
Training Loss: 0.018925826754420994
Epoch: 112 Batch: 1650
Training Loss: 0.018851448983857128
Epoch: 112 Batch: 1700
Training Loss: 0.018224383084213033
Epoch: 112 Batch: 1750
Training Loss: 0.017218318377222333
Epoch: 112 Batch: 1800
Training Loss: 0.01734898704621527
Epoch: 112 Batch: 1850
Training Loss: 0.016545661703960317
Epoch: 112 Batch: 1900
Training Loss: 0.015771955361491754
Epoch: 112 Batch: 1950
Training Loss: 0.016117577262413807
Epoch: 112 Batch: 2000
Training Loss: 0.015096018373966217
Epoch: 112 Batch: 2050
Training Loss: 0.01603242138536965
Epoch: 112 Batch: 2100
Training Loss: 0.015246253779956273
Epoch: 112 Batch: 2150
Training Loss: 0.013580648829770642
Epoch: 112 Batch: 2200
Training Loss: 0.014119194461540743
Epoch: 112 Batch: 2250
Training Loss: 0.013429147203763326
Epoch: 112 Batch: 2300
Training Loss: 0.013207598522953366
Epoch: 112 Batch: 2350
Training Loss: 0.013502443394762404
Epoch: 112 Batch: 2400
Training Loss: 0.012540223461886248
Epoch: 112 Batch: 2450
Training Loss: 0.011816484332084655
Epoch: 112 Batch: 2500
Training Loss: 0.011885948967933655
Epoch: 112 Batch: 2550
Training Loss: 0.012196866542685265
Epoch: 112 Batch: 2600
Training Loss: 0.011578643150054491
Epoch: 112 Batch: 2650
Training Loss: 0.011961102913010795
Epoch: 112 Batch: 2700
Training Loss: 0.011250915428002675
Epoch: 112 Batch: 2750
Training Loss: 0.01074904051694003
Epoch: 112 Batch: 2800
Training Loss: 0.011111425555178097
Epoch: 112 Batch: 2850
Training Loss: 0.01117865616815132
Epoch: 112 Batch: 2900
Training Loss: 0.011360782446532413
Epoch: 112 Batch: 2950
Training Loss: 0.01036731645212335
Epoch: 112 Batch: 3000
Training Loss: 0.010303608268499374
Epoch: 112 Batch: 3050
Training Loss: 0.010007340458572888
Epoch: 112 Batch: 3100
Training Loss: 0.009676993979561713
Epoch: 112 Batch: 3150
Training Loss: 0.009673009732412913
Epoch: 112 Batch: 3200
Training Loss: 0.009809651477262378
Epoch: 113 
 Validation Loss: 0.4768190383911133
---------------------------
Epoch: 113 Batch: 50
Training Loss: 0.6256060320138931
Epoch: 113 Batch: 100
Training Loss: 0.30904681771993636
Epoch: 113 Batch: 150
Training Loss: 0.2061181288957596
Epoch: 113 Batch: 200
Training Loss: 0.15789160162210464
Epoch: 113 Batch: 250
Training Loss: 0.12163968300819397
Epoch: 113 Batch: 300
Training Loss: 0.09895752787590027
Epoch: 113 Batch: 350
Training Loss: 0.08846901135785239
Epoch: 113 Batch: 400
Training Loss: 0.0776529810577631
Epoch: 113 Batch: 450
Training Loss: 0.06787402855025397
Epoch: 113 Batch: 500
Training Loss: 0.06203718441724777
Epoch: 113 Batch: 550
Training Loss: 0.053141173611987724
Epoch: 113 Batch: 600
Training Loss: 0.0528010056912899
Epoch: 113 Batch: 650
Training Loss: 0.05008689128435575
Epoch: 113 Batch: 700
Training Loss: 0.04513758429459163
Epoch: 113 Batch: 750
Training Loss: 0.04225870629151662
Epoch: 113 Batch: 800
Training Loss: 0.03773462358862162
Epoch: 113 Batch: 850
Training Loss: 0.03493730366230011
Epoch: 113 Batch: 900
Training Loss: 0.03637000958124797
Epoch: 113 Batch: 950
Training Loss: 0.03350189064678393
Epoch: 113 Batch: 1000
Training Loss: 0.030907989978790282
Epoch: 113 Batch: 1050
Training Loss: 0.029181726432981944
Epoch: 113 Batch: 1100
Training Loss: 0.028852573389356786
Epoch: 113 Batch: 1150
Training Loss: 0.02540969649086828
Epoch: 113 Batch: 1200
Training Loss: 0.026004335433244704
Epoch: 113 Batch: 1250
Training Loss: 0.023973445200920106
Epoch: 113 Batch: 1300
Training Loss: 0.02352874897993528
Epoch: 113 Batch: 1350
Training Loss: 0.022884651488727994
Epoch: 113 Batch: 1400
Training Loss: 0.022547311442238944
Epoch: 113 Batch: 1450
Training Loss: 0.02268327125187578
Epoch: 113 Batch: 1500
Training Loss: 0.021186186611652374
Epoch: 113 Batch: 1550
Training Loss: 0.01956064014665542
Epoch: 113 Batch: 1600
Training Loss: 0.01903962915763259
Epoch: 113 Batch: 1650
Training Loss: 0.01874891297383742
Epoch: 113 Batch: 1700
Training Loss: 0.01840821178520427
Epoch: 113 Batch: 1750
Training Loss: 0.019141458749771117
Epoch: 113 Batch: 1800
Training Loss: 0.017299405535062155
Epoch: 113 Batch: 1850
Training Loss: 0.016952646793545904
Epoch: 113 Batch: 1900
Training Loss: 0.016157688749463935
Epoch: 113 Batch: 1950
Training Loss: 0.016289641092985103
Epoch: 113 Batch: 2000
Training Loss: 0.014482561439275742
Epoch: 113 Batch: 2050
Training Loss: 0.015230038456800507
Epoch: 113 Batch: 2100
Training Loss: 0.014848273651940481
Epoch: 113 Batch: 2150
Training Loss: 0.013754216404848321
Epoch: 113 Batch: 2200
Training Loss: 0.01329238617962057
Epoch: 113 Batch: 2250
Training Loss: 0.012814092927508885
Epoch: 113 Batch: 2300
Training Loss: 0.013383304619270823
Epoch: 113 Batch: 2350
Training Loss: 0.013144953999113529
Epoch: 113 Batch: 2400
Training Loss: 0.012457724834481876
Epoch: 113 Batch: 2450
Training Loss: 0.012556758097239903
Epoch: 113 Batch: 2500
Training Loss: 0.012205482339859009
Epoch: 113 Batch: 2550
Training Loss: 0.012140783936369652
Epoch: 113 Batch: 2600
Training Loss: 0.012376211961874595
Epoch: 113 Batch: 2650
Training Loss: 0.011008098147950082
Epoch: 113 Batch: 2700
Training Loss: 0.011289398648120738
Epoch: 113 Batch: 2750
Training Loss: 0.011003723545507951
Epoch: 113 Batch: 2800
Training Loss: 0.011154157136167799
Epoch: 113 Batch: 2850
Training Loss: 0.011029472100107294
Epoch: 113 Batch: 2900
Training Loss: 0.010696249768651765
Epoch: 113 Batch: 2950
Training Loss: 0.010363710239782171
Epoch: 113 Batch: 3000
Training Loss: 0.010311099976301193
Epoch: 113 Batch: 3050
Training Loss: 0.009955562743984285
Epoch: 113 Batch: 3100
Training Loss: 0.00949512364402894
Epoch: 113 Batch: 3150
Training Loss: 0.01000734235559191
Epoch: 113 Batch: 3200
Training Loss: 0.009793899180367589
Epoch: 114 
 Validation Loss: 0.4770489364862442
---------------------------
Epoch: 114 Batch: 50
Training Loss: 0.6252804034948349
Epoch: 114 Batch: 100
Training Loss: 0.3088979971408844
Epoch: 114 Batch: 150
Training Loss: 0.203953342239062
Epoch: 114 Batch: 200
Training Loss: 0.15188967376947404
Epoch: 114 Batch: 250
Training Loss: 0.1211556349992752
Epoch: 114 Batch: 300
Training Loss: 0.10213065018256505
Epoch: 114 Batch: 350
Training Loss: 0.09002162984439305
Epoch: 114 Batch: 400
Training Loss: 0.07733617566525935
Epoch: 114 Batch: 450
Training Loss: 0.06881364544232686
Epoch: 114 Batch: 500
Training Loss: 0.06389065420627595
Epoch: 114 Batch: 550
Training Loss: 0.056135454232042484
Epoch: 114 Batch: 600
Training Loss: 0.05384994700551033
Epoch: 114 Batch: 650
Training Loss: 0.048349301356535694
Epoch: 114 Batch: 700
Training Loss: 0.04282684436866215
Epoch: 114 Batch: 750
Training Loss: 0.04313187722365062
Epoch: 114 Batch: 800
Training Loss: 0.03950821366161108
Epoch: 114 Batch: 850
Training Loss: 0.0353872863334768
Epoch: 114 Batch: 900
Training Loss: 0.03591239548391766
Epoch: 114 Batch: 950
Training Loss: 0.03264408466063048
Epoch: 114 Batch: 1000
Training Loss: 0.030229461401700974
Epoch: 114 Batch: 1050
Training Loss: 0.030046516883940923
Epoch: 114 Batch: 1100
Training Loss: 0.03001449612053958
Epoch: 114 Batch: 1150
Training Loss: 0.027866548299789427
Epoch: 114 Batch: 1200
Training Loss: 0.02522247965137164
Epoch: 114 Batch: 1250
Training Loss: 0.025730891156196593
Epoch: 114 Batch: 1300
Training Loss: 0.0234298398403021
Epoch: 114 Batch: 1350
Training Loss: 0.02218879322210948
Epoch: 114 Batch: 1400
Training Loss: 0.02208543834941728
Epoch: 114 Batch: 1450
Training Loss: 0.022298264811778892
Epoch: 114 Batch: 1500
Training Loss: 0.020429395377635957
Epoch: 114 Batch: 1550
Training Loss: 0.019318229171537585
Epoch: 114 Batch: 1600
Training Loss: 0.018743923269212245
Epoch: 114 Batch: 1650
Training Loss: 0.01845903049815785
Epoch: 114 Batch: 1700
Training Loss: 0.01788480635951547
Epoch: 114 Batch: 1750
Training Loss: 0.017785906348909652
Epoch: 114 Batch: 1800
Training Loss: 0.017024714367257224
Epoch: 114 Batch: 1850
Training Loss: 0.017471623162965518
Epoch: 114 Batch: 1900
Training Loss: 0.0161180486020289
Epoch: 114 Batch: 1950
Training Loss: 0.01562275544191018
Epoch: 114 Batch: 2000
Training Loss: 0.01490471152961254
Epoch: 114 Batch: 2050
Training Loss: 0.015131540792744334
Epoch: 114 Batch: 2100
Training Loss: 0.015054993615263985
Epoch: 114 Batch: 2150
Training Loss: 0.013411850416383079
Epoch: 114 Batch: 2200
Training Loss: 0.013326906981793316
Epoch: 114 Batch: 2250
Training Loss: 0.013934229003058539
Epoch: 114 Batch: 2300
Training Loss: 0.01345832928367283
Epoch: 114 Batch: 2350
Training Loss: 0.014154291076863066
Epoch: 114 Batch: 2400
Training Loss: 0.013301757698257765
Epoch: 114 Batch: 2450
Training Loss: 0.012882495595484364
Epoch: 114 Batch: 2500
Training Loss: 0.013061893916130066
Epoch: 114 Batch: 2550
Training Loss: 0.01172169123210159
Epoch: 114 Batch: 2600
Training Loss: 0.01173539991562183
Epoch: 114 Batch: 2650
Training Loss: 0.011630783857039686
Epoch: 114 Batch: 2700
Training Loss: 0.011951216260592142
Epoch: 114 Batch: 2750
Training Loss: 0.011218814102086154
Epoch: 114 Batch: 2800
Training Loss: 0.010430933375443732
Epoch: 114 Batch: 2850
Training Loss: 0.010798448585627372
Epoch: 114 Batch: 2900
Training Loss: 0.010454726959096975
Epoch: 114 Batch: 2950
Training Loss: 0.01018111536058329
Epoch: 114 Batch: 3000
Training Loss: 0.010178519099950791
Epoch: 114 Batch: 3050
Training Loss: 0.009918598102741554
Epoch: 114 Batch: 3100
Training Loss: 0.010006276042230668
Epoch: 114 Batch: 3150
Training Loss: 0.009663984255185203
Epoch: 114 Batch: 3200
Training Loss: 0.009854318145662545
Epoch: 115 
 Validation Loss: 0.4762280729081896
---------------------------
Epoch: 115 Batch: 50
Training Loss: 0.6277894288301468
Epoch: 115 Batch: 100
Training Loss: 0.3138796603679657
Epoch: 115 Batch: 150
Training Loss: 0.2132853784163793
Epoch: 115 Batch: 200
Training Loss: 0.15773836448788642
Epoch: 115 Batch: 250
Training Loss: 0.12226563084125519
Epoch: 115 Batch: 300
Training Loss: 0.10118052442868551
Epoch: 115 Batch: 350
Training Loss: 0.08663961299828121
Epoch: 115 Batch: 400
Training Loss: 0.07594517365098
Epoch: 115 Batch: 450
Training Loss: 0.0654750637213389
Epoch: 115 Batch: 500
Training Loss: 0.061724089682102204
Epoch: 115 Batch: 550
Training Loss: 0.05378055393695831
Epoch: 115 Batch: 600
Training Loss: 0.051989607115586596
Epoch: 115 Batch: 650
Training Loss: 0.048758158133580136
Epoch: 115 Batch: 700
Training Loss: 0.04516862405197961
Epoch: 115 Batch: 750
Training Loss: 0.04219112471739451
Epoch: 115 Batch: 800
Training Loss: 0.04034390926361084
Epoch: 115 Batch: 850
Training Loss: 0.03558282129904803
Epoch: 115 Batch: 900
Training Loss: 0.03338265498479207
Epoch: 115 Batch: 950
Training Loss: 0.032489317435967294
Epoch: 115 Batch: 1000
Training Loss: 0.03156273132562638
Epoch: 115 Batch: 1050
Training Loss: 0.03121857015859513
Epoch: 115 Batch: 1100
Training Loss: 0.028973831642757762
Epoch: 115 Batch: 1150
Training Loss: 0.026361588291499927
Epoch: 115 Batch: 1200
Training Loss: 0.026531592508157096
Epoch: 115 Batch: 1250
Training Loss: 0.025007589268684387
Epoch: 115 Batch: 1300
Training Loss: 0.023604777524104486
Epoch: 115 Batch: 1350
Training Loss: 0.02232870395536776
Epoch: 115 Batch: 1400
Training Loss: 0.02172545075416565
Epoch: 115 Batch: 1450
Training Loss: 0.021183970734990877
Epoch: 115 Batch: 1500
Training Loss: 0.0205454048315684
Epoch: 115 Batch: 1550
Training Loss: 0.019453504623905306
Epoch: 115 Batch: 1600
Training Loss: 0.018936694655567406
Epoch: 115 Batch: 1650
Training Loss: 0.018839882052305974
Epoch: 115 Batch: 1700
Training Loss: 0.017889156586983625
Epoch: 115 Batch: 1750
Training Loss: 0.01794986186708723
Epoch: 115 Batch: 1800
Training Loss: 0.01696070450875494
Epoch: 115 Batch: 1850
Training Loss: 0.01622576948758718
Epoch: 115 Batch: 1900
Training Loss: 0.016695938173093294
Epoch: 115 Batch: 1950
Training Loss: 0.01565271719908103
Epoch: 115 Batch: 2000
Training Loss: 0.015182245537638665
Epoch: 115 Batch: 2050
Training Loss: 0.014452946767574403
Epoch: 115 Batch: 2100
Training Loss: 0.014552702605724334
Epoch: 115 Batch: 2150
Training Loss: 0.014008768838505412
Epoch: 115 Batch: 2200
Training Loss: 0.013800481544299558
Epoch: 115 Batch: 2250
Training Loss: 0.014187243315908643
Epoch: 115 Batch: 2300
Training Loss: 0.013228493708631267
Epoch: 115 Batch: 2350
Training Loss: 0.013434029094716336
Epoch: 115 Batch: 2400
Training Loss: 0.01280168058971564
Epoch: 115 Batch: 2450
Training Loss: 0.011998798311973105
Epoch: 115 Batch: 2500
Training Loss: 0.011670383632183074
Epoch: 115 Batch: 2550
Training Loss: 0.012113181831789952
Epoch: 115 Batch: 2600
Training Loss: 0.012230966721589749
Epoch: 115 Batch: 2650
Training Loss: 0.011748043501152182
Epoch: 115 Batch: 2700
Training Loss: 0.010674343881783663
Epoch: 115 Batch: 2750
Training Loss: 0.010723657066171819
Epoch: 115 Batch: 2800
Training Loss: 0.010606683237212046
Epoch: 115 Batch: 2850
Training Loss: 0.011128163065826684
Epoch: 115 Batch: 2900
Training Loss: 0.010884530492897691
Epoch: 115 Batch: 2950
Training Loss: 0.010504041273715133
Epoch: 115 Batch: 3000
Training Loss: 0.010399857312440872
Epoch: 115 Batch: 3050
Training Loss: 0.010141543316059425
Epoch: 115 Batch: 3100
Training Loss: 0.009814708655880343
Epoch: 115 Batch: 3150
Training Loss: 0.009911680789220901
Epoch: 115 Batch: 3200
Training Loss: 0.009810199085623026
Epoch: 116 
 Validation Loss: 0.4765223311053382
---------------------------
Epoch: 116 Batch: 50
Training Loss: 0.608014822602272
Epoch: 116 Batch: 100
Training Loss: 0.3257314398884773
Epoch: 116 Batch: 150
Training Loss: 0.20381639997164408
Epoch: 116 Batch: 200
Training Loss: 0.15899729147553443
Epoch: 116 Batch: 250
Training Loss: 0.12196140205860138
Epoch: 116 Batch: 300
Training Loss: 0.1065552443265915
Epoch: 116 Batch: 350
Training Loss: 0.08970896065235139
Epoch: 116 Batch: 400
Training Loss: 0.07669930532574654
Epoch: 116 Batch: 450
Training Loss: 0.06839294327629937
Epoch: 116 Batch: 500
Training Loss: 0.06343527573347092
Epoch: 116 Batch: 550
Training Loss: 0.05863471594723788
Epoch: 116 Batch: 600
Training Loss: 0.051840760409832
Epoch: 116 Batch: 650
Training Loss: 0.04560457642261798
Epoch: 116 Batch: 700
Training Loss: 0.04443217354161399
Epoch: 116 Batch: 750
Training Loss: 0.0403905306259791
Epoch: 116 Batch: 800
Training Loss: 0.03924712926149368
Epoch: 116 Batch: 850
Training Loss: 0.035268389863126415
Epoch: 116 Batch: 900
Training Loss: 0.035798401667012106
Epoch: 116 Batch: 950
Training Loss: 0.03354443914011905
Epoch: 116 Batch: 1000
Training Loss: 0.03092636829614639
Epoch: 116 Batch: 1050
Training Loss: 0.030248979699044002
Epoch: 116 Batch: 1100
Training Loss: 0.027538404166698456
Epoch: 116 Batch: 1150
Training Loss: 0.027761646535085595
Epoch: 116 Batch: 1200
Training Loss: 0.025496908153096836
Epoch: 116 Batch: 1250
Training Loss: 0.023456875610351563
Epoch: 116 Batch: 1300
Training Loss: 0.02306746712097755
Epoch: 116 Batch: 1350
Training Loss: 0.023238607578807406
Epoch: 116 Batch: 1400
Training Loss: 0.023709221354552678
Epoch: 116 Batch: 1450
Training Loss: 0.021147672287349043
Epoch: 116 Batch: 1500
Training Loss: 0.020139308194319406
Epoch: 116 Batch: 1550
Training Loss: 0.019813029496900496
Epoch: 116 Batch: 1600
Training Loss: 0.019523876644670965
Epoch: 116 Batch: 1650
Training Loss: 0.01893845549135497
Epoch: 116 Batch: 1700
Training Loss: 0.01849249881856582
Epoch: 116 Batch: 1750
Training Loss: 0.018075743419783456
Epoch: 116 Batch: 1800
Training Loss: 0.017144478526380326
Epoch: 116 Batch: 1850
Training Loss: 0.016232933627592552
Epoch: 116 Batch: 1900
Training Loss: 0.01659777067209545
Epoch: 116 Batch: 1950
Training Loss: 0.016721427180828193
Epoch: 116 Batch: 2000
Training Loss: 0.015515483662486076
Epoch: 116 Batch: 2050
Training Loss: 0.015344376534950443
Epoch: 116 Batch: 2100
Training Loss: 0.014656422989709037
Epoch: 116 Batch: 2150
Training Loss: 0.013858384531597759
Epoch: 116 Batch: 2200
Training Loss: 0.013940482424064116
Epoch: 116 Batch: 2250
Training Loss: 0.013615834818945991
Epoch: 116 Batch: 2300
Training Loss: 0.013528841189716173
Epoch: 116 Batch: 2350
Training Loss: 0.012985436979760515
Epoch: 116 Batch: 2400
Training Loss: 0.01233117837458849
Epoch: 116 Batch: 2450
Training Loss: 0.012447683312455002
Epoch: 116 Batch: 2500
Training Loss: 0.012609051489830016
Epoch: 116 Batch: 2550
Training Loss: 0.01181319837476693
Epoch: 116 Batch: 2600
Training Loss: 0.01198589217204314
Epoch: 116 Batch: 2650
Training Loss: 0.012329898757754632
Epoch: 116 Batch: 2700
Training Loss: 0.012058384473677035
Epoch: 116 Batch: 2750
Training Loss: 0.011905984358354047
Epoch: 116 Batch: 2800
Training Loss: 0.010971019023231098
Epoch: 116 Batch: 2850
Training Loss: 0.010212587588711788
Epoch: 116 Batch: 2900
Training Loss: 0.010701507083300886
Epoch: 116 Batch: 2950
Training Loss: 0.009964077987913359
Epoch: 116 Batch: 3000
Training Loss: 0.009853360563516617
Epoch: 116 Batch: 3050
Training Loss: 0.010253900807411944
Epoch: 116 Batch: 3100
Training Loss: 0.009901389527705407
Epoch: 116 Batch: 3150
Training Loss: 0.009642049575608875
Epoch: 116 Batch: 3200
Training Loss: 0.009944651825353503
Epoch: 117 
 Validation Loss: 0.4764383577638202
---------------------------
Epoch: 117 Batch: 50
Training Loss: 0.6454727637767792
Epoch: 117 Batch: 100
Training Loss: 0.31317352086305617
Epoch: 117 Batch: 150
Training Loss: 0.21581989904244742
Epoch: 117 Batch: 200
Training Loss: 0.15832399636507033
Epoch: 117 Batch: 250
Training Loss: 0.11806101775169373
Epoch: 117 Batch: 300
Training Loss: 0.10379258950551351
Epoch: 117 Batch: 350
Training Loss: 0.08499976217746735
Epoch: 117 Batch: 400
Training Loss: 0.07587961941957473
Epoch: 117 Batch: 450
Training Loss: 0.06719676454861959
Epoch: 117 Batch: 500
Training Loss: 0.060390556156635286
Epoch: 117 Batch: 550
Training Loss: 0.05703289091587067
Epoch: 117 Batch: 600
Training Loss: 0.05132630894581477
Epoch: 117 Batch: 650
Training Loss: 0.04573120566514822
Epoch: 117 Batch: 700
Training Loss: 0.04567611902952194
Epoch: 117 Batch: 750
Training Loss: 0.04137979054450989
Epoch: 117 Batch: 800
Training Loss: 0.03777743645012379
Epoch: 117 Batch: 850
Training Loss: 0.034512202634530906
Epoch: 117 Batch: 900
Training Loss: 0.0325758210155699
Epoch: 117 Batch: 950
Training Loss: 0.03326602954613535
Epoch: 117 Batch: 1000
Training Loss: 0.03200400188565254
Epoch: 117 Batch: 1050
Training Loss: 0.02894111238774799
Epoch: 117 Batch: 1100
Training Loss: 0.028910420049320568
Epoch: 117 Batch: 1150
Training Loss: 0.025990442737289097
Epoch: 117 Batch: 1200
Training Loss: 0.025634823292493822
Epoch: 117 Batch: 1250
Training Loss: 0.0240605309009552
Epoch: 117 Batch: 1300
Training Loss: 0.024335879545945387
Epoch: 117 Batch: 1350
Training Loss: 0.023534842839947452
Epoch: 117 Batch: 1400
Training Loss: 0.022644864512341364
Epoch: 117 Batch: 1450
Training Loss: 0.021196485059014683
Epoch: 117 Batch: 1500
Training Loss: 0.020585715631643933
Epoch: 117 Batch: 1550
Training Loss: 0.020157317384596793
Epoch: 117 Batch: 1600
Training Loss: 0.019877415522933005
Epoch: 117 Batch: 1650
Training Loss: 0.018363928939356948
Epoch: 117 Batch: 1700
Training Loss: 0.018200695041347952
Epoch: 117 Batch: 1750
Training Loss: 0.017221977302006312
Epoch: 117 Batch: 1800
Training Loss: 0.01663616263204151
Epoch: 117 Batch: 1850
Training Loss: 0.017818743328790406
Epoch: 117 Batch: 1900
Training Loss: 0.01655122670688127
Epoch: 117 Batch: 1950
Training Loss: 0.015938552908408336
Epoch: 117 Batch: 2000
Training Loss: 0.014968596905469894
Epoch: 117 Batch: 2050
Training Loss: 0.014784024168805377
Epoch: 117 Batch: 2100
Training Loss: 0.01460396579333714
Epoch: 117 Batch: 2150
Training Loss: 0.014887681783631791
Epoch: 117 Batch: 2200
Training Loss: 0.014334012324159796
Epoch: 117 Batch: 2250
Training Loss: 0.013491430746184455
Epoch: 117 Batch: 2300
Training Loss: 0.013496898036936055
Epoch: 117 Batch: 2350
Training Loss: 0.013466589159153877
Epoch: 117 Batch: 2400
Training Loss: 0.012809689231216908
Epoch: 117 Batch: 2450
Training Loss: 0.012555038053162245
Epoch: 117 Batch: 2500
Training Loss: 0.012824977695941925
Epoch: 117 Batch: 2550
Training Loss: 0.012299915867693283
Epoch: 117 Batch: 2600
Training Loss: 0.012377205605690295
Epoch: 117 Batch: 2650
Training Loss: 0.011480064965643973
Epoch: 117 Batch: 2700
Training Loss: 0.011289720237255097
Epoch: 117 Batch: 2750
Training Loss: 0.010848929481072859
Epoch: 117 Batch: 2800
Training Loss: 0.011244890125734465
Epoch: 117 Batch: 2850
Training Loss: 0.01090540894290857
Epoch: 117 Batch: 2900
Training Loss: 0.01079452022396285
Epoch: 117 Batch: 2950
Training Loss: 0.01025005411293547
Epoch: 117 Batch: 3000
Training Loss: 0.010538405120372771
Epoch: 117 Batch: 3050
Training Loss: 0.010315132102028269
Epoch: 117 Batch: 3100
Training Loss: 0.010181839475708623
Epoch: 117 Batch: 3150
Training Loss: 0.009657737291048444
Epoch: 117 Batch: 3200
Training Loss: 0.009708369513973594
Epoch: 118 
 Validation Loss: 0.4760967724853092
---------------------------
Epoch: 118 Batch: 50
Training Loss: 0.6283849400281906
Epoch: 118 Batch: 100
Training Loss: 0.3110473546385765
Epoch: 118 Batch: 150
Training Loss: 0.19937020162741342
Epoch: 118 Batch: 200
Training Loss: 0.1578136771917343
Epoch: 118 Batch: 250
Training Loss: 0.12458988332748414
Epoch: 118 Batch: 300
Training Loss: 0.10262087494134903
Epoch: 118 Batch: 350
Training Loss: 0.08803271421364375
Epoch: 118 Batch: 400
Training Loss: 0.0763397242128849
Epoch: 118 Batch: 450
Training Loss: 0.06972554630703397
Epoch: 118 Batch: 500
Training Loss: 0.06380908542871475
Epoch: 118 Batch: 550
Training Loss: 0.054460080103440715
Epoch: 118 Batch: 600
Training Loss: 0.0523793151974678
Epoch: 118 Batch: 650
Training Loss: 0.048114410592959476
Epoch: 118 Batch: 700
Training Loss: 0.046884267926216125
Epoch: 118 Batch: 750
Training Loss: 0.04017890838781993
Epoch: 118 Batch: 800
Training Loss: 0.03918103482574224
Epoch: 118 Batch: 850
Training Loss: 0.03505491940414204
Epoch: 118 Batch: 900
Training Loss: 0.03371139105823305
Epoch: 118 Batch: 950
Training Loss: 0.032031409960044055
Epoch: 118 Batch: 1000
Training Loss: 0.0313727123439312
Epoch: 118 Batch: 1050
Training Loss: 0.0277064744915281
Epoch: 118 Batch: 1100
Training Loss: 0.027451675615527412
Epoch: 118 Batch: 1150
Training Loss: 0.02567706242851589
Epoch: 118 Batch: 1200
Training Loss: 0.026427984038988748
Epoch: 118 Batch: 1250
Training Loss: 0.02404759726524353
Epoch: 118 Batch: 1300
Training Loss: 0.02376095148233267
Epoch: 118 Batch: 1350
Training Loss: 0.02391796730182789
Epoch: 118 Batch: 1400
Training Loss: 0.02187801901783262
Epoch: 118 Batch: 1450
Training Loss: 0.021340470622325764
Epoch: 118 Batch: 1500
Training Loss: 0.02063340268532435
Epoch: 118 Batch: 1550
Training Loss: 0.01984991865773355
Epoch: 118 Batch: 1600
Training Loss: 0.018898820225149392
Epoch: 118 Batch: 1650
Training Loss: 0.018836062062870373
Epoch: 118 Batch: 1700
Training Loss: 0.018463276449371787
Epoch: 118 Batch: 1750
Training Loss: 0.018091276288032532
Epoch: 118 Batch: 1800
Training Loss: 0.017754885173506207
Epoch: 118 Batch: 1850
Training Loss: 0.016131669444006843
Epoch: 118 Batch: 1900
Training Loss: 0.016565881967544556
Epoch: 118 Batch: 1950
Training Loss: 0.01524110462421026
Epoch: 118 Batch: 2000
Training Loss: 0.015512506067752839
Epoch: 118 Batch: 2050
Training Loss: 0.014545363580308309
Epoch: 118 Batch: 2100
Training Loss: 0.01455191589537121
Epoch: 118 Batch: 2150
Training Loss: 0.01428243171337039
Epoch: 118 Batch: 2200
Training Loss: 0.014093664925206792
Epoch: 118 Batch: 2250
Training Loss: 0.014094752934243945
Epoch: 118 Batch: 2300
Training Loss: 0.013944620010645493
Epoch: 118 Batch: 2350
Training Loss: 0.013355598563843584
Epoch: 118 Batch: 2400
Training Loss: 0.012705520739157995
Epoch: 118 Batch: 2450
Training Loss: 0.012879496508715104
Epoch: 118 Batch: 2500
Training Loss: 0.011886748099327087
Epoch: 118 Batch: 2550
Training Loss: 0.011403575457778632
Epoch: 118 Batch: 2600
Training Loss: 0.011990379645274236
Epoch: 118 Batch: 2650
Training Loss: 0.011268244880550313
Epoch: 118 Batch: 2700
Training Loss: 0.011625538633929358
Epoch: 118 Batch: 2750
Training Loss: 0.010465859814123674
Epoch: 118 Batch: 2800
Training Loss: 0.011003518583519119
Epoch: 118 Batch: 2850
Training Loss: 0.010863748088217618
Epoch: 118 Batch: 2900
Training Loss: 0.010756209646833353
Epoch: 118 Batch: 2950
Training Loss: 0.010500615289655782
Epoch: 118 Batch: 3000
Training Loss: 0.010273824473222096
Epoch: 118 Batch: 3050
Training Loss: 0.01047867265881085
Epoch: 118 Batch: 3100
Training Loss: 0.010380585337838819
Epoch: 118 Batch: 3150
Training Loss: 0.009587757190068562
Epoch: 118 Batch: 3200
Training Loss: 0.010226404862478375
Epoch: 119 
 Validation Loss: 0.4763868818680445
---------------------------
Epoch: 119 Batch: 50
Training Loss: 0.6233970099687576
Epoch: 119 Batch: 100
Training Loss: 0.31017169028520586
Epoch: 119 Batch: 150
Training Loss: 0.20065748810768128
Epoch: 119 Batch: 200
Training Loss: 0.15424222722649575
Epoch: 119 Batch: 250
Training Loss: 0.12248178946971894
Epoch: 119 Batch: 300
Training Loss: 0.10095496157805126
Epoch: 119 Batch: 350
Training Loss: 0.09008470135075705
Epoch: 119 Batch: 400
Training Loss: 0.07339106850326062
Epoch: 119 Batch: 450
Training Loss: 0.06799560871389176
Epoch: 119 Batch: 500
Training Loss: 0.059553073704242704
Epoch: 119 Batch: 550
Training Loss: 0.05822966673157432
Epoch: 119 Batch: 600
Training Loss: 0.04931291162967682
Epoch: 119 Batch: 650
Training Loss: 0.047145423751610974
Epoch: 119 Batch: 700
Training Loss: 0.04441744021006993
Epoch: 119 Batch: 750
Training Loss: 0.041424562136332194
Epoch: 119 Batch: 800
Training Loss: 0.036885682046413425
Epoch: 119 Batch: 850
Training Loss: 0.03778474095989676
Epoch: 119 Batch: 900
Training Loss: 0.03409676326645745
Epoch: 119 Batch: 950
Training Loss: 0.03259219254318037
Epoch: 119 Batch: 1000
Training Loss: 0.030532802432775497
Epoch: 119 Batch: 1050
Training Loss: 0.029404433369636537
Epoch: 119 Batch: 1100
Training Loss: 0.026999263655055655
Epoch: 119 Batch: 1150
Training Loss: 0.02695056640583536
Epoch: 119 Batch: 1200
Training Loss: 0.026637866646051406
Epoch: 119 Batch: 1250
Training Loss: 0.024011641430854797
Epoch: 119 Batch: 1300
Training Loss: 0.024573067495456107
Epoch: 119 Batch: 1350
Training Loss: 0.02244119056948909
Epoch: 119 Batch: 1400
Training Loss: 0.022265001727002007
Epoch: 119 Batch: 1450
Training Loss: 0.021619788922112564
Epoch: 119 Batch: 1500
Training Loss: 0.02103954577445984
Epoch: 119 Batch: 1550
Training Loss: 0.01943791587506571
Epoch: 119 Batch: 1600
Training Loss: 0.01909011762589216
Epoch: 119 Batch: 1650
Training Loss: 0.018809419762004505
Epoch: 119 Batch: 1700
Training Loss: 0.018074095442014583
Epoch: 119 Batch: 1750
Training Loss: 0.018100246582712445
Epoch: 119 Batch: 1800
Training Loss: 0.01757232043478224
Epoch: 119 Batch: 1850
Training Loss: 0.0161130002060452
Epoch: 119 Batch: 1900
Training Loss: 0.015845785125305777
Epoch: 119 Batch: 1950
Training Loss: 0.015667726550346765
Epoch: 119 Batch: 2000
Training Loss: 0.015445093527436256
Epoch: 119 Batch: 2050
Training Loss: 0.01501490674367765
Epoch: 119 Batch: 2100
Training Loss: 0.013732799490292867
Epoch: 119 Batch: 2150
Training Loss: 0.015190218687057494
Epoch: 119 Batch: 2200
Training Loss: 0.013571068238128316
Epoch: 119 Batch: 2250
Training Loss: 0.013919412149323358
Epoch: 119 Batch: 2300
Training Loss: 0.013687186681705972
Epoch: 119 Batch: 2350
Training Loss: 0.012978449197525674
Epoch: 119 Batch: 2400
Training Loss: 0.013240329660475255
Epoch: 119 Batch: 2450
Training Loss: 0.012938491531780788
Epoch: 119 Batch: 2500
Training Loss: 0.012811156713962555
Epoch: 119 Batch: 2550
Training Loss: 0.01188122014204661
Epoch: 119 Batch: 2600
Training Loss: 0.011795910952182917
Epoch: 119 Batch: 2650
Training Loss: 0.011467067149450194
Epoch: 119 Batch: 2700
Training Loss: 0.011291625709445387
Epoch: 119 Batch: 2750
Training Loss: 0.01138594494082711
Epoch: 119 Batch: 2800
Training Loss: 0.011097342946699687
Epoch: 119 Batch: 2850
Training Loss: 0.011278091512228314
Epoch: 119 Batch: 2900
Training Loss: 0.01085364164977238
Epoch: 119 Batch: 2950
Training Loss: 0.01007395770590184
Epoch: 119 Batch: 3000
Training Loss: 0.010406554530064265
Epoch: 119 Batch: 3050
Training Loss: 0.010088390502773348
Epoch: 119 Batch: 3100
Training Loss: 0.01050658158717617
Epoch: 119 Batch: 3150
Training Loss: 0.009899782926317244
Epoch: 119 Batch: 3200
Training Loss: 0.009888822305947542
Epoch: 120 
 Validation Loss: 0.4758891873889499
---------------------------
Epoch: 120 Batch: 50
Training Loss: 0.6347925764322281
Epoch: 120 Batch: 100
Training Loss: 0.2986298713088036
Epoch: 120 Batch: 150
Training Loss: 0.20428238928318024
Epoch: 120 Batch: 200
Training Loss: 0.15629371181130408
Epoch: 120 Batch: 250
Training Loss: 0.11919456207752228
Epoch: 120 Batch: 300
Training Loss: 0.1005089642604192
Epoch: 120 Batch: 350
Training Loss: 0.08904818551880973
Epoch: 120 Batch: 400
Training Loss: 0.07741877421736718
Epoch: 120 Batch: 450
Training Loss: 0.07071827603711023
Epoch: 120 Batch: 500
Training Loss: 0.06295142328739166
Epoch: 120 Batch: 550
Training Loss: 0.05632202646949074
Epoch: 120 Batch: 600
Training Loss: 0.051761726786692934
Epoch: 120 Batch: 650
Training Loss: 0.04805016687283149
Epoch: 120 Batch: 700
Training Loss: 0.04446058609655925
Epoch: 120 Batch: 750
Training Loss: 0.043077157934506734
Epoch: 120 Batch: 800
Training Loss: 0.03993761315941811
Epoch: 120 Batch: 850
Training Loss: 0.03742446257787592
Epoch: 120 Batch: 900
Training Loss: 0.033809364802307555
Epoch: 120 Batch: 950
Training Loss: 0.03150895005778263
Epoch: 120 Batch: 1000
Training Loss: 0.03096570971608162
Epoch: 120 Batch: 1050
Training Loss: 0.03054023470197405
Epoch: 120 Batch: 1100
Training Loss: 0.02882780297236009
Epoch: 120 Batch: 1150
Training Loss: 0.027477053352024244
Epoch: 120 Batch: 1200
Training Loss: 0.026443743829925856
Epoch: 120 Batch: 1250
Training Loss: 0.02400917055606842
Epoch: 120 Batch: 1300
Training Loss: 0.024143473781072178
Epoch: 120 Batch: 1350
Training Loss: 0.023338117113819827
Epoch: 120 Batch: 1400
Training Loss: 0.0213832212133067
Epoch: 120 Batch: 1450
Training Loss: 0.020566794728410654
Epoch: 120 Batch: 1500
Training Loss: 0.02126992259422938
Epoch: 120 Batch: 1550
Training Loss: 0.020241525153959952
Epoch: 120 Batch: 1600
Training Loss: 0.019421679340302943
Epoch: 120 Batch: 1650
Training Loss: 0.018268235434185376
Epoch: 120 Batch: 1700
Training Loss: 0.016891946845194874
Epoch: 120 Batch: 1750
Training Loss: 0.017987839783940997
Epoch: 120 Batch: 1800
Training Loss: 0.017550201846493616
Epoch: 120 Batch: 1850
Training Loss: 0.016627184990290048
Epoch: 120 Batch: 1900
Training Loss: 0.01641828041327627
Epoch: 120 Batch: 1950
Training Loss: 0.015615942906110715
Epoch: 120 Batch: 2000
Training Loss: 0.014995048716664315
Epoch: 120 Batch: 2050
Training Loss: 0.014855473186911607
Epoch: 120 Batch: 2100
Training Loss: 0.015478936391217368
Epoch: 120 Batch: 2150
Training Loss: 0.014457071423530579
Epoch: 120 Batch: 2200
Training Loss: 0.013570780889554457
Epoch: 120 Batch: 2250
Training Loss: 0.01327172232998742
Epoch: 120 Batch: 2300
Training Loss: 0.013142179626485576
Epoch: 120 Batch: 2350
Training Loss: 0.013394427464363423
Epoch: 120 Batch: 2400
Training Loss: 0.012781010158360005
Epoch: 120 Batch: 2450
Training Loss: 0.012789142679195014
Epoch: 120 Batch: 2500
Training Loss: 0.01239269380569458
Epoch: 120 Batch: 2550
Training Loss: 0.012608771218973048
Epoch: 120 Batch: 2600
Training Loss: 0.012127934121168577
Epoch: 120 Batch: 2650
Training Loss: 0.011822049629013493
Epoch: 120 Batch: 2700
Training Loss: 0.01116626536404645
Epoch: 120 Batch: 2750
Training Loss: 0.01071130534735593
Epoch: 120 Batch: 2800
Training Loss: 0.01065100446343422
Epoch: 120 Batch: 2850
Training Loss: 0.01094288708870871
Epoch: 120 Batch: 2900
Training Loss: 0.010393960558134935
Epoch: 120 Batch: 2950
Training Loss: 0.010248547327720512
Epoch: 120 Batch: 3000
Training Loss: 0.010792730569839478
Epoch: 120 Batch: 3050
Training Loss: 0.009926803434481386
Epoch: 120 Batch: 3100
Training Loss: 0.010057629990962243
Epoch: 120 Batch: 3150
Training Loss: 0.009617444958005632
Epoch: 120 Batch: 3200
Training Loss: 0.009355581654235721
Epoch: 121 
 Validation Loss: 0.47624335885047914
---------------------------
Epoch: 121 Batch: 50
Training Loss: 0.6237013041973114
Epoch: 121 Batch: 100
Training Loss: 0.30091684848070144
Epoch: 121 Batch: 150
Training Loss: 0.21548341234525045
Epoch: 121 Batch: 200
Training Loss: 0.15712437465786933
Epoch: 121 Batch: 250
Training Loss: 0.12221148097515107
Epoch: 121 Batch: 300
Training Loss: 0.10331362068653106
Epoch: 121 Batch: 350
Training Loss: 0.08966781505516597
Epoch: 121 Batch: 400
Training Loss: 0.07647138245403767
Epoch: 121 Batch: 450
Training Loss: 0.07011270814471775
Epoch: 121 Batch: 500
Training Loss: 0.06072464168071747
Epoch: 121 Batch: 550
Training Loss: 0.0563678039745851
Epoch: 121 Batch: 600
Training Loss: 0.05256012166539828
Epoch: 121 Batch: 650
Training Loss: 0.04492542830797342
Epoch: 121 Batch: 700
Training Loss: 0.0450668831382479
Epoch: 121 Batch: 750
Training Loss: 0.04142790242036184
Epoch: 121 Batch: 800
Training Loss: 0.03909988880157471
Epoch: 121 Batch: 850
Training Loss: 0.03646204345366534
Epoch: 121 Batch: 900
Training Loss: 0.03237064494027032
Epoch: 121 Batch: 950
Training Loss: 0.03174064780536451
Epoch: 121 Batch: 1000
Training Loss: 0.03215301910042763
Epoch: 121 Batch: 1050
Training Loss: 0.02985278734139034
Epoch: 121 Batch: 1100
Training Loss: 0.028766542808576063
Epoch: 121 Batch: 1150
Training Loss: 0.027279214988584103
Epoch: 121 Batch: 1200
Training Loss: 0.02567651629447937
Epoch: 121 Batch: 1250
Training Loss: 0.02525394706726074
Epoch: 121 Batch: 1300
Training Loss: 0.02358813313337473
Epoch: 121 Batch: 1350
Training Loss: 0.023582126939738237
Epoch: 121 Batch: 1400
Training Loss: 0.021870326059205192
Epoch: 121 Batch: 1450
Training Loss: 0.021843717488749272
Epoch: 121 Batch: 1500
Training Loss: 0.019948661208152772
Epoch: 121 Batch: 1550
Training Loss: 0.019125242713958986
Epoch: 121 Batch: 1600
Training Loss: 0.019125333130359648
Epoch: 121 Batch: 1650
Training Loss: 0.01875111789414377
Epoch: 121 Batch: 1700
Training Loss: 0.018322760778314927
Epoch: 121 Batch: 1750
Training Loss: 0.018213744367871966
Epoch: 121 Batch: 1800
Training Loss: 0.016822547929154504
Epoch: 121 Batch: 1850
Training Loss: 0.016649361913268632
Epoch: 121 Batch: 1900
Training Loss: 0.016887635306308144
Epoch: 121 Batch: 1950
Training Loss: 0.016333484450976054
Epoch: 121 Batch: 2000
Training Loss: 0.015221088543534279
Epoch: 121 Batch: 2050
Training Loss: 0.014673610402316581
Epoch: 121 Batch: 2100
Training Loss: 0.014870394042560032
Epoch: 121 Batch: 2150
Training Loss: 0.013827426447424778
Epoch: 121 Batch: 2200
Training Loss: 0.013830117054960945
Epoch: 121 Batch: 2250
Training Loss: 0.014249727103445265
Epoch: 121 Batch: 2300
Training Loss: 0.01383769215449043
Epoch: 121 Batch: 2350
Training Loss: 0.013655675586233748
Epoch: 121 Batch: 2400
Training Loss: 0.012739515664676826
Epoch: 121 Batch: 2450
Training Loss: 0.012169613071850369
Epoch: 121 Batch: 2500
Training Loss: 0.012078790092468261
Epoch: 121 Batch: 2550
Training Loss: 0.011854077332160052
Epoch: 121 Batch: 2600
Training Loss: 0.012054055550923714
Epoch: 121 Batch: 2650
Training Loss: 0.01180292324075159
Epoch: 121 Batch: 2700
Training Loss: 0.011408009308355825
Epoch: 121 Batch: 2750
Training Loss: 0.01125137623873624
Epoch: 121 Batch: 2800
Training Loss: 0.011627052979809897
Epoch: 121 Batch: 2850
Training Loss: 0.010635236459865905
Epoch: 121 Batch: 2900
Training Loss: 0.01122958033249296
Epoch: 121 Batch: 2950
Training Loss: 0.01049534970420902
Epoch: 121 Batch: 3000
Training Loss: 0.010193839460611344
Epoch: 121 Batch: 3050
Training Loss: 0.009604112750194111
Epoch: 121 Batch: 3100
Training Loss: 0.009452559399989343
Epoch: 121 Batch: 3150
Training Loss: 0.009677196901942057
Epoch: 121 Batch: 3200
Training Loss: 0.009841440487653017
Epoch: 122 
 Validation Loss: 0.4760295738776525
---------------------------
Epoch: 122 Batch: 50
Training Loss: 0.6340132105350494
Epoch: 122 Batch: 100
Training Loss: 0.31237420201301574
Epoch: 122 Batch: 150
Training Loss: 0.21123112459977467
Epoch: 122 Batch: 200
Training Loss: 0.15682845428586006
Epoch: 122 Batch: 250
Training Loss: 0.125735054731369
Epoch: 122 Batch: 300
Training Loss: 0.10661282350619634
Epoch: 122 Batch: 350
Training Loss: 0.08782213909285409
Epoch: 122 Batch: 400
Training Loss: 0.07650282993912697
Epoch: 122 Batch: 450
Training Loss: 0.06693410495917003
Epoch: 122 Batch: 500
Training Loss: 0.06139408987760544
Epoch: 122 Batch: 550
Training Loss: 0.055923103148286996
Epoch: 122 Batch: 600
Training Loss: 0.050606574614842734
Epoch: 122 Batch: 650
Training Loss: 0.047158475701625534
Epoch: 122 Batch: 700
Training Loss: 0.04312395027705601
Epoch: 122 Batch: 750
Training Loss: 0.03930109620094299
Epoch: 122 Batch: 800
Training Loss: 0.04029939617961645
Epoch: 122 Batch: 850
Training Loss: 0.03688560780356912
Epoch: 122 Batch: 900
Training Loss: 0.034680230915546416
Epoch: 122 Batch: 950
Training Loss: 0.0332255430598008
Epoch: 122 Batch: 1000
Training Loss: 0.030458713591098786
Epoch: 122 Batch: 1050
Training Loss: 0.030012559748831248
Epoch: 122 Batch: 1100
Training Loss: 0.028045128042047673
Epoch: 122 Batch: 1150
Training Loss: 0.027284654404806055
Epoch: 122 Batch: 1200
Training Loss: 0.024250003546476363
Epoch: 122 Batch: 1250
Training Loss: 0.024932637882232666
Epoch: 122 Batch: 1300
Training Loss: 0.023894099730711715
Epoch: 122 Batch: 1350
Training Loss: 0.022810570134056938
Epoch: 122 Batch: 1400
Training Loss: 0.0215374199620315
Epoch: 122 Batch: 1450
Training Loss: 0.021598822721119584
Epoch: 122 Batch: 1500
Training Loss: 0.02054631495475769
Epoch: 122 Batch: 1550
Training Loss: 0.020307570176739845
Epoch: 122 Batch: 1600
Training Loss: 0.019018079396337272
Epoch: 122 Batch: 1650
Training Loss: 0.01832794290600401
Epoch: 122 Batch: 1700
Training Loss: 0.018625331959303687
Epoch: 122 Batch: 1750
Training Loss: 0.01786965506417411
Epoch: 122 Batch: 1800
Training Loss: 0.016573378096024195
Epoch: 122 Batch: 1850
Training Loss: 0.0173927224810059
Epoch: 122 Batch: 1900
Training Loss: 0.016498709273965736
Epoch: 122 Batch: 1950
Training Loss: 0.015668353728758983
Epoch: 122 Batch: 2000
Training Loss: 0.014591214776039124
Epoch: 122 Batch: 2050
Training Loss: 0.014089649857544318
Epoch: 122 Batch: 2100
Training Loss: 0.015135213051523481
Epoch: 122 Batch: 2150
Training Loss: 0.013806159842846005
Epoch: 122 Batch: 2200
Training Loss: 0.0145062677833167
Epoch: 122 Batch: 2250
Training Loss: 0.014152858482466803
Epoch: 122 Batch: 2300
Training Loss: 0.01300792280746543
Epoch: 122 Batch: 2350
Training Loss: 0.013736472117139938
Epoch: 122 Batch: 2400
Training Loss: 0.012628375887870788
Epoch: 122 Batch: 2450
Training Loss: 0.012392791387986164
Epoch: 122 Batch: 2500
Training Loss: 0.012007778084278107
Epoch: 122 Batch: 2550
Training Loss: 0.0118773921213898
Epoch: 122 Batch: 2600
Training Loss: 0.011426819952634665
Epoch: 122 Batch: 2650
Training Loss: 0.011969649004486372
Epoch: 122 Batch: 2700
Training Loss: 0.011316363403090723
Epoch: 122 Batch: 2750
Training Loss: 0.010975787975571373
Epoch: 122 Batch: 2800
Training Loss: 0.010250924367989812
Epoch: 122 Batch: 2850
Training Loss: 0.010757781758643033
Epoch: 122 Batch: 2900
Training Loss: 0.01053807734415449
Epoch: 122 Batch: 2950
Training Loss: 0.01031745050923299
Epoch: 122 Batch: 3000
Training Loss: 0.01043793108065923
Epoch: 122 Batch: 3050
Training Loss: 0.010042186330576412
Epoch: 122 Batch: 3100
Training Loss: 0.010000574627230245
Epoch: 122 Batch: 3150
Training Loss: 0.009400758308077616
Epoch: 122 Batch: 3200
Training Loss: 0.009392162263393402
Epoch: 123 
 Validation Loss: 0.4760809861951404
---------------------------
Epoch: 123 Batch: 50
Training Loss: 0.6151930218935013
Epoch: 123 Batch: 100
Training Loss: 0.30550920754671096
Epoch: 123 Batch: 150
Training Loss: 0.209290505250295
Epoch: 123 Batch: 200
Training Loss: 0.15434708595275878
Epoch: 123 Batch: 250
Training Loss: 0.12515957522392274
Epoch: 123 Batch: 300
Training Loss: 0.1025302016735077
Epoch: 123 Batch: 350
Training Loss: 0.0915569588967732
Epoch: 123 Batch: 400
Training Loss: 0.07446247182786464
Epoch: 123 Batch: 450
Training Loss: 0.06654684762159983
Epoch: 123 Batch: 500
Training Loss: 0.0590760572552681
Epoch: 123 Batch: 550
Training Loss: 0.05448754673654383
Epoch: 123 Batch: 600
Training Loss: 0.05297646482785543
Epoch: 123 Batch: 650
Training Loss: 0.049147372475037206
Epoch: 123 Batch: 700
Training Loss: 0.043643105370657784
Epoch: 123 Batch: 750
Training Loss: 0.042424510598182676
Epoch: 123 Batch: 800
Training Loss: 0.04044963054358959
Epoch: 123 Batch: 850
Training Loss: 0.03590124761357027
Epoch: 123 Batch: 900
Training Loss: 0.03394014103545083
Epoch: 123 Batch: 950
Training Loss: 0.03225775100682911
Epoch: 123 Batch: 1000
Training Loss: 0.031612347662448886
Epoch: 123 Batch: 1050
Training Loss: 0.028060726977529978
Epoch: 123 Batch: 1100
Training Loss: 0.028487989577380093
Epoch: 123 Batch: 1150
Training Loss: 0.0268025724525037
Epoch: 123 Batch: 1200
Training Loss: 0.026321631794174512
Epoch: 123 Batch: 1250
Training Loss: 0.02519349122047424
Epoch: 123 Batch: 1300
Training Loss: 0.02337095769552084
Epoch: 123 Batch: 1350
Training Loss: 0.022794813800741127
Epoch: 123 Batch: 1400
Training Loss: 0.022787037427936283
Epoch: 123 Batch: 1450
Training Loss: 0.022040898347723072
Epoch: 123 Batch: 1500
Training Loss: 0.019931774457295735
Epoch: 123 Batch: 1550
Training Loss: 0.019878665554908016
Epoch: 123 Batch: 1600
Training Loss: 0.018816014304757118
Epoch: 123 Batch: 1650
Training Loss: 0.01787476779836597
Epoch: 123 Batch: 1700
Training Loss: 0.019429237947744482
Epoch: 123 Batch: 1750
Training Loss: 0.017499421562467303
Epoch: 123 Batch: 1800
Training Loss: 0.016980763773123422
Epoch: 123 Batch: 1850
Training Loss: 0.017178272234426962
Epoch: 123 Batch: 1900
Training Loss: 0.016376920750266626
Epoch: 123 Batch: 1950
Training Loss: 0.01654363827827649
Epoch: 123 Batch: 2000
Training Loss: 0.014452551022171974
Epoch: 123 Batch: 2050
Training Loss: 0.015015737254445145
Epoch: 123 Batch: 2100
Training Loss: 0.014804874247028714
Epoch: 123 Batch: 2150
Training Loss: 0.013575218971385512
Epoch: 123 Batch: 2200
Training Loss: 0.014173320856961337
Epoch: 123 Batch: 2250
Training Loss: 0.013132758008109199
Epoch: 123 Batch: 2300
Training Loss: 0.013401702357375103
Epoch: 123 Batch: 2350
Training Loss: 0.012929332205589781
Epoch: 123 Batch: 2400
Training Loss: 0.013050341891745726
Epoch: 123 Batch: 2450
Training Loss: 0.012719774501664298
Epoch: 123 Batch: 2500
Training Loss: 0.011792201161384583
Epoch: 123 Batch: 2550
Training Loss: 0.011774374947828405
Epoch: 123 Batch: 2600
Training Loss: 0.0119028556232269
Epoch: 123 Batch: 2650
Training Loss: 0.011708303701202825
Epoch: 123 Batch: 2700
Training Loss: 0.011781270404656728
Epoch: 123 Batch: 2750
Training Loss: 0.010768976092338562
Epoch: 123 Batch: 2800
Training Loss: 0.011175193328942571
Epoch: 123 Batch: 2850
Training Loss: 0.010685575824034841
Epoch: 123 Batch: 2900
Training Loss: 0.010580708312577215
Epoch: 123 Batch: 2950
Training Loss: 0.010581440208321911
Epoch: 123 Batch: 3000
Training Loss: 0.01031362513701121
Epoch: 123 Batch: 3050
Training Loss: 0.009853027560671822
Epoch: 123 Batch: 3100
Training Loss: 0.01055566547378417
Epoch: 123 Batch: 3150
Training Loss: 0.010159154193741934
Epoch: 123 Batch: 3200
Training Loss: 0.009506287258118392
Epoch: 124 
 Validation Loss: 0.4761431260241403
---------------------------
Epoch: 124 Batch: 50
Training Loss: 0.6231973838806152
Epoch: 124 Batch: 100
Training Loss: 0.31241900503635406
Epoch: 124 Batch: 150
Training Loss: 0.20507329046726228
Epoch: 124 Batch: 200
Training Loss: 0.15332087576389314
Epoch: 124 Batch: 250
Training Loss: 0.12327713525295257
Epoch: 124 Batch: 300
Training Loss: 0.10099224915107091
Epoch: 124 Batch: 350
Training Loss: 0.08801528683730535
Epoch: 124 Batch: 400
Training Loss: 0.07758940763771534
Epoch: 124 Batch: 450
Training Loss: 0.06730940216117436
Epoch: 124 Batch: 500
Training Loss: 0.0616395868062973
Epoch: 124 Batch: 550
Training Loss: 0.05713987789370797
Epoch: 124 Batch: 600
Training Loss: 0.051519607653220494
Epoch: 124 Batch: 650
Training Loss: 0.04755485140360319
Epoch: 124 Batch: 700
Training Loss: 0.04484115421772003
Epoch: 124 Batch: 750
Training Loss: 0.040619529525438944
Epoch: 124 Batch: 800
Training Loss: 0.03773255225270986
Epoch: 124 Batch: 850
Training Loss: 0.03745589182657354
Epoch: 124 Batch: 900
Training Loss: 0.03371650175915824
Epoch: 124 Batch: 950
Training Loss: 0.03245900150976683
Epoch: 124 Batch: 1000
Training Loss: 0.02978220683336258
Epoch: 124 Batch: 1050
Training Loss: 0.02814153960772923
Epoch: 124 Batch: 1100
Training Loss: 0.0281508130106059
Epoch: 124 Batch: 1150
Training Loss: 0.026593976202218428
Epoch: 124 Batch: 1200
Training Loss: 0.02555992824335893
Epoch: 124 Batch: 1250
Training Loss: 0.024581276559829713
Epoch: 124 Batch: 1300
Training Loss: 0.023760273135625398
Epoch: 124 Batch: 1350
Training Loss: 0.02212809012995826
Epoch: 124 Batch: 1400
Training Loss: 0.022690310052462987
Epoch: 124 Batch: 1450
Training Loss: 0.021394623456330135
Epoch: 124 Batch: 1500
Training Loss: 0.020229236761728924
Epoch: 124 Batch: 1550
Training Loss: 0.02003841240559855
Epoch: 124 Batch: 1600
Training Loss: 0.017863918393850327
Epoch: 124 Batch: 1650
Training Loss: 0.018760732972260677
Epoch: 124 Batch: 1700
Training Loss: 0.018120872045264524
Epoch: 124 Batch: 1750
Training Loss: 0.0170902795451028
Epoch: 124 Batch: 1800
Training Loss: 0.017334161698818205
Epoch: 124 Batch: 1850
Training Loss: 0.016817061933311256
Epoch: 124 Batch: 1900
Training Loss: 0.0164837636445698
Epoch: 124 Batch: 1950
Training Loss: 0.0159115573993096
Epoch: 124 Batch: 2000
Training Loss: 0.014820613294839859
Epoch: 124 Batch: 2050
Training Loss: 0.01443936997797431
Epoch: 124 Batch: 2100
Training Loss: 0.014510386188824971
Epoch: 124 Batch: 2150
Training Loss: 0.014062362770701563
Epoch: 124 Batch: 2200
Training Loss: 0.014135188541629097
Epoch: 124 Batch: 2250
Training Loss: 0.013555428981781005
Epoch: 124 Batch: 2300
Training Loss: 0.013019661100014396
Epoch: 124 Batch: 2350
Training Loss: 0.013208000013168821
Epoch: 124 Batch: 2400
Training Loss: 0.01283339861780405
Epoch: 124 Batch: 2450
Training Loss: 0.01273443235426533
Epoch: 124 Batch: 2500
Training Loss: 0.012925908684730529
Epoch: 124 Batch: 2550
Training Loss: 0.012165487104771184
Epoch: 124 Batch: 2600
Training Loss: 0.012194982514931606
Epoch: 124 Batch: 2650
Training Loss: 0.011768738976064718
Epoch: 124 Batch: 2700
Training Loss: 0.011380654363720506
Epoch: 124 Batch: 2750
Training Loss: 0.010755503177642822
Epoch: 124 Batch: 2800
Training Loss: 0.011022685606564794
Epoch: 124 Batch: 2850
Training Loss: 0.010326206558629087
Epoch: 124 Batch: 2900
Training Loss: 0.010353641581946406
Epoch: 124 Batch: 2950
Training Loss: 0.011087929333670664
Epoch: 124 Batch: 3000
Training Loss: 0.010100753833850225
Epoch: 124 Batch: 3050
Training Loss: 0.010549106138651489
Epoch: 124 Batch: 3100
Training Loss: 0.010120351381840245
Epoch: 124 Batch: 3150
Training Loss: 0.01025784192577241
Epoch: 124 Batch: 3200
Training Loss: 0.009966795640066266
Epoch: 125 
 Validation Loss: 0.47561361094315846
---------------------------
Epoch: 125 Batch: 50
Training Loss: 0.6135975050926209
Epoch: 125 Batch: 100
Training Loss: 0.3129881379008293
Epoch: 125 Batch: 150
Training Loss: 0.21009476006031036
Epoch: 125 Batch: 200
Training Loss: 0.14874991044402122
Epoch: 125 Batch: 250
Training Loss: 0.12511467254161834
Epoch: 125 Batch: 300
Training Loss: 0.10464863707621892
Epoch: 125 Batch: 350
Training Loss: 0.08814060756138394
Epoch: 125 Batch: 400
Training Loss: 0.08022984452545642
Epoch: 125 Batch: 450
Training Loss: 0.06595206095112695
Epoch: 125 Batch: 500
Training Loss: 0.0646324679851532
Epoch: 125 Batch: 550
Training Loss: 0.05623842867937955
Epoch: 125 Batch: 600
Training Loss: 0.0529505596558253
Epoch: 125 Batch: 650
Training Loss: 0.048685138546503505
Epoch: 125 Batch: 700
Training Loss: 0.04305205119507653
Epoch: 125 Batch: 750
Training Loss: 0.04298844448725383
Epoch: 125 Batch: 800
Training Loss: 0.03883890621364117
Epoch: 125 Batch: 850
Training Loss: 0.03659834900323083
Epoch: 125 Batch: 900
Training Loss: 0.03289071987072627
Epoch: 125 Batch: 950
Training Loss: 0.03238726440228914
Epoch: 125 Batch: 1000
Training Loss: 0.030941704988479615
Epoch: 125 Batch: 1050
Training Loss: 0.029445355137189228
Epoch: 125 Batch: 1100
Training Loss: 0.028539398420940745
Epoch: 125 Batch: 1150
Training Loss: 0.026421423787656037
Epoch: 125 Batch: 1200
Training Loss: 0.025086083138982456
Epoch: 125 Batch: 1250
Training Loss: 0.024191372132301332
Epoch: 125 Batch: 1300
Training Loss: 0.023665410967973564
Epoch: 125 Batch: 1350
Training Loss: 0.023481481649257518
Epoch: 125 Batch: 1400
Training Loss: 0.020926067871706828
Epoch: 125 Batch: 1450
Training Loss: 0.020903894428549143
Epoch: 125 Batch: 1500
Training Loss: 0.019936327477296192
Epoch: 125 Batch: 1550
Training Loss: 0.020260252683393418
Epoch: 125 Batch: 1600
Training Loss: 0.01825630249455571
Epoch: 125 Batch: 1650
Training Loss: 0.018684457739194235
Epoch: 125 Batch: 1700
Training Loss: 0.017928280549890856
Epoch: 125 Batch: 1750
Training Loss: 0.01744021599633353
Epoch: 125 Batch: 1800
Training Loss: 0.0170411898361312
Epoch: 125 Batch: 1850
Training Loss: 0.016507505323435808
Epoch: 125 Batch: 1900
Training Loss: 0.016169140636920928
Epoch: 125 Batch: 1950
Training Loss: 0.01602448928050506
Epoch: 125 Batch: 2000
Training Loss: 0.014812079906463623
Epoch: 125 Batch: 2050
Training Loss: 0.01579357574625713
Epoch: 125 Batch: 2100
Training Loss: 0.014674548790568398
Epoch: 125 Batch: 2150
Training Loss: 0.01468306630156761
Epoch: 125 Batch: 2200
Training Loss: 0.014364797933535142
Epoch: 125 Batch: 2250
Training Loss: 0.013192756705813938
Epoch: 125 Batch: 2300
Training Loss: 0.014265615953051525
Epoch: 125 Batch: 2350
Training Loss: 0.013297011573263939
Epoch: 125 Batch: 2400
Training Loss: 0.012785364749530952
Epoch: 125 Batch: 2450
Training Loss: 0.012544913875813387
Epoch: 125 Batch: 2500
Training Loss: 0.012223680996894836
Epoch: 125 Batch: 2550
Training Loss: 0.012367165450956306
Epoch: 125 Batch: 2600
Training Loss: 0.012015173618610089
Epoch: 125 Batch: 2650
Training Loss: 0.011659858957776483
Epoch: 125 Batch: 2700
Training Loss: 0.011857955201908394
Epoch: 125 Batch: 2750
Training Loss: 0.01111605697328394
Epoch: 125 Batch: 2800
Training Loss: 0.011212920823267528
Epoch: 125 Batch: 2850
Training Loss: 0.010762239830535755
Epoch: 125 Batch: 2900
Training Loss: 0.01079300539246921
Epoch: 125 Batch: 2950
Training Loss: 0.010527007003961983
Epoch: 125 Batch: 3000
Training Loss: 0.009961127281188965
Epoch: 125 Batch: 3050
Training Loss: 0.010608363757367994
Epoch: 125 Batch: 3100
Training Loss: 0.009679302179044292
Epoch: 125 Batch: 3150
Training Loss: 0.009787222364592174
Epoch: 125 Batch: 3200
Training Loss: 0.009124478148296476
Epoch: 126 
 Validation Loss: 0.47532639106114705
---------------------------
Epoch: 126 Batch: 50
Training Loss: 0.606929172873497
Epoch: 126 Batch: 100
Training Loss: 0.2967705073952675
Epoch: 126 Batch: 150
Training Loss: 0.20327100197474163
Epoch: 126 Batch: 200
Training Loss: 0.15830626532435418
Epoch: 126 Batch: 250
Training Loss: 0.12189694130420685
Epoch: 126 Batch: 300
Training Loss: 0.10439409871896108
Epoch: 126 Batch: 350
Training Loss: 0.09140758837972368
Epoch: 126 Batch: 400
Training Loss: 0.07881389103829861
Epoch: 126 Batch: 450
Training Loss: 0.06336268703142801
Epoch: 126 Batch: 500
Training Loss: 0.060933695077896116
Epoch: 126 Batch: 550
Training Loss: 0.05715821547941728
Epoch: 126 Batch: 600
Training Loss: 0.05136687209208806
Epoch: 126 Batch: 650
Training Loss: 0.046349685696455145
Epoch: 126 Batch: 700
Training Loss: 0.04471756228378841
Epoch: 126 Batch: 750
Training Loss: 0.043182634711265565
Epoch: 126 Batch: 800
Training Loss: 0.037691225744783875
Epoch: 126 Batch: 850
Training Loss: 0.03580832940690658
Epoch: 126 Batch: 900
Training Loss: 0.034308256010214484
Epoch: 126 Batch: 950
Training Loss: 0.033051079009708606
Epoch: 126 Batch: 1000
Training Loss: 0.03198015728592873
Epoch: 126 Batch: 1050
Training Loss: 0.029284202995754422
Epoch: 126 Batch: 1100
Training Loss: 0.027074598642912777
Epoch: 126 Batch: 1150
Training Loss: 0.02746318959671518
Epoch: 126 Batch: 1200
Training Loss: 0.026865939795970916
Epoch: 126 Batch: 1250
Training Loss: 0.024172842741012574
Epoch: 126 Batch: 1300
Training Loss: 0.023408099986039675
Epoch: 126 Batch: 1350
Training Loss: 0.02273568833315814
Epoch: 126 Batch: 1400
Training Loss: 0.021788933255842753
Epoch: 126 Batch: 1450
Training Loss: 0.02147690758622926
Epoch: 126 Batch: 1500
Training Loss: 0.02085363519191742
Epoch: 126 Batch: 1550
Training Loss: 0.02050271334186677
Epoch: 126 Batch: 1600
Training Loss: 0.019521570950746536
Epoch: 126 Batch: 1650
Training Loss: 0.019110305453791762
Epoch: 126 Batch: 1700
Training Loss: 0.0174599148070111
Epoch: 126 Batch: 1750
Training Loss: 0.01795403712136405
Epoch: 126 Batch: 1800
Training Loss: 0.017358894530269835
Epoch: 126 Batch: 1850
Training Loss: 0.01707501247122481
Epoch: 126 Batch: 1900
Training Loss: 0.015999622344970703
Epoch: 126 Batch: 1950
Training Loss: 0.016138425775063345
Epoch: 126 Batch: 2000
Training Loss: 0.016323956340551376
Epoch: 126 Batch: 2050
Training Loss: 0.014157043564610365
Epoch: 126 Batch: 2100
Training Loss: 0.014800728786559332
Epoch: 126 Batch: 2150
Training Loss: 0.014921528059382771
Epoch: 126 Batch: 2200
Training Loss: 0.01435271141203967
Epoch: 126 Batch: 2250
Training Loss: 0.01344986347357432
Epoch: 126 Batch: 2300
Training Loss: 0.013584218698999156
Epoch: 126 Batch: 2350
Training Loss: 0.013814105328093183
Epoch: 126 Batch: 2400
Training Loss: 0.012848258477946123
Epoch: 126 Batch: 2450
Training Loss: 0.012083565045376212
Epoch: 126 Batch: 2500
Training Loss: 0.012337312352657318
Epoch: 126 Batch: 2550
Training Loss: 0.011844939227197685
Epoch: 126 Batch: 2600
Training Loss: 0.012233506521353354
Epoch: 126 Batch: 2650
Training Loss: 0.01135758027715503
Epoch: 126 Batch: 2700
Training Loss: 0.011324813895755344
Epoch: 126 Batch: 2750
Training Loss: 0.011304406794634732
Epoch: 126 Batch: 2800
Training Loss: 0.01085079767874309
Epoch: 126 Batch: 2850
Training Loss: 0.010650964923072279
Epoch: 126 Batch: 2900
Training Loss: 0.010905018027486472
Epoch: 126 Batch: 2950
Training Loss: 0.01068123235540875
Epoch: 126 Batch: 3000
Training Loss: 0.010412911921739577
Epoch: 126 Batch: 3050
Training Loss: 0.009684202426769694
Epoch: 126 Batch: 3100
Training Loss: 0.009529947955762187
Epoch: 126 Batch: 3150
Training Loss: 0.009509613400413876
Epoch: 126 Batch: 3200
Training Loss: 0.009891380807384848
Epoch: 127 
 Validation Loss: 0.47539817558394537
---------------------------
Epoch: 127 Batch: 50
Training Loss: 0.5912658101320267
Epoch: 127 Batch: 100
Training Loss: 0.31825317203998565
Epoch: 127 Batch: 150
Training Loss: 0.21154012044270834
Epoch: 127 Batch: 200
Training Loss: 0.1559097009897232
Epoch: 127 Batch: 250
Training Loss: 0.12388484513759614
Epoch: 127 Batch: 300
Training Loss: 0.09620481252670288
Epoch: 127 Batch: 350
Training Loss: 0.0916342783825738
Epoch: 127 Batch: 400
Training Loss: 0.07339847534894943
Epoch: 127 Batch: 450
Training Loss: 0.06462017960018582
Epoch: 127 Batch: 500
Training Loss: 0.06015645807981491
Epoch: 127 Batch: 550
Training Loss: 0.058087216453118755
Epoch: 127 Batch: 600
Training Loss: 0.05214933310945829
Epoch: 127 Batch: 650
Training Loss: 0.0455813511976829
Epoch: 127 Batch: 700
Training Loss: 0.04583064662558692
Epoch: 127 Batch: 750
Training Loss: 0.041480249325434364
Epoch: 127 Batch: 800
Training Loss: 0.036629038862884045
Epoch: 127 Batch: 850
Training Loss: 0.03617377989432391
Epoch: 127 Batch: 900
Training Loss: 0.03292067093981637
Epoch: 127 Batch: 950
Training Loss: 0.032589025685661716
Epoch: 127 Batch: 1000
Training Loss: 0.028802694737911225
Epoch: 127 Batch: 1050
Training Loss: 0.028000314093771433
Epoch: 127 Batch: 1100
Training Loss: 0.028985265086997638
Epoch: 127 Batch: 1150
Training Loss: 0.026830473438553187
Epoch: 127 Batch: 1200
Training Loss: 0.025446680362025898
Epoch: 127 Batch: 1250
Training Loss: 0.024904407858848572
Epoch: 127 Batch: 1300
Training Loss: 0.02392317416576239
Epoch: 127 Batch: 1350
Training Loss: 0.023237368596924674
Epoch: 127 Batch: 1400
Training Loss: 0.022753204043422428
Epoch: 127 Batch: 1450
Training Loss: 0.02215126483604826
Epoch: 127 Batch: 1500
Training Loss: 0.020196972807248432
Epoch: 127 Batch: 1550
Training Loss: 0.020196304725062463
Epoch: 127 Batch: 1600
Training Loss: 0.019244812503457068
Epoch: 127 Batch: 1650
Training Loss: 0.01877766267819838
Epoch: 127 Batch: 1700
Training Loss: 0.01768735391252181
Epoch: 127 Batch: 1750
Training Loss: 0.017844863959721156
Epoch: 127 Batch: 1800
Training Loss: 0.016807317982117334
Epoch: 127 Batch: 1850
Training Loss: 0.01644786783166834
Epoch: 127 Batch: 1900
Training Loss: 0.015425811253095927
Epoch: 127 Batch: 1950
Training Loss: 0.015755953513658962
Epoch: 127 Batch: 2000
Training Loss: 0.01560176508128643
Epoch: 127 Batch: 2050
Training Loss: 0.015061012506484985
Epoch: 127 Batch: 2100
Training Loss: 0.01494261120046888
Epoch: 127 Batch: 2150
Training Loss: 0.014045016876486845
Epoch: 127 Batch: 2200
Training Loss: 0.014415957792238756
Epoch: 127 Batch: 2250
Training Loss: 0.013853212515513103
Epoch: 127 Batch: 2300
Training Loss: 0.01317288955916529
Epoch: 127 Batch: 2350
Training Loss: 0.012492094204780903
Epoch: 127 Batch: 2400
Training Loss: 0.013550357210139434
Epoch: 127 Batch: 2450
Training Loss: 0.012252502149465133
Epoch: 127 Batch: 2500
Training Loss: 0.013023357343673707
Epoch: 127 Batch: 2550
Training Loss: 0.012344780669492834
Epoch: 127 Batch: 2600
Training Loss: 0.012311088454264861
Epoch: 127 Batch: 2650
Training Loss: 0.011307008064018105
Epoch: 127 Batch: 2700
Training Loss: 0.011320924626456367
Epoch: 127 Batch: 2750
Training Loss: 0.01065509301965887
Epoch: 127 Batch: 2800
Training Loss: 0.010807502440043857
Epoch: 127 Batch: 2850
Training Loss: 0.010848053139552736
Epoch: 127 Batch: 2900
Training Loss: 0.010445723533630371
Epoch: 127 Batch: 2950
Training Loss: 0.010396132196410227
Epoch: 127 Batch: 3000
Training Loss: 0.01037318072716395
Epoch: 127 Batch: 3050
Training Loss: 0.010057324073353751
Epoch: 127 Batch: 3100
Training Loss: 0.010174198064111893
Epoch: 127 Batch: 3150
Training Loss: 0.010021012520033215
Epoch: 127 Batch: 3200
Training Loss: 0.009004867933690548
Epoch: 128 
 Validation Loss: 0.4752329051494598
---------------------------
Epoch: 128 Batch: 50
Training Loss: 0.646132578253746
Epoch: 128 Batch: 100
Training Loss: 0.30916975617408754
Epoch: 128 Batch: 150
Training Loss: 0.20952806254227957
Epoch: 128 Batch: 200
Training Loss: 0.15740035861730575
Epoch: 128 Batch: 250
Training Loss: 0.11973466873168945
Epoch: 128 Batch: 300
Training Loss: 0.10422051012516022
Epoch: 128 Batch: 350
Training Loss: 0.08413313950811113
Epoch: 128 Batch: 400
Training Loss: 0.07467099323868752
Epoch: 128 Batch: 450
Training Loss: 0.06845537821451823
Epoch: 128 Batch: 500
Training Loss: 0.05814360135793686
Epoch: 128 Batch: 550
Training Loss: 0.055005621693351055
Epoch: 128 Batch: 600
Training Loss: 0.051704012254873914
Epoch: 128 Batch: 650
Training Loss: 0.04925845054479746
Epoch: 128 Batch: 700
Training Loss: 0.045423156321048735
Epoch: 128 Batch: 750
Training Loss: 0.04029464888572693
Epoch: 128 Batch: 800
Training Loss: 0.03986120034009218
Epoch: 128 Batch: 850
Training Loss: 0.03463815562865313
Epoch: 128 Batch: 900
Training Loss: 0.03304835581117206
Epoch: 128 Batch: 950
Training Loss: 0.032999431271302074
Epoch: 128 Batch: 1000
Training Loss: 0.031417153924703595
Epoch: 128 Batch: 1050
Training Loss: 0.03019413411617279
Epoch: 128 Batch: 1100
Training Loss: 0.028544166684150697
Epoch: 128 Batch: 1150
Training Loss: 0.026468406138212785
Epoch: 128 Batch: 1200
Training Loss: 0.02596769576271375
Epoch: 128 Batch: 1250
Training Loss: 0.02515449185371399
Epoch: 128 Batch: 1300
Training Loss: 0.023825931067650134
Epoch: 128 Batch: 1350
Training Loss: 0.022026604965881064
Epoch: 128 Batch: 1400
Training Loss: 0.022860818611724038
Epoch: 128 Batch: 1450
Training Loss: 0.021411004025360635
Epoch: 128 Batch: 1500
Training Loss: 0.020707208216190338
Epoch: 128 Batch: 1550
Training Loss: 0.01930922871635806
Epoch: 128 Batch: 1600
Training Loss: 0.019605397023260595
Epoch: 128 Batch: 1650
Training Loss: 0.01832919722253626
Epoch: 128 Batch: 1700
Training Loss: 0.018839286443065196
Epoch: 128 Batch: 1750
Training Loss: 0.017979106204850333
Epoch: 128 Batch: 1800
Training Loss: 0.016724421663416755
Epoch: 128 Batch: 1850
Training Loss: 0.015930881419697323
Epoch: 128 Batch: 1900
Training Loss: 0.016244136016619833
Epoch: 128 Batch: 1950
Training Loss: 0.015855374993422093
Epoch: 128 Batch: 2000
Training Loss: 0.015258382886648178
Epoch: 128 Batch: 2050
Training Loss: 0.014886224008188014
Epoch: 128 Batch: 2100
Training Loss: 0.014547836227076394
Epoch: 128 Batch: 2150
Training Loss: 0.013694792514623598
Epoch: 128 Batch: 2200
Training Loss: 0.014003025930036198
Epoch: 128 Batch: 2250
Training Loss: 0.01404634584320916
Epoch: 128 Batch: 2300
Training Loss: 0.013799259558967922
Epoch: 128 Batch: 2350
Training Loss: 0.013143949318439402
Epoch: 128 Batch: 2400
Training Loss: 0.01256178330630064
Epoch: 128 Batch: 2450
Training Loss: 0.01219680428504944
Epoch: 128 Batch: 2500
Training Loss: 0.012287337374687195
Epoch: 128 Batch: 2550
Training Loss: 0.011790733103658638
Epoch: 128 Batch: 2600
Training Loss: 0.011153560234950139
Epoch: 128 Batch: 2650
Training Loss: 0.011321480926477685
Epoch: 128 Batch: 2700
Training Loss: 0.0106570080253813
Epoch: 128 Batch: 2750
Training Loss: 0.011009375875646418
Epoch: 128 Batch: 2800
Training Loss: 0.010766100606748036
Epoch: 128 Batch: 2850
Training Loss: 0.011111158885453876
Epoch: 128 Batch: 2900
Training Loss: 0.01034207952433619
Epoch: 128 Batch: 2950
Training Loss: 0.01007165575431565
Epoch: 128 Batch: 3000
Training Loss: 0.010301312436660132
Epoch: 128 Batch: 3050
Training Loss: 0.010290783712121308
Epoch: 128 Batch: 3100
Training Loss: 0.00978376394317996
Epoch: 128 Batch: 3150
Training Loss: 0.009799665382930211
Epoch: 128 Batch: 3200
Training Loss: 0.009629671340808272
Epoch: 129 
 Validation Loss: 0.47470792068375484
---------------------------
Epoch: 129 Batch: 50
Training Loss: 0.6401039624214172
Epoch: 129 Batch: 100
Training Loss: 0.31023216307163237
Epoch: 129 Batch: 150
Training Loss: 0.20563596228758493
Epoch: 129 Batch: 200
Training Loss: 0.14974504917860032
Epoch: 129 Batch: 250
Training Loss: 0.1236731106042862
Epoch: 129 Batch: 300
Training Loss: 0.10231666336456935
Epoch: 129 Batch: 350
Training Loss: 0.0886870207956859
Epoch: 129 Batch: 400
Training Loss: 0.0764270082116127
Epoch: 129 Batch: 450
Training Loss: 0.06726765400833554
Epoch: 129 Batch: 500
Training Loss: 0.060101631343364714
Epoch: 129 Batch: 550
Training Loss: 0.05492110479961742
Epoch: 129 Batch: 600
Training Loss: 0.05010312601923943
Epoch: 129 Batch: 650
Training Loss: 0.048117629014528714
Epoch: 129 Batch: 700
Training Loss: 0.043906958954674856
Epoch: 129 Batch: 750
Training Loss: 0.03958648161093394
Epoch: 129 Batch: 800
Training Loss: 0.04103002350777388
Epoch: 129 Batch: 850
Training Loss: 0.034814520232817704
Epoch: 129 Batch: 900
Training Loss: 0.033006233076254526
Epoch: 129 Batch: 950
Training Loss: 0.03274523132725766
Epoch: 129 Batch: 1000
Training Loss: 0.03124242898821831
Epoch: 129 Batch: 1050
Training Loss: 0.02925435721874237
Epoch: 129 Batch: 1100
Training Loss: 0.028738677908073773
Epoch: 129 Batch: 1150
Training Loss: 0.02676736028298088
Epoch: 129 Batch: 1200
Training Loss: 0.025102260559797286
Epoch: 129 Batch: 1250
Training Loss: 0.024363079118728637
Epoch: 129 Batch: 1300
Training Loss: 0.023205618812487675
Epoch: 129 Batch: 1350
Training Loss: 0.022547560422508803
Epoch: 129 Batch: 1400
Training Loss: 0.02227713208113398
Epoch: 129 Batch: 1450
Training Loss: 0.02097610286597548
Epoch: 129 Batch: 1500
Training Loss: 0.02149850849310557
Epoch: 129 Batch: 1550
Training Loss: 0.020324877423624838
Epoch: 129 Batch: 1600
Training Loss: 0.019668827801942824
Epoch: 129 Batch: 1650
Training Loss: 0.01805487867557641
Epoch: 129 Batch: 1700
Training Loss: 0.01856409037814421
Epoch: 129 Batch: 1750
Training Loss: 0.01797835842200688
Epoch: 129 Batch: 1800
Training Loss: 0.016736708068185384
Epoch: 129 Batch: 1850
Training Loss: 0.01686169627550486
Epoch: 129 Batch: 1900
Training Loss: 0.016901210013188814
Epoch: 129 Batch: 1950
Training Loss: 0.015413943895926843
Epoch: 129 Batch: 2000
Training Loss: 0.015830796167254448
Epoch: 129 Batch: 2050
Training Loss: 0.014560377961251795
Epoch: 129 Batch: 2100
Training Loss: 0.014217933345408667
Epoch: 129 Batch: 2150
Training Loss: 0.014455900732861009
Epoch: 129 Batch: 2200
Training Loss: 0.014377273456616836
Epoch: 129 Batch: 2250
Training Loss: 0.014154741605122884
Epoch: 129 Batch: 2300
Training Loss: 0.013744612193625907
Epoch: 129 Batch: 2350
Training Loss: 0.012985187832345355
Epoch: 129 Batch: 2400
Training Loss: 0.012716812180976074
Epoch: 129 Batch: 2450
Training Loss: 0.012122823218910062
Epoch: 129 Batch: 2500
Training Loss: 0.011941733837127686
Epoch: 129 Batch: 2550
Training Loss: 0.011739981525084552
Epoch: 129 Batch: 2600
Training Loss: 0.011819527940108225
Epoch: 129 Batch: 2650
Training Loss: 0.011716798690130125
Epoch: 129 Batch: 2700
Training Loss: 0.01241295317808787
Epoch: 129 Batch: 2750
Training Loss: 0.011274522207000039
Epoch: 129 Batch: 2800
Training Loss: 0.01096190866615091
Epoch: 129 Batch: 2850
Training Loss: 0.010291342944429632
Epoch: 129 Batch: 2900
Training Loss: 0.010622531596956582
Epoch: 129 Batch: 2950
Training Loss: 0.01030433653774908
Epoch: 129 Batch: 3000
Training Loss: 0.009964249710241953
Epoch: 129 Batch: 3050
Training Loss: 0.009953910825682468
Epoch: 129 Batch: 3100
Training Loss: 0.010103277275639196
Epoch: 129 Batch: 3150
Training Loss: 0.009847410396924095
Epoch: 129 Batch: 3200
Training Loss: 0.009932387778535485
Epoch: 130 
 Validation Loss: 0.4747178673744202
---------------------------
Epoch: 130 Batch: 50
Training Loss: 0.6272904813289643
Epoch: 130 Batch: 100
Training Loss: 0.2996454399824142
Epoch: 130 Batch: 150
Training Loss: 0.20767757058143616
Epoch: 130 Batch: 200
Training Loss: 0.1495269453525543
Epoch: 130 Batch: 250
Training Loss: 0.11870216155052185
Epoch: 130 Batch: 300
Training Loss: 0.1002237265308698
Epoch: 130 Batch: 350
Training Loss: 0.09340899067265647
Epoch: 130 Batch: 400
Training Loss: 0.07823017306625843
Epoch: 130 Batch: 450
Training Loss: 0.06833151347107358
Epoch: 130 Batch: 500
Training Loss: 0.06070985215902328
Epoch: 130 Batch: 550
Training Loss: 0.05449236365881833
Epoch: 130 Batch: 600
Training Loss: 0.05279792994260788
Epoch: 130 Batch: 650
Training Loss: 0.04845350838624514
Epoch: 130 Batch: 700
Training Loss: 0.04566577698503222
Epoch: 130 Batch: 750
Training Loss: 0.041037498950958255
Epoch: 130 Batch: 800
Training Loss: 0.037798702865839
Epoch: 130 Batch: 850
Training Loss: 0.0365867979035658
Epoch: 130 Batch: 900
Training Loss: 0.03615851842694812
Epoch: 130 Batch: 950
Training Loss: 0.03270877414628079
Epoch: 130 Batch: 1000
Training Loss: 0.03136087968945503
Epoch: 130 Batch: 1050
Training Loss: 0.029806894376164392
Epoch: 130 Batch: 1100
Training Loss: 0.02847981317476793
Epoch: 130 Batch: 1150
Training Loss: 0.026481667368308356
Epoch: 130 Batch: 1200
Training Loss: 0.02558262340724468
Epoch: 130 Batch: 1250
Training Loss: 0.025759336400032043
Epoch: 130 Batch: 1300
Training Loss: 0.02314094499899791
Epoch: 130 Batch: 1350
Training Loss: 0.022375076324851424
Epoch: 130 Batch: 1400
Training Loss: 0.021965822385890142
Epoch: 130 Batch: 1450
Training Loss: 0.022358780009993192
Epoch: 130 Batch: 1500
Training Loss: 0.019882932305336
Epoch: 130 Batch: 1550
Training Loss: 0.020103080176538037
Epoch: 130 Batch: 1600
Training Loss: 0.019247301481664182
Epoch: 130 Batch: 1650
Training Loss: 0.018138077331311776
Epoch: 130 Batch: 1700
Training Loss: 0.018592419326305388
Epoch: 130 Batch: 1750
Training Loss: 0.01829457642350878
Epoch: 130 Batch: 1800
Training Loss: 0.016171214464637967
Epoch: 130 Batch: 1850
Training Loss: 0.01687533447871337
Epoch: 130 Batch: 1900
Training Loss: 0.01586048866573133
Epoch: 130 Batch: 1950
Training Loss: 0.01571448960365393
Epoch: 130 Batch: 2000
Training Loss: 0.015186062648892402
Epoch: 130 Batch: 2050
Training Loss: 0.014759621809168559
Epoch: 130 Batch: 2100
Training Loss: 0.015149744507812318
Epoch: 130 Batch: 2150
Training Loss: 0.013845067786615949
Epoch: 130 Batch: 2200
Training Loss: 0.013580419204451822
Epoch: 130 Batch: 2250
Training Loss: 0.013519664075639513
Epoch: 130 Batch: 2300
Training Loss: 0.013162127007608828
Epoch: 130 Batch: 2350
Training Loss: 0.013351570798995648
Epoch: 130 Batch: 2400
Training Loss: 0.012902285096546014
Epoch: 130 Batch: 2450
Training Loss: 0.012834018566170518
Epoch: 130 Batch: 2500
Training Loss: 0.012326899909973144
Epoch: 130 Batch: 2550
Training Loss: 0.01194501543746275
Epoch: 130 Batch: 2600
Training Loss: 0.012379811933407417
Epoch: 130 Batch: 2650
Training Loss: 0.011478349195336395
Epoch: 130 Batch: 2700
Training Loss: 0.011008721049185152
Epoch: 130 Batch: 2750
Training Loss: 0.011690516103397715
Epoch: 130 Batch: 2800
Training Loss: 0.011574484588844436
Epoch: 130 Batch: 2850
Training Loss: 0.010428851455972906
Epoch: 130 Batch: 2900
Training Loss: 0.011516922506792792
Epoch: 130 Batch: 2950
Training Loss: 0.01003356924501516
Epoch: 130 Batch: 3000
Training Loss: 0.01012695096929868
Epoch: 130 Batch: 3050
Training Loss: 0.010205152640577222
Epoch: 130 Batch: 3100
Training Loss: 0.009817287268177155
Epoch: 130 Batch: 3150
Training Loss: 0.009688808066504342
Epoch: 130 Batch: 3200
Training Loss: 0.00912976316176355
Epoch: 131 
 Validation Loss: 0.4747518390417099
---------------------------
Epoch: 131 Batch: 50
Training Loss: 0.6073265856504441
Epoch: 131 Batch: 100
Training Loss: 0.29597310721874237
Epoch: 131 Batch: 150
Training Loss: 0.20708898484706878
Epoch: 131 Batch: 200
Training Loss: 0.14665576115250586
Epoch: 131 Batch: 250
Training Loss: 0.12180508375167846
Epoch: 131 Batch: 300
Training Loss: 0.09918940295775731
Epoch: 131 Batch: 350
Training Loss: 0.08704421809741429
Epoch: 131 Batch: 400
Training Loss: 0.07648185595870018
Epoch: 131 Batch: 450
Training Loss: 0.06971902045938703
Epoch: 131 Batch: 500
Training Loss: 0.06346560341119767
Epoch: 131 Batch: 550
Training Loss: 0.05311090583151037
Epoch: 131 Batch: 600
Training Loss: 0.049397217432657875
Epoch: 131 Batch: 650
Training Loss: 0.04580554260657384
Epoch: 131 Batch: 700
Training Loss: 0.04114584377833775
Epoch: 131 Batch: 750
Training Loss: 0.04171867644786835
Epoch: 131 Batch: 800
Training Loss: 0.03850480373948813
Epoch: 131 Batch: 850
Training Loss: 0.03544161779039046
Epoch: 131 Batch: 900
Training Loss: 0.03415286312500636
Epoch: 131 Batch: 950
Training Loss: 0.03436137211950202
Epoch: 131 Batch: 1000
Training Loss: 0.031278835266828536
Epoch: 131 Batch: 1050
Training Loss: 0.02983049432436625
Epoch: 131 Batch: 1100
Training Loss: 0.028257248211990705
Epoch: 131 Batch: 1150
Training Loss: 0.02695792037507762
Epoch: 131 Batch: 1200
Training Loss: 0.026895364647110304
Epoch: 131 Batch: 1250
Training Loss: 0.024217943239212038
Epoch: 131 Batch: 1300
Training Loss: 0.02298771885725168
Epoch: 131 Batch: 1350
Training Loss: 0.022021843371567904
Epoch: 131 Batch: 1400
Training Loss: 0.02203768647142819
Epoch: 131 Batch: 1450
Training Loss: 0.021888496814102962
Epoch: 131 Batch: 1500
Training Loss: 0.020747594972451527
Epoch: 131 Batch: 1550
Training Loss: 0.01969792046854573
Epoch: 131 Batch: 1600
Training Loss: 0.01891199866309762
Epoch: 131 Batch: 1650
Training Loss: 0.01838423085935188
Epoch: 131 Batch: 1700
Training Loss: 0.018534275100511664
Epoch: 131 Batch: 1750
Training Loss: 0.017618000047547478
Epoch: 131 Batch: 1800
Training Loss: 0.01690330770280626
Epoch: 131 Batch: 1850
Training Loss: 0.01735970274822132
Epoch: 131 Batch: 1900
Training Loss: 0.01575177162885666
Epoch: 131 Batch: 1950
Training Loss: 0.0167057219071266
Epoch: 131 Batch: 2000
Training Loss: 0.015854199439287186
Epoch: 131 Batch: 2050
Training Loss: 0.01488840095880555
Epoch: 131 Batch: 2100
Training Loss: 0.014873092756384895
Epoch: 131 Batch: 2150
Training Loss: 0.014417806295461433
Epoch: 131 Batch: 2200
Training Loss: 0.013725750121203336
Epoch: 131 Batch: 2250
Training Loss: 0.014080296794573465
Epoch: 131 Batch: 2300
Training Loss: 0.013651347665683083
Epoch: 131 Batch: 2350
Training Loss: 0.012783419844952036
Epoch: 131 Batch: 2400
Training Loss: 0.013151144310832023
Epoch: 131 Batch: 2450
Training Loss: 0.012648380569049291
Epoch: 131 Batch: 2500
Training Loss: 0.01267357816696167
Epoch: 131 Batch: 2550
Training Loss: 0.012410363075779933
Epoch: 131 Batch: 2600
Training Loss: 0.011301941596544705
Epoch: 131 Batch: 2650
Training Loss: 0.01110809954832185
Epoch: 131 Batch: 2700
Training Loss: 0.01153788814942042
Epoch: 131 Batch: 2750
Training Loss: 0.011256182172081687
Epoch: 131 Batch: 2800
Training Loss: 0.01072067708841392
Epoch: 131 Batch: 2850
Training Loss: 0.010718820157803987
Epoch: 131 Batch: 2900
Training Loss: 0.010399615579637988
Epoch: 131 Batch: 2950
Training Loss: 0.010324634879322375
Epoch: 131 Batch: 3000
Training Loss: 0.010137544949849447
Epoch: 131 Batch: 3050
Training Loss: 0.009512100327210348
Epoch: 131 Batch: 3100
Training Loss: 0.01038971887480828
Epoch: 131 Batch: 3150
Training Loss: 0.010050152445596362
Epoch: 131 Batch: 3200
Training Loss: 0.009852325972169639
Epoch: 132 
 Validation Loss: 0.47496603594885933
---------------------------
Epoch: 132 Batch: 50
Training Loss: 0.638562570810318
Epoch: 132 Batch: 100
Training Loss: 0.3129479178786278
Epoch: 132 Batch: 150
Training Loss: 0.2090736867984136
Epoch: 132 Batch: 200
Training Loss: 0.15214790478348733
Epoch: 132 Batch: 250
Training Loss: 0.12979855525493622
Epoch: 132 Batch: 300
Training Loss: 0.1022906885544459
Epoch: 132 Batch: 350
Training Loss: 0.08459211315427508
Epoch: 132 Batch: 400
Training Loss: 0.07492728978395462
Epoch: 132 Batch: 450
Training Loss: 0.06866495258278317
Epoch: 132 Batch: 500
Training Loss: 0.06037765520811081
Epoch: 132 Batch: 550
Training Loss: 0.05560586636716669
Epoch: 132 Batch: 600
Training Loss: 0.05130698874592781
Epoch: 132 Batch: 650
Training Loss: 0.04624209789129404
Epoch: 132 Batch: 700
Training Loss: 0.044696125771318165
Epoch: 132 Batch: 750
Training Loss: 0.04289478798707326
Epoch: 132 Batch: 800
Training Loss: 0.03714157484471798
Epoch: 132 Batch: 850
Training Loss: 0.033952482868643365
Epoch: 132 Batch: 900
Training Loss: 0.03286218427949482
Epoch: 132 Batch: 950
Training Loss: 0.03210981080406591
Epoch: 132 Batch: 1000
Training Loss: 0.03202750140428543
Epoch: 132 Batch: 1050
Training Loss: 0.02970653093996502
Epoch: 132 Batch: 1100
Training Loss: 0.029121576303785496
Epoch: 132 Batch: 1150
Training Loss: 0.02640131307684857
Epoch: 132 Batch: 1200
Training Loss: 0.025086514775951703
Epoch: 132 Batch: 1250
Training Loss: 0.0242749475479126
Epoch: 132 Batch: 1300
Training Loss: 0.023776775483901683
Epoch: 132 Batch: 1350
Training Loss: 0.02250741340495922
Epoch: 132 Batch: 1400
Training Loss: 0.021455867971692768
Epoch: 132 Batch: 1450
Training Loss: 0.021180931083087263
Epoch: 132 Batch: 1500
Training Loss: 0.02021043986082077
Epoch: 132 Batch: 1550
Training Loss: 0.02127138493522521
Epoch: 132 Batch: 1600
Training Loss: 0.018895141817629336
Epoch: 132 Batch: 1650
Training Loss: 0.018905509746435917
Epoch: 132 Batch: 1700
Training Loss: 0.01814947783946991
Epoch: 132 Batch: 1750
Training Loss: 0.018123456971985953
Epoch: 132 Batch: 1800
Training Loss: 0.016204284065299564
Epoch: 132 Batch: 1850
Training Loss: 0.016791817916406167
Epoch: 132 Batch: 1900
Training Loss: 0.015901124571499073
Epoch: 132 Batch: 1950
Training Loss: 0.015288063914347918
Epoch: 132 Batch: 2000
Training Loss: 0.014968674212694167
Epoch: 132 Batch: 2050
Training Loss: 0.015476880524216629
Epoch: 132 Batch: 2100
Training Loss: 0.014965309642610097
Epoch: 132 Batch: 2150
Training Loss: 0.014204698717871377
Epoch: 132 Batch: 2200
Training Loss: 0.014475190585309808
Epoch: 132 Batch: 2250
Training Loss: 0.014264958580334981
Epoch: 132 Batch: 2300
Training Loss: 0.013327544901681983
Epoch: 132 Batch: 2350
Training Loss: 0.013350614017628609
Epoch: 132 Batch: 2400
Training Loss: 0.01256512951105833
Epoch: 132 Batch: 2450
Training Loss: 0.012240158660071236
Epoch: 132 Batch: 2500
Training Loss: 0.012175503849983215
Epoch: 132 Batch: 2550
Training Loss: 0.012242670631876179
Epoch: 132 Batch: 2600
Training Loss: 0.012377371753637607
Epoch: 132 Batch: 2650
Training Loss: 0.011556176014666287
Epoch: 132 Batch: 2700
Training Loss: 0.010841311326733341
Epoch: 132 Batch: 2750
Training Loss: 0.010698231003501198
Epoch: 132 Batch: 2800
Training Loss: 0.01082941252206053
Epoch: 132 Batch: 2850
Training Loss: 0.010573393420169228
Epoch: 132 Batch: 2900
Training Loss: 0.010523625797238843
Epoch: 132 Batch: 2950
Training Loss: 0.010216294668488583
Epoch: 132 Batch: 3000
Training Loss: 0.010220856636762619
Epoch: 132 Batch: 3050
Training Loss: 0.009503739466432666
Epoch: 132 Batch: 3100
Training Loss: 0.010081168499685102
Epoch: 132 Batch: 3150
Training Loss: 0.00944599519646357
Epoch: 132 Batch: 3200
Training Loss: 0.009769686926156283
Epoch: 133 
 Validation Loss: 0.4748284121354421
---------------------------
Epoch: 133 Batch: 50
Training Loss: 0.631786425113678
Epoch: 133 Batch: 100
Training Loss: 0.3057345551252365
Epoch: 133 Batch: 150
Training Loss: 0.21090961555639903
Epoch: 133 Batch: 200
Training Loss: 0.1558244113624096
Epoch: 133 Batch: 250
Training Loss: 0.1159497481584549
Epoch: 133 Batch: 300
Training Loss: 0.09601382335027059
Epoch: 133 Batch: 350
Training Loss: 0.08893842807837896
Epoch: 133 Batch: 400
Training Loss: 0.07571030929684638
Epoch: 133 Batch: 450
Training Loss: 0.06720738278494941
Epoch: 133 Batch: 500
Training Loss: 0.06167043167352677
Epoch: 133 Batch: 550
Training Loss: 0.05851404628970406
Epoch: 133 Batch: 600
Training Loss: 0.0489722640812397
Epoch: 133 Batch: 650
Training Loss: 0.04849372350252592
Epoch: 133 Batch: 700
Training Loss: 0.043278282710484096
Epoch: 133 Batch: 750
Training Loss: 0.0397575919230779
Epoch: 133 Batch: 800
Training Loss: 0.038512377478182314
Epoch: 133 Batch: 850
Training Loss: 0.035386036914937635
Epoch: 133 Batch: 900
Training Loss: 0.03579369488689634
Epoch: 133 Batch: 950
Training Loss: 0.03366406017228177
Epoch: 133 Batch: 1000
Training Loss: 0.030118633717298508
Epoch: 133 Batch: 1050
Training Loss: 0.030037682822772433
Epoch: 133 Batch: 1100
Training Loss: 0.027199822447516703
Epoch: 133 Batch: 1150
Training Loss: 0.026571042071218076
Epoch: 133 Batch: 1200
Training Loss: 0.02464157683153947
Epoch: 133 Batch: 1250
Training Loss: 0.024020349669456483
Epoch: 133 Batch: 1300
Training Loss: 0.0241144971205638
Epoch: 133 Batch: 1350
Training Loss: 0.022662920399948404
Epoch: 133 Batch: 1400
Training Loss: 0.02205536480460848
Epoch: 133 Batch: 1450
Training Loss: 0.021962029378989646
Epoch: 133 Batch: 1500
Training Loss: 0.020319312274456023
Epoch: 133 Batch: 1550
Training Loss: 0.020200838215889468
Epoch: 133 Batch: 1600
Training Loss: 0.01905842697247863
Epoch: 133 Batch: 1650
Training Loss: 0.018730450146126024
Epoch: 133 Batch: 1700
Training Loss: 0.017736356749254115
Epoch: 133 Batch: 1750
Training Loss: 0.017877887180873325
Epoch: 133 Batch: 1800
Training Loss: 0.01716503057214949
Epoch: 133 Batch: 1850
Training Loss: 0.015944922630851333
Epoch: 133 Batch: 1900
Training Loss: 0.016790893548413326
Epoch: 133 Batch: 1950
Training Loss: 0.015584428509076436
Epoch: 133 Batch: 2000
Training Loss: 0.014947315156459808
Epoch: 133 Batch: 2050
Training Loss: 0.01462681770324707
Epoch: 133 Batch: 2100
Training Loss: 0.014449763255459921
Epoch: 133 Batch: 2150
Training Loss: 0.014409450015356375
Epoch: 133 Batch: 2200
Training Loss: 0.014000697908076372
Epoch: 133 Batch: 2250
Training Loss: 0.013554229948255751
Epoch: 133 Batch: 2300
Training Loss: 0.013492806268774945
Epoch: 133 Batch: 2350
Training Loss: 0.01303568237639488
Epoch: 133 Batch: 2400
Training Loss: 0.012870253970225651
Epoch: 133 Batch: 2450
Training Loss: 0.012849373476845878
Epoch: 133 Batch: 2500
Training Loss: 0.012565765559673309
Epoch: 133 Batch: 2550
Training Loss: 0.011699297346320806
Epoch: 133 Batch: 2600
Training Loss: 0.011813575762968797
Epoch: 133 Batch: 2650
Training Loss: 0.011917491105367552
Epoch: 133 Batch: 2700
Training Loss: 0.011712255235071535
Epoch: 133 Batch: 2750
Training Loss: 0.011418336727402428
Epoch: 133 Batch: 2800
Training Loss: 0.010770878195762635
Epoch: 133 Batch: 2850
Training Loss: 0.010884079598543936
Epoch: 133 Batch: 2900
Training Loss: 0.010485615956372228
Epoch: 133 Batch: 2950
Training Loss: 0.011192302360373028
Epoch: 133 Batch: 3000
Training Loss: 0.010086052825053532
Epoch: 133 Batch: 3050
Training Loss: 0.009604803841622149
Epoch: 133 Batch: 3100
Training Loss: 0.009874988890463307
Epoch: 133 Batch: 3150
Training Loss: 0.01014860607328869
Epoch: 133 Batch: 3200
Training Loss: 0.009525237781926989
Epoch: 134 
 Validation Loss: 0.47437161968814
---------------------------
Epoch: 134 Batch: 50
Training Loss: 0.6099973493814468
Epoch: 134 Batch: 100
Training Loss: 0.30200357019901275
Epoch: 134 Batch: 150
Training Loss: 0.20896117488543192
Epoch: 134 Batch: 200
Training Loss: 0.15172507837414742
Epoch: 134 Batch: 250
Training Loss: 0.12107249355316162
Epoch: 134 Batch: 300
Training Loss: 0.10086029767990112
Epoch: 134 Batch: 350
Training Loss: 0.0892080955845969
Epoch: 134 Batch: 400
Training Loss: 0.07472320333123207
Epoch: 134 Batch: 450
Training Loss: 0.06662862413459354
Epoch: 134 Batch: 500
Training Loss: 0.060752001225948336
Epoch: 134 Batch: 550
Training Loss: 0.054513149911707096
Epoch: 134 Batch: 600
Training Loss: 0.05076290885607401
Epoch: 134 Batch: 650
Training Loss: 0.047000401065899776
Epoch: 134 Batch: 700
Training Loss: 0.045970132904393335
Epoch: 134 Batch: 750
Training Loss: 0.04237078281243642
Epoch: 134 Batch: 800
Training Loss: 0.03874977599829435
Epoch: 134 Batch: 850
Training Loss: 0.03488857830272001
Epoch: 134 Batch: 900
Training Loss: 0.03361351470152537
Epoch: 134 Batch: 950
Training Loss: 0.03247134192993766
Epoch: 134 Batch: 1000
Training Loss: 0.030585528254508974
Epoch: 134 Batch: 1050
Training Loss: 0.02837995898155939
Epoch: 134 Batch: 1100
Training Loss: 0.02783496618270874
Epoch: 134 Batch: 1150
Training Loss: 0.026116129315417746
Epoch: 134 Batch: 1200
Training Loss: 0.025693270166714986
Epoch: 134 Batch: 1250
Training Loss: 0.023971060276031495
Epoch: 134 Batch: 1300
Training Loss: 0.02384011713358072
Epoch: 134 Batch: 1350
Training Loss: 0.02319215973218282
Epoch: 134 Batch: 1400
Training Loss: 0.02278187027999333
Epoch: 134 Batch: 1450
Training Loss: 0.021191877825506803
Epoch: 134 Batch: 1500
Training Loss: 0.02067294575770696
Epoch: 134 Batch: 1550
Training Loss: 0.019027079939842224
Epoch: 134 Batch: 1600
Training Loss: 0.01911729505285621
Epoch: 134 Batch: 1650
Training Loss: 0.01843929167949792
Epoch: 134 Batch: 1700
Training Loss: 0.018174147062441883
Epoch: 134 Batch: 1750
Training Loss: 0.017053327628544397
Epoch: 134 Batch: 1800
Training Loss: 0.01631395571761661
Epoch: 134 Batch: 1850
Training Loss: 0.017199660072455537
Epoch: 134 Batch: 1900
Training Loss: 0.016239167498914818
Epoch: 134 Batch: 1950
Training Loss: 0.015914294337614987
Epoch: 134 Batch: 2000
Training Loss: 0.015383285030722619
Epoch: 134 Batch: 2050
Training Loss: 0.01478802525415653
Epoch: 134 Batch: 2100
Training Loss: 0.01460520911784399
Epoch: 134 Batch: 2150
Training Loss: 0.014140751084616017
Epoch: 134 Batch: 2200
Training Loss: 0.01483223016966473
Epoch: 134 Batch: 2250
Training Loss: 0.013773300886154175
Epoch: 134 Batch: 2300
Training Loss: 0.013693920153638591
Epoch: 134 Batch: 2350
Training Loss: 0.013134211948577394
Epoch: 134 Batch: 2400
Training Loss: 0.012722243964672088
Epoch: 134 Batch: 2450
Training Loss: 0.012427608820856834
Epoch: 134 Batch: 2500
Training Loss: 0.012054646039009095
Epoch: 134 Batch: 2550
Training Loss: 0.011713685685513066
Epoch: 134 Batch: 2600
Training Loss: 0.01219312623143196
Epoch: 134 Batch: 2650
Training Loss: 0.011890057718978738
Epoch: 134 Batch: 2700
Training Loss: 0.010651472829006336
Epoch: 134 Batch: 2750
Training Loss: 0.011292183735153892
Epoch: 134 Batch: 2800
Training Loss: 0.010851195018206325
Epoch: 134 Batch: 2850
Training Loss: 0.010372429845625895
Epoch: 134 Batch: 2900
Training Loss: 0.010716928593043623
Epoch: 134 Batch: 2950
Training Loss: 0.010691304115925806
Epoch: 134 Batch: 3000
Training Loss: 0.0104125334918499
Epoch: 134 Batch: 3050
Training Loss: 0.009706765432826808
Epoch: 134 Batch: 3100
Training Loss: 0.00990202731663181
Epoch: 134 Batch: 3150
Training Loss: 0.009894747156945487
Epoch: 134 Batch: 3200
Training Loss: 0.009541233209893108
Epoch: 135 
 Validation Loss: 0.4748679733938641
---------------------------
Epoch: 135 Batch: 50
Training Loss: 0.6271616929769516
Epoch: 135 Batch: 100
Training Loss: 0.29200475990772246
Epoch: 135 Batch: 150
Training Loss: 0.2041254961490631
Epoch: 135 Batch: 200
Training Loss: 0.15466050893068314
Epoch: 135 Batch: 250
Training Loss: 0.12289979791641235
Epoch: 135 Batch: 300
Training Loss: 0.1015325399239858
Epoch: 135 Batch: 350
Training Loss: 0.09000962666102819
Epoch: 135 Batch: 400
Training Loss: 0.07491197153925895
Epoch: 135 Batch: 450
Training Loss: 0.06841254307164087
Epoch: 135 Batch: 500
Training Loss: 0.06241014862060547
Epoch: 135 Batch: 550
Training Loss: 0.0549636832150546
Epoch: 135 Batch: 600
Training Loss: 0.0503554160396258
Epoch: 135 Batch: 650
Training Loss: 0.04805669165574587
Epoch: 135 Batch: 700
Training Loss: 0.04283282731260572
Epoch: 135 Batch: 750
Training Loss: 0.042821956197420755
Epoch: 135 Batch: 800
Training Loss: 0.038927023373544214
Epoch: 135 Batch: 850
Training Loss: 0.035836043918834014
Epoch: 135 Batch: 900
Training Loss: 0.03348353091213438
Epoch: 135 Batch: 950
Training Loss: 0.03247506493016293
Epoch: 135 Batch: 1000
Training Loss: 0.030089922100305557
Epoch: 135 Batch: 1050
Training Loss: 0.02939691123508272
Epoch: 135 Batch: 1100
Training Loss: 0.027235669710419394
Epoch: 135 Batch: 1150
Training Loss: 0.026692225544349007
Epoch: 135 Batch: 1200
Training Loss: 0.026311628992358845
Epoch: 135 Batch: 1250
Training Loss: 0.024236353826522828
Epoch: 135 Batch: 1300
Training Loss: 0.023309893699792715
Epoch: 135 Batch: 1350
Training Loss: 0.022776864016497575
Epoch: 135 Batch: 1400
Training Loss: 0.021909473687410353
Epoch: 135 Batch: 1450
Training Loss: 0.022026450675109336
Epoch: 135 Batch: 1500
Training Loss: 0.020388589978218078
Epoch: 135 Batch: 1550
Training Loss: 0.02039039025383611
Epoch: 135 Batch: 1600
Training Loss: 0.01956591781228781
Epoch: 135 Batch: 1650
Training Loss: 0.0180734193686283
Epoch: 135 Batch: 1700
Training Loss: 0.018623757344834943
Epoch: 135 Batch: 1750
Training Loss: 0.01762003551210676
Epoch: 135 Batch: 1800
Training Loss: 0.017151730027463702
Epoch: 135 Batch: 1850
Training Loss: 0.016425003103307776
Epoch: 135 Batch: 1900
Training Loss: 0.016863958914028972
Epoch: 135 Batch: 1950
Training Loss: 0.01606685989942306
Epoch: 135 Batch: 2000
Training Loss: 0.014743581131100655
Epoch: 135 Batch: 2050
Training Loss: 0.014673870104115184
Epoch: 135 Batch: 2100
Training Loss: 0.01424140711625417
Epoch: 135 Batch: 2150
Training Loss: 0.01382841939149901
Epoch: 135 Batch: 2200
Training Loss: 0.014014531766826457
Epoch: 135 Batch: 2250
Training Loss: 0.01437639311949412
Epoch: 135 Batch: 2300
Training Loss: 0.012535804121390633
Epoch: 135 Batch: 2350
Training Loss: 0.01292306510691947
Epoch: 135 Batch: 2400
Training Loss: 0.012724480380614599
Epoch: 135 Batch: 2450
Training Loss: 0.012807366446572908
Epoch: 135 Batch: 2500
Training Loss: 0.01278234052658081
Epoch: 135 Batch: 2550
Training Loss: 0.01227697963808097
Epoch: 135 Batch: 2600
Training Loss: 0.011824468901524178
Epoch: 135 Batch: 2650
Training Loss: 0.011626709024861175
Epoch: 135 Batch: 2700
Training Loss: 0.011173177814042127
Epoch: 135 Batch: 2750
Training Loss: 0.011911257050254129
Epoch: 135 Batch: 2800
Training Loss: 0.010996545915092741
Epoch: 135 Batch: 2850
Training Loss: 0.01073921775608732
Epoch: 135 Batch: 2900
Training Loss: 0.010530018436497656
Epoch: 135 Batch: 2950
Training Loss: 0.01043352563502425
Epoch: 135 Batch: 3000
Training Loss: 0.010586880058050155
Epoch: 135 Batch: 3050
Training Loss: 0.009836445470325283
Epoch: 135 Batch: 3100
Training Loss: 0.009844901340623055
Epoch: 135 Batch: 3150
Training Loss: 0.009818542344229563
Epoch: 135 Batch: 3200
Training Loss: 0.0093136992957443
Epoch: 136 
 Validation Loss: 0.47405380209287007
---------------------------
Epoch: 136 Batch: 50
Training Loss: 0.6415108621120453
Epoch: 136 Batch: 100
Training Loss: 0.3107477003335953
Epoch: 136 Batch: 150
Training Loss: 0.2054021324714025
Epoch: 136 Batch: 200
Training Loss: 0.15716758295893668
Epoch: 136 Batch: 250
Training Loss: 0.12267939579486847
Epoch: 136 Batch: 300
Training Loss: 0.10048739870389302
Epoch: 136 Batch: 350
Training Loss: 0.08979456262929099
Epoch: 136 Batch: 400
Training Loss: 0.07277782820165157
Epoch: 136 Batch: 450
Training Loss: 0.06916040645705329
Epoch: 136 Batch: 500
Training Loss: 0.06349588465690613
Epoch: 136 Batch: 550
Training Loss: 0.05677118436856703
Epoch: 136 Batch: 600
Training Loss: 0.05206785609324773
Epoch: 136 Batch: 650
Training Loss: 0.0479037738763369
Epoch: 136 Batch: 700
Training Loss: 0.04381279293979917
Epoch: 136 Batch: 750
Training Loss: 0.04177487309773763
Epoch: 136 Batch: 800
Training Loss: 0.03814786691218615
Epoch: 136 Batch: 850
Training Loss: 0.03573012916480794
Epoch: 136 Batch: 900
Training Loss: 0.034369597136974336
Epoch: 136 Batch: 950
Training Loss: 0.032390036896655434
Epoch: 136 Batch: 1000
Training Loss: 0.031798853248357774
Epoch: 136 Batch: 1050
Training Loss: 0.02968512909752982
Epoch: 136 Batch: 1100
Training Loss: 0.027922907878052104
Epoch: 136 Batch: 1150
Training Loss: 0.026256993335226308
Epoch: 136 Batch: 1200
Training Loss: 0.026488230352600416
Epoch: 136 Batch: 1250
Training Loss: 0.024260401940345764
Epoch: 136 Batch: 1300
Training Loss: 0.023434847318209134
Epoch: 136 Batch: 1350
Training Loss: 0.02268806011588485
Epoch: 136 Batch: 1400
Training Loss: 0.02101439961365291
Epoch: 136 Batch: 1450
Training Loss: 0.021238732029651774
Epoch: 136 Batch: 1500
Training Loss: 0.020221273998419442
Epoch: 136 Batch: 1550
Training Loss: 0.019015700163379792
Epoch: 136 Batch: 1600
Training Loss: 0.019446734711527823
Epoch: 136 Batch: 1650
Training Loss: 0.01886697626475132
Epoch: 136 Batch: 1700
Training Loss: 0.018672550829017864
Epoch: 136 Batch: 1750
Training Loss: 0.018270799943378994
Epoch: 136 Batch: 1800
Training Loss: 0.01675296804971165
Epoch: 136 Batch: 1850
Training Loss: 0.015616875661386026
Epoch: 136 Batch: 1900
Training Loss: 0.015592150343091865
Epoch: 136 Batch: 1950
Training Loss: 0.015223478430356735
Epoch: 136 Batch: 2000
Training Loss: 0.014862864777445793
Epoch: 136 Batch: 2050
Training Loss: 0.015505300454977082
Epoch: 136 Batch: 2100
Training Loss: 0.014858801535197667
Epoch: 136 Batch: 2150
Training Loss: 0.01415610176186229
Epoch: 136 Batch: 2200
Training Loss: 0.013929643685167485
Epoch: 136 Batch: 2250
Training Loss: 0.014245155930519103
Epoch: 136 Batch: 2300
Training Loss: 0.013638177254925603
Epoch: 136 Batch: 2350
Training Loss: 0.012730480838329235
Epoch: 136 Batch: 2400
Training Loss: 0.013431270122528077
Epoch: 136 Batch: 2450
Training Loss: 0.011772044057748756
Epoch: 136 Batch: 2500
Training Loss: 0.012382771790027619
Epoch: 136 Batch: 2550
Training Loss: 0.01201019921723534
Epoch: 136 Batch: 2600
Training Loss: 0.011790985969396739
Epoch: 136 Batch: 2650
Training Loss: 0.011312947430700626
Epoch: 136 Batch: 2700
Training Loss: 0.012058706404986205
Epoch: 136 Batch: 2750
Training Loss: 0.01153711619160392
Epoch: 136 Batch: 2800
Training Loss: 0.011126630497830255
Epoch: 136 Batch: 2850
Training Loss: 0.010813206632932027
Epoch: 136 Batch: 2900
Training Loss: 0.01006363989978001
Epoch: 136 Batch: 2950
Training Loss: 0.0104177879377947
Epoch: 136 Batch: 3000
Training Loss: 0.00991054861744245
Epoch: 136 Batch: 3050
Training Loss: 0.01001616653848867
Epoch: 136 Batch: 3100
Training Loss: 0.009729500835941684
Epoch: 136 Batch: 3150
Training Loss: 0.010124670238721939
Epoch: 136 Batch: 3200
Training Loss: 0.009605149235576392
Epoch: 137 
 Validation Loss: 0.47385467059082453
---------------------------
Epoch: 137 Batch: 50
Training Loss: 0.6164108365774155
Epoch: 137 Batch: 100
Training Loss: 0.3167291060090065
Epoch: 137 Batch: 150
Training Loss: 0.19710144956906636
Epoch: 137 Batch: 200
Training Loss: 0.149724298119545
Epoch: 137 Batch: 250
Training Loss: 0.12421167683601379
Epoch: 137 Batch: 300
Training Loss: 0.10323762724796931
Epoch: 137 Batch: 350
Training Loss: 0.08960233841623579
Epoch: 137 Batch: 400
Training Loss: 0.07781182192265987
Epoch: 137 Batch: 450
Training Loss: 0.06835778998004066
Epoch: 137 Batch: 500
Training Loss: 0.06379757761955261
Epoch: 137 Batch: 550
Training Loss: 0.05628606113520535
Epoch: 137 Batch: 600
Training Loss: 0.049435240825017296
Epoch: 137 Batch: 650
Training Loss: 0.04969865629306206
Epoch: 137 Batch: 700
Training Loss: 0.04549453105245318
Epoch: 137 Batch: 750
Training Loss: 0.0429889786640803
Epoch: 137 Batch: 800
Training Loss: 0.03701867178082466
Epoch: 137 Batch: 850
Training Loss: 0.0351402719932444
Epoch: 137 Batch: 900
Training Loss: 0.03382868978712294
Epoch: 137 Batch: 950
Training Loss: 0.031908791378924724
Epoch: 137 Batch: 1000
Training Loss: 0.031067580580711365
Epoch: 137 Batch: 1050
Training Loss: 0.029834897801989602
Epoch: 137 Batch: 1100
Training Loss: 0.028740635351701217
Epoch: 137 Batch: 1150
Training Loss: 0.02722052338330642
Epoch: 137 Batch: 1200
Training Loss: 0.02567528379460176
Epoch: 137 Batch: 1250
Training Loss: 0.024618660974502563
Epoch: 137 Batch: 1300
Training Loss: 0.022951334348091712
Epoch: 137 Batch: 1350
Training Loss: 0.022448758791994164
Epoch: 137 Batch: 1400
Training Loss: 0.021596877553633282
Epoch: 137 Batch: 1450
Training Loss: 0.02109224991551761
Epoch: 137 Batch: 1500
Training Loss: 0.021027054568131764
Epoch: 137 Batch: 1550
Training Loss: 0.019000186920166015
Epoch: 137 Batch: 1600
Training Loss: 0.01867429068312049
Epoch: 137 Batch: 1650
Training Loss: 0.01846849672722094
Epoch: 137 Batch: 1700
Training Loss: 0.018338589510496925
Epoch: 137 Batch: 1750
Training Loss: 0.01702701439176287
Epoch: 137 Batch: 1800
Training Loss: 0.017112266106737984
Epoch: 137 Batch: 1850
Training Loss: 0.015934845531308972
Epoch: 137 Batch: 1900
Training Loss: 0.01569877908418053
Epoch: 137 Batch: 1950
Training Loss: 0.01571257756306575
Epoch: 137 Batch: 2000
Training Loss: 0.014783050864934921
Epoch: 137 Batch: 2050
Training Loss: 0.014620119551332985
Epoch: 137 Batch: 2100
Training Loss: 0.014495548492386228
Epoch: 137 Batch: 2150
Training Loss: 0.01427161410797474
Epoch: 137 Batch: 2200
Training Loss: 0.013165365592999892
Epoch: 137 Batch: 2250
Training Loss: 0.01351967011557685
Epoch: 137 Batch: 2300
Training Loss: 0.012884939818278603
Epoch: 137 Batch: 2350
Training Loss: 0.012957076709321204
Epoch: 137 Batch: 2400
Training Loss: 0.01287612867852052
Epoch: 137 Batch: 2450
Training Loss: 0.012621874565980871
Epoch: 137 Batch: 2500
Training Loss: 0.01242403290271759
Epoch: 137 Batch: 2550
Training Loss: 0.012328287970786
Epoch: 137 Batch: 2600
Training Loss: 0.012022147339123946
Epoch: 137 Batch: 2650
Training Loss: 0.011749790592013665
Epoch: 137 Batch: 2700
Training Loss: 0.01156084109235693
Epoch: 137 Batch: 2750
Training Loss: 0.011304211204702205
Epoch: 137 Batch: 2800
Training Loss: 0.011258065381220409
Epoch: 137 Batch: 2850
Training Loss: 0.011116184581790055
Epoch: 137 Batch: 2900
Training Loss: 0.010802966746790656
Epoch: 137 Batch: 2950
Training Loss: 0.010247445904602439
Epoch: 137 Batch: 3000
Training Loss: 0.00975876392920812
Epoch: 137 Batch: 3050
Training Loss: 0.009949741705519254
Epoch: 137 Batch: 3100
Training Loss: 0.009561513335474076
Epoch: 137 Batch: 3150
Training Loss: 0.00981138152735574
Epoch: 137 Batch: 3200
Training Loss: 0.009419961860403418
Epoch: 138 
 Validation Loss: 0.4738031191958321
---------------------------
Epoch: 138 Batch: 50
Training Loss: 0.5868487775325775
Epoch: 138 Batch: 100
Training Loss: 0.31038143038749694
Epoch: 138 Batch: 150
Training Loss: 0.20664345661799113
Epoch: 138 Batch: 200
Training Loss: 0.15785615086555482
Epoch: 138 Batch: 250
Training Loss: 0.11892930173873902
Epoch: 138 Batch: 300
Training Loss: 0.10090194086233775
Epoch: 138 Batch: 350
Training Loss: 0.09176728802067893
Epoch: 138 Batch: 400
Training Loss: 0.07743153035640717
Epoch: 138 Batch: 450
Training Loss: 0.06729188978672028
Epoch: 138 Batch: 500
Training Loss: 0.06142917120456696
Epoch: 138 Batch: 550
Training Loss: 0.05721481892195615
Epoch: 138 Batch: 600
Training Loss: 0.05249797324339549
Epoch: 138 Batch: 650
Training Loss: 0.04793678297446324
Epoch: 138 Batch: 700
Training Loss: 0.04334790825843811
Epoch: 138 Batch: 750
Training Loss: 0.038915404558181765
Epoch: 138 Batch: 800
Training Loss: 0.03715841259807348
Epoch: 138 Batch: 850
Training Loss: 0.03649926438051111
Epoch: 138 Batch: 900
Training Loss: 0.0332035721010632
Epoch: 138 Batch: 950
Training Loss: 0.0319327103464227
Epoch: 138 Batch: 1000
Training Loss: 0.031141153037548067
Epoch: 138 Batch: 1050
Training Loss: 0.02906475870382218
Epoch: 138 Batch: 1100
Training Loss: 0.02879491323774511
Epoch: 138 Batch: 1150
Training Loss: 0.025164715414461882
Epoch: 138 Batch: 1200
Training Loss: 0.024594616269071896
Epoch: 138 Batch: 1250
Training Loss: 0.026800676584243774
Epoch: 138 Batch: 1300
Training Loss: 0.023466393901751592
Epoch: 138 Batch: 1350
Training Loss: 0.02310323543018765
Epoch: 138 Batch: 1400
Training Loss: 0.022186757389988217
Epoch: 138 Batch: 1450
Training Loss: 0.022313085897215482
Epoch: 138 Batch: 1500
Training Loss: 0.020119587739308675
Epoch: 138 Batch: 1550
Training Loss: 0.020407262636769202
Epoch: 138 Batch: 1600
Training Loss: 0.01916208729147911
Epoch: 138 Batch: 1650
Training Loss: 0.018381342364080025
Epoch: 138 Batch: 1700
Training Loss: 0.018013922747443702
Epoch: 138 Batch: 1750
Training Loss: 0.01767084046772548
Epoch: 138 Batch: 1800
Training Loss: 0.017281895793146557
Epoch: 138 Batch: 1850
Training Loss: 0.016546666122771597
Epoch: 138 Batch: 1900
Training Loss: 0.0156718405296928
Epoch: 138 Batch: 1950
Training Loss: 0.015702250920809232
Epoch: 138 Batch: 2000
Training Loss: 0.01564153276383877
Epoch: 138 Batch: 2050
Training Loss: 0.014640164462531486
Epoch: 138 Batch: 2100
Training Loss: 0.01485881134158089
Epoch: 138 Batch: 2150
Training Loss: 0.014711485200150068
Epoch: 138 Batch: 2200
Training Loss: 0.01383456971157681
Epoch: 138 Batch: 2250
Training Loss: 0.013324867672390408
Epoch: 138 Batch: 2300
Training Loss: 0.012989995350008426
Epoch: 138 Batch: 2350
Training Loss: 0.012931365066386284
Epoch: 138 Batch: 2400
Training Loss: 0.012354205461839836
Epoch: 138 Batch: 2450
Training Loss: 0.012749654748001877
Epoch: 138 Batch: 2500
Training Loss: 0.011739664506912231
Epoch: 138 Batch: 2550
Training Loss: 0.011809118951068205
Epoch: 138 Batch: 2600
Training Loss: 0.01215725988149643
Epoch: 138 Batch: 2650
Training Loss: 0.012148362476870699
Epoch: 138 Batch: 2700
Training Loss: 0.011457593021569429
Epoch: 138 Batch: 2750
Training Loss: 0.011159322229298678
Epoch: 138 Batch: 2800
Training Loss: 0.011063927646194186
Epoch: 138 Batch: 2850
Training Loss: 0.010657795991813927
Epoch: 138 Batch: 2900
Training Loss: 0.010638215418519645
Epoch: 138 Batch: 2950
Training Loss: 0.010362028398756254
Epoch: 138 Batch: 3000
Training Loss: 0.010463767896095912
Epoch: 138 Batch: 3050
Training Loss: 0.010233722104400884
Epoch: 138 Batch: 3100
Training Loss: 0.010174722421553827
Epoch: 138 Batch: 3150
Training Loss: 0.009827911778101845
Epoch: 138 Batch: 3200
Training Loss: 0.009301323015242814
Epoch: 139 
 Validation Loss: 0.47358327243063186
---------------------------
Epoch: 139 Batch: 50
Training Loss: 0.6245632010698319
Epoch: 139 Batch: 100
Training Loss: 0.3164722710847855
Epoch: 139 Batch: 150
Training Loss: 0.21169148882230124
Epoch: 139 Batch: 200
Training Loss: 0.15916474163532257
Epoch: 139 Batch: 250
Training Loss: 0.11343937587738037
Epoch: 139 Batch: 300
Training Loss: 0.10059733510017395
Epoch: 139 Batch: 350
Training Loss: 0.08580817418439048
Epoch: 139 Batch: 400
Training Loss: 0.07469768278300762
Epoch: 139 Batch: 450
Training Loss: 0.06785002688566844
Epoch: 139 Batch: 500
Training Loss: 0.06275619941949845
Epoch: 139 Batch: 550
Training Loss: 0.055378273292021316
Epoch: 139 Batch: 600
Training Loss: 0.05113859420021375
Epoch: 139 Batch: 650
Training Loss: 0.04793211785646585
Epoch: 139 Batch: 700
Training Loss: 0.043773389799254284
Epoch: 139 Batch: 750
Training Loss: 0.04029137567679087
Epoch: 139 Batch: 800
Training Loss: 0.03719536423683167
Epoch: 139 Batch: 850
Training Loss: 0.036647721634191624
Epoch: 139 Batch: 900
Training Loss: 0.033912826710277136
Epoch: 139 Batch: 950
Training Loss: 0.0316032673496949
Epoch: 139 Batch: 1000
Training Loss: 0.030231654793024063
Epoch: 139 Batch: 1050
Training Loss: 0.02903389391444978
Epoch: 139 Batch: 1100
Training Loss: 0.027234053015708925
Epoch: 139 Batch: 1150
Training Loss: 0.025590357780456544
Epoch: 139 Batch: 1200
Training Loss: 0.025319326544801395
Epoch: 139 Batch: 1250
Training Loss: 0.023390671730041505
Epoch: 139 Batch: 1300
Training Loss: 0.024267681791232183
Epoch: 139 Batch: 1350
Training Loss: 0.02342409721127263
Epoch: 139 Batch: 1400
Training Loss: 0.02267100127679961
Epoch: 139 Batch: 1450
Training Loss: 0.020826944976017392
Epoch: 139 Batch: 1500
Training Loss: 0.021848008175690967
Epoch: 139 Batch: 1550
Training Loss: 0.01990685672529282
Epoch: 139 Batch: 1600
Training Loss: 0.019255987908691166
Epoch: 139 Batch: 1650
Training Loss: 0.018371641635894775
Epoch: 139 Batch: 1700
Training Loss: 0.017743431399850285
Epoch: 139 Batch: 1750
Training Loss: 0.01742637886319842
Epoch: 139 Batch: 1800
Training Loss: 0.016847138073709276
Epoch: 139 Batch: 1850
Training Loss: 0.016562822187269056
Epoch: 139 Batch: 1900
Training Loss: 0.016593729947742664
Epoch: 139 Batch: 1950
Training Loss: 0.015649980642856696
Epoch: 139 Batch: 2000
Training Loss: 0.015302519395947456
Epoch: 139 Batch: 2050
Training Loss: 0.014531603964363657
Epoch: 139 Batch: 2100
Training Loss: 0.0144643111597924
Epoch: 139 Batch: 2150
Training Loss: 0.013584203706231228
Epoch: 139 Batch: 2200
Training Loss: 0.014004692909392444
Epoch: 139 Batch: 2250
Training Loss: 0.014660749249988132
Epoch: 139 Batch: 2300
Training Loss: 0.01328080152687819
Epoch: 139 Batch: 2350
Training Loss: 0.013052811546528592
Epoch: 139 Batch: 2400
Training Loss: 0.012875356785953045
Epoch: 139 Batch: 2450
Training Loss: 0.01229275685183856
Epoch: 139 Batch: 2500
Training Loss: 0.011829281771183014
Epoch: 139 Batch: 2550
Training Loss: 0.012164902114400676
Epoch: 139 Batch: 2600
Training Loss: 0.011484324347514372
Epoch: 139 Batch: 2650
Training Loss: 0.011810769922328445
Epoch: 139 Batch: 2700
Training Loss: 0.011917175551255543
Epoch: 139 Batch: 2750
Training Loss: 0.011290196537971496
Epoch: 139 Batch: 2800
Training Loss: 0.010776194238236973
Epoch: 139 Batch: 2850
Training Loss: 0.010765323795770344
Epoch: 139 Batch: 2900
Training Loss: 0.01038036780110721
Epoch: 139 Batch: 2950
Training Loss: 0.010561331884335664
Epoch: 139 Batch: 3000
Training Loss: 0.010494044403235118
Epoch: 139 Batch: 3050
Training Loss: 0.009992586139772759
Epoch: 139 Batch: 3100
Training Loss: 0.009686655527161013
Epoch: 139 Batch: 3150
Training Loss: 0.009546413204026601
Epoch: 139 Batch: 3200
Training Loss: 0.009619119288399815
Epoch: 140 
 Validation Loss: 0.4735255456633038
---------------------------
Epoch: 140 Batch: 50
Training Loss: 0.6148788952827453
Epoch: 140 Batch: 100
Training Loss: 0.3016202637553215
Epoch: 140 Batch: 150
Training Loss: 0.2002253387371699
Epoch: 140 Batch: 200
Training Loss: 0.1540788823366165
Epoch: 140 Batch: 250
Training Loss: 0.12304767298698426
Epoch: 140 Batch: 300
Training Loss: 0.10373212526241939
Epoch: 140 Batch: 350
Training Loss: 0.08908737582819802
Epoch: 140 Batch: 400
Training Loss: 0.0754835094511509
Epoch: 140 Batch: 450
Training Loss: 0.06602392037709554
Epoch: 140 Batch: 500
Training Loss: 0.06260688108205795
Epoch: 140 Batch: 550
Training Loss: 0.0539959361336448
Epoch: 140 Batch: 600
Training Loss: 0.05364557335774104
Epoch: 140 Batch: 650
Training Loss: 0.04753914447931143
Epoch: 140 Batch: 700
Training Loss: 0.04362834189619337
Epoch: 140 Batch: 750
Training Loss: 0.04016780765851339
Epoch: 140 Batch: 800
Training Loss: 0.0388448078930378
Epoch: 140 Batch: 850
Training Loss: 0.034737571653197796
Epoch: 140 Batch: 900
Training Loss: 0.03545205208990309
Epoch: 140 Batch: 950
Training Loss: 0.03179819201168261
Epoch: 140 Batch: 1000
Training Loss: 0.03096536299586296
Epoch: 140 Batch: 1050
Training Loss: 0.028047768303326198
Epoch: 140 Batch: 1100
Training Loss: 0.027823155495253476
Epoch: 140 Batch: 1150
Training Loss: 0.026848521154859792
Epoch: 140 Batch: 1200
Training Loss: 0.026765740339954693
Epoch: 140 Batch: 1250
Training Loss: 0.024309316158294677
Epoch: 140 Batch: 1300
Training Loss: 0.023060064430420215
Epoch: 140 Batch: 1350
Training Loss: 0.0225193186601003
Epoch: 140 Batch: 1400
Training Loss: 0.022468197771481107
Epoch: 140 Batch: 1450
Training Loss: 0.021197176123487538
Epoch: 140 Batch: 1500
Training Loss: 0.0198734197417895
Epoch: 140 Batch: 1550
Training Loss: 0.019965575472001108
Epoch: 140 Batch: 1600
Training Loss: 0.019353916682302953
Epoch: 140 Batch: 1650
Training Loss: 0.018848833802974585
Epoch: 140 Batch: 1700
Training Loss: 0.019114621544585507
Epoch: 140 Batch: 1750
Training Loss: 0.017473610077585494
Epoch: 140 Batch: 1800
Training Loss: 0.017495659705665377
Epoch: 140 Batch: 1850
Training Loss: 0.017352683769690024
Epoch: 140 Batch: 1900
Training Loss: 0.016417821645736696
Epoch: 140 Batch: 1950
Training Loss: 0.015807884136835735
Epoch: 140 Batch: 2000
Training Loss: 0.01452968207001686
Epoch: 140 Batch: 2050
Training Loss: 0.01470238951648154
Epoch: 140 Batch: 2100
Training Loss: 0.014664163192113241
Epoch: 140 Batch: 2150
Training Loss: 0.013506513487461002
Epoch: 140 Batch: 2200
Training Loss: 0.014227971244942059
Epoch: 140 Batch: 2250
Training Loss: 0.013356070730421278
Epoch: 140 Batch: 2300
Training Loss: 0.013252953964730968
Epoch: 140 Batch: 2350
Training Loss: 0.012611178159713745
Epoch: 140 Batch: 2400
Training Loss: 0.013095450301965078
Epoch: 140 Batch: 2450
Training Loss: 0.012686838021083753
Epoch: 140 Batch: 2500
Training Loss: 0.01180685648918152
Epoch: 140 Batch: 2550
Training Loss: 0.012379971789378746
Epoch: 140 Batch: 2600
Training Loss: 0.011257547633006022
Epoch: 140 Batch: 2650
Training Loss: 0.011782874150096245
Epoch: 140 Batch: 2700
Training Loss: 0.011342042552100287
Epoch: 140 Batch: 2750
Training Loss: 0.011459773009473628
Epoch: 140 Batch: 2800
Training Loss: 0.011007509785039084
Epoch: 140 Batch: 2850
Training Loss: 0.010793523056465283
Epoch: 140 Batch: 2900
Training Loss: 0.010353368839313244
Epoch: 140 Batch: 2950
Training Loss: 0.010517066343355986
Epoch: 140 Batch: 3000
Training Loss: 0.010283274203538894
Epoch: 140 Batch: 3050
Training Loss: 0.010417285043685163
Epoch: 140 Batch: 3100
Training Loss: 0.010057587315959316
Epoch: 140 Batch: 3150
Training Loss: 0.009965256651242574
Epoch: 140 Batch: 3200
Training Loss: 0.009450940573588014
Epoch: 141 
 Validation Loss: 0.47400028374460007
---------------------------
Epoch: 141 Batch: 50
Training Loss: 0.5861144310235977
Epoch: 141 Batch: 100
Training Loss: 0.30869446277618406
Epoch: 141 Batch: 150
Training Loss: 0.20658975958824158
Epoch: 141 Batch: 200
Training Loss: 0.16079579502344132
Epoch: 141 Batch: 250
Training Loss: 0.12290000236034393
Epoch: 141 Batch: 300
Training Loss: 0.10624046156803767
Epoch: 141 Batch: 350
Training Loss: 0.0861581005368914
Epoch: 141 Batch: 400
Training Loss: 0.0786569207906723
Epoch: 141 Batch: 450
Training Loss: 0.06721793399916755
Epoch: 141 Batch: 500
Training Loss: 0.058543422520160676
Epoch: 141 Batch: 550
Training Loss: 0.055448613546111365
Epoch: 141 Batch: 600
Training Loss: 0.05025856544574102
Epoch: 141 Batch: 650
Training Loss: 0.04592403916212229
Epoch: 141 Batch: 700
Training Loss: 0.04474861528192248
Epoch: 141 Batch: 750
Training Loss: 0.04110514533519745
Epoch: 141 Batch: 800
Training Loss: 0.0370111683756113
Epoch: 141 Batch: 850
Training Loss: 0.035775704383850096
Epoch: 141 Batch: 900
Training Loss: 0.035485888918240865
Epoch: 141 Batch: 950
Training Loss: 0.033037420950437844
Epoch: 141 Batch: 1000
Training Loss: 0.02993119704723358
Epoch: 141 Batch: 1050
Training Loss: 0.028025177745592027
Epoch: 141 Batch: 1100
Training Loss: 0.029623268084092574
Epoch: 141 Batch: 1150
Training Loss: 0.02622583003147789
Epoch: 141 Batch: 1200
Training Loss: 0.025456274524331093
Epoch: 141 Batch: 1250
Training Loss: 0.02430897843837738
Epoch: 141 Batch: 1300
Training Loss: 0.02316657034250406
Epoch: 141 Batch: 1350
Training Loss: 0.022367078728146025
Epoch: 141 Batch: 1400
Training Loss: 0.02086104778306825
Epoch: 141 Batch: 1450
Training Loss: 0.020834973063962214
Epoch: 141 Batch: 1500
Training Loss: 0.02093765276670456
Epoch: 141 Batch: 1550
Training Loss: 0.01973965617918199
Epoch: 141 Batch: 1600
Training Loss: 0.019108623396605254
Epoch: 141 Batch: 1650
Training Loss: 0.018957871957258746
Epoch: 141 Batch: 1700
Training Loss: 0.01742310509962194
Epoch: 141 Batch: 1750
Training Loss: 0.01715533675466265
Epoch: 141 Batch: 1800
Training Loss: 0.0167305009232627
Epoch: 141 Batch: 1850
Training Loss: 0.016592449513641562
Epoch: 141 Batch: 1900
Training Loss: 0.016139839824877288
Epoch: 141 Batch: 1950
Training Loss: 0.01536955897624676
Epoch: 141 Batch: 2000
Training Loss: 0.014419057950377465
Epoch: 141 Batch: 2050
Training Loss: 0.015563245880894545
Epoch: 141 Batch: 2100
Training Loss: 0.015494074296383631
Epoch: 141 Batch: 2150
Training Loss: 0.014302016413489053
Epoch: 141 Batch: 2200
Training Loss: 0.013997717662291093
Epoch: 141 Batch: 2250
Training Loss: 0.013607766628265381
Epoch: 141 Batch: 2300
Training Loss: 0.01281211301036503
Epoch: 141 Batch: 2350
Training Loss: 0.013064356217993067
Epoch: 141 Batch: 2400
Training Loss: 0.012637257327636082
Epoch: 141 Batch: 2450
Training Loss: 0.012907627808804415
Epoch: 141 Batch: 2500
Training Loss: 0.011824801158905029
Epoch: 141 Batch: 2550
Training Loss: 0.01189957441068163
Epoch: 141 Batch: 2600
Training Loss: 0.011722938212064596
Epoch: 141 Batch: 2650
Training Loss: 0.01157962440319781
Epoch: 141 Batch: 2700
Training Loss: 0.011060437968483678
Epoch: 141 Batch: 2750
Training Loss: 0.010896324753761292
Epoch: 141 Batch: 2800
Training Loss: 0.011101739555597305
Epoch: 141 Batch: 2850
Training Loss: 0.01082293735261549
Epoch: 141 Batch: 2900
Training Loss: 0.010467260130520524
Epoch: 141 Batch: 2950
Training Loss: 0.010282143566568019
Epoch: 141 Batch: 3000
Training Loss: 0.010302524209022522
Epoch: 141 Batch: 3050
Training Loss: 0.009849972265665648
Epoch: 141 Batch: 3100
Training Loss: 0.01009733404844038
Epoch: 141 Batch: 3150
Training Loss: 0.009937258088399493
Epoch: 141 Batch: 3200
Training Loss: 0.009261995125561952
Epoch: 142 
 Validation Loss: 0.4740772724151611
---------------------------
Epoch: 142 Batch: 50
Training Loss: 0.6115952759981156
Epoch: 142 Batch: 100
Training Loss: 0.2994239956140518
Epoch: 142 Batch: 150
Training Loss: 0.200354207555453
Epoch: 142 Batch: 200
Training Loss: 0.15516066029667855
Epoch: 142 Batch: 250
Training Loss: 0.12637535881996154
Epoch: 142 Batch: 300
Training Loss: 0.10252222826083501
Epoch: 142 Batch: 350
Training Loss: 0.08916411416871207
Epoch: 142 Batch: 400
Training Loss: 0.07712245546281338
Epoch: 142 Batch: 450
Training Loss: 0.06917804797490437
Epoch: 142 Batch: 500
Training Loss: 0.05957883566617966
Epoch: 142 Batch: 550
Training Loss: 0.054361669367009945
Epoch: 142 Batch: 600
Training Loss: 0.049370764742294945
Epoch: 142 Batch: 650
Training Loss: 0.04805129009943742
Epoch: 142 Batch: 700
Training Loss: 0.04469634758574622
Epoch: 142 Batch: 750
Training Loss: 0.04160964020093282
Epoch: 142 Batch: 800
Training Loss: 0.03859667528420687
Epoch: 142 Batch: 850
Training Loss: 0.036029932148316325
Epoch: 142 Batch: 900
Training Loss: 0.03482093072599835
Epoch: 142 Batch: 950
Training Loss: 0.03234592898895866
Epoch: 142 Batch: 1000
Training Loss: 0.030667372405529022
Epoch: 142 Batch: 1050
Training Loss: 0.028959546912284126
Epoch: 142 Batch: 1100
Training Loss: 0.027011909647421402
Epoch: 142 Batch: 1150
Training Loss: 0.02630545242972996
Epoch: 142 Batch: 1200
Training Loss: 0.025037403851747513
Epoch: 142 Batch: 1250
Training Loss: 0.023629211020469665
Epoch: 142 Batch: 1300
Training Loss: 0.022211473767574017
Epoch: 142 Batch: 1350
Training Loss: 0.021647304164038764
Epoch: 142 Batch: 1400
Training Loss: 0.0219629716021674
Epoch: 142 Batch: 1450
Training Loss: 0.020981937975719057
Epoch: 142 Batch: 1500
Training Loss: 0.020492919862270354
Epoch: 142 Batch: 1550
Training Loss: 0.018388906063572054
Epoch: 142 Batch: 1600
Training Loss: 0.018920975495129822
Epoch: 142 Batch: 1650
Training Loss: 0.018310492887641444
Epoch: 142 Batch: 1700
Training Loss: 0.018105841769891627
Epoch: 142 Batch: 1750
Training Loss: 0.017767159513064793
Epoch: 142 Batch: 1800
Training Loss: 0.01711786937382486
Epoch: 142 Batch: 1850
Training Loss: 0.016785558365486765
Epoch: 142 Batch: 1900
Training Loss: 0.016544917445433766
Epoch: 142 Batch: 1950
Training Loss: 0.015471579096256158
Epoch: 142 Batch: 2000
Training Loss: 0.014170313626527787
Epoch: 142 Batch: 2050
Training Loss: 0.015426711047567973
Epoch: 142 Batch: 2100
Training Loss: 0.013944388259024847
Epoch: 142 Batch: 2150
Training Loss: 0.014103571869606195
Epoch: 142 Batch: 2200
Training Loss: 0.013957866904410449
Epoch: 142 Batch: 2250
Training Loss: 0.013305124097400242
Epoch: 142 Batch: 2300
Training Loss: 0.013606623126112896
Epoch: 142 Batch: 2350
Training Loss: 0.01324212776853683
Epoch: 142 Batch: 2400
Training Loss: 0.013208677830795447
Epoch: 142 Batch: 2450
Training Loss: 0.012993689690317426
Epoch: 142 Batch: 2500
Training Loss: 0.01204718121290207
Epoch: 142 Batch: 2550
Training Loss: 0.012241637496387257
Epoch: 142 Batch: 2600
Training Loss: 0.012120819401282531
Epoch: 142 Batch: 2650
Training Loss: 0.01172289309636602
Epoch: 142 Batch: 2700
Training Loss: 0.011805899573696984
Epoch: 142 Batch: 2750
Training Loss: 0.01124449267170646
Epoch: 142 Batch: 2800
Training Loss: 0.011018567244921411
Epoch: 142 Batch: 2850
Training Loss: 0.010776118259680898
Epoch: 142 Batch: 2900
Training Loss: 0.01083894214753447
Epoch: 142 Batch: 2950
Training Loss: 0.01024809087737132
Epoch: 142 Batch: 3000
Training Loss: 0.01013303757707278
Epoch: 142 Batch: 3050
Training Loss: 0.0102282053232193
Epoch: 142 Batch: 3100
Training Loss: 0.009673658042184768
Epoch: 142 Batch: 3150
Training Loss: 0.010266789860195583
Epoch: 142 Batch: 3200
Training Loss: 0.009381381710991264
Epoch: 143 
 Validation Loss: 0.47363622387250265
---------------------------
Epoch: 143 Batch: 50
Training Loss: 0.6128235054016113
Epoch: 143 Batch: 100
Training Loss: 0.3073630851507187
Epoch: 143 Batch: 150
Training Loss: 0.20196133136749267
Epoch: 143 Batch: 200
Training Loss: 0.1467388892173767
Epoch: 143 Batch: 250
Training Loss: 0.12093720495700837
Epoch: 143 Batch: 300
Training Loss: 0.10132302572329839
Epoch: 143 Batch: 350
Training Loss: 0.0888625591993332
Epoch: 143 Batch: 400
Training Loss: 0.07709850274026393
Epoch: 143 Batch: 450
Training Loss: 0.06805538833141327
Epoch: 143 Batch: 500
Training Loss: 0.06100302278995514
Epoch: 143 Batch: 550
Training Loss: 0.05525671758434989
Epoch: 143 Batch: 600
Training Loss: 0.051831605484088265
Epoch: 143 Batch: 650
Training Loss: 0.047544958270513096
Epoch: 143 Batch: 700
Training Loss: 0.04389967952455793
Epoch: 143 Batch: 750
Training Loss: 0.0417607604265213
Epoch: 143 Batch: 800
Training Loss: 0.03871881403028965
Epoch: 143 Batch: 850
Training Loss: 0.03658658129327438
Epoch: 143 Batch: 900
Training Loss: 0.034503965609603456
Epoch: 143 Batch: 950
Training Loss: 0.032252653366640995
Epoch: 143 Batch: 1000
Training Loss: 0.030413822442293167
Epoch: 143 Batch: 1050
Training Loss: 0.02813270580200922
Epoch: 143 Batch: 1100
Training Loss: 0.027739807773720135
Epoch: 143 Batch: 1150
Training Loss: 0.027145713749139204
Epoch: 143 Batch: 1200
Training Loss: 0.02501907634238402
Epoch: 143 Batch: 1250
Training Loss: 0.024078811955451966
Epoch: 143 Batch: 1300
Training Loss: 0.02361295326397969
Epoch: 143 Batch: 1350
Training Loss: 0.02263105156245055
Epoch: 143 Batch: 1400
Training Loss: 0.021800647356680462
Epoch: 143 Batch: 1450
Training Loss: 0.021605956595519494
Epoch: 143 Batch: 1500
Training Loss: 0.02042912878592809
Epoch: 143 Batch: 1550
Training Loss: 0.019494971632957457
Epoch: 143 Batch: 1600
Training Loss: 0.01972969913855195
Epoch: 143 Batch: 1650
Training Loss: 0.019073309988686532
Epoch: 143 Batch: 1700
Training Loss: 0.01909782516605714
Epoch: 143 Batch: 1750
Training Loss: 0.01759501830169133
Epoch: 143 Batch: 1800
Training Loss: 0.017696580489476522
Epoch: 143 Batch: 1850
Training Loss: 0.016812306948610255
Epoch: 143 Batch: 1900
Training Loss: 0.015755694963430104
Epoch: 143 Batch: 1950
Training Loss: 0.01589885242474385
Epoch: 143 Batch: 2000
Training Loss: 0.014977081179618835
Epoch: 143 Batch: 2050
Training Loss: 0.014328374324775324
Epoch: 143 Batch: 2100
Training Loss: 0.014797646303971608
Epoch: 143 Batch: 2150
Training Loss: 0.014238257047741912
Epoch: 143 Batch: 2200
Training Loss: 0.013581964495507153
Epoch: 143 Batch: 2250
Training Loss: 0.01362039487891727
Epoch: 143 Batch: 2300
Training Loss: 0.01351167461146479
Epoch: 143 Batch: 2350
Training Loss: 0.012844679520485248
Epoch: 143 Batch: 2400
Training Loss: 0.012401376863320669
Epoch: 143 Batch: 2450
Training Loss: 0.012369467944514994
Epoch: 143 Batch: 2500
Training Loss: 0.012854761862754822
Epoch: 143 Batch: 2550
Training Loss: 0.01244695664620867
Epoch: 143 Batch: 2600
Training Loss: 0.0115819574204775
Epoch: 143 Batch: 2650
Training Loss: 0.01178945150015489
Epoch: 143 Batch: 2700
Training Loss: 0.011099146019529414
Epoch: 143 Batch: 2750
Training Loss: 0.011353044163097036
Epoch: 143 Batch: 2800
Training Loss: 0.010430183793817247
Epoch: 143 Batch: 2850
Training Loss: 0.011065031697875575
Epoch: 143 Batch: 2900
Training Loss: 0.01077694484899784
Epoch: 143 Batch: 2950
Training Loss: 0.01001301674519555
Epoch: 143 Batch: 3000
Training Loss: 0.010160279870033264
Epoch: 143 Batch: 3050
Training Loss: 0.010049514174461364
Epoch: 143 Batch: 3100
Training Loss: 0.01000918040352483
Epoch: 143 Batch: 3150
Training Loss: 0.009635172268700977
Epoch: 143 Batch: 3200
Training Loss: 0.009865957340225577
Epoch: 144 
 Validation Loss: 0.4733317342069414
---------------------------
Epoch: 144 Batch: 50
Training Loss: 0.6250291812419891
Epoch: 144 Batch: 100
Training Loss: 0.30652312964200973
Epoch: 144 Batch: 150
Training Loss: 0.21188684821128845
Epoch: 144 Batch: 200
Training Loss: 0.1579297935962677
Epoch: 144 Batch: 250
Training Loss: 0.125474791765213
Epoch: 144 Batch: 300
Training Loss: 0.10157415568828583
Epoch: 144 Batch: 350
Training Loss: 0.08605586315904344
Epoch: 144 Batch: 400
Training Loss: 0.07251356028020382
Epoch: 144 Batch: 450
Training Loss: 0.06583611806233725
Epoch: 144 Batch: 500
Training Loss: 0.06086845272779465
Epoch: 144 Batch: 550
Training Loss: 0.056909921656955374
Epoch: 144 Batch: 600
Training Loss: 0.04844267224272092
Epoch: 144 Batch: 650
Training Loss: 0.04834266933111044
Epoch: 144 Batch: 700
Training Loss: 0.04219985152993883
Epoch: 144 Batch: 750
Training Loss: 0.03884522616863251
Epoch: 144 Batch: 800
Training Loss: 0.03961517084389925
Epoch: 144 Batch: 850
Training Loss: 0.03557523415369146
Epoch: 144 Batch: 900
Training Loss: 0.034464717739158206
Epoch: 144 Batch: 950
Training Loss: 0.03163248240947723
Epoch: 144 Batch: 1000
Training Loss: 0.03051774224638939
Epoch: 144 Batch: 1050
Training Loss: 0.030938030169123696
Epoch: 144 Batch: 1100
Training Loss: 0.026346060904589567
Epoch: 144 Batch: 1150
Training Loss: 0.0271218443694322
Epoch: 144 Batch: 1200
Training Loss: 0.02532180299361547
Epoch: 144 Batch: 1250
Training Loss: 0.02581391501426697
Epoch: 144 Batch: 1300
Training Loss: 0.0233856591123801
Epoch: 144 Batch: 1350
Training Loss: 0.021961514419979518
Epoch: 144 Batch: 1400
Training Loss: 0.022569664035524642
Epoch: 144 Batch: 1450
Training Loss: 0.021199087685552138
Epoch: 144 Batch: 1500
Training Loss: 0.019371991217136383
Epoch: 144 Batch: 1550
Training Loss: 0.019047377532528293
Epoch: 144 Batch: 1600
Training Loss: 0.01911041112616658
Epoch: 144 Batch: 1650
Training Loss: 0.019483305829944033
Epoch: 144 Batch: 1700
Training Loss: 0.018850093571578755
Epoch: 144 Batch: 1750
Training Loss: 0.017727646946907042
Epoch: 144 Batch: 1800
Training Loss: 0.01671991068455908
Epoch: 144 Batch: 1850
Training Loss: 0.016868303498706303
Epoch: 144 Batch: 1900
Training Loss: 0.016360273486689517
Epoch: 144 Batch: 1950
Training Loss: 0.015054692656565935
Epoch: 144 Batch: 2000
Training Loss: 0.01555577763915062
Epoch: 144 Batch: 2050
Training Loss: 0.014582580240761362
Epoch: 144 Batch: 2100
Training Loss: 0.014432027893407005
Epoch: 144 Batch: 2150
Training Loss: 0.01402294724486595
Epoch: 144 Batch: 2200
Training Loss: 0.013765393996780569
Epoch: 144 Batch: 2250
Training Loss: 0.01312503355079227
Epoch: 144 Batch: 2300
Training Loss: 0.013657171454118646
Epoch: 144 Batch: 2350
Training Loss: 0.013447457260273873
Epoch: 144 Batch: 2400
Training Loss: 0.012308849195639293
Epoch: 144 Batch: 2450
Training Loss: 0.012389019447930005
Epoch: 144 Batch: 2500
Training Loss: 0.012308340764045715
Epoch: 144 Batch: 2550
Training Loss: 0.012285576591304704
Epoch: 144 Batch: 2600
Training Loss: 0.012200386168865058
Epoch: 144 Batch: 2650
Training Loss: 0.011437739403742665
Epoch: 144 Batch: 2700
Training Loss: 0.01111814143481078
Epoch: 144 Batch: 2750
Training Loss: 0.010804569818756797
Epoch: 144 Batch: 2800
Training Loss: 0.010672101559383529
Epoch: 144 Batch: 2850
Training Loss: 0.011118845239020231
Epoch: 144 Batch: 2900
Training Loss: 0.010411431255011722
Epoch: 144 Batch: 2950
Training Loss: 0.011040594901068736
Epoch: 144 Batch: 3000
Training Loss: 0.010251218289136886
Epoch: 144 Batch: 3050
Training Loss: 0.009757051213842923
Epoch: 144 Batch: 3100
Training Loss: 0.009722412939994566
Epoch: 144 Batch: 3150
Training Loss: 0.009522295338766916
Epoch: 144 Batch: 3200
Training Loss: 0.00864757778123021
Epoch: 145 
 Validation Loss: 0.47306222750080956
---------------------------
Epoch: 145 Batch: 50
Training Loss: 0.6185098361968994
Epoch: 145 Batch: 100
Training Loss: 0.3073024141788483
Epoch: 145 Batch: 150
Training Loss: 0.20211004296938578
Epoch: 145 Batch: 200
Training Loss: 0.15376042172312737
Epoch: 145 Batch: 250
Training Loss: 0.1288440957069397
Epoch: 145 Batch: 300
Training Loss: 0.10115506549676259
Epoch: 145 Batch: 350
Training Loss: 0.09123961951051439
Epoch: 145 Batch: 400
Training Loss: 0.0772348689287901
Epoch: 145 Batch: 450
Training Loss: 0.06755863785743714
Epoch: 145 Batch: 500
Training Loss: 0.061648819029331205
Epoch: 145 Batch: 550
Training Loss: 0.056640415245836434
Epoch: 145 Batch: 600
Training Loss: 0.05227239285906156
Epoch: 145 Batch: 650
Training Loss: 0.04808706274399391
Epoch: 145 Batch: 700
Training Loss: 0.043580933170659204
Epoch: 145 Batch: 750
Training Loss: 0.04167467534542084
Epoch: 145 Batch: 800
Training Loss: 0.0380373865365982
Epoch: 145 Batch: 850
Training Loss: 0.03556348137995776
Epoch: 145 Batch: 900
Training Loss: 0.03253731956084569
Epoch: 145 Batch: 950
Training Loss: 0.03187117046431491
Epoch: 145 Batch: 1000
Training Loss: 0.030935007452964784
Epoch: 145 Batch: 1050
Training Loss: 0.028360197969845364
Epoch: 145 Batch: 1100
Training Loss: 0.02838813296773217
Epoch: 145 Batch: 1150
Training Loss: 0.02762778779734736
Epoch: 145 Batch: 1200
Training Loss: 0.026801339263717333
Epoch: 145 Batch: 1250
Training Loss: 0.024132775068283083
Epoch: 145 Batch: 1300
Training Loss: 0.02291336667079192
Epoch: 145 Batch: 1350
Training Loss: 0.02333752649801749
Epoch: 145 Batch: 1400
Training Loss: 0.021964111988033566
Epoch: 145 Batch: 1450
Training Loss: 0.02104294254862029
Epoch: 145 Batch: 1500
Training Loss: 0.020395982881387074
Epoch: 145 Batch: 1550
Training Loss: 0.019712336217203447
Epoch: 145 Batch: 1600
Training Loss: 0.018027770705521105
Epoch: 145 Batch: 1650
Training Loss: 0.019398842779072847
Epoch: 145 Batch: 1700
Training Loss: 0.017710837476393754
Epoch: 145 Batch: 1750
Training Loss: 0.018537478600229536
Epoch: 145 Batch: 1800
Training Loss: 0.016679394278261397
Epoch: 145 Batch: 1850
Training Loss: 0.016208023445026296
Epoch: 145 Batch: 1900
Training Loss: 0.015769739731361993
Epoch: 145 Batch: 1950
Training Loss: 0.015289299763165988
Epoch: 145 Batch: 2000
Training Loss: 0.015441713571548462
Epoch: 145 Batch: 2050
Training Loss: 0.01544797238780231
Epoch: 145 Batch: 2100
Training Loss: 0.014324806758335659
Epoch: 145 Batch: 2150
Training Loss: 0.014434832972149517
Epoch: 145 Batch: 2200
Training Loss: 0.01398381464860656
Epoch: 145 Batch: 2250
Training Loss: 0.014200122674306233
Epoch: 145 Batch: 2300
Training Loss: 0.013417594497618469
Epoch: 145 Batch: 2350
Training Loss: 0.01259419081058908
Epoch: 145 Batch: 2400
Training Loss: 0.013665689465900263
Epoch: 145 Batch: 2450
Training Loss: 0.01278521504937386
Epoch: 145 Batch: 2500
Training Loss: 0.011464644837379455
Epoch: 145 Batch: 2550
Training Loss: 0.012102022545010437
Epoch: 145 Batch: 2600
Training Loss: 0.01148474462903463
Epoch: 145 Batch: 2650
Training Loss: 0.011087591749317241
Epoch: 145 Batch: 2700
Training Loss: 0.0115678369557416
Epoch: 145 Batch: 2750
Training Loss: 0.011218754909255288
Epoch: 145 Batch: 2800
Training Loss: 0.01103820906153747
Epoch: 145 Batch: 2850
Training Loss: 0.010521629180824547
Epoch: 145 Batch: 2900
Training Loss: 0.009912925146777054
Epoch: 145 Batch: 2950
Training Loss: 0.010294706013243077
Epoch: 145 Batch: 3000
Training Loss: 0.010230422814687093
Epoch: 145 Batch: 3050
Training Loss: 0.010376962579664637
Epoch: 145 Batch: 3100
Training Loss: 0.009625745233028166
Epoch: 145 Batch: 3150
Training Loss: 0.00994726300239563
Epoch: 145 Batch: 3200
Training Loss: 0.009536016602069139
Epoch: 146 
 Validation Loss: 0.4733032008012136
---------------------------
Epoch: 146 Batch: 50
Training Loss: 0.6127768629789352
Epoch: 146 Batch: 100
Training Loss: 0.3173381957411766
Epoch: 146 Batch: 150
Training Loss: 0.2020706444978714
Epoch: 146 Batch: 200
Training Loss: 0.155046559125185
Epoch: 146 Batch: 250
Training Loss: 0.11839796459674835
Epoch: 146 Batch: 300
Training Loss: 0.10296541591485342
Epoch: 146 Batch: 350
Training Loss: 0.08706446451800211
Epoch: 146 Batch: 400
Training Loss: 0.07324084110558032
Epoch: 146 Batch: 450
Training Loss: 0.06502951317363315
Epoch: 146 Batch: 500
Training Loss: 0.06390449303388596
Epoch: 146 Batch: 550
Training Loss: 0.05405677708712491
Epoch: 146 Batch: 600
Training Loss: 0.04819172203540802
Epoch: 146 Batch: 650
Training Loss: 0.045623582509847785
Epoch: 146 Batch: 700
Training Loss: 0.044420957607882366
Epoch: 146 Batch: 750
Training Loss: 0.040076655666033426
Epoch: 146 Batch: 800
Training Loss: 0.039206207394599915
Epoch: 146 Batch: 850
Training Loss: 0.03626495985423817
Epoch: 146 Batch: 900
Training Loss: 0.03404116269614962
Epoch: 146 Batch: 950
Training Loss: 0.03157135081918616
Epoch: 146 Batch: 1000
Training Loss: 0.03102978503704071
Epoch: 146 Batch: 1050
Training Loss: 0.028655102593558177
Epoch: 146 Batch: 1100
Training Loss: 0.028144254955378447
Epoch: 146 Batch: 1150
Training Loss: 0.026655418924663377
Epoch: 146 Batch: 1200
Training Loss: 0.025559443483750024
Epoch: 146 Batch: 1250
Training Loss: 0.025548020195961
Epoch: 146 Batch: 1300
Training Loss: 0.023287903368473053
Epoch: 146 Batch: 1350
Training Loss: 0.021923205764205367
Epoch: 146 Batch: 1400
Training Loss: 0.02038722153220858
Epoch: 146 Batch: 1450
Training Loss: 0.02127221537047419
Epoch: 146 Batch: 1500
Training Loss: 0.020784506897131603
Epoch: 146 Batch: 1550
Training Loss: 0.019802242652062447
Epoch: 146 Batch: 1600
Training Loss: 0.018084670901298523
Epoch: 146 Batch: 1650
Training Loss: 0.01905710635763226
Epoch: 146 Batch: 1700
Training Loss: 0.017749148624784807
Epoch: 146 Batch: 1750
Training Loss: 0.017572199957711355
Epoch: 146 Batch: 1800
Training Loss: 0.017511570536428028
Epoch: 146 Batch: 1850
Training Loss: 0.01757188202561559
Epoch: 146 Batch: 1900
Training Loss: 0.01637875663606744
Epoch: 146 Batch: 1950
Training Loss: 0.016857570455624506
Epoch: 146 Batch: 2000
Training Loss: 0.01653837387263775
Epoch: 146 Batch: 2050
Training Loss: 0.015031273030653233
Epoch: 146 Batch: 2100
Training Loss: 0.015164198832852499
Epoch: 146 Batch: 2150
Training Loss: 0.015011782271917476
Epoch: 146 Batch: 2200
Training Loss: 0.014262890070676803
Epoch: 146 Batch: 2250
Training Loss: 0.014015678789880541
Epoch: 146 Batch: 2300
Training Loss: 0.01344595458196557
Epoch: 146 Batch: 2350
Training Loss: 0.012415093513245278
Epoch: 146 Batch: 2400
Training Loss: 0.012780045593778292
Epoch: 146 Batch: 2450
Training Loss: 0.012408862734327512
Epoch: 146 Batch: 2500
Training Loss: 0.012163608694076539
Epoch: 146 Batch: 2550
Training Loss: 0.012193894783655802
Epoch: 146 Batch: 2600
Training Loss: 0.011835107207298278
Epoch: 146 Batch: 2650
Training Loss: 0.011281581138664822
Epoch: 146 Batch: 2700
Training Loss: 0.011314916400997728
Epoch: 146 Batch: 2750
Training Loss: 0.01144458707896146
Epoch: 146 Batch: 2800
Training Loss: 0.010920954176357814
Epoch: 146 Batch: 2850
Training Loss: 0.01111825071928794
Epoch: 146 Batch: 2900
Training Loss: 0.010596422491402462
Epoch: 146 Batch: 2950
Training Loss: 0.010555940169399067
Epoch: 146 Batch: 3000
Training Loss: 0.00987869037191073
Epoch: 146 Batch: 3050
Training Loss: 0.010023567969681787
Epoch: 146 Batch: 3100
Training Loss: 0.00983137935400009
Epoch: 146 Batch: 3150
Training Loss: 0.00959924506762671
Epoch: 146 Batch: 3200
Training Loss: 0.009855712279677391
Epoch: 147 
 Validation Loss: 0.4729224079185062
---------------------------
Epoch: 147 Batch: 50
Training Loss: 0.6256789183616638
Epoch: 147 Batch: 100
Training Loss: 0.31161763817071914
Epoch: 147 Batch: 150
Training Loss: 0.20791023313999177
Epoch: 147 Batch: 200
Training Loss: 0.1542874827980995
Epoch: 147 Batch: 250
Training Loss: 0.12006525135040283
Epoch: 147 Batch: 300
Training Loss: 0.09993093113104502
Epoch: 147 Batch: 350
Training Loss: 0.08664050246988024
Epoch: 147 Batch: 400
Training Loss: 0.07235001392662525
Epoch: 147 Batch: 450
Training Loss: 0.06828088727262285
Epoch: 147 Batch: 500
Training Loss: 0.05897377300262451
Epoch: 147 Batch: 550
Training Loss: 0.058144507137211886
Epoch: 147 Batch: 600
Training Loss: 0.05154384260376294
Epoch: 147 Batch: 650
Training Loss: 0.04744688932712261
Epoch: 147 Batch: 700
Training Loss: 0.04287490470068795
Epoch: 147 Batch: 750
Training Loss: 0.0422165466149648
Epoch: 147 Batch: 800
Training Loss: 0.03796483267098665
Epoch: 147 Batch: 850
Training Loss: 0.03464596176848692
Epoch: 147 Batch: 900
Training Loss: 0.03390629937251409
Epoch: 147 Batch: 950
Training Loss: 0.03248055006328382
Epoch: 147 Batch: 1000
Training Loss: 0.030673569202423097
Epoch: 147 Batch: 1050
Training Loss: 0.028838833684013004
Epoch: 147 Batch: 1100
Training Loss: 0.027880978367545387
Epoch: 147 Batch: 1150
Training Loss: 0.027090185802915823
Epoch: 147 Batch: 1200
Training Loss: 0.025264659076929093
Epoch: 147 Batch: 1250
Training Loss: 0.024926524233818054
Epoch: 147 Batch: 1300
Training Loss: 0.02370376726755729
Epoch: 147 Batch: 1350
Training Loss: 0.022715836741306162
Epoch: 147 Batch: 1400
Training Loss: 0.023895102036850795
Epoch: 147 Batch: 1450
Training Loss: 0.020438643323964088
Epoch: 147 Batch: 1500
Training Loss: 0.01972529321908951
Epoch: 147 Batch: 1550
Training Loss: 0.019996989273255873
Epoch: 147 Batch: 1600
Training Loss: 0.019839341025799512
Epoch: 147 Batch: 1650
Training Loss: 0.017796821251060024
Epoch: 147 Batch: 1700
Training Loss: 0.017829392587437348
Epoch: 147 Batch: 1750
Training Loss: 0.017087023309298923
Epoch: 147 Batch: 1800
Training Loss: 0.016441082424587673
Epoch: 147 Batch: 1850
Training Loss: 0.01614246908071879
Epoch: 147 Batch: 1900
Training Loss: 0.01684736491818177
Epoch: 147 Batch: 1950
Training Loss: 0.01500463047088721
Epoch: 147 Batch: 2000
Training Loss: 0.015214342057704926
Epoch: 147 Batch: 2050
Training Loss: 0.015234538781933668
Epoch: 147 Batch: 2100
Training Loss: 0.014616845746835073
Epoch: 147 Batch: 2150
Training Loss: 0.014803103444188141
Epoch: 147 Batch: 2200
Training Loss: 0.013714911152016033
Epoch: 147 Batch: 2250
Training Loss: 0.013497906857066684
Epoch: 147 Batch: 2300
Training Loss: 0.01371076850787453
Epoch: 147 Batch: 2350
Training Loss: 0.013744391038062725
Epoch: 147 Batch: 2400
Training Loss: 0.012695343419909477
Epoch: 147 Batch: 2450
Training Loss: 0.012514807642722616
Epoch: 147 Batch: 2500
Training Loss: 0.012189593768119813
Epoch: 147 Batch: 2550
Training Loss: 0.011970935452218149
Epoch: 147 Batch: 2600
Training Loss: 0.011893865832915672
Epoch: 147 Batch: 2650
Training Loss: 0.011389523699598492
Epoch: 147 Batch: 2700
Training Loss: 0.010947688972508465
Epoch: 147 Batch: 2750
Training Loss: 0.01068397372419184
Epoch: 147 Batch: 2800
Training Loss: 0.010963759709681784
Epoch: 147 Batch: 2850
Training Loss: 0.011370558968761511
Epoch: 147 Batch: 2900
Training Loss: 0.010446339732614057
Epoch: 147 Batch: 2950
Training Loss: 0.01015350668107049
Epoch: 147 Batch: 3000
Training Loss: 0.009849882284800212
Epoch: 147 Batch: 3050
Training Loss: 0.01002071729448975
Epoch: 147 Batch: 3100
Training Loss: 0.01032001951048451
Epoch: 147 Batch: 3150
Training Loss: 0.009339934738855513
Epoch: 147 Batch: 3200
Training Loss: 0.009924652306362986
Epoch: 148 
 Validation Loss: 0.4726286758979162
---------------------------
Epoch: 148 Batch: 50
Training Loss: 0.6251300126314163
Epoch: 148 Batch: 100
Training Loss: 0.293555184006691
Epoch: 148 Batch: 150
Training Loss: 0.19409521619478862
Epoch: 148 Batch: 200
Training Loss: 0.1609674671292305
Epoch: 148 Batch: 250
Training Loss: 0.12292671072483062
Epoch: 148 Batch: 300
Training Loss: 0.09818040718634924
Epoch: 148 Batch: 350
Training Loss: 0.08464324389185224
Epoch: 148 Batch: 400
Training Loss: 0.07717694722115993
Epoch: 148 Batch: 450
Training Loss: 0.07186254170205858
Epoch: 148 Batch: 500
Training Loss: 0.06094963228702545
Epoch: 148 Batch: 550
Training Loss: 0.05518025988882238
Epoch: 148 Batch: 600
Training Loss: 0.05517320558428764
Epoch: 148 Batch: 650
Training Loss: 0.048099293479552635
Epoch: 148 Batch: 700
Training Loss: 0.041777575143745964
Epoch: 148 Batch: 750
Training Loss: 0.041814685026804604
Epoch: 148 Batch: 800
Training Loss: 0.03793517366051674
Epoch: 148 Batch: 850
Training Loss: 0.036556182118023146
Epoch: 148 Batch: 900
Training Loss: 0.03370884868833754
Epoch: 148 Batch: 950
Training Loss: 0.031833354360178895
Epoch: 148 Batch: 1000
Training Loss: 0.030564490169286727
Epoch: 148 Batch: 1050
Training Loss: 0.029073654526755924
Epoch: 148 Batch: 1100
Training Loss: 0.027849649841135197
Epoch: 148 Batch: 1150
Training Loss: 0.026952204160068344
Epoch: 148 Batch: 1200
Training Loss: 0.025380175759394965
Epoch: 148 Batch: 1250
Training Loss: 0.025686199593544005
Epoch: 148 Batch: 1300
Training Loss: 0.02371829124597403
Epoch: 148 Batch: 1350
Training Loss: 0.02284576294598756
Epoch: 148 Batch: 1400
Training Loss: 0.02198661710534777
Epoch: 148 Batch: 1450
Training Loss: 0.021018889320307765
Epoch: 148 Batch: 1500
Training Loss: 0.01934698611497879
Epoch: 148 Batch: 1550
Training Loss: 0.02019010478450406
Epoch: 148 Batch: 1600
Training Loss: 0.019673928171396255
Epoch: 148 Batch: 1650
Training Loss: 0.018784738851316048
Epoch: 148 Batch: 1700
Training Loss: 0.018031642997966092
Epoch: 148 Batch: 1750
Training Loss: 0.018437790836606707
Epoch: 148 Batch: 1800
Training Loss: 0.01695853006508615
Epoch: 148 Batch: 1850
Training Loss: 0.01587849786152711
Epoch: 148 Batch: 1900
Training Loss: 0.015392117955182728
Epoch: 148 Batch: 1950
Training Loss: 0.014763308152174338
Epoch: 148 Batch: 2000
Training Loss: 0.014916675090789795
Epoch: 148 Batch: 2050
Training Loss: 0.015351781496187536
Epoch: 148 Batch: 2100
Training Loss: 0.014414077344394866
Epoch: 148 Batch: 2150
Training Loss: 0.014460610594860344
Epoch: 148 Batch: 2200
Training Loss: 0.013793234378099442
Epoch: 148 Batch: 2250
Training Loss: 0.013482304652531942
Epoch: 148 Batch: 2300
Training Loss: 0.013510759291441543
Epoch: 148 Batch: 2350
Training Loss: 0.012925101670813054
Epoch: 148 Batch: 2400
Training Loss: 0.013020875118672847
Epoch: 148 Batch: 2450
Training Loss: 0.012392339305001863
Epoch: 148 Batch: 2500
Training Loss: 0.011725215518474578
Epoch: 148 Batch: 2550
Training Loss: 0.01163477149664187
Epoch: 148 Batch: 2600
Training Loss: 0.01206225267969645
Epoch: 148 Batch: 2650
Training Loss: 0.011351073924100624
Epoch: 148 Batch: 2700
Training Loss: 0.012216351948402546
Epoch: 148 Batch: 2750
Training Loss: 0.01065545531836423
Epoch: 148 Batch: 2800
Training Loss: 0.01075661937040942
Epoch: 148 Batch: 2850
Training Loss: 0.010070053098494547
Epoch: 148 Batch: 2900
Training Loss: 0.010688073984507857
Epoch: 148 Batch: 2950
Training Loss: 0.010465066927974507
Epoch: 148 Batch: 3000
Training Loss: 0.010590426921844483
Epoch: 148 Batch: 3050
Training Loss: 0.009672065533575464
Epoch: 148 Batch: 3100
Training Loss: 0.009844921154360618
Epoch: 148 Batch: 3150
Training Loss: 0.009692374694915044
Epoch: 148 Batch: 3200
Training Loss: 0.009427936365827918
Epoch: 149 
 Validation Loss: 0.47285417318344114
---------------------------
Epoch: 149 Batch: 50
Training Loss: 0.6304531931877136
Epoch: 149 Batch: 100
Training Loss: 0.3077060082554817
Epoch: 149 Batch: 150
Training Loss: 0.19813700159390768
Epoch: 149 Batch: 200
Training Loss: 0.1560401526093483
Epoch: 149 Batch: 250
Training Loss: 0.1253420442342758
Epoch: 149 Batch: 300
Training Loss: 0.09855899304151534
Epoch: 149 Batch: 350
Training Loss: 0.09085021385124752
Epoch: 149 Batch: 400
Training Loss: 0.07291897013783455
Epoch: 149 Batch: 450
Training Loss: 0.0677451401286655
Epoch: 149 Batch: 500
Training Loss: 0.06147236430644989
Epoch: 149 Batch: 550
Training Loss: 0.057456185655160384
Epoch: 149 Batch: 600
Training Loss: 0.05294925918181737
Epoch: 149 Batch: 650
Training Loss: 0.04901494214167962
Epoch: 149 Batch: 700
Training Loss: 0.04280053211109979
Epoch: 149 Batch: 750
Training Loss: 0.041356766621271766
Epoch: 149 Batch: 800
Training Loss: 0.0391562420502305
Epoch: 149 Batch: 850
Training Loss: 0.037147736899993
Epoch: 149 Batch: 900
Training Loss: 0.034562239415115784
Epoch: 149 Batch: 950
Training Loss: 0.031095678147516754
Epoch: 149 Batch: 1000
Training Loss: 0.030239780604839325
Epoch: 149 Batch: 1050
Training Loss: 0.02976243214947837
Epoch: 149 Batch: 1100
Training Loss: 0.028407736420631408
Epoch: 149 Batch: 1150
Training Loss: 0.0260420154488605
Epoch: 149 Batch: 1200
Training Loss: 0.02624590295056502
Epoch: 149 Batch: 1250
Training Loss: 0.024200865030288697
Epoch: 149 Batch: 1300
Training Loss: 0.02388118858520801
Epoch: 149 Batch: 1350
Training Loss: 0.022248261239793565
Epoch: 149 Batch: 1400
Training Loss: 0.022586463817528317
Epoch: 149 Batch: 1450
Training Loss: 0.021080015897750853
Epoch: 149 Batch: 1500
Training Loss: 0.02070735917488734
Epoch: 149 Batch: 1550
Training Loss: 0.01909690176287005
Epoch: 149 Batch: 1600
Training Loss: 0.017988471407443286
Epoch: 149 Batch: 1650
Training Loss: 0.0184716494697513
Epoch: 149 Batch: 1700
Training Loss: 0.018008905947208404
Epoch: 149 Batch: 1750
Training Loss: 0.01703685620852879
Epoch: 149 Batch: 1800
Training Loss: 0.01668432177768813
Epoch: 149 Batch: 1850
Training Loss: 0.01606519181986113
Epoch: 149 Batch: 1900
Training Loss: 0.015579390416019842
Epoch: 149 Batch: 1950
Training Loss: 0.014883821744185228
Epoch: 149 Batch: 2000
Training Loss: 0.015122123435139656
Epoch: 149 Batch: 2050
Training Loss: 0.014823769389129266
Epoch: 149 Batch: 2100
Training Loss: 0.014797699380488622
Epoch: 149 Batch: 2150
Training Loss: 0.013984656112138616
Epoch: 149 Batch: 2200
Training Loss: 0.014292324754324827
Epoch: 149 Batch: 2250
Training Loss: 0.013613089389271206
Epoch: 149 Batch: 2300
Training Loss: 0.013344016619350599
Epoch: 149 Batch: 2350
Training Loss: 0.012738755484844776
Epoch: 149 Batch: 2400
Training Loss: 0.012623086720705032
Epoch: 149 Batch: 2450
Training Loss: 0.012624694972622152
Epoch: 149 Batch: 2500
Training Loss: 0.012309473490715027
Epoch: 149 Batch: 2550
Training Loss: 0.012189476571831049
Epoch: 149 Batch: 2600
Training Loss: 0.011209986622516925
Epoch: 149 Batch: 2650
Training Loss: 0.01197379987194853
Epoch: 149 Batch: 2700
Training Loss: 0.011497721638944414
Epoch: 149 Batch: 2750
Training Loss: 0.011495758490128951
Epoch: 149 Batch: 2800
Training Loss: 0.010735618535961423
Epoch: 149 Batch: 2850
Training Loss: 0.01123230466717168
Epoch: 149 Batch: 2900
Training Loss: 0.010485849637409736
Epoch: 149 Batch: 2950
Training Loss: 0.010007967261944787
Epoch: 149 Batch: 3000
Training Loss: 0.010209646433591842
Epoch: 149 Batch: 3050
Training Loss: 0.01008432402962544
Epoch: 149 Batch: 3100
Training Loss: 0.009866168556674835
Epoch: 149 Batch: 3150
Training Loss: 0.009502881074708605
Epoch: 149 Batch: 3200
Training Loss: 0.009763423977419734
Epoch: 150 
 Validation Loss: 0.47290500435564253
---------------------------
Epoch: 150 Batch: 50
Training Loss: 0.6089675664901734
Epoch: 150 Batch: 100
Training Loss: 0.30815972298383715
Epoch: 150 Batch: 150
Training Loss: 0.20496497571468353
Epoch: 150 Batch: 200
Training Loss: 0.1457533197104931
Epoch: 150 Batch: 250
Training Loss: 0.12021746969223022
Epoch: 150 Batch: 300
Training Loss: 0.10032410432895024
Epoch: 150 Batch: 350
Training Loss: 0.08788671323231288
Epoch: 150 Batch: 400
Training Loss: 0.07380524374544621
Epoch: 150 Batch: 450
Training Loss: 0.06948294056786432
Epoch: 150 Batch: 500
Training Loss: 0.06374199998378753
Epoch: 150 Batch: 550
Training Loss: 0.056244989796118305
Epoch: 150 Batch: 600
Training Loss: 0.052206155608097715
Epoch: 150 Batch: 650
Training Loss: 0.04589307693334726
Epoch: 150 Batch: 700
Training Loss: 0.043697533309459684
Epoch: 150 Batch: 750
Training Loss: 0.040271096110343935
Epoch: 150 Batch: 800
Training Loss: 0.039817147366702554
Epoch: 150 Batch: 850
Training Loss: 0.036349523803767034
Epoch: 150 Batch: 900
Training Loss: 0.035047318869166906
Epoch: 150 Batch: 950
Training Loss: 0.031561432261216014
Epoch: 150 Batch: 1000
Training Loss: 0.03060685181617737
Epoch: 150 Batch: 1050
Training Loss: 0.0299942409992218
Epoch: 150 Batch: 1100
Training Loss: 0.026816587339748035
Epoch: 150 Batch: 1150
Training Loss: 0.02739130507344785
Epoch: 150 Batch: 1200
Training Loss: 0.0252873378743728
Epoch: 150 Batch: 1250
Training Loss: 0.02435751814842224
Epoch: 150 Batch: 1300
Training Loss: 0.022691170642009147
Epoch: 150 Batch: 1350
Training Loss: 0.0230024990329036
Epoch: 150 Batch: 1400
Training Loss: 0.021631867800440106
Epoch: 150 Batch: 1450
Training Loss: 0.0204286651981288
Epoch: 150 Batch: 1500
Training Loss: 0.020168765286604565
Epoch: 150 Batch: 1550
Training Loss: 0.0197864463444679
Epoch: 150 Batch: 1600
Training Loss: 0.01930433413013816
Epoch: 150 Batch: 1650
Training Loss: 0.018919405648202606
Epoch: 150 Batch: 1700
Training Loss: 0.017763666861197527
Epoch: 150 Batch: 1750
Training Loss: 0.018623333368982586
Epoch: 150 Batch: 1800
Training Loss: 0.017632463210158877
Epoch: 150 Batch: 1850
Training Loss: 0.016670681447596165
Epoch: 150 Batch: 1900
Training Loss: 0.015618089406113875
Epoch: 150 Batch: 1950
Training Loss: 0.015677719773390355
Epoch: 150 Batch: 2000
Training Loss: 0.01495230932533741
Epoch: 150 Batch: 2050
Training Loss: 0.01495956137412932
Epoch: 150 Batch: 2100
Training Loss: 0.014526211250396001
Epoch: 150 Batch: 2150
Training Loss: 0.014333643400391867
Epoch: 150 Batch: 2200
Training Loss: 0.013911817520856858
Epoch: 150 Batch: 2250
Training Loss: 0.014031385633680556
Epoch: 150 Batch: 2300
Training Loss: 0.013084328472614289
Epoch: 150 Batch: 2350
Training Loss: 0.012991152481829867
Epoch: 150 Batch: 2400
Training Loss: 0.013183284836510817
Epoch: 150 Batch: 2450
Training Loss: 0.012238464902858345
Epoch: 150 Batch: 2500
Training Loss: 0.012017590200901032
Epoch: 150 Batch: 2550
Training Loss: 0.012099171292548085
Epoch: 150 Batch: 2600
Training Loss: 0.011875385607664402
Epoch: 150 Batch: 2650
Training Loss: 0.011267246750165832
Epoch: 150 Batch: 2700
Training Loss: 0.011672933311374098
Epoch: 150 Batch: 2750
Training Loss: 0.010828754620118574
Epoch: 150 Batch: 2800
Training Loss: 0.010182946303061076
Epoch: 150 Batch: 2850
Training Loss: 0.010742462476094564
Epoch: 150 Batch: 2900
Training Loss: 0.01040381282567978
Epoch: 150 Batch: 2950
Training Loss: 0.010196837883884624
Epoch: 150 Batch: 3000
Training Loss: 0.009947731932004293
Epoch: 150 Batch: 3050
Training Loss: 0.010011848885504927
Epoch: 150 Batch: 3100
Training Loss: 0.010225492533176176
Epoch: 150 Batch: 3150
Training Loss: 0.00962028097538721
Epoch: 150 Batch: 3200
Training Loss: 0.009508854877203703
Epoch: 151 
 Validation Loss: 0.4725667715072632
---------------------------
Epoch: 151 Batch: 50
Training Loss: 0.5976397651433945
Epoch: 151 Batch: 100
Training Loss: 0.30374986082315447
Epoch: 151 Batch: 150
Training Loss: 0.2067604031165441
Epoch: 151 Batch: 200
Training Loss: 0.15524773120880128
Epoch: 151 Batch: 250
Training Loss: 0.12274084270000457
Epoch: 151 Batch: 300
Training Loss: 0.09834520081679027
Epoch: 151 Batch: 350
Training Loss: 0.08876720334802356
Epoch: 151 Batch: 400
Training Loss: 0.07689685396850109
Epoch: 151 Batch: 450
Training Loss: 0.06673706372578939
Epoch: 151 Batch: 500
Training Loss: 0.05986296850442886
Epoch: 151 Batch: 550
Training Loss: 0.05147300655191595
Epoch: 151 Batch: 600
Training Loss: 0.050476757287979124
Epoch: 151 Batch: 650
Training Loss: 0.04621994733810425
Epoch: 151 Batch: 700
Training Loss: 0.04388686716556549
Epoch: 151 Batch: 750
Training Loss: 0.04143053956826528
Epoch: 151 Batch: 800
Training Loss: 0.038116400688886644
Epoch: 151 Batch: 850
Training Loss: 0.03530615224557764
Epoch: 151 Batch: 900
Training Loss: 0.03369246423244476
Epoch: 151 Batch: 950
Training Loss: 0.031064049036879288
Epoch: 151 Batch: 1000
Training Loss: 0.030949881851673126
Epoch: 151 Batch: 1050
Training Loss: 0.029275291164716086
Epoch: 151 Batch: 1100
Training Loss: 0.029160397188229995
Epoch: 151 Batch: 1150
Training Loss: 0.025972477301307348
Epoch: 151 Batch: 1200
Training Loss: 0.025123954614003498
Epoch: 151 Batch: 1250
Training Loss: 0.02446979265213013
Epoch: 151 Batch: 1300
Training Loss: 0.02478646931739954
Epoch: 151 Batch: 1350
Training Loss: 0.0221717913724758
Epoch: 151 Batch: 1400
Training Loss: 0.021515643277338573
Epoch: 151 Batch: 1450
Training Loss: 0.02099749558958514
Epoch: 151 Batch: 1500
Training Loss: 0.02039161958297094
Epoch: 151 Batch: 1550
Training Loss: 0.01906183994585468
Epoch: 151 Batch: 1600
Training Loss: 0.019143805224448442
Epoch: 151 Batch: 1650
Training Loss: 0.01847814865184553
Epoch: 151 Batch: 1700
Training Loss: 0.01853940816486583
Epoch: 151 Batch: 1750
Training Loss: 0.017708925996507918
Epoch: 151 Batch: 1800
Training Loss: 0.016734380341238447
Epoch: 151 Batch: 1850
Training Loss: 0.016555844190958385
Epoch: 151 Batch: 1900
Training Loss: 0.016039326786994934
Epoch: 151 Batch: 1950
Training Loss: 0.015495809882115096
Epoch: 151 Batch: 2000
Training Loss: 0.015568235993385315
Epoch: 151 Batch: 2050
Training Loss: 0.015736703799992074
Epoch: 151 Batch: 2100
Training Loss: 0.014416935713518233
Epoch: 151 Batch: 2150
Training Loss: 0.013900148965591608
Epoch: 151 Batch: 2200
Training Loss: 0.014381812052293257
Epoch: 151 Batch: 2250
Training Loss: 0.013673647814326817
Epoch: 151 Batch: 2300
Training Loss: 0.012998406887054443
Epoch: 151 Batch: 2350
Training Loss: 0.012741386991866091
Epoch: 151 Batch: 2400
Training Loss: 0.013751100289324919
Epoch: 151 Batch: 2450
Training Loss: 0.012248254138596204
Epoch: 151 Batch: 2500
Training Loss: 0.01218782787322998
Epoch: 151 Batch: 2550
Training Loss: 0.012043905059496561
Epoch: 151 Batch: 2600
Training Loss: 0.01148033320903778
Epoch: 151 Batch: 2650
Training Loss: 0.010916988770916777
Epoch: 151 Batch: 2700
Training Loss: 0.011343432090900562
Epoch: 151 Batch: 2750
Training Loss: 0.011208135778253728
Epoch: 151 Batch: 2800
Training Loss: 0.010665800475648472
Epoch: 151 Batch: 2850
Training Loss: 0.010943124817128768
Epoch: 151 Batch: 2900
Training Loss: 0.010192719461589023
Epoch: 151 Batch: 2950
Training Loss: 0.010603319659071454
Epoch: 151 Batch: 3000
Training Loss: 0.010467556834220886
Epoch: 151 Batch: 3050
Training Loss: 0.010258806347846984
Epoch: 151 Batch: 3100
Training Loss: 0.009537298554374326
Epoch: 151 Batch: 3150
Training Loss: 0.009471020452559941
Epoch: 151 Batch: 3200
Training Loss: 0.00947481238283217
Epoch: 152 
 Validation Loss: 0.4731639484564463
---------------------------
Epoch: 152 Batch: 50
Training Loss: 0.6269164848327636
Epoch: 152 Batch: 100
Training Loss: 0.3134617325663567
Epoch: 152 Batch: 150
Training Loss: 0.21796685218811035
Epoch: 152 Batch: 200
Training Loss: 0.15498180523514749
Epoch: 152 Batch: 250
Training Loss: 0.12448131620883941
Epoch: 152 Batch: 300
Training Loss: 0.10326180199782053
Epoch: 152 Batch: 350
Training Loss: 0.08502721888678415
Epoch: 152 Batch: 400
Training Loss: 0.0790074510872364
Epoch: 152 Batch: 450
Training Loss: 0.06841346118185256
Epoch: 152 Batch: 500
Training Loss: 0.06060060161352158
Epoch: 152 Batch: 550
Training Loss: 0.05256563538854772
Epoch: 152 Batch: 600
Training Loss: 0.04937691971659661
Epoch: 152 Batch: 650
Training Loss: 0.04840567217423366
Epoch: 152 Batch: 700
Training Loss: 0.04384157265935625
Epoch: 152 Batch: 750
Training Loss: 0.04092300041516622
Epoch: 152 Batch: 800
Training Loss: 0.03814678616821766
Epoch: 152 Batch: 850
Training Loss: 0.03676204730482662
Epoch: 152 Batch: 900
Training Loss: 0.033092257115576
Epoch: 152 Batch: 950
Training Loss: 0.03255212774402217
Epoch: 152 Batch: 1000
Training Loss: 0.030891409635543824
Epoch: 152 Batch: 1050
Training Loss: 0.027758344837597437
Epoch: 152 Batch: 1100
Training Loss: 0.027963819449598137
Epoch: 152 Batch: 1150
Training Loss: 0.02698443511258001
Epoch: 152 Batch: 1200
Training Loss: 0.024968206311265628
Epoch: 152 Batch: 1250
Training Loss: 0.023989054107666014
Epoch: 152 Batch: 1300
Training Loss: 0.02281749101785513
Epoch: 152 Batch: 1350
Training Loss: 0.022378464473618402
Epoch: 152 Batch: 1400
Training Loss: 0.021005533869777406
Epoch: 152 Batch: 1450
Training Loss: 0.02142234835131415
Epoch: 152 Batch: 1500
Training Loss: 0.0209721404115359
Epoch: 152 Batch: 1550
Training Loss: 0.019772991807230057
Epoch: 152 Batch: 1600
Training Loss: 0.0197156947478652
Epoch: 152 Batch: 1650
Training Loss: 0.01866516487164931
Epoch: 152 Batch: 1700
Training Loss: 0.018292168448953067
Epoch: 152 Batch: 1750
Training Loss: 0.017434638381004335
Epoch: 152 Batch: 1800
Training Loss: 0.016701391488313674
Epoch: 152 Batch: 1850
Training Loss: 0.016157190880259954
Epoch: 152 Batch: 1900
Training Loss: 0.015279247368636884
Epoch: 152 Batch: 1950
Training Loss: 0.015695426418231084
Epoch: 152 Batch: 2000
Training Loss: 0.015949712350964547
Epoch: 152 Batch: 2050
Training Loss: 0.014295690888311804
Epoch: 152 Batch: 2100
Training Loss: 0.014472339068140303
Epoch: 152 Batch: 2150
Training Loss: 0.013944327013437138
Epoch: 152 Batch: 2200
Training Loss: 0.014224775663831016
Epoch: 152 Batch: 2250
Training Loss: 0.013739627440770468
Epoch: 152 Batch: 2300
Training Loss: 0.013538221626178078
Epoch: 152 Batch: 2350
Training Loss: 0.01326379436127683
Epoch: 152 Batch: 2400
Training Loss: 0.012833074231942495
Epoch: 152 Batch: 2450
Training Loss: 0.011931002736091614
Epoch: 152 Batch: 2500
Training Loss: 0.012082762968540191
Epoch: 152 Batch: 2550
Training Loss: 0.01200863079697478
Epoch: 152 Batch: 2600
Training Loss: 0.011868301652945005
Epoch: 152 Batch: 2650
Training Loss: 0.011028274084037205
Epoch: 152 Batch: 2700
Training Loss: 0.011245868746881132
Epoch: 152 Batch: 2750
Training Loss: 0.011575293638489463
Epoch: 152 Batch: 2800
Training Loss: 0.011267720652478082
Epoch: 152 Batch: 2850
Training Loss: 0.010333371674805357
Epoch: 152 Batch: 2900
Training Loss: 0.009942864292654497
Epoch: 152 Batch: 2950
Training Loss: 0.010639703667769998
Epoch: 152 Batch: 3000
Training Loss: 0.01023458190759023
Epoch: 152 Batch: 3050
Training Loss: 0.009826631878243118
Epoch: 152 Batch: 3100
Training Loss: 0.009643182129629196
Epoch: 152 Batch: 3150
Training Loss: 0.00941800262246813
Epoch: 152 Batch: 3200
Training Loss: 0.009476078031584621
Epoch: 153 
 Validation Loss: 0.4725612673494551
---------------------------
Epoch: 153 Batch: 50
Training Loss: 0.6111199641227723
Epoch: 153 Batch: 100
Training Loss: 0.2927618283033371
Epoch: 153 Batch: 150
Training Loss: 0.2073570587237676
Epoch: 153 Batch: 200
Training Loss: 0.15499118193984032
Epoch: 153 Batch: 250
Training Loss: 0.12352011442184448
Epoch: 153 Batch: 300
Training Loss: 0.0963441076874733
Epoch: 153 Batch: 350
Training Loss: 0.08551303940159934
Epoch: 153 Batch: 400
Training Loss: 0.0734295554459095
Epoch: 153 Batch: 450
Training Loss: 0.0640650925371382
Epoch: 153 Batch: 500
Training Loss: 0.061644147872924805
Epoch: 153 Batch: 550
Training Loss: 0.05688789367675781
Epoch: 153 Batch: 600
Training Loss: 0.050663987547159194
Epoch: 153 Batch: 650
Training Loss: 0.0469737374324065
Epoch: 153 Batch: 700
Training Loss: 0.0449464527623994
Epoch: 153 Batch: 750
Training Loss: 0.04035922574996948
Epoch: 153 Batch: 800
Training Loss: 0.03995362117886543
Epoch: 153 Batch: 850
Training Loss: 0.0355012141606387
Epoch: 153 Batch: 900
Training Loss: 0.03151764545175764
Epoch: 153 Batch: 950
Training Loss: 0.03133190374625357
Epoch: 153 Batch: 1000
Training Loss: 0.030193428456783293
Epoch: 153 Batch: 1050
Training Loss: 0.02979554457323892
Epoch: 153 Batch: 1100
Training Loss: 0.02847258990461176
Epoch: 153 Batch: 1150
Training Loss: 0.026927116720572762
Epoch: 153 Batch: 1200
Training Loss: 0.026590651472409566
Epoch: 153 Batch: 1250
Training Loss: 0.024587156867980957
Epoch: 153 Batch: 1300
Training Loss: 0.023856075841646927
Epoch: 153 Batch: 1350
Training Loss: 0.022295935463022304
Epoch: 153 Batch: 1400
Training Loss: 0.021935548526900155
Epoch: 153 Batch: 1450
Training Loss: 0.021501645815783534
Epoch: 153 Batch: 1500
Training Loss: 0.020881605843702952
Epoch: 153 Batch: 1550
Training Loss: 0.020368641095776713
Epoch: 153 Batch: 1600
Training Loss: 0.018744391556829214
Epoch: 153 Batch: 1650
Training Loss: 0.018335849552443533
Epoch: 153 Batch: 1700
Training Loss: 0.017716880493304308
Epoch: 153 Batch: 1750
Training Loss: 0.017609219721385412
Epoch: 153 Batch: 1800
Training Loss: 0.017508057024743822
Epoch: 153 Batch: 1850
Training Loss: 0.01639535040468783
Epoch: 153 Batch: 1900
Training Loss: 0.015525361660279726
Epoch: 153 Batch: 1950
Training Loss: 0.015709786460949823
Epoch: 153 Batch: 2000
Training Loss: 0.015699366584420203
Epoch: 153 Batch: 2050
Training Loss: 0.014450420882643722
Epoch: 153 Batch: 2100
Training Loss: 0.014050455632663909
Epoch: 153 Batch: 2150
Training Loss: 0.013864156423613083
Epoch: 153 Batch: 2200
Training Loss: 0.014477855454791676
Epoch: 153 Batch: 2250
Training Loss: 0.01376043238904741
Epoch: 153 Batch: 2300
Training Loss: 0.01318909553082093
Epoch: 153 Batch: 2350
Training Loss: 0.01285945809902029
Epoch: 153 Batch: 2400
Training Loss: 0.012803261155883471
Epoch: 153 Batch: 2450
Training Loss: 0.012633445567014265
Epoch: 153 Batch: 2500
Training Loss: 0.012875869607925415
Epoch: 153 Batch: 2550
Training Loss: 0.011980523396940793
Epoch: 153 Batch: 2600
Training Loss: 0.011568124787165568
Epoch: 153 Batch: 2650
Training Loss: 0.012343245024950999
Epoch: 153 Batch: 2700
Training Loss: 0.01118977313792264
Epoch: 153 Batch: 2750
Training Loss: 0.011487781708890742
Epoch: 153 Batch: 2800
Training Loss: 0.01098281673022679
Epoch: 153 Batch: 2850
Training Loss: 0.01082517563251027
Epoch: 153 Batch: 2900
Training Loss: 0.010700008129251414
Epoch: 153 Batch: 2950
Training Loss: 0.009966878577814264
Epoch: 153 Batch: 3000
Training Loss: 0.01007983852426211
Epoch: 153 Batch: 3050
Training Loss: 0.009707227349281311
Epoch: 153 Batch: 3100
Training Loss: 0.009402256761827776
Epoch: 153 Batch: 3150
Training Loss: 0.00973850655177283
Epoch: 153 Batch: 3200
Training Loss: 0.009743373449891806
Epoch: 154 
 Validation Loss: 0.47232519818676844
---------------------------
Epoch: 154 Batch: 50
Training Loss: 0.6204482382535934
Epoch: 154 Batch: 100
Training Loss: 0.30259717255830765
Epoch: 154 Batch: 150
Training Loss: 0.20048061271508535
Epoch: 154 Batch: 200
Training Loss: 0.15537120997905732
Epoch: 154 Batch: 250
Training Loss: 0.12462678790092468
Epoch: 154 Batch: 300
Training Loss: 0.0991657414038976
Epoch: 154 Batch: 350
Training Loss: 0.08703035107680729
Epoch: 154 Batch: 400
Training Loss: 0.07753318406641484
Epoch: 154 Batch: 450
Training Loss: 0.06757524602942996
Epoch: 154 Batch: 500
Training Loss: 0.062433561503887174
Epoch: 154 Batch: 550
Training Loss: 0.055375230800021776
Epoch: 154 Batch: 600
Training Loss: 0.052403031239906944
Epoch: 154 Batch: 650
Training Loss: 0.04768577612363375
Epoch: 154 Batch: 700
Training Loss: 0.042929260560444424
Epoch: 154 Batch: 750
Training Loss: 0.03989169494311015
Epoch: 154 Batch: 800
Training Loss: 0.03775313917547465
Epoch: 154 Batch: 850
Training Loss: 0.03737443124546724
Epoch: 154 Batch: 900
Training Loss: 0.03390882419215308
Epoch: 154 Batch: 950
Training Loss: 0.03254567961943777
Epoch: 154 Batch: 1000
Training Loss: 0.03129419758915901
Epoch: 154 Batch: 1050
Training Loss: 0.029191843697002957
Epoch: 154 Batch: 1100
Training Loss: 0.0271622878854925
Epoch: 154 Batch: 1150
Training Loss: 0.02565641128498575
Epoch: 154 Batch: 1200
Training Loss: 0.02500960665444533
Epoch: 154 Batch: 1250
Training Loss: 0.025020836973190307
Epoch: 154 Batch: 1300
Training Loss: 0.023712335114295668
Epoch: 154 Batch: 1350
Training Loss: 0.022880562367262665
Epoch: 154 Batch: 1400
Training Loss: 0.02198363374386515
Epoch: 154 Batch: 1450
Training Loss: 0.020679485181282306
Epoch: 154 Batch: 1500
Training Loss: 0.019984517236550647
Epoch: 154 Batch: 1550
Training Loss: 0.02026357844952614
Epoch: 154 Batch: 1600
Training Loss: 0.019551121164113283
Epoch: 154 Batch: 1650
Training Loss: 0.01955552740530534
Epoch: 154 Batch: 1700
Training Loss: 0.017476895574261162
Epoch: 154 Batch: 1750
Training Loss: 0.017127959200314114
Epoch: 154 Batch: 1800
Training Loss: 0.017386134250296486
Epoch: 154 Batch: 1850
Training Loss: 0.016574486526283058
Epoch: 154 Batch: 1900
Training Loss: 0.016405571432490097
Epoch: 154 Batch: 1950
Training Loss: 0.015040248250349974
Epoch: 154 Batch: 2000
Training Loss: 0.014919185891747474
Epoch: 154 Batch: 2050
Training Loss: 0.01552287699245825
Epoch: 154 Batch: 2100
Training Loss: 0.014440890068099612
Epoch: 154 Batch: 2150
Training Loss: 0.014183672846749772
Epoch: 154 Batch: 2200
Training Loss: 0.013281329003247348
Epoch: 154 Batch: 2250
Training Loss: 0.013589717825253804
Epoch: 154 Batch: 2300
Training Loss: 0.013294306125329888
Epoch: 154 Batch: 2350
Training Loss: 0.013401611130288307
Epoch: 154 Batch: 2400
Training Loss: 0.012450112563868364
Epoch: 154 Batch: 2450
Training Loss: 0.012519582887085117
Epoch: 154 Batch: 2500
Training Loss: 0.012372605049610137
Epoch: 154 Batch: 2550
Training Loss: 0.012384081868564382
Epoch: 154 Batch: 2600
Training Loss: 0.012230562349924675
Epoch: 154 Batch: 2650
Training Loss: 0.012123098474628521
Epoch: 154 Batch: 2700
Training Loss: 0.01119957819029137
Epoch: 154 Batch: 2750
Training Loss: 0.010913321180777116
Epoch: 154 Batch: 2800
Training Loss: 0.011057930184262139
Epoch: 154 Batch: 2850
Training Loss: 0.01009864742295784
Epoch: 154 Batch: 2900
Training Loss: 0.010535576507962984
Epoch: 154 Batch: 2950
Training Loss: 0.010416867995666245
Epoch: 154 Batch: 3000
Training Loss: 0.010220389048258463
Epoch: 154 Batch: 3050
Training Loss: 0.010009300229979343
Epoch: 154 Batch: 3100
Training Loss: 0.010242574118798779
Epoch: 154 Batch: 3150
Training Loss: 0.009511649116637214
Epoch: 154 Batch: 3200
Training Loss: 0.009654956515878438
Epoch: 155 
 Validation Loss: 0.4725042250421312
---------------------------
Epoch: 155 Batch: 50
Training Loss: 0.6188585293293
Epoch: 155 Batch: 100
Training Loss: 0.289806906580925
Epoch: 155 Batch: 150
Training Loss: 0.20616373320420583
Epoch: 155 Batch: 200
Training Loss: 0.15102241382002832
Epoch: 155 Batch: 250
Training Loss: 0.1279130448102951
Epoch: 155 Batch: 300
Training Loss: 0.10445107936859131
Epoch: 155 Batch: 350
Training Loss: 0.08692893334797451
Epoch: 155 Batch: 400
Training Loss: 0.07562243856489659
Epoch: 155 Batch: 450
Training Loss: 0.0664405365784963
Epoch: 155 Batch: 500
Training Loss: 0.06079170614480972
Epoch: 155 Batch: 550
Training Loss: 0.054581786556677385
Epoch: 155 Batch: 600
Training Loss: 0.04958217213551203
Epoch: 155 Batch: 650
Training Loss: 0.046910249224075905
Epoch: 155 Batch: 700
Training Loss: 0.04109275324004037
Epoch: 155 Batch: 750
Training Loss: 0.03933476893107096
Epoch: 155 Batch: 800
Training Loss: 0.037711534686386584
Epoch: 155 Batch: 850
Training Loss: 0.036295285049606774
Epoch: 155 Batch: 900
Training Loss: 0.03304632574319839
Epoch: 155 Batch: 950
Training Loss: 0.031009858595697503
Epoch: 155 Batch: 1000
Training Loss: 0.03098118495941162
Epoch: 155 Batch: 1050
Training Loss: 0.028113584830647423
Epoch: 155 Batch: 1100
Training Loss: 0.030079166293144225
Epoch: 155 Batch: 1150
Training Loss: 0.026273007263307988
Epoch: 155 Batch: 1200
Training Loss: 0.02599963073929151
Epoch: 155 Batch: 1250
Training Loss: 0.024411438179016115
Epoch: 155 Batch: 1300
Training Loss: 0.024334307679763207
Epoch: 155 Batch: 1350
Training Loss: 0.02257126450538635
Epoch: 155 Batch: 1400
Training Loss: 0.02088014279093061
Epoch: 155 Batch: 1450
Training Loss: 0.02039958076230411
Epoch: 155 Batch: 1500
Training Loss: 0.020182763040065766
Epoch: 155 Batch: 1550
Training Loss: 0.01951385876824779
Epoch: 155 Batch: 1600
Training Loss: 0.019072028920054437
Epoch: 155 Batch: 1650
Training Loss: 0.019220354069362986
Epoch: 155 Batch: 1700
Training Loss: 0.01839013059349621
Epoch: 155 Batch: 1750
Training Loss: 0.01801618276323591
Epoch: 155 Batch: 1800
Training Loss: 0.01658711228105757
Epoch: 155 Batch: 1850
Training Loss: 0.01706398451650465
Epoch: 155 Batch: 1900
Training Loss: 0.016354587815309827
Epoch: 155 Batch: 1950
Training Loss: 0.01508635109815842
Epoch: 155 Batch: 2000
Training Loss: 0.015290451824665069
Epoch: 155 Batch: 2050
Training Loss: 0.014355217247474484
Epoch: 155 Batch: 2100
Training Loss: 0.014523698957193465
Epoch: 155 Batch: 2150
Training Loss: 0.013795993771663932
Epoch: 155 Batch: 2200
Training Loss: 0.014124594832008536
Epoch: 155 Batch: 2250
Training Loss: 0.013565001156595018
Epoch: 155 Batch: 2300
Training Loss: 0.01302506553090137
Epoch: 155 Batch: 2350
Training Loss: 0.01251213031880399
Epoch: 155 Batch: 2400
Training Loss: 0.012271450906991959
Epoch: 155 Batch: 2450
Training Loss: 0.012806457937980185
Epoch: 155 Batch: 2500
Training Loss: 0.011685746598243714
Epoch: 155 Batch: 2550
Training Loss: 0.012191284018404344
Epoch: 155 Batch: 2600
Training Loss: 0.011918920370248647
Epoch: 155 Batch: 2650
Training Loss: 0.011679283324277626
Epoch: 155 Batch: 2700
Training Loss: 0.012098698516686757
Epoch: 155 Batch: 2750
Training Loss: 0.011216553590514444
Epoch: 155 Batch: 2800
Training Loss: 0.011238991320133208
Epoch: 155 Batch: 2850
Training Loss: 0.010917772629804779
Epoch: 155 Batch: 2900
Training Loss: 0.010254561325599407
Epoch: 155 Batch: 2950
Training Loss: 0.010063202886258142
Epoch: 155 Batch: 3000
Training Loss: 0.009875745634237926
Epoch: 155 Batch: 3050
Training Loss: 0.009605250065443946
Epoch: 155 Batch: 3100
Training Loss: 0.010005281798301204
Epoch: 155 Batch: 3150
Training Loss: 0.009665246757249984
Epoch: 155 Batch: 3200
Training Loss: 0.009591318871825934
Epoch: 156 
 Validation Loss: 0.47193042304780747
---------------------------
Epoch: 156 Batch: 50
Training Loss: 0.6424184727668762
Epoch: 156 Batch: 100
Training Loss: 0.31365711212158204
Epoch: 156 Batch: 150
Training Loss: 0.20043270587921141
Epoch: 156 Batch: 200
Training Loss: 0.15889640927314758
Epoch: 156 Batch: 250
Training Loss: 0.1179897369146347
Epoch: 156 Batch: 300
Training Loss: 0.10462899655103683
Epoch: 156 Batch: 350
Training Loss: 0.08498154606137957
Epoch: 156 Batch: 400
Training Loss: 0.0741330287605524
Epoch: 156 Batch: 450
Training Loss: 0.0669090633922153
Epoch: 156 Batch: 500
Training Loss: 0.06236002147197724
Epoch: 156 Batch: 550
Training Loss: 0.056634937849911776
Epoch: 156 Batch: 600
Training Loss: 0.05008696511387825
Epoch: 156 Batch: 650
Training Loss: 0.04525366310889904
Epoch: 156 Batch: 700
Training Loss: 0.04307161991085325
Epoch: 156 Batch: 750
Training Loss: 0.04084151530265808
Epoch: 156 Batch: 800
Training Loss: 0.0382005700096488
Epoch: 156 Batch: 850
Training Loss: 0.03417475788032307
Epoch: 156 Batch: 900
Training Loss: 0.03398727734883626
Epoch: 156 Batch: 950
Training Loss: 0.0331473151633614
Epoch: 156 Batch: 1000
Training Loss: 0.031142318159341813
Epoch: 156 Batch: 1050
Training Loss: 0.030053594424611047
Epoch: 156 Batch: 1100
Training Loss: 0.028141795694828033
Epoch: 156 Batch: 1150
Training Loss: 0.026130723175795182
Epoch: 156 Batch: 1200
Training Loss: 0.025057343070705732
Epoch: 156 Batch: 1250
Training Loss: 0.02420984742641449
Epoch: 156 Batch: 1300
Training Loss: 0.023587371317239907
Epoch: 156 Batch: 1350
Training Loss: 0.021858990479398658
Epoch: 156 Batch: 1400
Training Loss: 0.021212394769702637
Epoch: 156 Batch: 1450
Training Loss: 0.020861520088952163
Epoch: 156 Batch: 1500
Training Loss: 0.020645067910353344
Epoch: 156 Batch: 1550
Training Loss: 0.020691271616566566
Epoch: 156 Batch: 1600
Training Loss: 0.01860498147085309
Epoch: 156 Batch: 1650
Training Loss: 0.018702001824523463
Epoch: 156 Batch: 1700
Training Loss: 0.017962885761962217
Epoch: 156 Batch: 1750
Training Loss: 0.01770121398993901
Epoch: 156 Batch: 1800
Training Loss: 0.017510076231426662
Epoch: 156 Batch: 1850
Training Loss: 0.01652593382307001
Epoch: 156 Batch: 1900
Training Loss: 0.015566456349272477
Epoch: 156 Batch: 1950
Training Loss: 0.015770676579230872
Epoch: 156 Batch: 2000
Training Loss: 0.01507761549949646
Epoch: 156 Batch: 2050
Training Loss: 0.014939411602369168
Epoch: 156 Batch: 2100
Training Loss: 0.014641915006296975
Epoch: 156 Batch: 2150
Training Loss: 0.014284267785937287
Epoch: 156 Batch: 2200
Training Loss: 0.014476699178869073
Epoch: 156 Batch: 2250
Training Loss: 0.013277947637769912
Epoch: 156 Batch: 2300
Training Loss: 0.012920074566550878
Epoch: 156 Batch: 2350
Training Loss: 0.012704028677433095
Epoch: 156 Batch: 2400
Training Loss: 0.01267274223268032
Epoch: 156 Batch: 2450
Training Loss: 0.012375695085038944
Epoch: 156 Batch: 2500
Training Loss: 0.012735844933986664
Epoch: 156 Batch: 2550
Training Loss: 0.011967781022483227
Epoch: 156 Batch: 2600
Training Loss: 0.01148739246221689
Epoch: 156 Batch: 2650
Training Loss: 0.011949128845952593
Epoch: 156 Batch: 2700
Training Loss: 0.01168422692351871
Epoch: 156 Batch: 2750
Training Loss: 0.011417622251944108
Epoch: 156 Batch: 2800
Training Loss: 0.010777031523840769
Epoch: 156 Batch: 2850
Training Loss: 0.010574116884616383
Epoch: 156 Batch: 2900
Training Loss: 0.010597789513653722
Epoch: 156 Batch: 2950
Training Loss: 0.011016095038187707
Epoch: 156 Batch: 3000
Training Loss: 0.01033781228462855
Epoch: 156 Batch: 3050
Training Loss: 0.00994354805985435
Epoch: 156 Batch: 3100
Training Loss: 0.009644166173473481
Epoch: 156 Batch: 3150
Training Loss: 0.009530200295978123
Epoch: 156 Batch: 3200
Training Loss: 0.00935285714454949
Epoch: 157 
 Validation Loss: 0.47262777752346463
---------------------------
Epoch: 157 Batch: 50
Training Loss: 0.6252015739679336
Epoch: 157 Batch: 100
Training Loss: 0.30356599390506744
Epoch: 157 Batch: 150
Training Loss: 0.20988089859485626
Epoch: 157 Batch: 200
Training Loss: 0.14638574436306953
Epoch: 157 Batch: 250
Training Loss: 0.12306635916233062
Epoch: 157 Batch: 300
Training Loss: 0.10266070614258448
Epoch: 157 Batch: 350
Training Loss: 0.08677802758557455
Epoch: 157 Batch: 400
Training Loss: 0.07549313805997372
Epoch: 157 Batch: 450
Training Loss: 0.0701824301481247
Epoch: 157 Batch: 500
Training Loss: 0.06184299582242966
Epoch: 157 Batch: 550
Training Loss: 0.05677626398476687
Epoch: 157 Batch: 600
Training Loss: 0.05013260553280512
Epoch: 157 Batch: 650
Training Loss: 0.047232911174113934
Epoch: 157 Batch: 700
Training Loss: 0.04130503267049789
Epoch: 157 Batch: 750
Training Loss: 0.041570674022038775
Epoch: 157 Batch: 800
Training Loss: 0.03812465120106936
Epoch: 157 Batch: 850
Training Loss: 0.03686528942164253
Epoch: 157 Batch: 900
Training Loss: 0.034180666539404124
Epoch: 157 Batch: 950
Training Loss: 0.03156064842876635
Epoch: 157 Batch: 1000
Training Loss: 0.03070246812701225
Epoch: 157 Batch: 1050
Training Loss: 0.028168227246829443
Epoch: 157 Batch: 1100
Training Loss: 0.027425152551044118
Epoch: 157 Batch: 1150
Training Loss: 0.02779989392861076
Epoch: 157 Batch: 1200
Training Loss: 0.025196052491664886
Epoch: 157 Batch: 1250
Training Loss: 0.0244932368516922
Epoch: 157 Batch: 1300
Training Loss: 0.024183391745273883
Epoch: 157 Batch: 1350
Training Loss: 0.023551297496866297
Epoch: 157 Batch: 1400
Training Loss: 0.021276865197079522
Epoch: 157 Batch: 1450
Training Loss: 0.020025793190660147
Epoch: 157 Batch: 1500
Training Loss: 0.01916257796684901
Epoch: 157 Batch: 1550
Training Loss: 0.01977691763831723
Epoch: 157 Batch: 1600
Training Loss: 0.019110827594995498
Epoch: 157 Batch: 1650
Training Loss: 0.01915944187930136
Epoch: 157 Batch: 1700
Training Loss: 0.018454732947489793
Epoch: 157 Batch: 1750
Training Loss: 0.01744308125972748
Epoch: 157 Batch: 1800
Training Loss: 0.01750728502869606
Epoch: 157 Batch: 1850
Training Loss: 0.01638287729508168
Epoch: 157 Batch: 1900
Training Loss: 0.016174169182777406
Epoch: 157 Batch: 1950
Training Loss: 0.015937131444613138
Epoch: 157 Batch: 2000
Training Loss: 0.015192940801382065
Epoch: 157 Batch: 2050
Training Loss: 0.015308506198045684
Epoch: 157 Batch: 2100
Training Loss: 0.014128249699161165
Epoch: 157 Batch: 2150
Training Loss: 0.014364476980164994
Epoch: 157 Batch: 2200
Training Loss: 0.014761482761664824
Epoch: 157 Batch: 2250
Training Loss: 0.013904338545269436
Epoch: 157 Batch: 2300
Training Loss: 0.013869401695935623
Epoch: 157 Batch: 2350
Training Loss: 0.013974693874095348
Epoch: 157 Batch: 2400
Training Loss: 0.012866866489251455
Epoch: 157 Batch: 2450
Training Loss: 0.012235216924122402
Epoch: 157 Batch: 2500
Training Loss: 0.011645110344886779
Epoch: 157 Batch: 2550
Training Loss: 0.011135989661310233
Epoch: 157 Batch: 2600
Training Loss: 0.011639108474438007
Epoch: 157 Batch: 2650
Training Loss: 0.01146049605225617
Epoch: 157 Batch: 2700
Training Loss: 0.0116848263586009
Epoch: 157 Batch: 2750
Training Loss: 0.011260201421650974
Epoch: 157 Batch: 2800
Training Loss: 0.01130412512591907
Epoch: 157 Batch: 2850
Training Loss: 0.009996052969966019
Epoch: 157 Batch: 2900
Training Loss: 0.010635428603353172
Epoch: 157 Batch: 2950
Training Loss: 0.010644766597424523
Epoch: 157 Batch: 3000
Training Loss: 0.010129422664642333
Epoch: 157 Batch: 3050
Training Loss: 0.009844089179742532
Epoch: 157 Batch: 3100
Training Loss: 0.009761771569328923
Epoch: 157 Batch: 3150
Training Loss: 0.009418532933507646
Epoch: 157 Batch: 3200
Training Loss: 0.009558936255052686
Epoch: 158 
 Validation Loss: 0.47251737780041164
---------------------------
Epoch: 158 Batch: 50
Training Loss: 0.612838802933693
Epoch: 158 Batch: 100
Training Loss: 0.29739390552043915
Epoch: 158 Batch: 150
Training Loss: 0.2064207512140274
Epoch: 158 Batch: 200
Training Loss: 0.15933172166347503
Epoch: 158 Batch: 250
Training Loss: 0.12196471607685089
Epoch: 158 Batch: 300
Training Loss: 0.10475048810243606
Epoch: 158 Batch: 350
Training Loss: 0.08867537021636963
Epoch: 158 Batch: 400
Training Loss: 0.0776708384603262
Epoch: 158 Batch: 450
Training Loss: 0.06610622551706102
Epoch: 158 Batch: 500
Training Loss: 0.06146028500795364
Epoch: 158 Batch: 550
Training Loss: 0.0552219033241272
Epoch: 158 Batch: 600
Training Loss: 0.05122304623325666
Epoch: 158 Batch: 650
Training Loss: 0.04819096092994397
Epoch: 158 Batch: 700
Training Loss: 0.04381255515984127
Epoch: 158 Batch: 750
Training Loss: 0.041535846789677935
Epoch: 158 Batch: 800
Training Loss: 0.03801821190863848
Epoch: 158 Batch: 850
Training Loss: 0.03583043438546798
Epoch: 158 Batch: 900
Training Loss: 0.03379425611760881
Epoch: 158 Batch: 950
Training Loss: 0.032179490955252396
Epoch: 158 Batch: 1000
Training Loss: 0.03083302628993988
Epoch: 158 Batch: 1050
Training Loss: 0.029475590671811786
Epoch: 158 Batch: 1100
Training Loss: 0.026634711000052366
Epoch: 158 Batch: 1150
Training Loss: 0.026509595383768497
Epoch: 158 Batch: 1200
Training Loss: 0.025681937088569005
Epoch: 158 Batch: 1250
Training Loss: 0.024468861722946166
Epoch: 158 Batch: 1300
Training Loss: 0.024231806236964006
Epoch: 158 Batch: 1350
Training Loss: 0.023098824244958384
Epoch: 158 Batch: 1400
Training Loss: 0.022183345513684408
Epoch: 158 Batch: 1450
Training Loss: 0.02027111330936695
Epoch: 158 Batch: 1500
Training Loss: 0.020653971831003825
Epoch: 158 Batch: 1550
Training Loss: 0.019508598792937495
Epoch: 158 Batch: 1600
Training Loss: 0.01775083499029279
Epoch: 158 Batch: 1650
Training Loss: 0.018745788155180035
Epoch: 158 Batch: 1700
Training Loss: 0.01886884010889951
Epoch: 158 Batch: 1750
Training Loss: 0.018053986975124905
Epoch: 158 Batch: 1800
Training Loss: 0.01659193077021175
Epoch: 158 Batch: 1850
Training Loss: 0.016816178818006774
Epoch: 158 Batch: 1900
Training Loss: 0.015815182657618273
Epoch: 158 Batch: 1950
Training Loss: 0.016207891886050884
Epoch: 158 Batch: 2000
Training Loss: 0.014726116970181466
Epoch: 158 Batch: 2050
Training Loss: 0.014738458787522665
Epoch: 158 Batch: 2100
Training Loss: 0.013842523168949853
Epoch: 158 Batch: 2150
Training Loss: 0.014239838275798531
Epoch: 158 Batch: 2200
Training Loss: 0.01405188498171893
Epoch: 158 Batch: 2250
Training Loss: 0.013257877786954244
Epoch: 158 Batch: 2300
Training Loss: 0.013155908377274223
Epoch: 158 Batch: 2350
Training Loss: 0.012319814004796616
Epoch: 158 Batch: 2400
Training Loss: 0.013166512586176396
Epoch: 158 Batch: 2450
Training Loss: 0.012503530407438472
Epoch: 158 Batch: 2500
Training Loss: 0.012627909219264984
Epoch: 158 Batch: 2550
Training Loss: 0.012209252621613298
Epoch: 158 Batch: 2600
Training Loss: 0.011729204001334998
Epoch: 158 Batch: 2650
Training Loss: 0.011526438357695094
Epoch: 158 Batch: 2700
Training Loss: 0.010998105693746495
Epoch: 158 Batch: 2750
Training Loss: 0.011545682332732461
Epoch: 158 Batch: 2800
Training Loss: 0.010790181383490562
Epoch: 158 Batch: 2850
Training Loss: 0.010723459908836766
Epoch: 158 Batch: 2900
Training Loss: 0.010538200253042682
Epoch: 158 Batch: 2950
Training Loss: 0.010372852016303499
Epoch: 158 Batch: 3000
Training Loss: 0.009915400038162867
Epoch: 158 Batch: 3050
Training Loss: 0.010146954079143337
Epoch: 158 Batch: 3100
Training Loss: 0.00957477609957418
Epoch: 158 Batch: 3150
Training Loss: 0.009617252491769337
Epoch: 158 Batch: 3200
Training Loss: 0.009413478933274745
Epoch: 159 
 Validation Loss: 0.4717669619454278
---------------------------
Epoch: 159 Batch: 50
Training Loss: 0.6367334455251694
Epoch: 159 Batch: 100
Training Loss: 0.30692792415618897
Epoch: 159 Batch: 150
Training Loss: 0.2073687674601873
Epoch: 159 Batch: 200
Training Loss: 0.1548988202214241
Epoch: 159 Batch: 250
Training Loss: 0.12464247977733613
Epoch: 159 Batch: 300
Training Loss: 0.10237516880035401
Epoch: 159 Batch: 350
Training Loss: 0.08451764822006226
Epoch: 159 Batch: 400
Training Loss: 0.07678997717797756
Epoch: 159 Batch: 450
Training Loss: 0.06935376107692719
Epoch: 159 Batch: 500
Training Loss: 0.06114182877540588
Epoch: 159 Batch: 550
Training Loss: 0.05172353706576607
Epoch: 159 Batch: 600
Training Loss: 0.04947155987222989
Epoch: 159 Batch: 650
Training Loss: 0.048013746050687936
Epoch: 159 Batch: 700
Training Loss: 0.045255414119788576
Epoch: 159 Batch: 750
Training Loss: 0.04019051138559977
Epoch: 159 Batch: 800
Training Loss: 0.03717432599514723
Epoch: 159 Batch: 850
Training Loss: 0.0351799677400028
Epoch: 159 Batch: 900
Training Loss: 0.03357818921407064
Epoch: 159 Batch: 950
Training Loss: 0.03194994079439264
Epoch: 159 Batch: 1000
Training Loss: 0.031231240391731262
Epoch: 159 Batch: 1050
Training Loss: 0.028878096881366912
Epoch: 159 Batch: 1100
Training Loss: 0.028003744889389386
Epoch: 159 Batch: 1150
Training Loss: 0.0259107220950334
Epoch: 159 Batch: 1200
Training Loss: 0.024795165384809175
Epoch: 159 Batch: 1250
Training Loss: 0.02468317928314209
Epoch: 159 Batch: 1300
Training Loss: 0.024073538161241092
Epoch: 159 Batch: 1350
Training Loss: 0.023500786666516903
Epoch: 159 Batch: 1400
Training Loss: 0.02211818262934685
Epoch: 159 Batch: 1450
Training Loss: 0.020569000922400375
Epoch: 159 Batch: 1500
Training Loss: 0.020655942181746163
Epoch: 159 Batch: 1550
Training Loss: 0.019599435194846124
Epoch: 159 Batch: 1600
Training Loss: 0.01872002985328436
Epoch: 159 Batch: 1650
Training Loss: 0.018098623788718023
Epoch: 159 Batch: 1700
Training Loss: 0.017531188603709724
Epoch: 159 Batch: 1750
Training Loss: 0.01697607776096889
Epoch: 159 Batch: 1800
Training Loss: 0.01628023804889785
Epoch: 159 Batch: 1850
Training Loss: 0.01582635413955998
Epoch: 159 Batch: 1900
Training Loss: 0.016516819643346888
Epoch: 159 Batch: 1950
Training Loss: 0.015554235440034132
Epoch: 159 Batch: 2000
Training Loss: 0.01466950711607933
Epoch: 159 Batch: 2050
Training Loss: 0.014872998755152633
Epoch: 159 Batch: 2100
Training Loss: 0.014597009065605345
Epoch: 159 Batch: 2150
Training Loss: 0.014160552121872125
Epoch: 159 Batch: 2200
Training Loss: 0.013470245112072338
Epoch: 159 Batch: 2250
Training Loss: 0.013396351628833347
Epoch: 159 Batch: 2300
Training Loss: 0.013602959397046463
Epoch: 159 Batch: 2350
Training Loss: 0.012933595497557458
Epoch: 159 Batch: 2400
Training Loss: 0.012615797234078248
Epoch: 159 Batch: 2450
Training Loss: 0.012638954860823495
Epoch: 159 Batch: 2500
Training Loss: 0.012444631850719453
Epoch: 159 Batch: 2550
Training Loss: 0.011703469648080713
Epoch: 159 Batch: 2600
Training Loss: 0.011853229701519013
Epoch: 159 Batch: 2650
Training Loss: 0.011754422513943798
Epoch: 159 Batch: 2700
Training Loss: 0.011135583542011402
Epoch: 159 Batch: 2750
Training Loss: 0.011306155746633356
Epoch: 159 Batch: 2800
Training Loss: 0.011294740810990333
Epoch: 159 Batch: 2850
Training Loss: 0.010480031026037116
Epoch: 159 Batch: 2900
Training Loss: 0.010327926925544082
Epoch: 159 Batch: 2950
Training Loss: 0.010625914294840926
Epoch: 159 Batch: 3000
Training Loss: 0.009898102392752964
Epoch: 159 Batch: 3050
Training Loss: 0.010776231992440146
Epoch: 159 Batch: 3100
Training Loss: 0.00978967220552506
Epoch: 159 Batch: 3150
Training Loss: 0.009308528862302266
Epoch: 159 Batch: 3200
Training Loss: 0.009194484138861298
Epoch: 160 
 Validation Loss: 0.47227072748872967
---------------------------
Epoch: 160 Batch: 50
Training Loss: 0.6205261951684952
Epoch: 160 Batch: 100
Training Loss: 0.2998059743642807
Epoch: 160 Batch: 150
Training Loss: 0.20315348227818808
Epoch: 160 Batch: 200
Training Loss: 0.14938524901866912
Epoch: 160 Batch: 250
Training Loss: 0.11826955807209015
Epoch: 160 Batch: 300
Training Loss: 0.09809061388174693
Epoch: 160 Batch: 350
Training Loss: 0.08785134945596967
Epoch: 160 Batch: 400
Training Loss: 0.07636965736746788
Epoch: 160 Batch: 450
Training Loss: 0.06703904933399624
Epoch: 160 Batch: 500
Training Loss: 0.0611105694770813
Epoch: 160 Batch: 550
Training Loss: 0.0572865783626383
Epoch: 160 Batch: 600
Training Loss: 0.051271333744128546
Epoch: 160 Batch: 650
Training Loss: 0.04883922347655663
Epoch: 160 Batch: 700
Training Loss: 0.04314874913011278
Epoch: 160 Batch: 750
Training Loss: 0.04146532150109609
Epoch: 160 Batch: 800
Training Loss: 0.03864941839128733
Epoch: 160 Batch: 850
Training Loss: 0.0356800100032021
Epoch: 160 Batch: 900
Training Loss: 0.032647085852093166
Epoch: 160 Batch: 950
Training Loss: 0.03237590871359173
Epoch: 160 Batch: 1000
Training Loss: 0.030934373944997786
Epoch: 160 Batch: 1050
Training Loss: 0.029915756498064314
Epoch: 160 Batch: 1100
Training Loss: 0.027669666572050614
Epoch: 160 Batch: 1150
Training Loss: 0.027162204151568205
Epoch: 160 Batch: 1200
Training Loss: 0.02509479857981205
Epoch: 160 Batch: 1250
Training Loss: 0.02394728968143463
Epoch: 160 Batch: 1300
Training Loss: 0.023919605566905096
Epoch: 160 Batch: 1350
Training Loss: 0.021794937407528914
Epoch: 160 Batch: 1400
Training Loss: 0.021397742586476462
Epoch: 160 Batch: 1450
Training Loss: 0.02176712379373353
Epoch: 160 Batch: 1500
Training Loss: 0.021213834365208944
Epoch: 160 Batch: 1550
Training Loss: 0.020215553468273532
Epoch: 160 Batch: 1600
Training Loss: 0.019117935318499802
Epoch: 160 Batch: 1650
Training Loss: 0.018776155800530405
Epoch: 160 Batch: 1700
Training Loss: 0.017817861613105324
Epoch: 160 Batch: 1750
Training Loss: 0.01762453201838902
Epoch: 160 Batch: 1800
Training Loss: 0.017092976338333555
Epoch: 160 Batch: 1850
Training Loss: 0.016185430384971
Epoch: 160 Batch: 1900
Training Loss: 0.01610356424984179
Epoch: 160 Batch: 1950
Training Loss: 0.014980644033505366
Epoch: 160 Batch: 2000
Training Loss: 0.015861423656344412
Epoch: 160 Batch: 2050
Training Loss: 0.014411357859285866
Epoch: 160 Batch: 2100
Training Loss: 0.014673258023602621
Epoch: 160 Batch: 2150
Training Loss: 0.01409052383067996
Epoch: 160 Batch: 2200
Training Loss: 0.013781535869294946
Epoch: 160 Batch: 2250
Training Loss: 0.013131360703044468
Epoch: 160 Batch: 2300
Training Loss: 0.013766746119312619
Epoch: 160 Batch: 2350
Training Loss: 0.013098690902933162
Epoch: 160 Batch: 2400
Training Loss: 0.012678944244980813
Epoch: 160 Batch: 2450
Training Loss: 0.012369871200347434
Epoch: 160 Batch: 2500
Training Loss: 0.011833648145198822
Epoch: 160 Batch: 2550
Training Loss: 0.012150973934753269
Epoch: 160 Batch: 2600
Training Loss: 0.011942109339512312
Epoch: 160 Batch: 2650
Training Loss: 0.011241502244517488
Epoch: 160 Batch: 2700
Training Loss: 0.011149676375918918
Epoch: 160 Batch: 2750
Training Loss: 0.01036629348451441
Epoch: 160 Batch: 2800
Training Loss: 0.010962664431759288
Epoch: 160 Batch: 2850
Training Loss: 0.010816430878220942
Epoch: 160 Batch: 2900
Training Loss: 0.01028869835467174
Epoch: 160 Batch: 2950
Training Loss: 0.010626294784626717
Epoch: 160 Batch: 3000
Training Loss: 0.009885826379060745
Epoch: 160 Batch: 3050
Training Loss: 0.010114218876010082
Epoch: 160 Batch: 3100
Training Loss: 0.010281650183662292
Epoch: 160 Batch: 3150
Training Loss: 0.009544847210248312
Epoch: 160 Batch: 3200
Training Loss: 0.009268960040062666
Epoch: 161 
 Validation Loss: 0.471610849764612
---------------------------
Epoch: 161 Batch: 50
Training Loss: 0.5941732752323151
Epoch: 161 Batch: 100
Training Loss: 0.31492333322763444
Epoch: 161 Batch: 150
Training Loss: 0.20100158154964448
Epoch: 161 Batch: 200
Training Loss: 0.15121058315038682
Epoch: 161 Batch: 250
Training Loss: 0.11960109508037567
Epoch: 161 Batch: 300
Training Loss: 0.1007294429341952
Epoch: 161 Batch: 350
Training Loss: 0.08868760219642094
Epoch: 161 Batch: 400
Training Loss: 0.07406969800591469
Epoch: 161 Batch: 450
Training Loss: 0.06805122541056739
Epoch: 161 Batch: 500
Training Loss: 0.05951545006036758
Epoch: 161 Batch: 550
Training Loss: 0.053960990905761716
Epoch: 161 Batch: 600
Training Loss: 0.04889106929302216
Epoch: 161 Batch: 650
Training Loss: 0.04678189410613133
Epoch: 161 Batch: 700
Training Loss: 0.046244605651923586
Epoch: 161 Batch: 750
Training Loss: 0.04286143012841542
Epoch: 161 Batch: 800
Training Loss: 0.037976680025458336
Epoch: 161 Batch: 850
Training Loss: 0.036585441862835605
Epoch: 161 Batch: 900
Training Loss: 0.03280279792017407
Epoch: 161 Batch: 950
Training Loss: 0.03240925741823096
Epoch: 161 Batch: 1000
Training Loss: 0.03141166535019874
Epoch: 161 Batch: 1050
Training Loss: 0.03071472182160332
Epoch: 161 Batch: 1100
Training Loss: 0.02702526078982787
Epoch: 161 Batch: 1150
Training Loss: 0.026716654585755388
Epoch: 161 Batch: 1200
Training Loss: 0.02624615522722403
Epoch: 161 Batch: 1250
Training Loss: 0.024611415576934813
Epoch: 161 Batch: 1300
Training Loss: 0.023385739693274864
Epoch: 161 Batch: 1350
Training Loss: 0.022308067701481006
Epoch: 161 Batch: 1400
Training Loss: 0.022013518065214156
Epoch: 161 Batch: 1450
Training Loss: 0.020908712929692763
Epoch: 161 Batch: 1500
Training Loss: 0.019685621281464893
Epoch: 161 Batch: 1550
Training Loss: 0.019777117948378286
Epoch: 161 Batch: 1600
Training Loss: 0.018393575679510832
Epoch: 161 Batch: 1650
Training Loss: 0.01847411937785871
Epoch: 161 Batch: 1700
Training Loss: 0.01849953481379677
Epoch: 161 Batch: 1750
Training Loss: 0.018197989565985544
Epoch: 161 Batch: 1800
Training Loss: 0.017100006937980652
Epoch: 161 Batch: 1850
Training Loss: 0.016693518741710767
Epoch: 161 Batch: 1900
Training Loss: 0.01593653989465613
Epoch: 161 Batch: 1950
Training Loss: 0.01585771870918763
Epoch: 161 Batch: 2000
Training Loss: 0.015577593624591827
Epoch: 161 Batch: 2050
Training Loss: 0.014765094242444852
Epoch: 161 Batch: 2100
Training Loss: 0.014533588531471435
Epoch: 161 Batch: 2150
Training Loss: 0.013713831457980843
Epoch: 161 Batch: 2200
Training Loss: 0.013860963623632084
Epoch: 161 Batch: 2250
Training Loss: 0.013697553515434266
Epoch: 161 Batch: 2300
Training Loss: 0.013336635063523831
Epoch: 161 Batch: 2350
Training Loss: 0.012729160050128368
Epoch: 161 Batch: 2400
Training Loss: 0.012668878224988779
Epoch: 161 Batch: 2450
Training Loss: 0.012013583256273854
Epoch: 161 Batch: 2500
Training Loss: 0.012604012846946717
Epoch: 161 Batch: 2550
Training Loss: 0.012362347560770372
Epoch: 161 Batch: 2600
Training Loss: 0.011587695995202431
Epoch: 161 Batch: 2650
Training Loss: 0.01153809368610382
Epoch: 161 Batch: 2700
Training Loss: 0.011935462289386325
Epoch: 161 Batch: 2750
Training Loss: 0.010373525814576583
Epoch: 161 Batch: 2800
Training Loss: 0.010636469359908786
Epoch: 161 Batch: 2850
Training Loss: 0.01070878667789593
Epoch: 161 Batch: 2900
Training Loss: 0.010343668666379206
Epoch: 161 Batch: 2950
Training Loss: 0.010821533799171447
Epoch: 161 Batch: 3000
Training Loss: 0.009808682779471079
Epoch: 161 Batch: 3050
Training Loss: 0.009941694365173089
Epoch: 161 Batch: 3100
Training Loss: 0.009797105856480137
Epoch: 161 Batch: 3150
Training Loss: 0.009942395441115848
Epoch: 161 Batch: 3200
Training Loss: 0.009528127741068601
Epoch: 162 
 Validation Loss: 0.47160854935646057
---------------------------
Epoch: 162 Batch: 50
Training Loss: 0.6459662264585495
Epoch: 162 Batch: 100
Training Loss: 0.3077906447649002
Epoch: 162 Batch: 150
Training Loss: 0.20463168521722158
Epoch: 162 Batch: 200
Training Loss: 0.15578180223703383
Epoch: 162 Batch: 250
Training Loss: 0.12865491247177124
Epoch: 162 Batch: 300
Training Loss: 0.10575255264838536
Epoch: 162 Batch: 350
Training Loss: 0.09381431826523372
Epoch: 162 Batch: 400
Training Loss: 0.07491773158311844
Epoch: 162 Batch: 450
Training Loss: 0.06860875791973538
Epoch: 162 Batch: 500
Training Loss: 0.0603095098733902
Epoch: 162 Batch: 550
Training Loss: 0.05558552362702109
Epoch: 162 Batch: 600
Training Loss: 0.05298103556036949
Epoch: 162 Batch: 650
Training Loss: 0.04834727227687836
Epoch: 162 Batch: 700
Training Loss: 0.044465232150895255
Epoch: 162 Batch: 750
Training Loss: 0.04023251994450887
Epoch: 162 Batch: 800
Training Loss: 0.03936533790081739
Epoch: 162 Batch: 850
Training Loss: 0.03611843491301817
Epoch: 162 Batch: 900
Training Loss: 0.03548882514238358
Epoch: 162 Batch: 950
Training Loss: 0.031197771430015564
Epoch: 162 Batch: 1000
Training Loss: 0.030804595589637756
Epoch: 162 Batch: 1050
Training Loss: 0.029048923481078376
Epoch: 162 Batch: 1100
Training Loss: 0.02674015933817083
Epoch: 162 Batch: 1150
Training Loss: 0.027089180376218713
Epoch: 162 Batch: 1200
Training Loss: 0.025503018101056416
Epoch: 162 Batch: 1250
Training Loss: 0.024259294986724852
Epoch: 162 Batch: 1300
Training Loss: 0.022307728804074802
Epoch: 162 Batch: 1350
Training Loss: 0.021207777239658214
Epoch: 162 Batch: 1400
Training Loss: 0.02036556661128998
Epoch: 162 Batch: 1450
Training Loss: 0.02026306834714166
Epoch: 162 Batch: 1500
Training Loss: 0.02078270689646403
Epoch: 162 Batch: 1550
Training Loss: 0.020077831629783875
Epoch: 162 Batch: 1600
Training Loss: 0.018442788831889628
Epoch: 162 Batch: 1650
Training Loss: 0.017853711717056506
Epoch: 162 Batch: 1700
Training Loss: 0.01739201801664689
Epoch: 162 Batch: 1750
Training Loss: 0.01745088025501796
Epoch: 162 Batch: 1800
Training Loss: 0.017784438961082033
Epoch: 162 Batch: 1850
Training Loss: 0.01685194463343234
Epoch: 162 Batch: 1900
Training Loss: 0.015352357233825482
Epoch: 162 Batch: 1950
Training Loss: 0.016163473908717817
Epoch: 162 Batch: 2000
Training Loss: 0.015273248866200448
Epoch: 162 Batch: 2050
Training Loss: 0.013877439280835594
Epoch: 162 Batch: 2100
Training Loss: 0.014804438324201676
Epoch: 162 Batch: 2150
Training Loss: 0.015011093214500781
Epoch: 162 Batch: 2200
Training Loss: 0.012966373630545355
Epoch: 162 Batch: 2250
Training Loss: 0.01330826649400923
Epoch: 162 Batch: 2300
Training Loss: 0.01319357860347499
Epoch: 162 Batch: 2350
Training Loss: 0.013310843820267535
Epoch: 162 Batch: 2400
Training Loss: 0.012419444719950358
Epoch: 162 Batch: 2450
Training Loss: 0.012508340064360171
Epoch: 162 Batch: 2500
Training Loss: 0.0116659153342247
Epoch: 162 Batch: 2550
Training Loss: 0.012048757461940542
Epoch: 162 Batch: 2600
Training Loss: 0.011094356293861683
Epoch: 162 Batch: 2650
Training Loss: 0.011695221954921507
Epoch: 162 Batch: 2700
Training Loss: 0.010999436731691714
Epoch: 162 Batch: 2750
Training Loss: 0.011215513305230573
Epoch: 162 Batch: 2800
Training Loss: 0.011516372646604265
Epoch: 162 Batch: 2850
Training Loss: 0.01111808298972615
Epoch: 162 Batch: 2900
Training Loss: 0.010412684802351326
Epoch: 162 Batch: 2950
Training Loss: 0.010394717151835813
Epoch: 162 Batch: 3000
Training Loss: 0.009499799827734629
Epoch: 162 Batch: 3050
Training Loss: 0.010460692067615322
Epoch: 162 Batch: 3100
Training Loss: 0.009642143662898771
Epoch: 162 Batch: 3150
Training Loss: 0.009905346310327924
Epoch: 162 Batch: 3200
Training Loss: 0.009363602055236698
Epoch: 163 
 Validation Loss: 0.4716828207174937
---------------------------
Epoch: 163 Batch: 50
Training Loss: 0.5934759521484375
Epoch: 163 Batch: 100
Training Loss: 0.33429823845624923
Epoch: 163 Batch: 150
Training Loss: 0.20413409312566122
Epoch: 163 Batch: 200
Training Loss: 0.15507024511694908
Epoch: 163 Batch: 250
Training Loss: 0.12756922626495362
Epoch: 163 Batch: 300
Training Loss: 0.10106259365876516
Epoch: 163 Batch: 350
Training Loss: 0.0912653272492545
Epoch: 163 Batch: 400
Training Loss: 0.07682372123003006
Epoch: 163 Batch: 450
Training Loss: 0.06569359792603387
Epoch: 163 Batch: 500
Training Loss: 0.061832974791526794
Epoch: 163 Batch: 550
Training Loss: 0.05647942418401892
Epoch: 163 Batch: 600
Training Loss: 0.0519947346051534
Epoch: 163 Batch: 650
Training Loss: 0.04651561961724208
Epoch: 163 Batch: 700
Training Loss: 0.04414310736315591
Epoch: 163 Batch: 750
Training Loss: 0.04007521140575409
Epoch: 163 Batch: 800
Training Loss: 0.03731968823820352
Epoch: 163 Batch: 850
Training Loss: 0.03569292261320002
Epoch: 163 Batch: 900
Training Loss: 0.03522445201873779
Epoch: 163 Batch: 950
Training Loss: 0.03377816601803428
Epoch: 163 Batch: 1000
Training Loss: 0.029459190756082533
Epoch: 163 Batch: 1050
Training Loss: 0.028816060934747968
Epoch: 163 Batch: 1100
Training Loss: 0.02717290301214565
Epoch: 163 Batch: 1150
Training Loss: 0.026386719657027203
Epoch: 163 Batch: 1200
Training Loss: 0.025747309376796087
Epoch: 163 Batch: 1250
Training Loss: 0.024522820162773133
Epoch: 163 Batch: 1300
Training Loss: 0.02195997600372021
Epoch: 163 Batch: 1350
Training Loss: 0.023455354593418264
Epoch: 163 Batch: 1400
Training Loss: 0.021653063339846474
Epoch: 163 Batch: 1450
Training Loss: 0.020714809524601904
Epoch: 163 Batch: 1500
Training Loss: 0.020491312364737193
Epoch: 163 Batch: 1550
Training Loss: 0.02054392985759243
Epoch: 163 Batch: 1600
Training Loss: 0.018038790095597505
Epoch: 163 Batch: 1650
Training Loss: 0.018185655944275132
Epoch: 163 Batch: 1700
Training Loss: 0.019220445033381967
Epoch: 163 Batch: 1750
Training Loss: 0.017707527654511588
Epoch: 163 Batch: 1800
Training Loss: 0.01707198056909773
Epoch: 163 Batch: 1850
Training Loss: 0.016551594170364173
Epoch: 163 Batch: 1900
Training Loss: 0.015637208568422416
Epoch: 163 Batch: 1950
Training Loss: 0.015174866746633481
Epoch: 163 Batch: 2000
Training Loss: 0.015023308113217353
Epoch: 163 Batch: 2050
Training Loss: 0.014615029111141112
Epoch: 163 Batch: 2100
Training Loss: 0.014421199631123316
Epoch: 163 Batch: 2150
Training Loss: 0.014335094205168791
Epoch: 163 Batch: 2200
Training Loss: 0.01409746221520684
Epoch: 163 Batch: 2250
Training Loss: 0.013931729078292846
Epoch: 163 Batch: 2300
Training Loss: 0.013323096516339675
Epoch: 163 Batch: 2350
Training Loss: 0.013360097319521803
Epoch: 163 Batch: 2400
Training Loss: 0.012608304346601169
Epoch: 163 Batch: 2450
Training Loss: 0.012518751390126287
Epoch: 163 Batch: 2500
Training Loss: 0.012926752161979676
Epoch: 163 Batch: 2550
Training Loss: 0.01179762835596122
Epoch: 163 Batch: 2600
Training Loss: 0.011902390168263362
Epoch: 163 Batch: 2650
Training Loss: 0.012092815064034371
Epoch: 163 Batch: 2700
Training Loss: 0.010946934046568694
Epoch: 163 Batch: 2750
Training Loss: 0.011067279360511086
Epoch: 163 Batch: 2800
Training Loss: 0.011032041577356202
Epoch: 163 Batch: 2850
Training Loss: 0.010595709806994388
Epoch: 163 Batch: 2900
Training Loss: 0.010324865240475227
Epoch: 163 Batch: 2950
Training Loss: 0.010017570034932283
Epoch: 163 Batch: 3000
Training Loss: 0.010032376706600189
Epoch: 163 Batch: 3050
Training Loss: 0.009566428319352572
Epoch: 163 Batch: 3100
Training Loss: 0.009638582669919536
Epoch: 163 Batch: 3150
Training Loss: 0.009678271223628332
Epoch: 163 Batch: 3200
Training Loss: 0.009418872874230147
Epoch: 164 
 Validation Loss: 0.4715582599242528
---------------------------
Epoch: 164 Batch: 50
Training Loss: 0.5980002105236053
Epoch: 164 Batch: 100
Training Loss: 0.31358696669340136
Epoch: 164 Batch: 150
Training Loss: 0.19715954045454662
Epoch: 164 Batch: 200
Training Loss: 0.16174656614661218
Epoch: 164 Batch: 250
Training Loss: 0.12260526001453399
Epoch: 164 Batch: 300
Training Loss: 0.09961656947930654
Epoch: 164 Batch: 350
Training Loss: 0.08738435626029968
Epoch: 164 Batch: 400
Training Loss: 0.07761242240667343
Epoch: 164 Batch: 450
Training Loss: 0.06807948489983877
Epoch: 164 Batch: 500
Training Loss: 0.06210247796773911
Epoch: 164 Batch: 550
Training Loss: 0.05656695864417336
Epoch: 164 Batch: 600
Training Loss: 0.05080271114905675
Epoch: 164 Batch: 650
Training Loss: 0.045073868586466864
Epoch: 164 Batch: 700
Training Loss: 0.04252911614520209
Epoch: 164 Batch: 750
Training Loss: 0.04151351499557495
Epoch: 164 Batch: 800
Training Loss: 0.036611363068223
Epoch: 164 Batch: 850
Training Loss: 0.03512101036660811
Epoch: 164 Batch: 900
Training Loss: 0.03309899148013857
Epoch: 164 Batch: 950
Training Loss: 0.0330401873588562
Epoch: 164 Batch: 1000
Training Loss: 0.030966180235147477
Epoch: 164 Batch: 1050
Training Loss: 0.028704330211593992
Epoch: 164 Batch: 1100
Training Loss: 0.028317794528874483
Epoch: 164 Batch: 1150
Training Loss: 0.02591053009033203
Epoch: 164 Batch: 1200
Training Loss: 0.024559205745657287
Epoch: 164 Batch: 1250
Training Loss: 0.02413637104034424
Epoch: 164 Batch: 1300
Training Loss: 0.023530872051532453
Epoch: 164 Batch: 1350
Training Loss: 0.021838978484824852
Epoch: 164 Batch: 1400
Training Loss: 0.02157446590917451
Epoch: 164 Batch: 1450
Training Loss: 0.02090797348269101
Epoch: 164 Batch: 1500
Training Loss: 0.020637939612070718
Epoch: 164 Batch: 1550
Training Loss: 0.01962336238353483
Epoch: 164 Batch: 1600
Training Loss: 0.01891912054270506
Epoch: 164 Batch: 1650
Training Loss: 0.01922577903126225
Epoch: 164 Batch: 1700
Training Loss: 0.01742384603794883
Epoch: 164 Batch: 1750
Training Loss: 0.01747859105042049
Epoch: 164 Batch: 1800
Training Loss: 0.017270232919189665
Epoch: 164 Batch: 1850
Training Loss: 0.01675585416523186
Epoch: 164 Batch: 1900
Training Loss: 0.015991772366197487
Epoch: 164 Batch: 1950
Training Loss: 0.015169865626555222
Epoch: 164 Batch: 2000
Training Loss: 0.01578827713429928
Epoch: 164 Batch: 2050
Training Loss: 0.01468603500505773
Epoch: 164 Batch: 2100
Training Loss: 0.014511770308017731
Epoch: 164 Batch: 2150
Training Loss: 0.014392267243806706
Epoch: 164 Batch: 2200
Training Loss: 0.01457888581536033
Epoch: 164 Batch: 2250
Training Loss: 0.01389850976732042
Epoch: 164 Batch: 2300
Training Loss: 0.013253789582978124
Epoch: 164 Batch: 2350
Training Loss: 0.012466137269709972
Epoch: 164 Batch: 2400
Training Loss: 0.013035022715727488
Epoch: 164 Batch: 2450
Training Loss: 0.01266755291393825
Epoch: 164 Batch: 2500
Training Loss: 0.012323753154277802
Epoch: 164 Batch: 2550
Training Loss: 0.012305202799684861
Epoch: 164 Batch: 2600
Training Loss: 0.011498758884576651
Epoch: 164 Batch: 2650
Training Loss: 0.011434268816462104
Epoch: 164 Batch: 2700
Training Loss: 0.011308360828293694
Epoch: 164 Batch: 2750
Training Loss: 0.01143224106051705
Epoch: 164 Batch: 2800
Training Loss: 0.010805704753313746
Epoch: 164 Batch: 2850
Training Loss: 0.0112568665073629
Epoch: 164 Batch: 2900
Training Loss: 0.01060560689917926
Epoch: 164 Batch: 2950
Training Loss: 0.010192087080519078
Epoch: 164 Batch: 3000
Training Loss: 0.010499467591444652
Epoch: 164 Batch: 3050
Training Loss: 0.00997098910026863
Epoch: 164 Batch: 3100
Training Loss: 0.00976797382677755
Epoch: 164 Batch: 3150
Training Loss: 0.009850168237610469
Epoch: 164 Batch: 3200
Training Loss: 0.0094387464877218
Epoch: 165 
 Validation Loss: 0.47109529707166886
---------------------------
Epoch: 165 Batch: 50
Training Loss: 0.6223171985149384
Epoch: 165 Batch: 100
Training Loss: 0.31324607580900193
Epoch: 165 Batch: 150
Training Loss: 0.1958192898829778
Epoch: 165 Batch: 200
Training Loss: 0.15429038628935815
Epoch: 165 Batch: 250
Training Loss: 0.11991298115253449
Epoch: 165 Batch: 300
Training Loss: 0.09931698153416316
Epoch: 165 Batch: 350
Training Loss: 0.08617590052740914
Epoch: 165 Batch: 400
Training Loss: 0.07872890800237656
Epoch: 165 Batch: 450
Training Loss: 0.06863664733039009
Epoch: 165 Batch: 500
Training Loss: 0.061622636675834654
Epoch: 165 Batch: 550
Training Loss: 0.05551711981946772
Epoch: 165 Batch: 600
Training Loss: 0.05175670633713404
Epoch: 165 Batch: 650
Training Loss: 0.050717485226117645
Epoch: 165 Batch: 700
Training Loss: 0.04267141010080065
Epoch: 165 Batch: 750
Training Loss: 0.04118049601713816
Epoch: 165 Batch: 800
Training Loss: 0.03789231866598129
Epoch: 165 Batch: 850
Training Loss: 0.036343614984961116
Epoch: 165 Batch: 900
Training Loss: 0.033317381805843774
Epoch: 165 Batch: 950
Training Loss: 0.033185835041497885
Epoch: 165 Batch: 1000
Training Loss: 0.02984978449344635
Epoch: 165 Batch: 1050
Training Loss: 0.029526484069370088
Epoch: 165 Batch: 1100
Training Loss: 0.02718678357926282
Epoch: 165 Batch: 1150
Training Loss: 0.026983349711998648
Epoch: 165 Batch: 1200
Training Loss: 0.026052102223038675
Epoch: 165 Batch: 1250
Training Loss: 0.024292431902885436
Epoch: 165 Batch: 1300
Training Loss: 0.023629802419589115
Epoch: 165 Batch: 1350
Training Loss: 0.02306920102349034
Epoch: 165 Batch: 1400
Training Loss: 0.022086773748908724
Epoch: 165 Batch: 1450
Training Loss: 0.020944185071978077
Epoch: 165 Batch: 1500
Training Loss: 0.020471324304739635
Epoch: 165 Batch: 1550
Training Loss: 0.019403654086974358
Epoch: 165 Batch: 1600
Training Loss: 0.019697236716747283
Epoch: 165 Batch: 1650
Training Loss: 0.01822787866447911
Epoch: 165 Batch: 1700
Training Loss: 0.01711984979755738
Epoch: 165 Batch: 1750
Training Loss: 0.018033764907291958
Epoch: 165 Batch: 1800
Training Loss: 0.017384703639480803
Epoch: 165 Batch: 1850
Training Loss: 0.016643218623625267
Epoch: 165 Batch: 1900
Training Loss: 0.01584352356822867
Epoch: 165 Batch: 1950
Training Loss: 0.015314970031762734
Epoch: 165 Batch: 2000
Training Loss: 0.015696547895669936
Epoch: 165 Batch: 2050
Training Loss: 0.014869661054960112
Epoch: 165 Batch: 2100
Training Loss: 0.014935823735736665
Epoch: 165 Batch: 2150
Training Loss: 0.013818971043409303
Epoch: 165 Batch: 2200
Training Loss: 0.014209754670208151
Epoch: 165 Batch: 2250
Training Loss: 0.01369909640153249
Epoch: 165 Batch: 2300
Training Loss: 0.013366497469984967
Epoch: 165 Batch: 2350
Training Loss: 0.012570296069408985
Epoch: 165 Batch: 2400
Training Loss: 0.012884294725954532
Epoch: 165 Batch: 2450
Training Loss: 0.012933488950437428
Epoch: 165 Batch: 2500
Training Loss: 0.012293083477020263
Epoch: 165 Batch: 2550
Training Loss: 0.011507942162308039
Epoch: 165 Batch: 2600
Training Loss: 0.01153940187050746
Epoch: 165 Batch: 2650
Training Loss: 0.011467446887268211
Epoch: 165 Batch: 2700
Training Loss: 0.01145360893673367
Epoch: 165 Batch: 2750
Training Loss: 0.010986219828779047
Epoch: 165 Batch: 2800
Training Loss: 0.011110346146992274
Epoch: 165 Batch: 2850
Training Loss: 0.010795287333036724
Epoch: 165 Batch: 2900
Training Loss: 0.010390969648443419
Epoch: 165 Batch: 2950
Training Loss: 0.010738028348502467
Epoch: 165 Batch: 3000
Training Loss: 0.009894787659247716
Epoch: 165 Batch: 3050
Training Loss: 0.010145841748988043
Epoch: 165 Batch: 3100
Training Loss: 0.009660004627320074
Epoch: 165 Batch: 3150
Training Loss: 0.009695282370325119
Epoch: 165 Batch: 3200
Training Loss: 0.009693871363997459
Epoch: 166 
 Validation Loss: 0.4711730119254854
---------------------------
Epoch: 166 Batch: 50
Training Loss: 0.6348895370960236
Epoch: 166 Batch: 100
Training Loss: 0.2957552409172058
Epoch: 166 Batch: 150
Training Loss: 0.20131420095761618
Epoch: 166 Batch: 200
Training Loss: 0.15686728656291962
Epoch: 166 Batch: 250
Training Loss: 0.11779850518703461
Epoch: 166 Batch: 300
Training Loss: 0.10333776086568833
Epoch: 166 Batch: 350
Training Loss: 0.08720496194703238
Epoch: 166 Batch: 400
Training Loss: 0.07501324824988842
Epoch: 166 Batch: 450
Training Loss: 0.06750048769844903
Epoch: 166 Batch: 500
Training Loss: 0.06042857420444488
Epoch: 166 Batch: 550
Training Loss: 0.055955635851079766
Epoch: 166 Batch: 600
Training Loss: 0.04886763374010722
Epoch: 166 Batch: 650
Training Loss: 0.047471462946671705
Epoch: 166 Batch: 700
Training Loss: 0.04417270728519985
Epoch: 166 Batch: 750
Training Loss: 0.039436474482218424
Epoch: 166 Batch: 800
Training Loss: 0.03710328493267298
Epoch: 166 Batch: 850
Training Loss: 0.03610212515382206
Epoch: 166 Batch: 900
Training Loss: 0.0340658500790596
Epoch: 166 Batch: 950
Training Loss: 0.030191298284028705
Epoch: 166 Batch: 1000
Training Loss: 0.030003293603658675
Epoch: 166 Batch: 1050
Training Loss: 0.030010327923865546
Epoch: 166 Batch: 1100
Training Loss: 0.028479441621086815
Epoch: 166 Batch: 1150
Training Loss: 0.025540375994599383
Epoch: 166 Batch: 1200
Training Loss: 0.02647868645687898
Epoch: 166 Batch: 1250
Training Loss: 0.025051410007476807
Epoch: 166 Batch: 1300
Training Loss: 0.025028752363645112
Epoch: 166 Batch: 1350
Training Loss: 0.022723367147975498
Epoch: 166 Batch: 1400
Training Loss: 0.021709304600954056
Epoch: 166 Batch: 1450
Training Loss: 0.020970394508592012
Epoch: 166 Batch: 1500
Training Loss: 0.020251133739948274
Epoch: 166 Batch: 1550
Training Loss: 0.018694398191667374
Epoch: 166 Batch: 1600
Training Loss: 0.018545239455997942
Epoch: 166 Batch: 1650
Training Loss: 0.017986908410534713
Epoch: 166 Batch: 1700
Training Loss: 0.017602654835757087
Epoch: 166 Batch: 1750
Training Loss: 0.01742191667216165
Epoch: 166 Batch: 1800
Training Loss: 0.01648151460621092
Epoch: 166 Batch: 1850
Training Loss: 0.016861154662596214
Epoch: 166 Batch: 1900
Training Loss: 0.01612152673696217
Epoch: 166 Batch: 1950
Training Loss: 0.015355806197875585
Epoch: 166 Batch: 2000
Training Loss: 0.014898878619074822
Epoch: 166 Batch: 2050
Training Loss: 0.01487012530245432
Epoch: 166 Batch: 2100
Training Loss: 0.015259479099795931
Epoch: 166 Batch: 2150
Training Loss: 0.014633112494335618
Epoch: 166 Batch: 2200
Training Loss: 0.013542078679258173
Epoch: 166 Batch: 2250
Training Loss: 0.01302825215127733
Epoch: 166 Batch: 2300
Training Loss: 0.012761988782364389
Epoch: 166 Batch: 2350
Training Loss: 0.012434068601182166
Epoch: 166 Batch: 2400
Training Loss: 0.012787337812284628
Epoch: 166 Batch: 2450
Training Loss: 0.012401065108727436
Epoch: 166 Batch: 2500
Training Loss: 0.011746675086021424
Epoch: 166 Batch: 2550
Training Loss: 0.011955863868488984
Epoch: 166 Batch: 2600
Training Loss: 0.01211254209280014
Epoch: 166 Batch: 2650
Training Loss: 0.01187482051129611
Epoch: 166 Batch: 2700
Training Loss: 0.011927382758370152
Epoch: 166 Batch: 2750
Training Loss: 0.011154017199169506
Epoch: 166 Batch: 2800
Training Loss: 0.010693837521331651
Epoch: 166 Batch: 2850
Training Loss: 0.010711153484227364
Epoch: 166 Batch: 2900
Training Loss: 0.010221695930793367
Epoch: 166 Batch: 2950
Training Loss: 0.010540049348847341
Epoch: 166 Batch: 3000
Training Loss: 0.010673272212346395
Epoch: 166 Batch: 3050
Training Loss: 0.01008060386923493
Epoch: 166 Batch: 3100
Training Loss: 0.010173311233520508
Epoch: 166 Batch: 3150
Training Loss: 0.009679982189148192
Epoch: 166 Batch: 3200
Training Loss: 0.009340088479220868
Epoch: 167 
 Validation Loss: 0.4709222455819448
---------------------------
Epoch: 167 Batch: 50
Training Loss: 0.582931552529335
Epoch: 167 Batch: 100
Training Loss: 0.290637823343277
Epoch: 167 Batch: 150
Training Loss: 0.20474582970142363
Epoch: 167 Batch: 200
Training Loss: 0.14904319152235984
Epoch: 167 Batch: 250
Training Loss: 0.1216960334777832
Epoch: 167 Batch: 300
Training Loss: 0.1040200126171112
Epoch: 167 Batch: 350
Training Loss: 0.08492375263145992
Epoch: 167 Batch: 400
Training Loss: 0.07854325756430626
Epoch: 167 Batch: 450
Training Loss: 0.06749413245254093
Epoch: 167 Batch: 500
Training Loss: 0.06049556541442871
Epoch: 167 Batch: 550
Training Loss: 0.055671894171021204
Epoch: 167 Batch: 600
Training Loss: 0.05456974357366562
Epoch: 167 Batch: 650
Training Loss: 0.047644224992165196
Epoch: 167 Batch: 700
Training Loss: 0.043647415212222505
Epoch: 167 Batch: 750
Training Loss: 0.04156416845321655
Epoch: 167 Batch: 800
Training Loss: 0.03889677330851555
Epoch: 167 Batch: 850
Training Loss: 0.0357475285319721
Epoch: 167 Batch: 900
Training Loss: 0.03523642384343677
Epoch: 167 Batch: 950
Training Loss: 0.03308934550536306
Epoch: 167 Batch: 1000
Training Loss: 0.030672962963581085
Epoch: 167 Batch: 1050
Training Loss: 0.028940926619938443
Epoch: 167 Batch: 1100
Training Loss: 0.02821045076305216
Epoch: 167 Batch: 1150
Training Loss: 0.025579052748887436
Epoch: 167 Batch: 1200
Training Loss: 0.025056777025262513
Epoch: 167 Batch: 1250
Training Loss: 0.024507261395454408
Epoch: 167 Batch: 1300
Training Loss: 0.02337047741963313
Epoch: 167 Batch: 1350
Training Loss: 0.022076580391989814
Epoch: 167 Batch: 1400
Training Loss: 0.021635774565594536
Epoch: 167 Batch: 1450
Training Loss: 0.02143266229793943
Epoch: 167 Batch: 1500
Training Loss: 0.020299785137176514
Epoch: 167 Batch: 1550
Training Loss: 0.019817307072301064
Epoch: 167 Batch: 1600
Training Loss: 0.01793212752789259
Epoch: 167 Batch: 1650
Training Loss: 0.018372266726060345
Epoch: 167 Batch: 1700
Training Loss: 0.01703588564606274
Epoch: 167 Batch: 1750
Training Loss: 0.017463137251990182
Epoch: 167 Batch: 1800
Training Loss: 0.017349248263570996
Epoch: 167 Batch: 1850
Training Loss: 0.01638435091521289
Epoch: 167 Batch: 1900
Training Loss: 0.015949235150688572
Epoch: 167 Batch: 1950
Training Loss: 0.015919957069250253
Epoch: 167 Batch: 2000
Training Loss: 0.015387829929590225
Epoch: 167 Batch: 2050
Training Loss: 0.015009049627839065
Epoch: 167 Batch: 2100
Training Loss: 0.014998693934508733
Epoch: 167 Batch: 2150
Training Loss: 0.01422620952129364
Epoch: 167 Batch: 2200
Training Loss: 0.013650069412860004
Epoch: 167 Batch: 2250
Training Loss: 0.013191671093304953
Epoch: 167 Batch: 2300
Training Loss: 0.012953350310740264
Epoch: 167 Batch: 2350
Training Loss: 0.012918147038906178
Epoch: 167 Batch: 2400
Training Loss: 0.012578390588363011
Epoch: 167 Batch: 2450
Training Loss: 0.012349765945454033
Epoch: 167 Batch: 2500
Training Loss: 0.012383811497688293
Epoch: 167 Batch: 2550
Training Loss: 0.012214918008037642
Epoch: 167 Batch: 2600
Training Loss: 0.011896967601317626
Epoch: 167 Batch: 2650
Training Loss: 0.011442212181271246
Epoch: 167 Batch: 2700
Training Loss: 0.011381501456101735
Epoch: 167 Batch: 2750
Training Loss: 0.011108282869512385
Epoch: 167 Batch: 2800
Training Loss: 0.011099681769098554
Epoch: 167 Batch: 2850
Training Loss: 0.010194633822692069
Epoch: 167 Batch: 2900
Training Loss: 0.010345578779434336
Epoch: 167 Batch: 2950
Training Loss: 0.01015943934351711
Epoch: 167 Batch: 3000
Training Loss: 0.00957290048400561
Epoch: 167 Batch: 3050
Training Loss: 0.010042214843093372
Epoch: 167 Batch: 3100
Training Loss: 0.01007934566467039
Epoch: 167 Batch: 3150
Training Loss: 0.009555032423564366
Epoch: 167 Batch: 3200
Training Loss: 0.009276845240965486
Epoch: 168 
 Validation Loss: 0.47106554309527077
---------------------------
Epoch: 168 Batch: 50
Training Loss: 0.6116403877735138
Epoch: 168 Batch: 100
Training Loss: 0.31187223106622697
Epoch: 168 Batch: 150
Training Loss: 0.19379797359307607
Epoch: 168 Batch: 200
Training Loss: 0.1501963509619236
Epoch: 168 Batch: 250
Training Loss: 0.11940137553215027
Epoch: 168 Batch: 300
Training Loss: 0.10407702535390854
Epoch: 168 Batch: 350
Training Loss: 0.0854875522000449
Epoch: 168 Batch: 400
Training Loss: 0.07205753527581692
Epoch: 168 Batch: 450
Training Loss: 0.0655843080414666
Epoch: 168 Batch: 500
Training Loss: 0.05985214811563492
Epoch: 168 Batch: 550
Training Loss: 0.05544288776137612
Epoch: 168 Batch: 600
Training Loss: 0.052106783588727314
Epoch: 168 Batch: 650
Training Loss: 0.04735935733868526
Epoch: 168 Batch: 700
Training Loss: 0.042846942118235996
Epoch: 168 Batch: 750
Training Loss: 0.0392486629486084
Epoch: 168 Batch: 800
Training Loss: 0.03887419071048498
Epoch: 168 Batch: 850
Training Loss: 0.03513384521007538
Epoch: 168 Batch: 900
Training Loss: 0.032436383366584776
Epoch: 168 Batch: 950
Training Loss: 0.03143353710049077
Epoch: 168 Batch: 1000
Training Loss: 0.030377530694007872
Epoch: 168 Batch: 1050
Training Loss: 0.028821144444601876
Epoch: 168 Batch: 1100
Training Loss: 0.027572821839289233
Epoch: 168 Batch: 1150
Training Loss: 0.027055888435114986
Epoch: 168 Batch: 1200
Training Loss: 0.02552260642250379
Epoch: 168 Batch: 1250
Training Loss: 0.024222774004936217
Epoch: 168 Batch: 1300
Training Loss: 0.023386275768280028
Epoch: 168 Batch: 1350
Training Loss: 0.021493333379427593
Epoch: 168 Batch: 1400
Training Loss: 0.020261796350990024
Epoch: 168 Batch: 1450
Training Loss: 0.020853188633918763
Epoch: 168 Batch: 1500
Training Loss: 0.020039543469746908
Epoch: 168 Batch: 1550
Training Loss: 0.019220122310423082
Epoch: 168 Batch: 1600
Training Loss: 0.018830296006053685
Epoch: 168 Batch: 1650
Training Loss: 0.017766891302484454
Epoch: 168 Batch: 1700
Training Loss: 0.017844686245217043
Epoch: 168 Batch: 1750
Training Loss: 0.017180992433003018
Epoch: 168 Batch: 1800
Training Loss: 0.016764449344740973
Epoch: 168 Batch: 1850
Training Loss: 0.015992842332736866
Epoch: 168 Batch: 1900
Training Loss: 0.016062490265620382
Epoch: 168 Batch: 1950
Training Loss: 0.015538058907557757
Epoch: 168 Batch: 2000
Training Loss: 0.01586749692261219
Epoch: 168 Batch: 2050
Training Loss: 0.014806553852267382
Epoch: 168 Batch: 2100
Training Loss: 0.014650868219988686
Epoch: 168 Batch: 2150
Training Loss: 0.0140523369506348
Epoch: 168 Batch: 2200
Training Loss: 0.013375865952535109
Epoch: 168 Batch: 2250
Training Loss: 0.014091618167029486
Epoch: 168 Batch: 2300
Training Loss: 0.013236947279909383
Epoch: 168 Batch: 2350
Training Loss: 0.013387831893373043
Epoch: 168 Batch: 2400
Training Loss: 0.012691829564670722
Epoch: 168 Batch: 2450
Training Loss: 0.012518956722045432
Epoch: 168 Batch: 2500
Training Loss: 0.01205066339969635
Epoch: 168 Batch: 2550
Training Loss: 0.012031402658013737
Epoch: 168 Batch: 2600
Training Loss: 0.012269787547680048
Epoch: 168 Batch: 2650
Training Loss: 0.011811282533519672
Epoch: 168 Batch: 2700
Training Loss: 0.011557738030398333
Epoch: 168 Batch: 2750
Training Loss: 0.01146174541386691
Epoch: 168 Batch: 2800
Training Loss: 0.010173197122556822
Epoch: 168 Batch: 2850
Training Loss: 0.010032238489703128
Epoch: 168 Batch: 2900
Training Loss: 0.010667829924616321
Epoch: 168 Batch: 2950
Training Loss: 0.010398984106920533
Epoch: 168 Batch: 3000
Training Loss: 0.009791364759206772
Epoch: 168 Batch: 3050
Training Loss: 0.010256703613234348
Epoch: 168 Batch: 3100
Training Loss: 0.009945291348042026
Epoch: 168 Batch: 3150
Training Loss: 0.009472165098265995
Epoch: 168 Batch: 3200
Training Loss: 0.009712763512507081
Epoch: 169 
 Validation Loss: 0.47081755333476594
---------------------------
Epoch: 169 Batch: 50
Training Loss: 0.6270890778303146
Epoch: 169 Batch: 100
Training Loss: 0.3134161570668221
Epoch: 169 Batch: 150
Training Loss: 0.21213527937730153
Epoch: 169 Batch: 200
Training Loss: 0.14463348105549811
Epoch: 169 Batch: 250
Training Loss: 0.12432116866111756
Epoch: 169 Batch: 300
Training Loss: 0.10478287438551585
Epoch: 169 Batch: 350
Training Loss: 0.08890247762203217
Epoch: 169 Batch: 400
Training Loss: 0.0766329587996006
Epoch: 169 Batch: 450
Training Loss: 0.0678800481557846
Epoch: 169 Batch: 500
Training Loss: 0.06421397638320923
Epoch: 169 Batch: 550
Training Loss: 0.055821466716853056
Epoch: 169 Batch: 600
Training Loss: 0.0514488655825456
Epoch: 169 Batch: 650
Training Loss: 0.04853426786569449
Epoch: 169 Batch: 700
Training Loss: 0.04463953239577157
Epoch: 169 Batch: 750
Training Loss: 0.039802118182182315
Epoch: 169 Batch: 800
Training Loss: 0.03714133400470018
Epoch: 169 Batch: 850
Training Loss: 0.03550001852652606
Epoch: 169 Batch: 900
Training Loss: 0.034208328028519946
Epoch: 169 Batch: 950
Training Loss: 0.03212667066799967
Epoch: 169 Batch: 1000
Training Loss: 0.0306408674120903
Epoch: 169 Batch: 1050
Training Loss: 0.02985011699653807
Epoch: 169 Batch: 1100
Training Loss: 0.028839871937578376
Epoch: 169 Batch: 1150
Training Loss: 0.02628417061722797
Epoch: 169 Batch: 1200
Training Loss: 0.025079212561249734
Epoch: 169 Batch: 1250
Training Loss: 0.024434974050521852
Epoch: 169 Batch: 1300
Training Loss: 0.02429925017631971
Epoch: 169 Batch: 1350
Training Loss: 0.022922227757948416
Epoch: 169 Batch: 1400
Training Loss: 0.021805584111383985
Epoch: 169 Batch: 1450
Training Loss: 0.02088154634525036
Epoch: 169 Batch: 1500
Training Loss: 0.02087537846962611
Epoch: 169 Batch: 1550
Training Loss: 0.019373983990761543
Epoch: 169 Batch: 1600
Training Loss: 0.019266584906727075
Epoch: 169 Batch: 1650
Training Loss: 0.01860420817678625
Epoch: 169 Batch: 1700
Training Loss: 0.017791442818501417
Epoch: 169 Batch: 1750
Training Loss: 0.017152740461485728
Epoch: 169 Batch: 1800
Training Loss: 0.016520897812313504
Epoch: 169 Batch: 1850
Training Loss: 0.016464432220201233
Epoch: 169 Batch: 1900
Training Loss: 0.016496008760050723
Epoch: 169 Batch: 1950
Training Loss: 0.015709239183328092
Epoch: 169 Batch: 2000
Training Loss: 0.01468581485748291
Epoch: 169 Batch: 2050
Training Loss: 0.014980874047046754
Epoch: 169 Batch: 2100
Training Loss: 0.014425803025563557
Epoch: 169 Batch: 2150
Training Loss: 0.014581306410390278
Epoch: 169 Batch: 2200
Training Loss: 0.013708518946712668
Epoch: 169 Batch: 2250
Training Loss: 0.013885662237803142
Epoch: 169 Batch: 2300
Training Loss: 0.01355686921140422
Epoch: 169 Batch: 2350
Training Loss: 0.012798556419129067
Epoch: 169 Batch: 2400
Training Loss: 0.012577627922097842
Epoch: 169 Batch: 2450
Training Loss: 0.01260732072956708
Epoch: 169 Batch: 2500
Training Loss: 0.012544274938106536
Epoch: 169 Batch: 2550
Training Loss: 0.011942003942003437
Epoch: 169 Batch: 2600
Training Loss: 0.011785015761852265
Epoch: 169 Batch: 2650
Training Loss: 0.011435866996927082
Epoch: 169 Batch: 2700
Training Loss: 0.011200781906092609
Epoch: 169 Batch: 2750
Training Loss: 0.010805824225599116
Epoch: 169 Batch: 2800
Training Loss: 0.010812065984521594
Epoch: 169 Batch: 2850
Training Loss: 0.01072162762022855
Epoch: 169 Batch: 2900
Training Loss: 0.010250366319870127
Epoch: 169 Batch: 2950
Training Loss: 0.010271815101979142
Epoch: 169 Batch: 3000
Training Loss: 0.010276954611142477
Epoch: 169 Batch: 3050
Training Loss: 0.010113249430890944
Epoch: 169 Batch: 3100
Training Loss: 0.009577125426261655
Epoch: 169 Batch: 3150
Training Loss: 0.009584907728528219
Epoch: 169 Batch: 3200
Training Loss: 0.010153466137126088
Epoch: 170 
 Validation Loss: 0.47098219328456453
---------------------------
Epoch: 170 Batch: 50
Training Loss: 0.6351377785205841
Epoch: 170 Batch: 100
Training Loss: 0.2986910539865494
Epoch: 170 Batch: 150
Training Loss: 0.20342490752538045
Epoch: 170 Batch: 200
Training Loss: 0.15576178282499314
Epoch: 170 Batch: 250
Training Loss: 0.12319890117645263
Epoch: 170 Batch: 300
Training Loss: 0.09616308758656184
Epoch: 170 Batch: 350
Training Loss: 0.08653298999582018
Epoch: 170 Batch: 400
Training Loss: 0.07586060963571072
Epoch: 170 Batch: 450
Training Loss: 0.0664464427365197
Epoch: 170 Batch: 500
Training Loss: 0.05896431893110275
Epoch: 170 Batch: 550
Training Loss: 0.055324972813779655
Epoch: 170 Batch: 600
Training Loss: 0.05265334149201711
Epoch: 170 Batch: 650
Training Loss: 0.04837274115819198
Epoch: 170 Batch: 700
Training Loss: 0.04467234781810216
Epoch: 170 Batch: 750
Training Loss: 0.04133047564824422
Epoch: 170 Batch: 800
Training Loss: 0.03837171670049429
Epoch: 170 Batch: 850
Training Loss: 0.034637073278427126
Epoch: 170 Batch: 900
Training Loss: 0.033332061171531674
Epoch: 170 Batch: 950
Training Loss: 0.031549397236422486
Epoch: 170 Batch: 1000
Training Loss: 0.030038095355033875
Epoch: 170 Batch: 1050
Training Loss: 0.02885857431661515
Epoch: 170 Batch: 1100
Training Loss: 0.02767313323237679
Epoch: 170 Batch: 1150
Training Loss: 0.026981319085411404
Epoch: 170 Batch: 1200
Training Loss: 0.02411036585768064
Epoch: 170 Batch: 1250
Training Loss: 0.024478290486335753
Epoch: 170 Batch: 1300
Training Loss: 0.022758136231165665
Epoch: 170 Batch: 1350
Training Loss: 0.021669372894145823
Epoch: 170 Batch: 1400
Training Loss: 0.021812061348131725
Epoch: 170 Batch: 1450
Training Loss: 0.019903610019848264
Epoch: 170 Batch: 1500
Training Loss: 0.019993719398975374
Epoch: 170 Batch: 1550
Training Loss: 0.019804498053366138
Epoch: 170 Batch: 1600
Training Loss: 0.01991991152986884
Epoch: 170 Batch: 1650
Training Loss: 0.019166592847217213
Epoch: 170 Batch: 1700
Training Loss: 0.017318773392368765
Epoch: 170 Batch: 1750
Training Loss: 0.017298561538968766
Epoch: 170 Batch: 1800
Training Loss: 0.016843147360616262
Epoch: 170 Batch: 1850
Training Loss: 0.016563840395695455
Epoch: 170 Batch: 1900
Training Loss: 0.015820399993344357
Epoch: 170 Batch: 1950
Training Loss: 0.015487514978800064
Epoch: 170 Batch: 2000
Training Loss: 0.01553267565369606
Epoch: 170 Batch: 2050
Training Loss: 0.015027376151666409
Epoch: 170 Batch: 2100
Training Loss: 0.01529934143736249
Epoch: 170 Batch: 2150
Training Loss: 0.013612349865048431
Epoch: 170 Batch: 2200
Training Loss: 0.013098895780064844
Epoch: 170 Batch: 2250
Training Loss: 0.012992077827453614
Epoch: 170 Batch: 2300
Training Loss: 0.013356400494990142
Epoch: 170 Batch: 2350
Training Loss: 0.013037040170202864
Epoch: 170 Batch: 2400
Training Loss: 0.012274751973648867
Epoch: 170 Batch: 2450
Training Loss: 0.012484201983529694
Epoch: 170 Batch: 2500
Training Loss: 0.012547306323051453
Epoch: 170 Batch: 2550
Training Loss: 0.011537658420263552
Epoch: 170 Batch: 2600
Training Loss: 0.011707301678565832
Epoch: 170 Batch: 2650
Training Loss: 0.01150575264444891
Epoch: 170 Batch: 2700
Training Loss: 0.011129806880597715
Epoch: 170 Batch: 2750
Training Loss: 0.011436319394545121
Epoch: 170 Batch: 2800
Training Loss: 0.011440008048500334
Epoch: 170 Batch: 2850
Training Loss: 0.010226448680225172
Epoch: 170 Batch: 2900
Training Loss: 0.010334456234142699
Epoch: 170 Batch: 2950
Training Loss: 0.010179589875673844
Epoch: 170 Batch: 3000
Training Loss: 0.01072338036696116
Epoch: 170 Batch: 3050
Training Loss: 0.00981189748302835
Epoch: 170 Batch: 3100
Training Loss: 0.009752527831062193
Epoch: 170 Batch: 3150
Training Loss: 0.010161240318464854
Epoch: 170 Batch: 3200
Training Loss: 0.009997939923778176
Epoch: 171 
 Validation Loss: 0.4708760417169995
---------------------------
Epoch: 171 Batch: 50
Training Loss: 0.6058359146118164
Epoch: 171 Batch: 100
Training Loss: 0.3212655836343765
Epoch: 171 Batch: 150
Training Loss: 0.20382240414619446
Epoch: 171 Batch: 200
Training Loss: 0.14444353878498079
Epoch: 171 Batch: 250
Training Loss: 0.12532196462154388
Epoch: 171 Batch: 300
Training Loss: 0.09560209433237711
Epoch: 171 Batch: 350
Training Loss: 0.08818384119442531
Epoch: 171 Batch: 400
Training Loss: 0.07505260638892651
Epoch: 171 Batch: 450
Training Loss: 0.06649365365505218
Epoch: 171 Batch: 500
Training Loss: 0.06374773323535919
Epoch: 171 Batch: 550
Training Loss: 0.056022668860175394
Epoch: 171 Batch: 600
Training Loss: 0.05242508525649706
Epoch: 171 Batch: 650
Training Loss: 0.04417436347557948
Epoch: 171 Batch: 700
Training Loss: 0.044052460193634034
Epoch: 171 Batch: 750
Training Loss: 0.03966483000914256
Epoch: 171 Batch: 800
Training Loss: 0.037015242725610735
Epoch: 171 Batch: 850
Training Loss: 0.034878441586213955
Epoch: 171 Batch: 900
Training Loss: 0.03474224083953434
Epoch: 171 Batch: 950
Training Loss: 0.030547036434474744
Epoch: 171 Batch: 1000
Training Loss: 0.029060033828020096
Epoch: 171 Batch: 1050
Training Loss: 0.028902293415296647
Epoch: 171 Batch: 1100
Training Loss: 0.028130517195571553
Epoch: 171 Batch: 1150
Training Loss: 0.024979336002598638
Epoch: 171 Batch: 1200
Training Loss: 0.026034671862920127
Epoch: 171 Batch: 1250
Training Loss: 0.025556822109222412
Epoch: 171 Batch: 1300
Training Loss: 0.02280246771298922
Epoch: 171 Batch: 1350
Training Loss: 0.02352546645535363
Epoch: 171 Batch: 1400
Training Loss: 0.022206403017044067
Epoch: 171 Batch: 1450
Training Loss: 0.020765649943516172
Epoch: 171 Batch: 1500
Training Loss: 0.02044565063714981
Epoch: 171 Batch: 1550
Training Loss: 0.019665122109074746
Epoch: 171 Batch: 1600
Training Loss: 0.01917683897539973
Epoch: 171 Batch: 1650
Training Loss: 0.01773683968818549
Epoch: 171 Batch: 1700
Training Loss: 0.018369221038678114
Epoch: 171 Batch: 1750
Training Loss: 0.017533685939652578
Epoch: 171 Batch: 1800
Training Loss: 0.017811203565862445
Epoch: 171 Batch: 1850
Training Loss: 0.016096655533120438
Epoch: 171 Batch: 1900
Training Loss: 0.015885888510628752
Epoch: 171 Batch: 1950
Training Loss: 0.01481843583094768
Epoch: 171 Batch: 2000
Training Loss: 0.015706766203045845
Epoch: 171 Batch: 2050
Training Loss: 0.015576072567846717
Epoch: 171 Batch: 2100
Training Loss: 0.014936977596510024
Epoch: 171 Batch: 2150
Training Loss: 0.01442226278227429
Epoch: 171 Batch: 2200
Training Loss: 0.013568194061517715
Epoch: 171 Batch: 2250
Training Loss: 0.013151099363962809
Epoch: 171 Batch: 2300
Training Loss: 0.013752412925595823
Epoch: 171 Batch: 2350
Training Loss: 0.013328840390164801
Epoch: 171 Batch: 2400
Training Loss: 0.012428215766946474
Epoch: 171 Batch: 2450
Training Loss: 0.012058740975905438
Epoch: 171 Batch: 2500
Training Loss: 0.011871677482128144
Epoch: 171 Batch: 2550
Training Loss: 0.011450225418689204
Epoch: 171 Batch: 2600
Training Loss: 0.011688166146094983
Epoch: 171 Batch: 2650
Training Loss: 0.011450134808162473
Epoch: 171 Batch: 2700
Training Loss: 0.011281898176228558
Epoch: 171 Batch: 2750
Training Loss: 0.010974259192293341
Epoch: 171 Batch: 2800
Training Loss: 0.011207655838557653
Epoch: 171 Batch: 2850
Training Loss: 0.010782356272663987
Epoch: 171 Batch: 2900
Training Loss: 0.010953378862348095
Epoch: 171 Batch: 2950
Training Loss: 0.01077937182733568
Epoch: 171 Batch: 3000
Training Loss: 0.010525947650273641
Epoch: 171 Batch: 3050
Training Loss: 0.010372272641932379
Epoch: 171 Batch: 3100
Training Loss: 0.009673726904776789
Epoch: 171 Batch: 3150
Training Loss: 0.010006887818139697
Epoch: 171 Batch: 3200
Training Loss: 0.009450842216610909
Epoch: 172 
 Validation Loss: 0.4711263044012917
---------------------------
Epoch: 172 Batch: 50
Training Loss: 0.6146647930145264
Epoch: 172 Batch: 100
Training Loss: 0.30322474241256714
Epoch: 172 Batch: 150
Training Loss: 0.19889051377773284
Epoch: 172 Batch: 200
Training Loss: 0.1472529512643814
Epoch: 172 Batch: 250
Training Loss: 0.11628133690357208
Epoch: 172 Batch: 300
Training Loss: 0.10310221274693807
Epoch: 172 Batch: 350
Training Loss: 0.09041297716753824
Epoch: 172 Batch: 400
Training Loss: 0.07760840572416783
Epoch: 172 Batch: 450
Training Loss: 0.06714220232433743
Epoch: 172 Batch: 500
Training Loss: 0.060555935204029084
Epoch: 172 Batch: 550
Training Loss: 0.05691676529971036
Epoch: 172 Batch: 600
Training Loss: 0.04879246448477109
Epoch: 172 Batch: 650
Training Loss: 0.0460599285364151
Epoch: 172 Batch: 700
Training Loss: 0.044361234307289124
Epoch: 172 Batch: 750
Training Loss: 0.04115503903230031
Epoch: 172 Batch: 800
Training Loss: 0.039557707086205485
Epoch: 172 Batch: 850
Training Loss: 0.034814730812521545
Epoch: 172 Batch: 900
Training Loss: 0.0346946981549263
Epoch: 172 Batch: 950
Training Loss: 0.031161194098623175
Epoch: 172 Batch: 1000
Training Loss: 0.03101813825964928
Epoch: 172 Batch: 1050
Training Loss: 0.02876994521844955
Epoch: 172 Batch: 1100
Training Loss: 0.028217245529998432
Epoch: 172 Batch: 1150
Training Loss: 0.02726496458053589
Epoch: 172 Batch: 1200
Training Loss: 0.02507777084906896
Epoch: 172 Batch: 1250
Training Loss: 0.02337239716053009
Epoch: 172 Batch: 1300
Training Loss: 0.023513894081115722
Epoch: 172 Batch: 1350
Training Loss: 0.023413774613980892
Epoch: 172 Batch: 1400
Training Loss: 0.021717560227428163
Epoch: 172 Batch: 1450
Training Loss: 0.02122999668121338
Epoch: 172 Batch: 1500
Training Loss: 0.019866305510203044
Epoch: 172 Batch: 1550
Training Loss: 0.019346920828665456
Epoch: 172 Batch: 1600
Training Loss: 0.01865565624088049
Epoch: 172 Batch: 1650
Training Loss: 0.01792028035178329
Epoch: 172 Batch: 1700
Training Loss: 0.01859796809799531
Epoch: 172 Batch: 1750
Training Loss: 0.016955945014953615
Epoch: 172 Batch: 1800
Training Loss: 0.016869768814908134
Epoch: 172 Batch: 1850
Training Loss: 0.015913631287780967
Epoch: 172 Batch: 1900
Training Loss: 0.016214687322315416
Epoch: 172 Batch: 1950
Training Loss: 0.015799339841573667
Epoch: 172 Batch: 2000
Training Loss: 0.015585306346416474
Epoch: 172 Batch: 2050
Training Loss: 0.015116687664171544
Epoch: 172 Batch: 2100
Training Loss: 0.014356645544370016
Epoch: 172 Batch: 2150
Training Loss: 0.014378642190334408
Epoch: 172 Batch: 2200
Training Loss: 0.0140604382482442
Epoch: 172 Batch: 2250
Training Loss: 0.013463037318653531
Epoch: 172 Batch: 2300
Training Loss: 0.013155302185079326
Epoch: 172 Batch: 2350
Training Loss: 0.01271268885186378
Epoch: 172 Batch: 2400
Training Loss: 0.012980236411094666
Epoch: 172 Batch: 2450
Training Loss: 0.013122396237996159
Epoch: 172 Batch: 2500
Training Loss: 0.01218215800523758
Epoch: 172 Batch: 2550
Training Loss: 0.011757706742660672
Epoch: 172 Batch: 2600
Training Loss: 0.012030343654064031
Epoch: 172 Batch: 2650
Training Loss: 0.011227798383190947
Epoch: 172 Batch: 2700
Training Loss: 0.010701229009363386
Epoch: 172 Batch: 2750
Training Loss: 0.01098948714949868
Epoch: 172 Batch: 2800
Training Loss: 0.011318262187497957
Epoch: 172 Batch: 2850
Training Loss: 0.010801951257806076
Epoch: 172 Batch: 2900
Training Loss: 0.010679634583407436
Epoch: 172 Batch: 2950
Training Loss: 0.010358458434121084
Epoch: 172 Batch: 3000
Training Loss: 0.010017487625281016
Epoch: 172 Batch: 3050
Training Loss: 0.010029585029258103
Epoch: 172 Batch: 3100
Training Loss: 0.009723511492052386
Epoch: 172 Batch: 3150
Training Loss: 0.00954541630215115
Epoch: 172 Batch: 3200
Training Loss: 0.009843576131388546
Epoch: 173 
 Validation Loss: 0.47049279510974884
---------------------------
Epoch: 173 Batch: 50
Training Loss: 0.6356116688251495
Epoch: 173 Batch: 100
Training Loss: 0.3071020469069481
Epoch: 173 Batch: 150
Training Loss: 0.20128377000490824
Epoch: 173 Batch: 200
Training Loss: 0.14950416952371598
Epoch: 173 Batch: 250
Training Loss: 0.11824782454967499
Epoch: 173 Batch: 300
Training Loss: 0.09677510490020116
Epoch: 173 Batch: 350
Training Loss: 0.08808558412960597
Epoch: 173 Batch: 400
Training Loss: 0.07631412096321583
Epoch: 173 Batch: 450
Training Loss: 0.06821782377031115
Epoch: 173 Batch: 500
Training Loss: 0.06067783439159393
Epoch: 173 Batch: 550
Training Loss: 0.053144361810250715
Epoch: 173 Batch: 600
Training Loss: 0.04930255855123202
Epoch: 173 Batch: 650
Training Loss: 0.04963301356022175
Epoch: 173 Batch: 700
Training Loss: 0.043186742322785514
Epoch: 173 Batch: 750
Training Loss: 0.04088765307267507
Epoch: 173 Batch: 800
Training Loss: 0.03795111399143934
Epoch: 173 Batch: 850
Training Loss: 0.035981258925269634
Epoch: 173 Batch: 900
Training Loss: 0.03451195753282971
Epoch: 173 Batch: 950
Training Loss: 0.03253889080725218
Epoch: 173 Batch: 1000
Training Loss: 0.030501527637243273
Epoch: 173 Batch: 1050
Training Loss: 0.029668061165582568
Epoch: 173 Batch: 1100
Training Loss: 0.0279212488098578
Epoch: 173 Batch: 1150
Training Loss: 0.026203224347985308
Epoch: 173 Batch: 1200
Training Loss: 0.026440161267916363
Epoch: 173 Batch: 1250
Training Loss: 0.02357018904685974
Epoch: 173 Batch: 1300
Training Loss: 0.02357896937773778
Epoch: 173 Batch: 1350
Training Loss: 0.02139225414505711
Epoch: 173 Batch: 1400
Training Loss: 0.021461248972586222
Epoch: 173 Batch: 1450
Training Loss: 0.020678779667821424
Epoch: 173 Batch: 1500
Training Loss: 0.020690922339757282
Epoch: 173 Batch: 1550
Training Loss: 0.018612474952974627
Epoch: 173 Batch: 1600
Training Loss: 0.019440617728978395
Epoch: 173 Batch: 1650
Training Loss: 0.01854582024343086
Epoch: 173 Batch: 1700
Training Loss: 0.018183060428675485
Epoch: 173 Batch: 1750
Training Loss: 0.017009506055286953
Epoch: 173 Batch: 1800
Training Loss: 0.01746779875622855
Epoch: 173 Batch: 1850
Training Loss: 0.01708678616059793
Epoch: 173 Batch: 1900
Training Loss: 0.01604384963449679
Epoch: 173 Batch: 1950
Training Loss: 0.014820119387064224
Epoch: 173 Batch: 2000
Training Loss: 0.015043073937296867
Epoch: 173 Batch: 2050
Training Loss: 0.01572181165218353
Epoch: 173 Batch: 2100
Training Loss: 0.013993096209707715
Epoch: 173 Batch: 2150
Training Loss: 0.014369679758715075
Epoch: 173 Batch: 2200
Training Loss: 0.013220327753912318
Epoch: 173 Batch: 2250
Training Loss: 0.013791162199444242
Epoch: 173 Batch: 2300
Training Loss: 0.013490868431070576
Epoch: 173 Batch: 2350
Training Loss: 0.01295942505623432
Epoch: 173 Batch: 2400
Training Loss: 0.012764266083637872
Epoch: 173 Batch: 2450
Training Loss: 0.012335937035327055
Epoch: 173 Batch: 2500
Training Loss: 0.011843661201000213
Epoch: 173 Batch: 2550
Training Loss: 0.01234525860524645
Epoch: 173 Batch: 2600
Training Loss: 0.011690891614327063
Epoch: 173 Batch: 2650
Training Loss: 0.012009389220543627
Epoch: 173 Batch: 2700
Training Loss: 0.010990833353113245
Epoch: 173 Batch: 2750
Training Loss: 0.011591894377361644
Epoch: 173 Batch: 2800
Training Loss: 0.01108109436929226
Epoch: 173 Batch: 2850
Training Loss: 0.010156195205554627
Epoch: 173 Batch: 2900
Training Loss: 0.010044058674368365
Epoch: 173 Batch: 2950
Training Loss: 0.010629866183814356
Epoch: 173 Batch: 3000
Training Loss: 0.01015979857246081
Epoch: 173 Batch: 3050
Training Loss: 0.009908044689991435
Epoch: 173 Batch: 3100
Training Loss: 0.009998120165640308
Epoch: 173 Batch: 3150
Training Loss: 0.009549817972713046
Epoch: 173 Batch: 3200
Training Loss: 0.010075692757964135
Epoch: 174 
 Validation Loss: 0.47106452816062505
---------------------------
Epoch: 174 Batch: 50
Training Loss: 0.6209590584039688
Epoch: 174 Batch: 100
Training Loss: 0.295844541490078
Epoch: 174 Batch: 150
Training Loss: 0.20758171995480856
Epoch: 174 Batch: 200
Training Loss: 0.1522200670838356
Epoch: 174 Batch: 250
Training Loss: 0.1258101851940155
Epoch: 174 Batch: 300
Training Loss: 0.09571433633565903
Epoch: 174 Batch: 350
Training Loss: 0.0881412535905838
Epoch: 174 Batch: 400
Training Loss: 0.07835122972726821
Epoch: 174 Batch: 450
Training Loss: 0.0703176713652081
Epoch: 174 Batch: 500
Training Loss: 0.06176828968524933
Epoch: 174 Batch: 550
Training Loss: 0.05701291891661557
Epoch: 174 Batch: 600
Training Loss: 0.04899210313955943
Epoch: 174 Batch: 650
Training Loss: 0.0502292008124865
Epoch: 174 Batch: 700
Training Loss: 0.04561541983059474
Epoch: 174 Batch: 750
Training Loss: 0.03880918387571971
Epoch: 174 Batch: 800
Training Loss: 0.038020352870225906
Epoch: 174 Batch: 850
Training Loss: 0.03662182341603672
Epoch: 174 Batch: 900
Training Loss: 0.03419879761007097
Epoch: 174 Batch: 950
Training Loss: 0.03173176464281584
Epoch: 174 Batch: 1000
Training Loss: 0.031209675133228303
Epoch: 174 Batch: 1050
Training Loss: 0.02908583924883888
Epoch: 174 Batch: 1100
Training Loss: 0.02732050351121209
Epoch: 174 Batch: 1150
Training Loss: 0.02557425511919934
Epoch: 174 Batch: 1200
Training Loss: 0.024230062067508697
Epoch: 174 Batch: 1250
Training Loss: 0.02435644166469574
Epoch: 174 Batch: 1300
Training Loss: 0.02275185167789459
Epoch: 174 Batch: 1350
Training Loss: 0.02183791134092543
Epoch: 174 Batch: 1400
Training Loss: 0.021364049187728336
Epoch: 174 Batch: 1450
Training Loss: 0.02053029200126385
Epoch: 174 Batch: 1500
Training Loss: 0.020023714264233907
Epoch: 174 Batch: 1550
Training Loss: 0.01943613288864013
Epoch: 174 Batch: 1600
Training Loss: 0.020006615072488784
Epoch: 174 Batch: 1650
Training Loss: 0.018080482735778346
Epoch: 174 Batch: 1700
Training Loss: 0.017918668564628152
Epoch: 174 Batch: 1750
Training Loss: 0.01763906845024654
Epoch: 174 Batch: 1800
Training Loss: 0.016967343356874255
Epoch: 174 Batch: 1850
Training Loss: 0.01664340976122263
Epoch: 174 Batch: 1900
Training Loss: 0.016388169260401476
Epoch: 174 Batch: 1950
Training Loss: 0.01600285354333046
Epoch: 174 Batch: 2000
Training Loss: 0.01431164774298668
Epoch: 174 Batch: 2050
Training Loss: 0.015231946837611315
Epoch: 174 Batch: 2100
Training Loss: 0.014914700786272685
Epoch: 174 Batch: 2150
Training Loss: 0.014109071839687437
Epoch: 174 Batch: 2200
Training Loss: 0.013674548850818114
Epoch: 174 Batch: 2250
Training Loss: 0.013802802218331231
Epoch: 174 Batch: 2300
Training Loss: 0.013069749422695325
Epoch: 174 Batch: 2350
Training Loss: 0.01318125548514914
Epoch: 174 Batch: 2400
Training Loss: 0.013303326976795992
Epoch: 174 Batch: 2450
Training Loss: 0.012500099186994592
Epoch: 174 Batch: 2500
Training Loss: 0.0122414093375206
Epoch: 174 Batch: 2550
Training Loss: 0.012147864734425265
Epoch: 174 Batch: 2600
Training Loss: 0.011938864050003198
Epoch: 174 Batch: 2650
Training Loss: 0.011704051359644476
Epoch: 174 Batch: 2700
Training Loss: 0.010689951346980201
Epoch: 174 Batch: 2750
Training Loss: 0.010801034851507707
Epoch: 174 Batch: 2800
Training Loss: 0.011081129653113229
Epoch: 174 Batch: 2850
Training Loss: 0.010619297508607831
Epoch: 174 Batch: 2900
Training Loss: 0.010390296403704018
Epoch: 174 Batch: 2950
Training Loss: 0.010180276622206478
Epoch: 174 Batch: 3000
Training Loss: 0.010150633762280146
Epoch: 174 Batch: 3050
Training Loss: 0.009956335288579346
Epoch: 174 Batch: 3100
Training Loss: 0.009834402876515541
Epoch: 174 Batch: 3150
Training Loss: 0.009459437207570151
Epoch: 174 Batch: 3200
Training Loss: 0.009445628924295306
Epoch: 175 
 Validation Loss: 0.47103801369667053
---------------------------
Epoch: 175 Batch: 50
Training Loss: 0.6461021095514298
Epoch: 175 Batch: 100
Training Loss: 0.2942281836271286
Epoch: 175 Batch: 150
Training Loss: 0.20032055735588072
Epoch: 175 Batch: 200
Training Loss: 0.15078129038214683
Epoch: 175 Batch: 250
Training Loss: 0.1265296094417572
Epoch: 175 Batch: 300
Training Loss: 0.10529711375633875
Epoch: 175 Batch: 350
Training Loss: 0.08799711184842246
Epoch: 175 Batch: 400
Training Loss: 0.07348172597587109
Epoch: 175 Batch: 450
Training Loss: 0.06735771079858144
Epoch: 175 Batch: 500
Training Loss: 0.060097531914711
Epoch: 175 Batch: 550
Training Loss: 0.055402619621970434
Epoch: 175 Batch: 600
Training Loss: 0.049472234944502515
Epoch: 175 Batch: 650
Training Loss: 0.04895606591151311
Epoch: 175 Batch: 700
Training Loss: 0.042752659107957565
Epoch: 175 Batch: 750
Training Loss: 0.042087324778238934
Epoch: 175 Batch: 800
Training Loss: 0.03914921458810568
Epoch: 175 Batch: 850
Training Loss: 0.03673650180592256
Epoch: 175 Batch: 900
Training Loss: 0.03274060815572739
Epoch: 175 Batch: 950
Training Loss: 0.03347599358935105
Epoch: 175 Batch: 1000
Training Loss: 0.03044619682431221
Epoch: 175 Batch: 1050
Training Loss: 0.0282865796202705
Epoch: 175 Batch: 1100
Training Loss: 0.027339942075989464
Epoch: 175 Batch: 1150
Training Loss: 0.024478083797123122
Epoch: 175 Batch: 1200
Training Loss: 0.026197866400082905
Epoch: 175 Batch: 1250
Training Loss: 0.024821397399902342
Epoch: 175 Batch: 1300
Training Loss: 0.023579498644058523
Epoch: 175 Batch: 1350
Training Loss: 0.023617020514276294
Epoch: 175 Batch: 1400
Training Loss: 0.02182077041694096
Epoch: 175 Batch: 1450
Training Loss: 0.020621034544089745
Epoch: 175 Batch: 1500
Training Loss: 0.020588857690493265
Epoch: 175 Batch: 1550
Training Loss: 0.019484385501953864
Epoch: 175 Batch: 1600
Training Loss: 0.01910064335912466
Epoch: 175 Batch: 1650
Training Loss: 0.017368380138368316
Epoch: 175 Batch: 1700
Training Loss: 0.01853124250383938
Epoch: 175 Batch: 1750
Training Loss: 0.017981881260871888
Epoch: 175 Batch: 1800
Training Loss: 0.016990562197234896
Epoch: 175 Batch: 1850
Training Loss: 0.01715006391744356
Epoch: 175 Batch: 1900
Training Loss: 0.015823719925002047
Epoch: 175 Batch: 1950
Training Loss: 0.014201884926893771
Epoch: 175 Batch: 2000
Training Loss: 0.01575293593108654
Epoch: 175 Batch: 2050
Training Loss: 0.015103740110629942
Epoch: 175 Batch: 2100
Training Loss: 0.014439610498292106
Epoch: 175 Batch: 2150
Training Loss: 0.014554311383602232
Epoch: 175 Batch: 2200
Training Loss: 0.014014277431097897
Epoch: 175 Batch: 2250
Training Loss: 0.013692630754576789
Epoch: 175 Batch: 2300
Training Loss: 0.012678867889487225
Epoch: 175 Batch: 2350
Training Loss: 0.012865769076854624
Epoch: 175 Batch: 2400
Training Loss: 0.012190579622983932
Epoch: 175 Batch: 2450
Training Loss: 0.01253882230544577
Epoch: 175 Batch: 2500
Training Loss: 0.012383593654632569
Epoch: 175 Batch: 2550
Training Loss: 0.011102169611874749
Epoch: 175 Batch: 2600
Training Loss: 0.011843141512228893
Epoch: 175 Batch: 2650
Training Loss: 0.011323895173252753
Epoch: 175 Batch: 2700
Training Loss: 0.011364460046644565
Epoch: 175 Batch: 2750
Training Loss: 0.011010680632157759
Epoch: 175 Batch: 2800
Training Loss: 0.010595387039440018
Epoch: 175 Batch: 2850
Training Loss: 0.010651768655107733
Epoch: 175 Batch: 2900
Training Loss: 0.010221476123250764
Epoch: 175 Batch: 2950
Training Loss: 0.010701941100217528
Epoch: 175 Batch: 3000
Training Loss: 0.010016869177420933
Epoch: 175 Batch: 3050
Training Loss: 0.009759532643146203
Epoch: 175 Batch: 3100
Training Loss: 0.009817559978654307
Epoch: 175 Batch: 3150
Training Loss: 0.009549330851388356
Epoch: 175 Batch: 3200
Training Loss: 0.009405721696093678
Epoch: 176 
 Validation Loss: 0.4713690701458189
---------------------------
Epoch: 176 Batch: 50
Training Loss: 0.6303666108846664
Epoch: 176 Batch: 100
Training Loss: 0.3078988179564476
Epoch: 176 Batch: 150
Training Loss: 0.2122341380516688
Epoch: 176 Batch: 200
Training Loss: 0.1522116543352604
Epoch: 176 Batch: 250
Training Loss: 0.12270021045207977
Epoch: 176 Batch: 300
Training Loss: 0.10357418209314347
Epoch: 176 Batch: 350
Training Loss: 0.08071413619177682
Epoch: 176 Batch: 400
Training Loss: 0.07432684078812599
Epoch: 176 Batch: 450
Training Loss: 0.06892056895626916
Epoch: 176 Batch: 500
Training Loss: 0.05872574454545975
Epoch: 176 Batch: 550
Training Loss: 0.05323158670555461
Epoch: 176 Batch: 600
Training Loss: 0.05034414947032929
Epoch: 176 Batch: 650
Training Loss: 0.047739712320841274
Epoch: 176 Batch: 700
Training Loss: 0.044199783205986026
Epoch: 176 Batch: 750
Training Loss: 0.03815555644035339
Epoch: 176 Batch: 800
Training Loss: 0.03771569821983576
Epoch: 176 Batch: 850
Training Loss: 0.03474615097045899
Epoch: 176 Batch: 900
Training Loss: 0.03310217493110233
Epoch: 176 Batch: 950
Training Loss: 0.03070447789995294
Epoch: 176 Batch: 1000
Training Loss: 0.030492127627134324
Epoch: 176 Batch: 1050
Training Loss: 0.029168313571384975
Epoch: 176 Batch: 1100
Training Loss: 0.028032717271284622
Epoch: 176 Batch: 1150
Training Loss: 0.026816565809042556
Epoch: 176 Batch: 1200
Training Loss: 0.025045740678906442
Epoch: 176 Batch: 1250
Training Loss: 0.02382248203754425
Epoch: 176 Batch: 1300
Training Loss: 0.02434958698657843
Epoch: 176 Batch: 1350
Training Loss: 0.021869124461103367
Epoch: 176 Batch: 1400
Training Loss: 0.021810038962534497
Epoch: 176 Batch: 1450
Training Loss: 0.02080474228694521
Epoch: 176 Batch: 1500
Training Loss: 0.02038711468378703
Epoch: 176 Batch: 1550
Training Loss: 0.02029754384871452
Epoch: 176 Batch: 1600
Training Loss: 0.01961783355101943
Epoch: 176 Batch: 1650
Training Loss: 0.01803669122132388
Epoch: 176 Batch: 1700
Training Loss: 0.018948069828398088
Epoch: 176 Batch: 1750
Training Loss: 0.01728130900859833
Epoch: 176 Batch: 1800
Training Loss: 0.01671442922618654
Epoch: 176 Batch: 1850
Training Loss: 0.016631419159270622
Epoch: 176 Batch: 1900
Training Loss: 0.016085659312574486
Epoch: 176 Batch: 1950
Training Loss: 0.015164069869579412
Epoch: 176 Batch: 2000
Training Loss: 0.015809245198965073
Epoch: 176 Batch: 2050
Training Loss: 0.014523375601303287
Epoch: 176 Batch: 2100
Training Loss: 0.014060749170326051
Epoch: 176 Batch: 2150
Training Loss: 0.01435316845428112
Epoch: 176 Batch: 2200
Training Loss: 0.013388782997022975
Epoch: 176 Batch: 2250
Training Loss: 0.01320688988102807
Epoch: 176 Batch: 2300
Training Loss: 0.013716520705948705
Epoch: 176 Batch: 2350
Training Loss: 0.013035282251682687
Epoch: 176 Batch: 2400
Training Loss: 0.012758868771294753
Epoch: 176 Batch: 2450
Training Loss: 0.012975205487134505
Epoch: 176 Batch: 2500
Training Loss: 0.011863279235363006
Epoch: 176 Batch: 2550
Training Loss: 0.011798775254511366
Epoch: 176 Batch: 2600
Training Loss: 0.012062994161477455
Epoch: 176 Batch: 2650
Training Loss: 0.011661509567836546
Epoch: 176 Batch: 2700
Training Loss: 0.011392467938087604
Epoch: 176 Batch: 2750
Training Loss: 0.011620730259201743
Epoch: 176 Batch: 2800
Training Loss: 0.010760455440197672
Epoch: 176 Batch: 2850
Training Loss: 0.010412769558136923
Epoch: 176 Batch: 2900
Training Loss: 0.010703252297023246
Epoch: 176 Batch: 2950
Training Loss: 0.01001151414240821
Epoch: 176 Batch: 3000
Training Loss: 0.009951713313659032
Epoch: 176 Batch: 3050
Training Loss: 0.00982354579401798
Epoch: 176 Batch: 3100
Training Loss: 0.009893893032304701
Epoch: 176 Batch: 3150
Training Loss: 0.009661316237752399
Epoch: 176 Batch: 3200
Training Loss: 0.009105800054967403
Epoch: 177 
 Validation Loss: 0.47057472632990943
---------------------------
Epoch: 177 Batch: 50
Training Loss: 0.6500581157207489
Epoch: 177 Batch: 100
Training Loss: 0.3083916690945625
Epoch: 177 Batch: 150
Training Loss: 0.20568261365095775
Epoch: 177 Batch: 200
Training Loss: 0.1541744215786457
Epoch: 177 Batch: 250
Training Loss: 0.11789057946205139
Epoch: 177 Batch: 300
Training Loss: 0.1041239998737971
Epoch: 177 Batch: 350
Training Loss: 0.08248170239584787
Epoch: 177 Batch: 400
Training Loss: 0.0765146093070507
Epoch: 177 Batch: 450
Training Loss: 0.0676212959157096
Epoch: 177 Batch: 500
Training Loss: 0.05954895627498627
Epoch: 177 Batch: 550
Training Loss: 0.05567973478273912
Epoch: 177 Batch: 600
Training Loss: 0.05166206588347753
Epoch: 177 Batch: 650
Training Loss: 0.046334335116239696
Epoch: 177 Batch: 700
Training Loss: 0.04386460423469543
Epoch: 177 Batch: 750
Training Loss: 0.039831246932347616
Epoch: 177 Batch: 800
Training Loss: 0.03702486079186201
Epoch: 177 Batch: 850
Training Loss: 0.03572177648544311
Epoch: 177 Batch: 900
Training Loss: 0.033713499969906274
Epoch: 177 Batch: 950
Training Loss: 0.03405783750508961
Epoch: 177 Batch: 1000
Training Loss: 0.03218903997540474
Epoch: 177 Batch: 1050
Training Loss: 0.02852292307785579
Epoch: 177 Batch: 1100
Training Loss: 0.02741177729584954
Epoch: 177 Batch: 1150
Training Loss: 0.02644232309382895
Epoch: 177 Batch: 1200
Training Loss: 0.02502764917910099
Epoch: 177 Batch: 1250
Training Loss: 0.024475779962539673
Epoch: 177 Batch: 1300
Training Loss: 0.023431646823883056
Epoch: 177 Batch: 1350
Training Loss: 0.0222499320462898
Epoch: 177 Batch: 1400
Training Loss: 0.022558459086077554
Epoch: 177 Batch: 1450
Training Loss: 0.02026978665384753
Epoch: 177 Batch: 1500
Training Loss: 0.02119235728184382
Epoch: 177 Batch: 1550
Training Loss: 0.019128220042874736
Epoch: 177 Batch: 1600
Training Loss: 0.01931072959676385
Epoch: 177 Batch: 1650
Training Loss: 0.018294919982100977
Epoch: 177 Batch: 1700
Training Loss: 0.0175833102534799
Epoch: 177 Batch: 1750
Training Loss: 0.01732717227935791
Epoch: 177 Batch: 1800
Training Loss: 0.016415231542454826
Epoch: 177 Batch: 1850
Training Loss: 0.01618597555804897
Epoch: 177 Batch: 1900
Training Loss: 0.01612421209874906
Epoch: 177 Batch: 1950
Training Loss: 0.015242121341900948
Epoch: 177 Batch: 2000
Training Loss: 0.01457886789739132
Epoch: 177 Batch: 2050
Training Loss: 0.014822190331249702
Epoch: 177 Batch: 2100
Training Loss: 0.015208389560381572
Epoch: 177 Batch: 2150
Training Loss: 0.014121661740680073
Epoch: 177 Batch: 2200
Training Loss: 0.01343467738140713
Epoch: 177 Batch: 2250
Training Loss: 0.013717246002621121
Epoch: 177 Batch: 2300
Training Loss: 0.012756571510563726
Epoch: 177 Batch: 2350
Training Loss: 0.012622776627540588
Epoch: 177 Batch: 2400
Training Loss: 0.012669160775840283
Epoch: 177 Batch: 2450
Training Loss: 0.012286772594159964
Epoch: 177 Batch: 2500
Training Loss: 0.011896496069431306
Epoch: 177 Batch: 2550
Training Loss: 0.011554159335061615
Epoch: 177 Batch: 2600
Training Loss: 0.012184494206538566
Epoch: 177 Batch: 2650
Training Loss: 0.011914279168506839
Epoch: 177 Batch: 2700
Training Loss: 0.011346212614465643
Epoch: 177 Batch: 2750
Training Loss: 0.011270625385371122
Epoch: 177 Batch: 2800
Training Loss: 0.010973989399416106
Epoch: 177 Batch: 2850
Training Loss: 0.010758371593659385
Epoch: 177 Batch: 2900
Training Loss: 0.010768756290961957
Epoch: 177 Batch: 2950
Training Loss: 0.010294980770450527
Epoch: 177 Batch: 3000
Training Loss: 0.00961980833609899
Epoch: 177 Batch: 3050
Training Loss: 0.009861063165742843
Epoch: 177 Batch: 3100
Training Loss: 0.009676270244583007
Epoch: 177 Batch: 3150
Training Loss: 0.00949970146966359
Epoch: 177 Batch: 3200
Training Loss: 0.00950156381353736
Epoch: 178 
 Validation Loss: 0.47057396272818247
---------------------------
Epoch: 178 Batch: 50
Training Loss: 0.6301271218061447
Epoch: 178 Batch: 100
Training Loss: 0.30949234575033185
Epoch: 178 Batch: 150
Training Loss: 0.19510880490144095
Epoch: 178 Batch: 200
Training Loss: 0.1535979400575161
Epoch: 178 Batch: 250
Training Loss: 0.11632501649856568
Epoch: 178 Batch: 300
Training Loss: 0.10846075683832168
Epoch: 178 Batch: 350
Training Loss: 0.08668960681983402
Epoch: 178 Batch: 400
Training Loss: 0.07735115766525269
Epoch: 178 Batch: 450
Training Loss: 0.0658242913087209
Epoch: 178 Batch: 500
Training Loss: 0.06062047868967056
Epoch: 178 Batch: 550
Training Loss: 0.05457290042530406
Epoch: 178 Batch: 600
Training Loss: 0.052555684198935824
Epoch: 178 Batch: 650
Training Loss: 0.04888190164015843
Epoch: 178 Batch: 700
Training Loss: 0.04322669855185918
Epoch: 178 Batch: 750
Training Loss: 0.04142860190073649
Epoch: 178 Batch: 800
Training Loss: 0.03864454489201307
Epoch: 178 Batch: 850
Training Loss: 0.03460902887232163
Epoch: 178 Batch: 900
Training Loss: 0.034919347233242456
Epoch: 178 Batch: 950
Training Loss: 0.031105672466127495
Epoch: 178 Batch: 1000
Training Loss: 0.029855192035436632
Epoch: 178 Batch: 1050
Training Loss: 0.03030057858853113
Epoch: 178 Batch: 1100
Training Loss: 0.025912627984176984
Epoch: 178 Batch: 1150
Training Loss: 0.026823729354402293
Epoch: 178 Batch: 1200
Training Loss: 0.024417420948545137
Epoch: 178 Batch: 1250
Training Loss: 0.02454727716445923
Epoch: 178 Batch: 1300
Training Loss: 0.02339210583613469
Epoch: 178 Batch: 1350
Training Loss: 0.022632286217477586
Epoch: 178 Batch: 1400
Training Loss: 0.021729417932885034
Epoch: 178 Batch: 1450
Training Loss: 0.02075559416721607
Epoch: 178 Batch: 1500
Training Loss: 0.019414684971173605
Epoch: 178 Batch: 1550
Training Loss: 0.019248844712011277
Epoch: 178 Batch: 1600
Training Loss: 0.018842023108154537
Epoch: 178 Batch: 1650
Training Loss: 0.01862115776900089
Epoch: 178 Batch: 1700
Training Loss: 0.01814060058663873
Epoch: 178 Batch: 1750
Training Loss: 0.01707853215081351
Epoch: 178 Batch: 1800
Training Loss: 0.017386086086432138
Epoch: 178 Batch: 1850
Training Loss: 0.01645477481790491
Epoch: 178 Batch: 1900
Training Loss: 0.015499016030838616
Epoch: 178 Batch: 1950
Training Loss: 0.015363943301714384
Epoch: 178 Batch: 2000
Training Loss: 0.014721687018871308
Epoch: 178 Batch: 2050
Training Loss: 0.014505006583725534
Epoch: 178 Batch: 2100
Training Loss: 0.01412778750771568
Epoch: 178 Batch: 2150
Training Loss: 0.013866876169692639
Epoch: 178 Batch: 2200
Training Loss: 0.013139963949268514
Epoch: 178 Batch: 2250
Training Loss: 0.013079036659664579
Epoch: 178 Batch: 2300
Training Loss: 0.01310436241004778
Epoch: 178 Batch: 2350
Training Loss: 0.013015346032507875
Epoch: 178 Batch: 2400
Training Loss: 0.01293319046497345
Epoch: 178 Batch: 2450
Training Loss: 0.012198821306228638
Epoch: 178 Batch: 2500
Training Loss: 0.012039011824131012
Epoch: 178 Batch: 2550
Training Loss: 0.012026278575261434
Epoch: 178 Batch: 2600
Training Loss: 0.012376717351950131
Epoch: 178 Batch: 2650
Training Loss: 0.011853502089122557
Epoch: 178 Batch: 2700
Training Loss: 0.011540100397887052
Epoch: 178 Batch: 2750
Training Loss: 0.010864898042245344
Epoch: 178 Batch: 2800
Training Loss: 0.011037143841385842
Epoch: 178 Batch: 2850
Training Loss: 0.010877684051530404
Epoch: 178 Batch: 2900
Training Loss: 0.010583106217713191
Epoch: 178 Batch: 2950
Training Loss: 0.010299544132361978
Epoch: 178 Batch: 3000
Training Loss: 0.010197798917690912
Epoch: 178 Batch: 3050
Training Loss: 0.010338188615001615
Epoch: 178 Batch: 3100
Training Loss: 0.009481486389713903
Epoch: 178 Batch: 3150
Training Loss: 0.009859922821559603
Epoch: 178 Batch: 3200
Training Loss: 0.009503641789779067
Epoch: 179 
 Validation Loss: 0.4703217691845364
---------------------------
Epoch: 179 Batch: 50
Training Loss: 0.6249560594558716
Epoch: 179 Batch: 100
Training Loss: 0.31824493139982224
Epoch: 179 Batch: 150
Training Loss: 0.20315815766652426
Epoch: 179 Batch: 200
Training Loss: 0.1555771853029728
Epoch: 179 Batch: 250
Training Loss: 0.12061136543750763
Epoch: 179 Batch: 300
Training Loss: 0.10248448878526688
Epoch: 179 Batch: 350
Training Loss: 0.08428046439375196
Epoch: 179 Batch: 400
Training Loss: 0.07441953957080841
Epoch: 179 Batch: 450
Training Loss: 0.06724990381134881
Epoch: 179 Batch: 500
Training Loss: 0.058714655697345734
Epoch: 179 Batch: 550
Training Loss: 0.05378003537654877
Epoch: 179 Batch: 600
Training Loss: 0.05078030854463577
Epoch: 179 Batch: 650
Training Loss: 0.04640928483926333
Epoch: 179 Batch: 700
Training Loss: 0.04223758301564625
Epoch: 179 Batch: 750
Training Loss: 0.04253773283958435
Epoch: 179 Batch: 800
Training Loss: 0.037305536903440954
Epoch: 179 Batch: 850
Training Loss: 0.035726267695426944
Epoch: 179 Batch: 900
Training Loss: 0.035164351463317874
Epoch: 179 Batch: 950
Training Loss: 0.03125797588574259
Epoch: 179 Batch: 1000
Training Loss: 0.030263264924287797
Epoch: 179 Batch: 1050
Training Loss: 0.02872354303087507
Epoch: 179 Batch: 1100
Training Loss: 0.028170729794285514
Epoch: 179 Batch: 1150
Training Loss: 0.026566428205241327
Epoch: 179 Batch: 1200
Training Loss: 0.024725845977663995
Epoch: 179 Batch: 1250
Training Loss: 0.024180721497535706
Epoch: 179 Batch: 1300
Training Loss: 0.023567814895739923
Epoch: 179 Batch: 1350
Training Loss: 0.022679846110167327
Epoch: 179 Batch: 1400
Training Loss: 0.02109151133469173
Epoch: 179 Batch: 1450
Training Loss: 0.021284694198904365
Epoch: 179 Batch: 1500
Training Loss: 0.02131034642457962
Epoch: 179 Batch: 1550
Training Loss: 0.019647761737146684
Epoch: 179 Batch: 1600
Training Loss: 0.019416264854371548
Epoch: 179 Batch: 1650
Training Loss: 0.01819684108098348
Epoch: 179 Batch: 1700
Training Loss: 0.01674669919645085
Epoch: 179 Batch: 1750
Training Loss: 0.017002087644168308
Epoch: 179 Batch: 1800
Training Loss: 0.01670528694987297
Epoch: 179 Batch: 1850
Training Loss: 0.016766512490607597
Epoch: 179 Batch: 1900
Training Loss: 0.016437236572566787
Epoch: 179 Batch: 1950
Training Loss: 0.015265017167115823
Epoch: 179 Batch: 2000
Training Loss: 0.015011658668518066
Epoch: 179 Batch: 2050
Training Loss: 0.01434834624209055
Epoch: 179 Batch: 2100
Training Loss: 0.013666476876962753
Epoch: 179 Batch: 2150
Training Loss: 0.014155719238658285
Epoch: 179 Batch: 2200
Training Loss: 0.014023698676716198
Epoch: 179 Batch: 2250
Training Loss: 0.013405376328362358
Epoch: 179 Batch: 2300
Training Loss: 0.013220217603704204
Epoch: 179 Batch: 2350
Training Loss: 0.013101399934038202
Epoch: 179 Batch: 2400
Training Loss: 0.012570524290204048
Epoch: 179 Batch: 2450
Training Loss: 0.011958228933567903
Epoch: 179 Batch: 2500
Training Loss: 0.012143734228610993
Epoch: 179 Batch: 2550
Training Loss: 0.012412469036438886
Epoch: 179 Batch: 2600
Training Loss: 0.01141433786887389
Epoch: 179 Batch: 2650
Training Loss: 0.011141688531299807
Epoch: 179 Batch: 2700
Training Loss: 0.011776860179724517
Epoch: 179 Batch: 2750
Training Loss: 0.011722361900589683
Epoch: 179 Batch: 2800
Training Loss: 0.011253362713115556
Epoch: 179 Batch: 2850
Training Loss: 0.010621266605561239
Epoch: 179 Batch: 2900
Training Loss: 0.010506978877659503
Epoch: 179 Batch: 2950
Training Loss: 0.010321574817269535
Epoch: 179 Batch: 3000
Training Loss: 0.010006537973880767
Epoch: 179 Batch: 3050
Training Loss: 0.010136993898720038
Epoch: 179 Batch: 3100
Training Loss: 0.009879944112993056
Epoch: 179 Batch: 3150
Training Loss: 0.009702269340318348
Epoch: 179 Batch: 3200
Training Loss: 0.009309278428554535
Epoch: 180 
 Validation Loss: 0.4701018604967329
---------------------------
Epoch: 180 Batch: 50
Training Loss: 0.6110351300239563
Epoch: 180 Batch: 100
Training Loss: 0.3155381828546524
Epoch: 180 Batch: 150
Training Loss: 0.20118171632289886
Epoch: 180 Batch: 200
Training Loss: 0.15315071254968643
Epoch: 180 Batch: 250
Training Loss: 0.1228172528743744
Epoch: 180 Batch: 300
Training Loss: 0.10234005669752756
Epoch: 180 Batch: 350
Training Loss: 0.08812320964676994
Epoch: 180 Batch: 400
Training Loss: 0.0783506541699171
Epoch: 180 Batch: 450
Training Loss: 0.06860819194051955
Epoch: 180 Batch: 500
Training Loss: 0.059896986961364744
Epoch: 180 Batch: 550
Training Loss: 0.05541961501945149
Epoch: 180 Batch: 600
Training Loss: 0.050649551997582115
Epoch: 180 Batch: 650
Training Loss: 0.04237063595881829
Epoch: 180 Batch: 700
Training Loss: 0.04437537176268441
Epoch: 180 Batch: 750
Training Loss: 0.03738576054573059
Epoch: 180 Batch: 800
Training Loss: 0.03901867229491472
Epoch: 180 Batch: 850
Training Loss: 0.03615694224834442
Epoch: 180 Batch: 900
Training Loss: 0.03226071178913117
Epoch: 180 Batch: 950
Training Loss: 0.031175225314341092
Epoch: 180 Batch: 1000
Training Loss: 0.03131680500507355
Epoch: 180 Batch: 1050
Training Loss: 0.029315131448564076
Epoch: 180 Batch: 1100
Training Loss: 0.028798425007950174
Epoch: 180 Batch: 1150
Training Loss: 0.02652055859565735
Epoch: 180 Batch: 1200
Training Loss: 0.02475112095475197
Epoch: 180 Batch: 1250
Training Loss: 0.025521804428100585
Epoch: 180 Batch: 1300
Training Loss: 0.02400067517390618
Epoch: 180 Batch: 1350
Training Loss: 0.022490458974131832
Epoch: 180 Batch: 1400
Training Loss: 0.02039025115115302
Epoch: 180 Batch: 1450
Training Loss: 0.02155893784144829
Epoch: 180 Batch: 1500
Training Loss: 0.02021904530127843
Epoch: 180 Batch: 1550
Training Loss: 0.019080018574191678
Epoch: 180 Batch: 1600
Training Loss: 0.019085100628435613
Epoch: 180 Batch: 1650
Training Loss: 0.017968985709277067
Epoch: 180 Batch: 1700
Training Loss: 0.017567662491517907
Epoch: 180 Batch: 1750
Training Loss: 0.01758952854360853
Epoch: 180 Batch: 1800
Training Loss: 0.01771518260240555
Epoch: 180 Batch: 1850
Training Loss: 0.0156723591282561
Epoch: 180 Batch: 1900
Training Loss: 0.016469601753510928
Epoch: 180 Batch: 1950
Training Loss: 0.015663825151247857
Epoch: 180 Batch: 2000
Training Loss: 0.016222751289606093
Epoch: 180 Batch: 2050
Training Loss: 0.014726693397615015
Epoch: 180 Batch: 2100
Training Loss: 0.014315359464713505
Epoch: 180 Batch: 2150
Training Loss: 0.014244372927865318
Epoch: 180 Batch: 2200
Training Loss: 0.013818346614187413
Epoch: 180 Batch: 2250
Training Loss: 0.01323055566681756
Epoch: 180 Batch: 2300
Training Loss: 0.01316924555146176
Epoch: 180 Batch: 2350
Training Loss: 0.012688302575273716
Epoch: 180 Batch: 2400
Training Loss: 0.01265594686071078
Epoch: 180 Batch: 2450
Training Loss: 0.012259539803680108
Epoch: 180 Batch: 2500
Training Loss: 0.01236877669095993
Epoch: 180 Batch: 2550
Training Loss: 0.012369113880045274
Epoch: 180 Batch: 2600
Training Loss: 0.011184285122614641
Epoch: 180 Batch: 2650
Training Loss: 0.011479821081431406
Epoch: 180 Batch: 2700
Training Loss: 0.011050851345062255
Epoch: 180 Batch: 2750
Training Loss: 0.011384547753767535
Epoch: 180 Batch: 2800
Training Loss: 0.010999911363635744
Epoch: 180 Batch: 2850
Training Loss: 0.010962951005550853
Epoch: 180 Batch: 2900
Training Loss: 0.010581331520244992
Epoch: 180 Batch: 2950
Training Loss: 0.00981067386724181
Epoch: 180 Batch: 3000
Training Loss: 0.01104826041062673
Epoch: 180 Batch: 3050
Training Loss: 0.010089010137026427
Epoch: 180 Batch: 3100
Training Loss: 0.010397941047145474
Epoch: 180 Batch: 3150
Training Loss: 0.009102334209850856
Epoch: 180 Batch: 3200
Training Loss: 0.009243666678667068
Epoch: 181 
 Validation Loss: 0.46996025509304473
---------------------------
Epoch: 181 Batch: 50
Training Loss: 0.6501725620031357
Epoch: 181 Batch: 100
Training Loss: 0.3018482485413551
Epoch: 181 Batch: 150
Training Loss: 0.2081144497791926
Epoch: 181 Batch: 200
Training Loss: 0.1515612231194973
Epoch: 181 Batch: 250
Training Loss: 0.12552549159526824
Epoch: 181 Batch: 300
Training Loss: 0.09589941372474034
Epoch: 181 Batch: 350
Training Loss: 0.08637936856065477
Epoch: 181 Batch: 400
Training Loss: 0.07598361261188984
Epoch: 181 Batch: 450
Training Loss: 0.06419958651065827
Epoch: 181 Batch: 500
Training Loss: 0.06053382456302643
Epoch: 181 Batch: 550
Training Loss: 0.057285989252003754
Epoch: 181 Batch: 600
Training Loss: 0.049268901993831
Epoch: 181 Batch: 650
Training Loss: 0.0483804315328598
Epoch: 181 Batch: 700
Training Loss: 0.04371580170733588
Epoch: 181 Batch: 750
Training Loss: 0.040223990837732954
Epoch: 181 Batch: 800
Training Loss: 0.039774247109889985
Epoch: 181 Batch: 850
Training Loss: 0.03559463458902696
Epoch: 181 Batch: 900
Training Loss: 0.033752322130733065
Epoch: 181 Batch: 950
Training Loss: 0.032457536553081714
Epoch: 181 Batch: 1000
Training Loss: 0.03050469732284546
Epoch: 181 Batch: 1050
Training Loss: 0.030144318200293042
Epoch: 181 Batch: 1100
Training Loss: 0.028622489327734167
Epoch: 181 Batch: 1150
Training Loss: 0.02659925624080326
Epoch: 181 Batch: 1200
Training Loss: 0.025645273625850677
Epoch: 181 Batch: 1250
Training Loss: 0.024088209676742554
Epoch: 181 Batch: 1300
Training Loss: 0.021720960690424992
Epoch: 181 Batch: 1350
Training Loss: 0.023210404139977915
Epoch: 181 Batch: 1400
Training Loss: 0.021658418327569963
Epoch: 181 Batch: 1450
Training Loss: 0.021224010401758656
Epoch: 181 Batch: 1500
Training Loss: 0.020130276282628378
Epoch: 181 Batch: 1550
Training Loss: 0.019223954100762643
Epoch: 181 Batch: 1600
Training Loss: 0.019501819163560866
Epoch: 181 Batch: 1650
Training Loss: 0.018135839971629056
Epoch: 181 Batch: 1700
Training Loss: 0.019117644814883962
Epoch: 181 Batch: 1750
Training Loss: 0.017997860295431954
Epoch: 181 Batch: 1800
Training Loss: 0.01677357011371189
Epoch: 181 Batch: 1850
Training Loss: 0.016217527437854456
Epoch: 181 Batch: 1900
Training Loss: 0.016267506715498474
Epoch: 181 Batch: 1950
Training Loss: 0.015450481298642281
Epoch: 181 Batch: 2000
Training Loss: 0.015538374930620193
Epoch: 181 Batch: 2050
Training Loss: 0.015085739420681464
Epoch: 181 Batch: 2100
Training Loss: 0.014269871016343435
Epoch: 181 Batch: 2150
Training Loss: 0.014356073903483015
Epoch: 181 Batch: 2200
Training Loss: 0.013739244748245586
Epoch: 181 Batch: 2250
Training Loss: 0.013321157058080037
Epoch: 181 Batch: 2300
Training Loss: 0.01327396484820739
Epoch: 181 Batch: 2350
Training Loss: 0.012439563616793206
Epoch: 181 Batch: 2400
Training Loss: 0.012750252981980641
Epoch: 181 Batch: 2450
Training Loss: 0.012938232397546573
Epoch: 181 Batch: 2500
Training Loss: 0.012032033383846282
Epoch: 181 Batch: 2550
Training Loss: 0.011714410723424426
Epoch: 181 Batch: 2600
Training Loss: 0.012031485679057928
Epoch: 181 Batch: 2650
Training Loss: 0.011657485017236674
Epoch: 181 Batch: 2700
Training Loss: 0.011251916642542239
Epoch: 181 Batch: 2750
Training Loss: 0.01092488991130482
Epoch: 181 Batch: 2800
Training Loss: 0.010793936061007635
Epoch: 181 Batch: 2850
Training Loss: 0.010629615260843646
Epoch: 181 Batch: 2900
Training Loss: 0.01079600733929667
Epoch: 181 Batch: 2950
Training Loss: 0.010714472602989715
Epoch: 181 Batch: 3000
Training Loss: 0.010423850158850351
Epoch: 181 Batch: 3050
Training Loss: 0.010216777744840403
Epoch: 181 Batch: 3100
Training Loss: 0.009471611880487011
Epoch: 181 Batch: 3150
Training Loss: 0.009696899122662014
Epoch: 181 Batch: 3200
Training Loss: 0.01016584857366979
Epoch: 182 
 Validation Loss: 0.46989378829797107
---------------------------
Epoch: 182 Batch: 50
Training Loss: 0.6139594662189484
Epoch: 182 Batch: 100
Training Loss: 0.30350683063268663
Epoch: 182 Batch: 150
Training Loss: 0.19940998733043672
Epoch: 182 Batch: 200
Training Loss: 0.16038238734006882
Epoch: 182 Batch: 250
Training Loss: 0.12062815380096435
Epoch: 182 Batch: 300
Training Loss: 0.09724075684944788
Epoch: 182 Batch: 350
Training Loss: 0.09030363500118256
Epoch: 182 Batch: 400
Training Loss: 0.07510549776256084
Epoch: 182 Batch: 450
Training Loss: 0.06939270046022203
Epoch: 182 Batch: 500
Training Loss: 0.05997788232564926
Epoch: 182 Batch: 550
Training Loss: 0.05641386568546295
Epoch: 182 Batch: 600
Training Loss: 0.05024968345959981
Epoch: 182 Batch: 650
Training Loss: 0.04902150346682622
Epoch: 182 Batch: 700
Training Loss: 0.043324994019099644
Epoch: 182 Batch: 750
Training Loss: 0.039592779954274494
Epoch: 182 Batch: 800
Training Loss: 0.037660972848534584
Epoch: 182 Batch: 850
Training Loss: 0.033512102190185994
Epoch: 182 Batch: 900
Training Loss: 0.03343310207128525
Epoch: 182 Batch: 950
Training Loss: 0.032045112001268486
Epoch: 182 Batch: 1000
Training Loss: 0.030173752218484878
Epoch: 182 Batch: 1050
Training Loss: 0.028707155812354314
Epoch: 182 Batch: 1100
Training Loss: 0.027782877629453487
Epoch: 182 Batch: 1150
Training Loss: 0.02563403062198473
Epoch: 182 Batch: 1200
Training Loss: 0.025334972590208053
Epoch: 182 Batch: 1250
Training Loss: 0.025200682830810547
Epoch: 182 Batch: 1300
Training Loss: 0.024277195701232324
Epoch: 182 Batch: 1350
Training Loss: 0.021897182067235312
Epoch: 182 Batch: 1400
Training Loss: 0.021971232465335302
Epoch: 182 Batch: 1450
Training Loss: 0.02046906210225204
Epoch: 182 Batch: 1500
Training Loss: 0.020814796169598897
Epoch: 182 Batch: 1550
Training Loss: 0.020482027934443568
Epoch: 182 Batch: 1600
Training Loss: 0.019111409280449153
Epoch: 182 Batch: 1650
Training Loss: 0.019058821580626748
Epoch: 182 Batch: 1700
Training Loss: 0.017920373082160948
Epoch: 182 Batch: 1750
Training Loss: 0.016463470373834884
Epoch: 182 Batch: 1800
Training Loss: 0.016563469535774655
Epoch: 182 Batch: 1850
Training Loss: 0.01596995822481207
Epoch: 182 Batch: 1900
Training Loss: 0.01627802279434706
Epoch: 182 Batch: 1950
Training Loss: 0.0150348162651062
Epoch: 182 Batch: 2000
Training Loss: 0.015916056960821153
Epoch: 182 Batch: 2050
Training Loss: 0.014358098637766954
Epoch: 182 Batch: 2100
Training Loss: 0.014334111682006292
Epoch: 182 Batch: 2150
Training Loss: 0.013839233614677607
Epoch: 182 Batch: 2200
Training Loss: 0.014052156047387557
Epoch: 182 Batch: 2250
Training Loss: 0.012937045097351075
Epoch: 182 Batch: 2300
Training Loss: 0.013814600110054016
Epoch: 182 Batch: 2350
Training Loss: 0.013615278512873549
Epoch: 182 Batch: 2400
Training Loss: 0.012820110817750295
Epoch: 182 Batch: 2450
Training Loss: 0.012816166001923231
Epoch: 182 Batch: 2500
Training Loss: 0.01246740198135376
Epoch: 182 Batch: 2550
Training Loss: 0.011678541980537714
Epoch: 182 Batch: 2600
Training Loss: 0.011738430914970545
Epoch: 182 Batch: 2650
Training Loss: 0.01200715755516628
Epoch: 182 Batch: 2700
Training Loss: 0.010591864177474269
Epoch: 182 Batch: 2750
Training Loss: 0.010797599597410723
Epoch: 182 Batch: 2800
Training Loss: 0.0108448610241924
Epoch: 182 Batch: 2850
Training Loss: 0.010805961008657489
Epoch: 182 Batch: 2900
Training Loss: 0.010956149779517074
Epoch: 182 Batch: 2950
Training Loss: 0.01015224338588068
Epoch: 182 Batch: 3000
Training Loss: 0.01002675794561704
Epoch: 182 Batch: 3050
Training Loss: 0.010052796963785516
Epoch: 182 Batch: 3100
Training Loss: 0.009628667091169664
Epoch: 182 Batch: 3150
Training Loss: 0.009729164290049719
Epoch: 182 Batch: 3200
Training Loss: 0.009488091124221682
Epoch: 183 
 Validation Loss: 0.4700458784898122
---------------------------
Epoch: 183 Batch: 50
Training Loss: 0.6554507011175156
Epoch: 183 Batch: 100
Training Loss: 0.29558219820261
Epoch: 183 Batch: 150
Training Loss: 0.20613736172517141
Epoch: 183 Batch: 200
Training Loss: 0.14353183761239052
Epoch: 183 Batch: 250
Training Loss: 0.12395574581623077
Epoch: 183 Batch: 300
Training Loss: 0.10200730244318644
Epoch: 183 Batch: 350
Training Loss: 0.08240033933094569
Epoch: 183 Batch: 400
Training Loss: 0.077708383128047
Epoch: 183 Batch: 450
Training Loss: 0.06807300104035272
Epoch: 183 Batch: 500
Training Loss: 0.06101612937450409
Epoch: 183 Batch: 550
Training Loss: 0.05695054758678783
Epoch: 183 Batch: 600
Training Loss: 0.051516788204511006
Epoch: 183 Batch: 650
Training Loss: 0.04551308572292328
Epoch: 183 Batch: 700
Training Loss: 0.04613947480916977
Epoch: 183 Batch: 750
Training Loss: 0.04040530741214752
Epoch: 183 Batch: 800
Training Loss: 0.038720826879143716
Epoch: 183 Batch: 850
Training Loss: 0.035484038345954
Epoch: 183 Batch: 900
Training Loss: 0.03314412789212333
Epoch: 183 Batch: 950
Training Loss: 0.03196085195792349
Epoch: 183 Batch: 1000
Training Loss: 0.030918413877487182
Epoch: 183 Batch: 1050
Training Loss: 0.02801290004026322
Epoch: 183 Batch: 1100
Training Loss: 0.02748602956533432
Epoch: 183 Batch: 1150
Training Loss: 0.026189272636952608
Epoch: 183 Batch: 1200
Training Loss: 0.02544000538686911
Epoch: 183 Batch: 1250
Training Loss: 0.025139659333229064
Epoch: 183 Batch: 1300
Training Loss: 0.024167548761917994
Epoch: 183 Batch: 1350
Training Loss: 0.02271271213337227
Epoch: 183 Batch: 1400
Training Loss: 0.02170847366963114
Epoch: 183 Batch: 1450
Training Loss: 0.021862716695358015
Epoch: 183 Batch: 1500
Training Loss: 0.020608820339043935
Epoch: 183 Batch: 1550
Training Loss: 0.019986578649090182
Epoch: 183 Batch: 1600
Training Loss: 0.01855105532333255
Epoch: 183 Batch: 1650
Training Loss: 0.017969954862739102
Epoch: 183 Batch: 1700
Training Loss: 0.01785036746193381
Epoch: 183 Batch: 1750
Training Loss: 0.0176084132705416
Epoch: 183 Batch: 1800
Training Loss: 0.017408165633678436
Epoch: 183 Batch: 1850
Training Loss: 0.01645781135236895
Epoch: 183 Batch: 1900
Training Loss: 0.016080576416693235
Epoch: 183 Batch: 1950
Training Loss: 0.015882350680155633
Epoch: 183 Batch: 2000
Training Loss: 0.015120484560728073
Epoch: 183 Batch: 2050
Training Loss: 0.01481999009120755
Epoch: 183 Batch: 2100
Training Loss: 0.014229269311541603
Epoch: 183 Batch: 2150
Training Loss: 0.014405778008838032
Epoch: 183 Batch: 2200
Training Loss: 0.013666759417815642
Epoch: 183 Batch: 2250
Training Loss: 0.01334364906946818
Epoch: 183 Batch: 2300
Training Loss: 0.012994253104147703
Epoch: 183 Batch: 2350
Training Loss: 0.012840755924265436
Epoch: 183 Batch: 2400
Training Loss: 0.012524087900916735
Epoch: 183 Batch: 2450
Training Loss: 0.01229498882682956
Epoch: 183 Batch: 2500
Training Loss: 0.012118843746185303
Epoch: 183 Batch: 2550
Training Loss: 0.011852987759253558
Epoch: 183 Batch: 2600
Training Loss: 0.01183796994961225
Epoch: 183 Batch: 2650
Training Loss: 0.011373784497099103
Epoch: 183 Batch: 2700
Training Loss: 0.011550272747322366
Epoch: 183 Batch: 2750
Training Loss: 0.011026776432991027
Epoch: 183 Batch: 2800
Training Loss: 0.010203501667295183
Epoch: 183 Batch: 2850
Training Loss: 0.010554638776862831
Epoch: 183 Batch: 2900
Training Loss: 0.01044768789718891
Epoch: 183 Batch: 2950
Training Loss: 0.009777705022844217
Epoch: 183 Batch: 3000
Training Loss: 0.01049420627951622
Epoch: 183 Batch: 3050
Training Loss: 0.010130037235431984
Epoch: 183 Batch: 3100
Training Loss: 0.00991246372461319
Epoch: 183 Batch: 3150
Training Loss: 0.009298512670728896
Epoch: 183 Batch: 3200
Training Loss: 0.009567632814869284
Epoch: 184 
 Validation Loss: 0.47007053130202825
---------------------------
Epoch: 184 Batch: 50
Training Loss: 0.6528498166799546
Epoch: 184 Batch: 100
Training Loss: 0.30278958976268766
Epoch: 184 Batch: 150
Training Loss: 0.1999940210580826
Epoch: 184 Batch: 200
Training Loss: 0.15414194613695145
Epoch: 184 Batch: 250
Training Loss: 0.12674628925323486
Epoch: 184 Batch: 300
Training Loss: 0.10217654863993327
Epoch: 184 Batch: 350
Training Loss: 0.08818825662136077
Epoch: 184 Batch: 400
Training Loss: 0.07386630237102508
Epoch: 184 Batch: 450
Training Loss: 0.06529023561212752
Epoch: 184 Batch: 500
Training Loss: 0.05929635238647461
Epoch: 184 Batch: 550
Training Loss: 0.054989827925508675
Epoch: 184 Batch: 600
Training Loss: 0.049612392832835514
Epoch: 184 Batch: 650
Training Loss: 0.04405711031877078
Epoch: 184 Batch: 700
Training Loss: 0.04337564238480159
Epoch: 184 Batch: 750
Training Loss: 0.03999366557598114
Epoch: 184 Batch: 800
Training Loss: 0.03835767913609743
Epoch: 184 Batch: 850
Training Loss: 0.03642611843698165
Epoch: 184 Batch: 900
Training Loss: 0.03378700604041417
Epoch: 184 Batch: 950
Training Loss: 0.03293682917168266
Epoch: 184 Batch: 1000
Training Loss: 0.03194501379132271
Epoch: 184 Batch: 1050
Training Loss: 0.029064417736870903
Epoch: 184 Batch: 1100
Training Loss: 0.028066964880986648
Epoch: 184 Batch: 1150
Training Loss: 0.02653177313182665
Epoch: 184 Batch: 1200
Training Loss: 0.02573527199526628
Epoch: 184 Batch: 1250
Training Loss: 0.024497994208335875
Epoch: 184 Batch: 1300
Training Loss: 0.02349200759942715
Epoch: 184 Batch: 1350
Training Loss: 0.02294149758639159
Epoch: 184 Batch: 1400
Training Loss: 0.022352056482008525
Epoch: 184 Batch: 1450
Training Loss: 0.02114795719755107
Epoch: 184 Batch: 1500
Training Loss: 0.02026134620110194
Epoch: 184 Batch: 1550
Training Loss: 0.019600257469761757
Epoch: 184 Batch: 1600
Training Loss: 0.019386734161525965
Epoch: 184 Batch: 1650
Training Loss: 0.018386986887816228
Epoch: 184 Batch: 1700
Training Loss: 0.017736798875472123
Epoch: 184 Batch: 1750
Training Loss: 0.017473556331225805
Epoch: 184 Batch: 1800
Training Loss: 0.016343359897534052
Epoch: 184 Batch: 1850
Training Loss: 0.015881544931514842
Epoch: 184 Batch: 1900
Training Loss: 0.016722895628527593
Epoch: 184 Batch: 1950
Training Loss: 0.015791409244904152
Epoch: 184 Batch: 2000
Training Loss: 0.01553856447339058
Epoch: 184 Batch: 2050
Training Loss: 0.01368382139903743
Epoch: 184 Batch: 2100
Training Loss: 0.014139914980956486
Epoch: 184 Batch: 2150
Training Loss: 0.014002883323403293
Epoch: 184 Batch: 2200
Training Loss: 0.013176119828766042
Epoch: 184 Batch: 2250
Training Loss: 0.013340396033393011
Epoch: 184 Batch: 2300
Training Loss: 0.013288947693679644
Epoch: 184 Batch: 2350
Training Loss: 0.012728531487444614
Epoch: 184 Batch: 2400
Training Loss: 0.012449802073339622
Epoch: 184 Batch: 2450
Training Loss: 0.012295753834198932
Epoch: 184 Batch: 2500
Training Loss: 0.012128856146335602
Epoch: 184 Batch: 2550
Training Loss: 0.012129469174964755
Epoch: 184 Batch: 2600
Training Loss: 0.01198538200213359
Epoch: 184 Batch: 2650
Training Loss: 0.011602313383570257
Epoch: 184 Batch: 2700
Training Loss: 0.011491657672105013
Epoch: 184 Batch: 2750
Training Loss: 0.011003533146598122
Epoch: 184 Batch: 2800
Training Loss: 0.010897891404373306
Epoch: 184 Batch: 2850
Training Loss: 0.010605575170433312
Epoch: 184 Batch: 2900
Training Loss: 0.010691343391763752
Epoch: 184 Batch: 2950
Training Loss: 0.010491903899079662
Epoch: 184 Batch: 3000
Training Loss: 0.010035586535930634
Epoch: 184 Batch: 3050
Training Loss: 0.010204109197757284
Epoch: 184 Batch: 3100
Training Loss: 0.010543685613139984
Epoch: 184 Batch: 3150
Training Loss: 0.00972186299543532
Epoch: 184 Batch: 3200
Training Loss: 0.009538726108148694
Epoch: 185 
 Validation Loss: 0.46976557738251157
---------------------------
Epoch: 185 Batch: 50
Training Loss: 0.5957794213294982
Epoch: 185 Batch: 100
Training Loss: 0.29366773039102556
Epoch: 185 Batch: 150
Training Loss: 0.20106276412804922
Epoch: 185 Batch: 200
Training Loss: 0.1539778845012188
Epoch: 185 Batch: 250
Training Loss: 0.11774971950054168
Epoch: 185 Batch: 300
Training Loss: 0.10764055192470551
Epoch: 185 Batch: 350
Training Loss: 0.08534699865749903
Epoch: 185 Batch: 400
Training Loss: 0.07261193849146366
Epoch: 185 Batch: 450
Training Loss: 0.06514336109161377
Epoch: 185 Batch: 500
Training Loss: 0.06078116416931152
Epoch: 185 Batch: 550
Training Loss: 0.05303384358232672
Epoch: 185 Batch: 600
Training Loss: 0.05193705568710963
Epoch: 185 Batch: 650
Training Loss: 0.04670322427382836
Epoch: 185 Batch: 700
Training Loss: 0.04314367639166968
Epoch: 185 Batch: 750
Training Loss: 0.039961711247762044
Epoch: 185 Batch: 800
Training Loss: 0.037905317842960355
Epoch: 185 Batch: 850
Training Loss: 0.03620785288950976
Epoch: 185 Batch: 900
Training Loss: 0.0334388816025522
Epoch: 185 Batch: 950
Training Loss: 0.03107865731967123
Epoch: 185 Batch: 1000
Training Loss: 0.029829921215772628
Epoch: 185 Batch: 1050
Training Loss: 0.02984653319631304
Epoch: 185 Batch: 1100
Training Loss: 0.02873747950250452
Epoch: 185 Batch: 1150
Training Loss: 0.026519559181254843
Epoch: 185 Batch: 1200
Training Loss: 0.025081343774994214
Epoch: 185 Batch: 1250
Training Loss: 0.024612604331970215
Epoch: 185 Batch: 1300
Training Loss: 0.023821554275659414
Epoch: 185 Batch: 1350
Training Loss: 0.02381585317629355
Epoch: 185 Batch: 1400
Training Loss: 0.02150093355349132
Epoch: 185 Batch: 1450
Training Loss: 0.020625772578962916
Epoch: 185 Batch: 1500
Training Loss: 0.020278318762779236
Epoch: 185 Batch: 1550
Training Loss: 0.019956736987636937
Epoch: 185 Batch: 1600
Training Loss: 0.02008202634751797
Epoch: 185 Batch: 1650
Training Loss: 0.01884652610981103
Epoch: 185 Batch: 1700
Training Loss: 0.017734737291055565
Epoch: 185 Batch: 1750
Training Loss: 0.017901170287813458
Epoch: 185 Batch: 1800
Training Loss: 0.01612561058666971
Epoch: 185 Batch: 1850
Training Loss: 0.015562705139856081
Epoch: 185 Batch: 1900
Training Loss: 0.0155921712360884
Epoch: 185 Batch: 1950
Training Loss: 0.014838366691882795
Epoch: 185 Batch: 2000
Training Loss: 0.015174402594566346
Epoch: 185 Batch: 2050
Training Loss: 0.015306718334919069
Epoch: 185 Batch: 2100
Training Loss: 0.01475356434072767
Epoch: 185 Batch: 2150
Training Loss: 0.01386041268359783
Epoch: 185 Batch: 2200
Training Loss: 0.014213893887671558
Epoch: 185 Batch: 2250
Training Loss: 0.013407686789830526
Epoch: 185 Batch: 2300
Training Loss: 0.013419888006604236
Epoch: 185 Batch: 2350
Training Loss: 0.013289402723312378
Epoch: 185 Batch: 2400
Training Loss: 0.013021669313311578
Epoch: 185 Batch: 2450
Training Loss: 0.013068008897255878
Epoch: 185 Batch: 2500
Training Loss: 0.012526900708675385
Epoch: 185 Batch: 2550
Training Loss: 0.011649075232300105
Epoch: 185 Batch: 2600
Training Loss: 0.011301784664392472
Epoch: 185 Batch: 2650
Training Loss: 0.01138520691754683
Epoch: 185 Batch: 2700
Training Loss: 0.011102799585571995
Epoch: 185 Batch: 2750
Training Loss: 0.011037042541937395
Epoch: 185 Batch: 2800
Training Loss: 0.010792994467275484
Epoch: 185 Batch: 2850
Training Loss: 0.010480010112126669
Epoch: 185 Batch: 2900
Training Loss: 0.010307661911536908
Epoch: 185 Batch: 2950
Training Loss: 0.010329502699738842
Epoch: 185 Batch: 3000
Training Loss: 0.009699285109837849
Epoch: 185 Batch: 3050
Training Loss: 0.010273489356040954
Epoch: 185 Batch: 3100
Training Loss: 0.010289814875971887
Epoch: 185 Batch: 3150
Training Loss: 0.009476129601872156
Epoch: 185 Batch: 3200
Training Loss: 0.009786474080756306
Epoch: 186 
 Validation Loss: 0.4695697396993637
---------------------------
Epoch: 186 Batch: 50
Training Loss: 0.5993142747879028
Epoch: 186 Batch: 100
Training Loss: 0.30488125890493395
Epoch: 186 Batch: 150
Training Loss: 0.2040965586900711
Epoch: 186 Batch: 200
Training Loss: 0.15629251092672347
Epoch: 186 Batch: 250
Training Loss: 0.1226010318994522
Epoch: 186 Batch: 300
Training Loss: 0.09968585620323817
Epoch: 186 Batch: 350
Training Loss: 0.08777691968849727
Epoch: 186 Batch: 400
Training Loss: 0.07760420948266983
Epoch: 186 Batch: 450
Training Loss: 0.06844675971402062
Epoch: 186 Batch: 500
Training Loss: 0.059582508981227876
Epoch: 186 Batch: 550
Training Loss: 0.054262230558828874
Epoch: 186 Batch: 600
Training Loss: 0.05253425369660059
Epoch: 186 Batch: 650
Training Loss: 0.046094972078616805
Epoch: 186 Batch: 700
Training Loss: 0.04094441111598696
Epoch: 186 Batch: 750
Training Loss: 0.041012279987335205
Epoch: 186 Batch: 800
Training Loss: 0.03788576602935791
Epoch: 186 Batch: 850
Training Loss: 0.036536514969433054
Epoch: 186 Batch: 900
Training Loss: 0.034003410471810235
Epoch: 186 Batch: 950
Training Loss: 0.030214643760731345
Epoch: 186 Batch: 1000
Training Loss: 0.02912386202812195
Epoch: 186 Batch: 1050
Training Loss: 0.030779934184891836
Epoch: 186 Batch: 1100
Training Loss: 0.02786295703866265
Epoch: 186 Batch: 1150
Training Loss: 0.026133586323779563
Epoch: 186 Batch: 1200
Training Loss: 0.025850122744838396
Epoch: 186 Batch: 1250
Training Loss: 0.023924931764602662
Epoch: 186 Batch: 1300
Training Loss: 0.023606292467850905
Epoch: 186 Batch: 1350
Training Loss: 0.0228634774684906
Epoch: 186 Batch: 1400
Training Loss: 0.02241452123437609
Epoch: 186 Batch: 1450
Training Loss: 0.020818503684011
Epoch: 186 Batch: 1500
Training Loss: 0.019911330699920654
Epoch: 186 Batch: 1550
Training Loss: 0.019657548377590796
Epoch: 186 Batch: 1600
Training Loss: 0.01882190428674221
Epoch: 186 Batch: 1650
Training Loss: 0.018385347561402754
Epoch: 186 Batch: 1700
Training Loss: 0.017718908523812013
Epoch: 186 Batch: 1750
Training Loss: 0.017126239572252545
Epoch: 186 Batch: 1800
Training Loss: 0.01734138756990433
Epoch: 186 Batch: 1850
Training Loss: 0.017228005560668738
Epoch: 186 Batch: 1900
Training Loss: 0.015627454189877762
Epoch: 186 Batch: 1950
Training Loss: 0.015901020215107843
Epoch: 186 Batch: 2000
Training Loss: 0.015418629318475723
Epoch: 186 Batch: 2050
Training Loss: 0.014425241045835542
Epoch: 186 Batch: 2100
Training Loss: 0.01463458749509993
Epoch: 186 Batch: 2150
Training Loss: 0.013476093550061071
Epoch: 186 Batch: 2200
Training Loss: 0.013410924713719975
Epoch: 186 Batch: 2250
Training Loss: 0.01390523205863105
Epoch: 186 Batch: 2300
Training Loss: 0.013165402982545935
Epoch: 186 Batch: 2350
Training Loss: 0.012733567471199848
Epoch: 186 Batch: 2400
Training Loss: 0.01235737354805072
Epoch: 186 Batch: 2450
Training Loss: 0.012749431400882954
Epoch: 186 Batch: 2500
Training Loss: 0.012410684227943421
Epoch: 186 Batch: 2550
Training Loss: 0.01149871919669357
Epoch: 186 Batch: 2600
Training Loss: 0.011628779367758678
Epoch: 186 Batch: 2650
Training Loss: 0.01178499994412908
Epoch: 186 Batch: 2700
Training Loss: 0.010978492498397828
Epoch: 186 Batch: 2750
Training Loss: 0.010850061546672474
Epoch: 186 Batch: 2800
Training Loss: 0.010959695875644684
Epoch: 186 Batch: 2850
Training Loss: 0.010684964625458968
Epoch: 186 Batch: 2900
Training Loss: 0.010292882261605098
Epoch: 186 Batch: 2950
Training Loss: 0.010633908520310612
Epoch: 186 Batch: 3000
Training Loss: 0.009694879005352656
Epoch: 186 Batch: 3050
Training Loss: 0.009705103928925561
Epoch: 186 Batch: 3100
Training Loss: 0.009888593025745884
Epoch: 186 Batch: 3150
Training Loss: 0.009501617531927804
Epoch: 186 Batch: 3200
Training Loss: 0.009433404980227352
Epoch: 187 
 Validation Loss: 0.4699633094999525
---------------------------
Epoch: 187 Batch: 50
Training Loss: 0.6147417521476746
Epoch: 187 Batch: 100
Training Loss: 0.31609987407922746
Epoch: 187 Batch: 150
Training Loss: 0.19738526662190756
Epoch: 187 Batch: 200
Training Loss: 0.1519703722000122
Epoch: 187 Batch: 250
Training Loss: 0.1224680427312851
Epoch: 187 Batch: 300
Training Loss: 0.10552248934904734
Epoch: 187 Batch: 350
Training Loss: 0.0867888708625521
Epoch: 187 Batch: 400
Training Loss: 0.07333515137434006
Epoch: 187 Batch: 450
Training Loss: 0.06664878030618032
Epoch: 187 Batch: 500
Training Loss: 0.06213127863407135
Epoch: 187 Batch: 550
Training Loss: 0.05350589627569372
Epoch: 187 Batch: 600
Training Loss: 0.050327587475379305
Epoch: 187 Batch: 650
Training Loss: 0.04771431849553035
Epoch: 187 Batch: 700
Training Loss: 0.04177878873688834
Epoch: 187 Batch: 750
Training Loss: 0.039712894241015116
Epoch: 187 Batch: 800
Training Loss: 0.037846779935061935
Epoch: 187 Batch: 850
Training Loss: 0.03529221057891846
Epoch: 187 Batch: 900
Training Loss: 0.035262812972068784
Epoch: 187 Batch: 950
Training Loss: 0.03135134342469667
Epoch: 187 Batch: 1000
Training Loss: 0.030993724882602693
Epoch: 187 Batch: 1050
Training Loss: 0.028215013997895378
Epoch: 187 Batch: 1100
Training Loss: 0.026375053226947786
Epoch: 187 Batch: 1150
Training Loss: 0.026985868422881417
Epoch: 187 Batch: 1200
Training Loss: 0.024119925027092298
Epoch: 187 Batch: 1250
Training Loss: 0.023387011051177978
Epoch: 187 Batch: 1300
Training Loss: 0.024323351566608135
Epoch: 187 Batch: 1350
Training Loss: 0.023149197300275166
Epoch: 187 Batch: 1400
Training Loss: 0.022623183727264403
Epoch: 187 Batch: 1450
Training Loss: 0.020923609980221452
Epoch: 187 Batch: 1500
Training Loss: 0.02011099698146184
Epoch: 187 Batch: 1550
Training Loss: 0.01970256511242159
Epoch: 187 Batch: 1600
Training Loss: 0.01941105192527175
Epoch: 187 Batch: 1650
Training Loss: 0.018388808640566738
Epoch: 187 Batch: 1700
Training Loss: 0.018794541341416976
Epoch: 187 Batch: 1750
Training Loss: 0.016891439591135298
Epoch: 187 Batch: 1800
Training Loss: 0.016780672801865473
Epoch: 187 Batch: 1850
Training Loss: 0.016186472741333214
Epoch: 187 Batch: 1900
Training Loss: 0.016592944044815868
Epoch: 187 Batch: 1950
Training Loss: 0.015093629421331944
Epoch: 187 Batch: 2000
Training Loss: 0.014801974251866341
Epoch: 187 Batch: 2050
Training Loss: 0.014750494811593032
Epoch: 187 Batch: 2100
Training Loss: 0.014670963159629277
Epoch: 187 Batch: 2150
Training Loss: 0.013988014140794444
Epoch: 187 Batch: 2200
Training Loss: 0.01390917871486057
Epoch: 187 Batch: 2250
Training Loss: 0.013359301500850254
Epoch: 187 Batch: 2300
Training Loss: 0.013556582953618921
Epoch: 187 Batch: 2350
Training Loss: 0.01263857182036055
Epoch: 187 Batch: 2400
Training Loss: 0.012601008179287115
Epoch: 187 Batch: 2450
Training Loss: 0.011872766455825494
Epoch: 187 Batch: 2500
Training Loss: 0.011919491910934448
Epoch: 187 Batch: 2550
Training Loss: 0.011803903509588803
Epoch: 187 Batch: 2600
Training Loss: 0.011256333589553833
Epoch: 187 Batch: 2650
Training Loss: 0.011178069924408535
Epoch: 187 Batch: 2700
Training Loss: 0.011829952244405393
Epoch: 187 Batch: 2750
Training Loss: 0.01118754133311185
Epoch: 187 Batch: 2800
Training Loss: 0.011174699163862637
Epoch: 187 Batch: 2850
Training Loss: 0.010333691741290845
Epoch: 187 Batch: 2900
Training Loss: 0.010868214666843415
Epoch: 187 Batch: 2950
Training Loss: 0.010535227252265154
Epoch: 187 Batch: 3000
Training Loss: 0.0106726320485274
Epoch: 187 Batch: 3050
Training Loss: 0.010148823642339863
Epoch: 187 Batch: 3100
Training Loss: 0.009777634816785013
Epoch: 187 Batch: 3150
Training Loss: 0.009928297797838846
Epoch: 187 Batch: 3200
Training Loss: 0.009814071571454405
Epoch: 188 
 Validation Loss: 0.46935272713502246
---------------------------
Epoch: 188 Batch: 50
Training Loss: 0.6113994055986405
Epoch: 188 Batch: 100
Training Loss: 0.3059588575363159
Epoch: 188 Batch: 150
Training Loss: 0.19453362365563712
Epoch: 188 Batch: 200
Training Loss: 0.15721408575773238
Epoch: 188 Batch: 250
Training Loss: 0.12659479367733
Epoch: 188 Batch: 300
Training Loss: 0.10101100822289785
Epoch: 188 Batch: 350
Training Loss: 0.08401114276477269
Epoch: 188 Batch: 400
Training Loss: 0.07500518336892129
Epoch: 188 Batch: 450
Training Loss: 0.06680245452457004
Epoch: 188 Batch: 500
Training Loss: 0.05911709922552109
Epoch: 188 Batch: 550
Training Loss: 0.05558301335031336
Epoch: 188 Batch: 600
Training Loss: 0.050520157366991045
Epoch: 188 Batch: 650
Training Loss: 0.046234103303689225
Epoch: 188 Batch: 700
Training Loss: 0.04562081285885402
Epoch: 188 Batch: 750
Training Loss: 0.04100100549062093
Epoch: 188 Batch: 800
Training Loss: 0.03692589987069368
Epoch: 188 Batch: 850
Training Loss: 0.0385751967570361
Epoch: 188 Batch: 900
Training Loss: 0.033561747868855796
Epoch: 188 Batch: 950
Training Loss: 0.03162993468736348
Epoch: 188 Batch: 1000
Training Loss: 0.028559552639722825
Epoch: 188 Batch: 1050
Training Loss: 0.02979183072135562
Epoch: 188 Batch: 1100
Training Loss: 0.027976253710009835
Epoch: 188 Batch: 1150
Training Loss: 0.02551363035388615
Epoch: 188 Batch: 1200
Training Loss: 0.025424215371410052
Epoch: 188 Batch: 1250
Training Loss: 0.02410919027328491
Epoch: 188 Batch: 1300
Training Loss: 0.02385032784480315
Epoch: 188 Batch: 1350
Training Loss: 0.022512175771925186
Epoch: 188 Batch: 1400
Training Loss: 0.020603066171918597
Epoch: 188 Batch: 1450
Training Loss: 0.02034414632567044
Epoch: 188 Batch: 1500
Training Loss: 0.02144680521885554
Epoch: 188 Batch: 1550
Training Loss: 0.019720462445289858
Epoch: 188 Batch: 1600
Training Loss: 0.01914508333429694
Epoch: 188 Batch: 1650
Training Loss: 0.017964412407441573
Epoch: 188 Batch: 1700
Training Loss: 0.017589722363387836
Epoch: 188 Batch: 1750
Training Loss: 0.017436839495386398
Epoch: 188 Batch: 1800
Training Loss: 0.017067474557293787
Epoch: 188 Batch: 1850
Training Loss: 0.01696458811695511
Epoch: 188 Batch: 1900
Training Loss: 0.0167152055470567
Epoch: 188 Batch: 1950
Training Loss: 0.015763937907341197
Epoch: 188 Batch: 2000
Training Loss: 0.015330253854393959
Epoch: 188 Batch: 2050
Training Loss: 0.01456919335737461
Epoch: 188 Batch: 2100
Training Loss: 0.014550112627801442
Epoch: 188 Batch: 2150
Training Loss: 0.01428272571674613
Epoch: 188 Batch: 2200
Training Loss: 0.014555904269218444
Epoch: 188 Batch: 2250
Training Loss: 0.013574399775928921
Epoch: 188 Batch: 2300
Training Loss: 0.013298334323841593
Epoch: 188 Batch: 2350
Training Loss: 0.014348496794700622
Epoch: 188 Batch: 2400
Training Loss: 0.013027156392733255
Epoch: 188 Batch: 2450
Training Loss: 0.012573918724546627
Epoch: 188 Batch: 2500
Training Loss: 0.011851316738128661
Epoch: 188 Batch: 2550
Training Loss: 0.011637947056807724
Epoch: 188 Batch: 2600
Training Loss: 0.01161526776277102
Epoch: 188 Batch: 2650
Training Loss: 0.011540393728130269
Epoch: 188 Batch: 2700
Training Loss: 0.011176662643750508
Epoch: 188 Batch: 2750
Training Loss: 0.010768702951344578
Epoch: 188 Batch: 2800
Training Loss: 0.010909281234656062
Epoch: 188 Batch: 2850
Training Loss: 0.01055878998940451
Epoch: 188 Batch: 2900
Training Loss: 0.010696959320841165
Epoch: 188 Batch: 2950
Training Loss: 0.010088361420873868
Epoch: 188 Batch: 3000
Training Loss: 0.01019720329840978
Epoch: 188 Batch: 3050
Training Loss: 0.009132524660376251
Epoch: 188 Batch: 3100
Training Loss: 0.010040682611926909
Epoch: 188 Batch: 3150
Training Loss: 0.00931761710416703
Epoch: 188 Batch: 3200
Training Loss: 0.009092702511698008
Epoch: 189 
 Validation Loss: 0.46943016946315763
---------------------------
Epoch: 189 Batch: 50
Training Loss: 0.6066459327936172
Epoch: 189 Batch: 100
Training Loss: 0.3214878880977631
Epoch: 189 Batch: 150
Training Loss: 0.19437910854816437
Epoch: 189 Batch: 200
Training Loss: 0.14905510365962982
Epoch: 189 Batch: 250
Training Loss: 0.11958556163311004
Epoch: 189 Batch: 300
Training Loss: 0.10346533993879954
Epoch: 189 Batch: 350
Training Loss: 0.08742836126259396
Epoch: 189 Batch: 400
Training Loss: 0.07470413751900196
Epoch: 189 Batch: 450
Training Loss: 0.06585117717583974
Epoch: 189 Batch: 500
Training Loss: 0.06544450408220291
Epoch: 189 Batch: 550
Training Loss: 0.05445462075146762
Epoch: 189 Batch: 600
Training Loss: 0.051263672610123955
Epoch: 189 Batch: 650
Training Loss: 0.043751268432690546
Epoch: 189 Batch: 700
Training Loss: 0.04549694921289171
Epoch: 189 Batch: 750
Training Loss: 0.0401263039112091
Epoch: 189 Batch: 800
Training Loss: 0.03816235017031431
Epoch: 189 Batch: 850
Training Loss: 0.035660876631736754
Epoch: 189 Batch: 900
Training Loss: 0.03409134222401513
Epoch: 189 Batch: 950
Training Loss: 0.03194453145328321
Epoch: 189 Batch: 1000
Training Loss: 0.02946073043346405
Epoch: 189 Batch: 1050
Training Loss: 0.028354882938521248
Epoch: 189 Batch: 1100
Training Loss: 0.02706787729805166
Epoch: 189 Batch: 1150
Training Loss: 0.026660507813743923
Epoch: 189 Batch: 1200
Training Loss: 0.023791834736863773
Epoch: 189 Batch: 1250
Training Loss: 0.024232192492485045
Epoch: 189 Batch: 1300
Training Loss: 0.023476328666393573
Epoch: 189 Batch: 1350
Training Loss: 0.022996593294320283
Epoch: 189 Batch: 1400
Training Loss: 0.02203003900391715
Epoch: 189 Batch: 1450
Training Loss: 0.02045991706437078
Epoch: 189 Batch: 1500
Training Loss: 0.020077007949352264
Epoch: 189 Batch: 1550
Training Loss: 0.019524132032548228
Epoch: 189 Batch: 1600
Training Loss: 0.01906141508370638
Epoch: 189 Batch: 1650
Training Loss: 0.018996583440087058
Epoch: 189 Batch: 1700
Training Loss: 0.01785756232107387
Epoch: 189 Batch: 1750
Training Loss: 0.016728641067232406
Epoch: 189 Batch: 1800
Training Loss: 0.016833888292312624
Epoch: 189 Batch: 1850
Training Loss: 0.016441617108680105
Epoch: 189 Batch: 1900
Training Loss: 0.01635708118739881
Epoch: 189 Batch: 1950
Training Loss: 0.01586140392682491
Epoch: 189 Batch: 2000
Training Loss: 0.015254219651222228
Epoch: 189 Batch: 2050
Training Loss: 0.014389443877266674
Epoch: 189 Batch: 2100
Training Loss: 0.014677227834860484
Epoch: 189 Batch: 2150
Training Loss: 0.013655246246692745
Epoch: 189 Batch: 2200
Training Loss: 0.014199647320942445
Epoch: 189 Batch: 2250
Training Loss: 0.013468338555759855
Epoch: 189 Batch: 2300
Training Loss: 0.012935481058514636
Epoch: 189 Batch: 2350
Training Loss: 0.01282101943137798
Epoch: 189 Batch: 2400
Training Loss: 0.01311769721408685
Epoch: 189 Batch: 2450
Training Loss: 0.01227422096291367
Epoch: 189 Batch: 2500
Training Loss: 0.011334304869174958
Epoch: 189 Batch: 2550
Training Loss: 0.012294926877115288
Epoch: 189 Batch: 2600
Training Loss: 0.012103970692707942
Epoch: 189 Batch: 2650
Training Loss: 0.011765630368916494
Epoch: 189 Batch: 2700
Training Loss: 0.011301212222487839
Epoch: 189 Batch: 2750
Training Loss: 0.011757923548871821
Epoch: 189 Batch: 2800
Training Loss: 0.010549150045428958
Epoch: 189 Batch: 2850
Training Loss: 0.010970844239519353
Epoch: 189 Batch: 2900
Training Loss: 0.010174356678436542
Epoch: 189 Batch: 2950
Training Loss: 0.0103287530248448
Epoch: 189 Batch: 3000
Training Loss: 0.009830813874801
Epoch: 189 Batch: 3050
Training Loss: 0.01028521680441059
Epoch: 189 Batch: 3100
Training Loss: 0.009992970978060076
Epoch: 189 Batch: 3150
Training Loss: 0.009680825273195902
Epoch: 189 Batch: 3200
Training Loss: 0.00949211691506207
Epoch: 190 
 Validation Loss: 0.46959855655829114
---------------------------
Epoch: 190 Batch: 50
Training Loss: 0.6218669527769088
Epoch: 190 Batch: 100
Training Loss: 0.28811021089553834
Epoch: 190 Batch: 150
Training Loss: 0.194699596563975
Epoch: 190 Batch: 200
Training Loss: 0.15842057079076766
Epoch: 190 Batch: 250
Training Loss: 0.12318626022338867
Epoch: 190 Batch: 300
Training Loss: 0.10118712961673737
Epoch: 190 Batch: 350
Training Loss: 0.0863690688780376
Epoch: 190 Batch: 400
Training Loss: 0.07667460858821869
Epoch: 190 Batch: 450
Training Loss: 0.0674397787782881
Epoch: 190 Batch: 500
Training Loss: 0.06253793317079544
Epoch: 190 Batch: 550
Training Loss: 0.057588672908869654
Epoch: 190 Batch: 600
Training Loss: 0.05176910579204559
Epoch: 190 Batch: 650
Training Loss: 0.04605132254270407
Epoch: 190 Batch: 700
Training Loss: 0.04296999841928482
Epoch: 190 Batch: 750
Training Loss: 0.041140140414237976
Epoch: 190 Batch: 800
Training Loss: 0.03660604573786259
Epoch: 190 Batch: 850
Training Loss: 0.03524261684978709
Epoch: 190 Batch: 900
Training Loss: 0.033425154089927675
Epoch: 190 Batch: 950
Training Loss: 0.03028542371172654
Epoch: 190 Batch: 1000
Training Loss: 0.029819759249687196
Epoch: 190 Batch: 1050
Training Loss: 0.027926199010440283
Epoch: 190 Batch: 1100
Training Loss: 0.02820688472552733
Epoch: 190 Batch: 1150
Training Loss: 0.026721359646838645
Epoch: 190 Batch: 1200
Training Loss: 0.026144398426016173
Epoch: 190 Batch: 1250
Training Loss: 0.02342929697036743
Epoch: 190 Batch: 1300
Training Loss: 0.023797155160170336
Epoch: 190 Batch: 1350
Training Loss: 0.022450412489749768
Epoch: 190 Batch: 1400
Training Loss: 0.02123032572013991
Epoch: 190 Batch: 1450
Training Loss: 0.02037062127014686
Epoch: 190 Batch: 1500
Training Loss: 0.02059182858467102
Epoch: 190 Batch: 1550
Training Loss: 0.019121250048760446
Epoch: 190 Batch: 1600
Training Loss: 0.019446079842746256
Epoch: 190 Batch: 1650
Training Loss: 0.01920774058862166
Epoch: 190 Batch: 1700
Training Loss: 0.016984602125251994
Epoch: 190 Batch: 1750
Training Loss: 0.017174679960523333
Epoch: 190 Batch: 1800
Training Loss: 0.018096927520301606
Epoch: 190 Batch: 1850
Training Loss: 0.016648938543087726
Epoch: 190 Batch: 1900
Training Loss: 0.016410172942437625
Epoch: 190 Batch: 1950
Training Loss: 0.014772662902489686
Epoch: 190 Batch: 2000
Training Loss: 0.015403291016817092
Epoch: 190 Batch: 2050
Training Loss: 0.014863773279073761
Epoch: 190 Batch: 2100
Training Loss: 0.014113576185135614
Epoch: 190 Batch: 2150
Training Loss: 0.014213381076967993
Epoch: 190 Batch: 2200
Training Loss: 0.013650078123266046
Epoch: 190 Batch: 2250
Training Loss: 0.013795460409588284
Epoch: 190 Batch: 2300
Training Loss: 0.013582469263802403
Epoch: 190 Batch: 2350
Training Loss: 0.013173375700382475
Epoch: 190 Batch: 2400
Training Loss: 0.012410434149205684
Epoch: 190 Batch: 2450
Training Loss: 0.011811949245783748
Epoch: 190 Batch: 2500
Training Loss: 0.013213885641098022
Epoch: 190 Batch: 2550
Training Loss: 0.01190481397451139
Epoch: 190 Batch: 2600
Training Loss: 0.011639609405627617
Epoch: 190 Batch: 2650
Training Loss: 0.0114119239235824
Epoch: 190 Batch: 2700
Training Loss: 0.010887854982305456
Epoch: 190 Batch: 2750
Training Loss: 0.01077346676046198
Epoch: 190 Batch: 2800
Training Loss: 0.010664299726486207
Epoch: 190 Batch: 2850
Training Loss: 0.010723412308776588
Epoch: 190 Batch: 2900
Training Loss: 0.010607387351578679
Epoch: 190 Batch: 2950
Training Loss: 0.010726630778635962
Epoch: 190 Batch: 3000
Training Loss: 0.010591575642426809
Epoch: 190 Batch: 3050
Training Loss: 0.009723824966149252
Epoch: 190 Batch: 3100
Training Loss: 0.009881835272235255
Epoch: 190 Batch: 3150
Training Loss: 0.009249285601434254
Epoch: 190 Batch: 3200
Training Loss: 0.009462686199694871
Epoch: 191 
 Validation Loss: 0.46936062210135987
---------------------------
Epoch: 191 Batch: 50
Training Loss: 0.6042457246780395
Epoch: 191 Batch: 100
Training Loss: 0.30694738507270813
Epoch: 191 Batch: 150
Training Loss: 0.2017747277021408
Epoch: 191 Batch: 200
Training Loss: 0.14810173988342284
Epoch: 191 Batch: 250
Training Loss: 0.11971234118938447
Epoch: 191 Batch: 300
Training Loss: 0.10053591132164001
Epoch: 191 Batch: 350
Training Loss: 0.08400880541120256
Epoch: 191 Batch: 400
Training Loss: 0.07597759306430817
Epoch: 191 Batch: 450
Training Loss: 0.06658464438385434
Epoch: 191 Batch: 500
Training Loss: 0.06244825792312622
Epoch: 191 Batch: 550
Training Loss: 0.0555265733870593
Epoch: 191 Batch: 600
Training Loss: 0.04971931959191958
Epoch: 191 Batch: 650
Training Loss: 0.045419355172377364
Epoch: 191 Batch: 700
Training Loss: 0.042667231346879685
Epoch: 191 Batch: 750
Training Loss: 0.041097954551378886
Epoch: 191 Batch: 800
Training Loss: 0.03576952002942562
Epoch: 191 Batch: 850
Training Loss: 0.03380995129837709
Epoch: 191 Batch: 900
Training Loss: 0.03421173317564858
Epoch: 191 Batch: 950
Training Loss: 0.03186408667187942
Epoch: 191 Batch: 1000
Training Loss: 0.03039837843179703
Epoch: 191 Batch: 1050
Training Loss: 0.02931887734503973
Epoch: 191 Batch: 1100
Training Loss: 0.028498766070062465
Epoch: 191 Batch: 1150
Training Loss: 0.026483010364615398
Epoch: 191 Batch: 1200
Training Loss: 0.02473276071250439
Epoch: 191 Batch: 1250
Training Loss: 0.02238100538253784
Epoch: 191 Batch: 1300
Training Loss: 0.024009134265092703
Epoch: 191 Batch: 1350
Training Loss: 0.023454008080341197
Epoch: 191 Batch: 1400
Training Loss: 0.02151879472391946
Epoch: 191 Batch: 1450
Training Loss: 0.02139858578813487
Epoch: 191 Batch: 1500
Training Loss: 0.019691166420777638
Epoch: 191 Batch: 1550
Training Loss: 0.01963698460209754
Epoch: 191 Batch: 1600
Training Loss: 0.019167932625859977
Epoch: 191 Batch: 1650
Training Loss: 0.018955700704545685
Epoch: 191 Batch: 1700
Training Loss: 0.01796236702624489
Epoch: 191 Batch: 1750
Training Loss: 0.01708112839290074
Epoch: 191 Batch: 1800
Training Loss: 0.016833349714676538
Epoch: 191 Batch: 1850
Training Loss: 0.016209571071573205
Epoch: 191 Batch: 1900
Training Loss: 0.015901167831922833
Epoch: 191 Batch: 1950
Training Loss: 0.01598860665773734
Epoch: 191 Batch: 2000
Training Loss: 0.015086882874369621
Epoch: 191 Batch: 2050
Training Loss: 0.015345626662417155
Epoch: 191 Batch: 2100
Training Loss: 0.01388054255928312
Epoch: 191 Batch: 2150
Training Loss: 0.014425663407458815
Epoch: 191 Batch: 2200
Training Loss: 0.01341806949539618
Epoch: 191 Batch: 2250
Training Loss: 0.013216349853409661
Epoch: 191 Batch: 2300
Training Loss: 0.01392109110303547
Epoch: 191 Batch: 2350
Training Loss: 0.01268433782648533
Epoch: 191 Batch: 2400
Training Loss: 0.012848704991241296
Epoch: 191 Batch: 2450
Training Loss: 0.012722496937732307
Epoch: 191 Batch: 2500
Training Loss: 0.012499075961112976
Epoch: 191 Batch: 2550
Training Loss: 0.011762143876038345
Epoch: 191 Batch: 2600
Training Loss: 0.01199292725095382
Epoch: 191 Batch: 2650
Training Loss: 0.011302300129296644
Epoch: 191 Batch: 2700
Training Loss: 0.01063016238035979
Epoch: 191 Batch: 2750
Training Loss: 0.011254114747047425
Epoch: 191 Batch: 2800
Training Loss: 0.010564042063696044
Epoch: 191 Batch: 2850
Training Loss: 0.010055707734927796
Epoch: 191 Batch: 2900
Training Loss: 0.010777802302919585
Epoch: 191 Batch: 2950
Training Loss: 0.010500012363417674
Epoch: 191 Batch: 3000
Training Loss: 0.010173643559217453
Epoch: 191 Batch: 3050
Training Loss: 0.009702662542218068
Epoch: 191 Batch: 3100
Training Loss: 0.009977549420249077
Epoch: 191 Batch: 3150
Training Loss: 0.009595189917655218
Epoch: 191 Batch: 3200
Training Loss: 0.009250316070392728
Epoch: 192 
 Validation Loss: 0.46928407351175944
---------------------------
Epoch: 192 Batch: 50
Training Loss: 0.6357093048095703
Epoch: 192 Batch: 100
Training Loss: 0.3054072150588036
Epoch: 192 Batch: 150
Training Loss: 0.2056463575363159
Epoch: 192 Batch: 200
Training Loss: 0.14499625518918038
Epoch: 192 Batch: 250
Training Loss: 0.127310600399971
Epoch: 192 Batch: 300
Training Loss: 0.10452827036380768
Epoch: 192 Batch: 350
Training Loss: 0.08596675132002149
Epoch: 192 Batch: 400
Training Loss: 0.07820558331906796
Epoch: 192 Batch: 450
Training Loss: 0.06387541174888611
Epoch: 192 Batch: 500
Training Loss: 0.060355858683586124
Epoch: 192 Batch: 550
Training Loss: 0.059908273978666826
Epoch: 192 Batch: 600
Training Loss: 0.05067870741089185
Epoch: 192 Batch: 650
Training Loss: 0.04500469171083891
Epoch: 192 Batch: 700
Training Loss: 0.04562317086117608
Epoch: 192 Batch: 750
Training Loss: 0.03921195451418559
Epoch: 192 Batch: 800
Training Loss: 0.03765423510223627
Epoch: 192 Batch: 850
Training Loss: 0.03486413110704983
Epoch: 192 Batch: 900
Training Loss: 0.033677453796068825
Epoch: 192 Batch: 950
Training Loss: 0.031087912979878878
Epoch: 192 Batch: 1000
Training Loss: 0.02937439039349556
Epoch: 192 Batch: 1050
Training Loss: 0.028389278934115456
Epoch: 192 Batch: 1100
Training Loss: 0.027786884199489246
Epoch: 192 Batch: 1150
Training Loss: 0.02555577145970386
Epoch: 192 Batch: 1200
Training Loss: 0.025328861052791276
Epoch: 192 Batch: 1250
Training Loss: 0.024799531722068786
Epoch: 192 Batch: 1300
Training Loss: 0.022903790955360118
Epoch: 192 Batch: 1350
Training Loss: 0.022069519978982433
Epoch: 192 Batch: 1400
Training Loss: 0.02114266474332128
Epoch: 192 Batch: 1450
Training Loss: 0.020558318886263617
Epoch: 192 Batch: 1500
Training Loss: 0.02046952341000239
Epoch: 192 Batch: 1550
Training Loss: 0.019620171823809224
Epoch: 192 Batch: 1600
Training Loss: 0.019783339258283378
Epoch: 192 Batch: 1650
Training Loss: 0.01905156939318686
Epoch: 192 Batch: 1700
Training Loss: 0.017449412047863006
Epoch: 192 Batch: 1750
Training Loss: 0.0177999849149159
Epoch: 192 Batch: 1800
Training Loss: 0.016700793074236976
Epoch: 192 Batch: 1850
Training Loss: 0.016352043699573826
Epoch: 192 Batch: 1900
Training Loss: 0.01613969718155108
Epoch: 192 Batch: 1950
Training Loss: 0.015756215147483044
Epoch: 192 Batch: 2000
Training Loss: 0.014825667321681977
Epoch: 192 Batch: 2050
Training Loss: 0.014805138052963629
Epoch: 192 Batch: 2100
Training Loss: 0.014303028697059269
Epoch: 192 Batch: 2150
Training Loss: 0.014497667024301927
Epoch: 192 Batch: 2200
Training Loss: 0.013854217624122447
Epoch: 192 Batch: 2250
Training Loss: 0.01402705192565918
Epoch: 192 Batch: 2300
Training Loss: 0.012646168690660726
Epoch: 192 Batch: 2350
Training Loss: 0.013241064129991735
Epoch: 192 Batch: 2400
Training Loss: 0.012138795567055544
Epoch: 192 Batch: 2450
Training Loss: 0.012171546330257337
Epoch: 192 Batch: 2500
Training Loss: 0.012214161217212678
Epoch: 192 Batch: 2550
Training Loss: 0.01248064115935681
Epoch: 192 Batch: 2600
Training Loss: 0.011841229899571492
Epoch: 192 Batch: 2650
Training Loss: 0.011133748720277031
Epoch: 192 Batch: 2700
Training Loss: 0.011889753032613684
Epoch: 192 Batch: 2750
Training Loss: 0.011131433421915228
Epoch: 192 Batch: 2800
Training Loss: 0.010684603367533003
Epoch: 192 Batch: 2850
Training Loss: 0.010625102917353312
Epoch: 192 Batch: 2900
Training Loss: 0.010736660875123122
Epoch: 192 Batch: 2950
Training Loss: 0.010296375084731538
Epoch: 192 Batch: 3000
Training Loss: 0.009787939876317977
Epoch: 192 Batch: 3050
Training Loss: 0.00999173140916668
Epoch: 192 Batch: 3100
Training Loss: 0.009661427928555396
Epoch: 192 Batch: 3150
Training Loss: 0.0096164370813067
Epoch: 192 Batch: 3200
Training Loss: 0.009186118636280298
Epoch: 193 
 Validation Loss: 0.46882153385215336
---------------------------
Epoch: 193 Batch: 50
Training Loss: 0.6162220060825347
Epoch: 193 Batch: 100
Training Loss: 0.30189382284879684
Epoch: 193 Batch: 150
Training Loss: 0.20272460281848909
Epoch: 193 Batch: 200
Training Loss: 0.14826540410518646
Epoch: 193 Batch: 250
Training Loss: 0.12448877882957458
Epoch: 193 Batch: 300
Training Loss: 0.10201882153749466
Epoch: 193 Batch: 350
Training Loss: 0.0860009583405086
Epoch: 193 Batch: 400
Training Loss: 0.07424640379846097
Epoch: 193 Batch: 450
Training Loss: 0.06780744949976603
Epoch: 193 Batch: 500
Training Loss: 0.06010362124443054
Epoch: 193 Batch: 550
Training Loss: 0.05466821686788039
Epoch: 193 Batch: 600
Training Loss: 0.05147964974244436
Epoch: 193 Batch: 650
Training Loss: 0.04557874280672807
Epoch: 193 Batch: 700
Training Loss: 0.043556056533541
Epoch: 193 Batch: 750
Training Loss: 0.03975102746486664
Epoch: 193 Batch: 800
Training Loss: 0.03817566245794296
Epoch: 193 Batch: 850
Training Loss: 0.033327672902275535
Epoch: 193 Batch: 900
Training Loss: 0.03355623265107473
Epoch: 193 Batch: 950
Training Loss: 0.032922404973130474
Epoch: 193 Batch: 1000
Training Loss: 0.03091663420200348
Epoch: 193 Batch: 1050
Training Loss: 0.02986116622175489
Epoch: 193 Batch: 1100
Training Loss: 0.027513410963795403
Epoch: 193 Batch: 1150
Training Loss: 0.026041059701339058
Epoch: 193 Batch: 1200
Training Loss: 0.026318241407473884
Epoch: 193 Batch: 1250
Training Loss: 0.024127953958511354
Epoch: 193 Batch: 1300
Training Loss: 0.023233329493265887
Epoch: 193 Batch: 1350
Training Loss: 0.0230537145446848
Epoch: 193 Batch: 1400
Training Loss: 0.022358462469918388
Epoch: 193 Batch: 1450
Training Loss: 0.020494394898414613
Epoch: 193 Batch: 1500
Training Loss: 0.019360703428586323
Epoch: 193 Batch: 1550
Training Loss: 0.01948354944106071
Epoch: 193 Batch: 1600
Training Loss: 0.0194928234256804
Epoch: 193 Batch: 1650
Training Loss: 0.018469841877619426
Epoch: 193 Batch: 1700
Training Loss: 0.017991977228837854
Epoch: 193 Batch: 1750
Training Loss: 0.016815003769738334
Epoch: 193 Batch: 1800
Training Loss: 0.016112030264404085
Epoch: 193 Batch: 1850
Training Loss: 0.017263699769973755
Epoch: 193 Batch: 1900
Training Loss: 0.016411230124925312
Epoch: 193 Batch: 1950
Training Loss: 0.01600723659380888
Epoch: 193 Batch: 2000
Training Loss: 0.01577475969493389
Epoch: 193 Batch: 2050
Training Loss: 0.014615069831289896
Epoch: 193 Batch: 2100
Training Loss: 0.01376492033402125
Epoch: 193 Batch: 2150
Training Loss: 0.014338797580364139
Epoch: 193 Batch: 2200
Training Loss: 0.01451443532651121
Epoch: 193 Batch: 2250
Training Loss: 0.0137059193054835
Epoch: 193 Batch: 2300
Training Loss: 0.012902783440506977
Epoch: 193 Batch: 2350
Training Loss: 0.012659109386991947
Epoch: 193 Batch: 2400
Training Loss: 0.012469541070361932
Epoch: 193 Batch: 2450
Training Loss: 0.01223104004957238
Epoch: 193 Batch: 2500
Training Loss: 0.01230324867963791
Epoch: 193 Batch: 2550
Training Loss: 0.011696536985098148
Epoch: 193 Batch: 2600
Training Loss: 0.011529201452548687
Epoch: 193 Batch: 2650
Training Loss: 0.011264784268613132
Epoch: 193 Batch: 2700
Training Loss: 0.011720513447567268
Epoch: 193 Batch: 2750
Training Loss: 0.011202784158966759
Epoch: 193 Batch: 2800
Training Loss: 0.010574236437678337
Epoch: 193 Batch: 2850
Training Loss: 0.01051235136232878
Epoch: 193 Batch: 2900
Training Loss: 0.010555327031119116
Epoch: 193 Batch: 2950
Training Loss: 0.010188880502167395
Epoch: 193 Batch: 3000
Training Loss: 0.010370930343866349
Epoch: 193 Batch: 3050
Training Loss: 0.010043786822772417
Epoch: 193 Batch: 3100
Training Loss: 0.00994868359258098
Epoch: 193 Batch: 3150
Training Loss: 0.009508759246932137
Epoch: 193 Batch: 3200
Training Loss: 0.009332251567393542
Epoch: 194 
 Validation Loss: 0.46948291261990865
---------------------------
Epoch: 194 Batch: 50
Training Loss: 0.6077987217903137
Epoch: 194 Batch: 100
Training Loss: 0.296250524520874
Epoch: 194 Batch: 150
Training Loss: 0.19917530218760174
Epoch: 194 Batch: 200
Training Loss: 0.1487300544977188
Epoch: 194 Batch: 250
Training Loss: 0.12051090228557587
Epoch: 194 Batch: 300
Training Loss: 0.10378848512967427
Epoch: 194 Batch: 350
Training Loss: 0.09029598874705179
Epoch: 194 Batch: 400
Training Loss: 0.07580259501934052
Epoch: 194 Batch: 450
Training Loss: 0.06828099158075121
Epoch: 194 Batch: 500
Training Loss: 0.06036918461322784
Epoch: 194 Batch: 550
Training Loss: 0.05660089882937345
Epoch: 194 Batch: 600
Training Loss: 0.053505428383747736
Epoch: 194 Batch: 650
Training Loss: 0.046812039980521566
Epoch: 194 Batch: 700
Training Loss: 0.04375390670129231
Epoch: 194 Batch: 750
Training Loss: 0.04018074758847554
Epoch: 194 Batch: 800
Training Loss: 0.03789091903716326
Epoch: 194 Batch: 850
Training Loss: 0.03528189473292407
Epoch: 194 Batch: 900
Training Loss: 0.033781722353564365
Epoch: 194 Batch: 950
Training Loss: 0.030391640223954852
Epoch: 194 Batch: 1000
Training Loss: 0.030340485632419586
Epoch: 194 Batch: 1050
Training Loss: 0.029314470943950472
Epoch: 194 Batch: 1100
Training Loss: 0.027533736201849852
Epoch: 194 Batch: 1150
Training Loss: 0.02750035498453223
Epoch: 194 Batch: 1200
Training Loss: 0.025239795222878458
Epoch: 194 Batch: 1250
Training Loss: 0.02499838924407959
Epoch: 194 Batch: 1300
Training Loss: 0.022404831326924838
Epoch: 194 Batch: 1350
Training Loss: 0.022501548329989116
Epoch: 194 Batch: 1400
Training Loss: 0.02160348179084914
Epoch: 194 Batch: 1450
Training Loss: 0.020958054723410773
Epoch: 194 Batch: 1500
Training Loss: 0.020359781165917714
Epoch: 194 Batch: 1550
Training Loss: 0.019244565944517813
Epoch: 194 Batch: 1600
Training Loss: 0.0184200269728899
Epoch: 194 Batch: 1650
Training Loss: 0.01842257326299494
Epoch: 194 Batch: 1700
Training Loss: 0.01737211634131039
Epoch: 194 Batch: 1750
Training Loss: 0.017696756226675853
Epoch: 194 Batch: 1800
Training Loss: 0.016424586822589238
Epoch: 194 Batch: 1850
Training Loss: 0.017056206690298545
Epoch: 194 Batch: 1900
Training Loss: 0.016212967994966004
Epoch: 194 Batch: 1950
Training Loss: 0.014506843991768666
Epoch: 194 Batch: 2000
Training Loss: 0.015391179516911506
Epoch: 194 Batch: 2050
Training Loss: 0.015021041922452974
Epoch: 194 Batch: 2100
Training Loss: 0.014790289998054505
Epoch: 194 Batch: 2150
Training Loss: 0.013974994074466616
Epoch: 194 Batch: 2200
Training Loss: 0.014419485438953746
Epoch: 194 Batch: 2250
Training Loss: 0.01381235941251119
Epoch: 194 Batch: 2300
Training Loss: 0.013184716390526813
Epoch: 194 Batch: 2350
Training Loss: 0.013327084944603291
Epoch: 194 Batch: 2400
Training Loss: 0.012383260118464629
Epoch: 194 Batch: 2450
Training Loss: 0.01203388935449172
Epoch: 194 Batch: 2500
Training Loss: 0.012144605541229248
Epoch: 194 Batch: 2550
Training Loss: 0.01239869468352374
Epoch: 194 Batch: 2600
Training Loss: 0.011681347970779125
Epoch: 194 Batch: 2650
Training Loss: 0.010457349307132217
Epoch: 194 Batch: 2700
Training Loss: 0.011277666533434833
Epoch: 194 Batch: 2750
Training Loss: 0.011201460859992287
Epoch: 194 Batch: 2800
Training Loss: 0.011064974750791276
Epoch: 194 Batch: 2850
Training Loss: 0.010553459012717531
Epoch: 194 Batch: 2900
Training Loss: 0.010485115750082609
Epoch: 194 Batch: 2950
Training Loss: 0.010388003911002208
Epoch: 194 Batch: 3000
Training Loss: 0.010027027090390524
Epoch: 194 Batch: 3050
Training Loss: 0.009917177479775226
Epoch: 194 Batch: 3100
Training Loss: 0.009951906540701466
Epoch: 194 Batch: 3150
Training Loss: 0.009674482884861174
Epoch: 194 Batch: 3200
Training Loss: 0.009003964206203818
Epoch: 195 
 Validation Loss: 0.46941918267144095
---------------------------
Epoch: 195 Batch: 50
Training Loss: 0.6254828798770905
Epoch: 195 Batch: 100
Training Loss: 0.3030310943722725
Epoch: 195 Batch: 150
Training Loss: 0.20963942428429921
Epoch: 195 Batch: 200
Training Loss: 0.14922329917550087
Epoch: 195 Batch: 250
Training Loss: 0.11756691539287567
Epoch: 195 Batch: 300
Training Loss: 0.10443903625011444
Epoch: 195 Batch: 350
Training Loss: 0.08945961398737771
Epoch: 195 Batch: 400
Training Loss: 0.07464180536568164
Epoch: 195 Batch: 450
Training Loss: 0.06865579300456577
Epoch: 195 Batch: 500
Training Loss: 0.061244727730751035
Epoch: 195 Batch: 550
Training Loss: 0.05835154592990875
Epoch: 195 Batch: 600
Training Loss: 0.04905881330370903
Epoch: 195 Batch: 650
Training Loss: 0.04483092032946073
Epoch: 195 Batch: 700
Training Loss: 0.04441407301596233
Epoch: 195 Batch: 750
Training Loss: 0.03915210656325022
Epoch: 195 Batch: 800
Training Loss: 0.03685477875173092
Epoch: 195 Batch: 850
Training Loss: 0.03692843580947203
Epoch: 195 Batch: 900
Training Loss: 0.03313146197133594
Epoch: 195 Batch: 950
Training Loss: 0.03249913416410748
Epoch: 195 Batch: 1000
Training Loss: 0.02945781147480011
Epoch: 195 Batch: 1050
Training Loss: 0.02869055112202962
Epoch: 195 Batch: 1100
Training Loss: 0.027339560335332698
Epoch: 195 Batch: 1150
Training Loss: 0.02612144024475761
Epoch: 195 Batch: 1200
Training Loss: 0.02458755744000276
Epoch: 195 Batch: 1250
Training Loss: 0.02451025080680847
Epoch: 195 Batch: 1300
Training Loss: 0.022632470772816583
Epoch: 195 Batch: 1350
Training Loss: 0.022721301560048705
Epoch: 195 Batch: 1400
Training Loss: 0.02136481429849352
Epoch: 195 Batch: 1450
Training Loss: 0.020855813848561253
Epoch: 195 Batch: 1500
Training Loss: 0.02019675483306249
Epoch: 195 Batch: 1550
Training Loss: 0.019758631067891275
Epoch: 195 Batch: 1600
Training Loss: 0.017752556391060353
Epoch: 195 Batch: 1650
Training Loss: 0.018222757921074376
Epoch: 195 Batch: 1700
Training Loss: 0.01750050143283956
Epoch: 195 Batch: 1750
Training Loss: 0.01723192547048841
Epoch: 195 Batch: 1800
Training Loss: 0.016848847137557136
Epoch: 195 Batch: 1850
Training Loss: 0.016369242587605037
Epoch: 195 Batch: 1900
Training Loss: 0.015760016331547184
Epoch: 195 Batch: 1950
Training Loss: 0.016041552118766
Epoch: 195 Batch: 2000
Training Loss: 0.015704004570841788
Epoch: 195 Batch: 2050
Training Loss: 0.014288501303370406
Epoch: 195 Batch: 2100
Training Loss: 0.014395541946093241
Epoch: 195 Batch: 2150
Training Loss: 0.013608756217845651
Epoch: 195 Batch: 2200
Training Loss: 0.014352970841256056
Epoch: 195 Batch: 2250
Training Loss: 0.013927521003617181
Epoch: 195 Batch: 2300
Training Loss: 0.014102932745995729
Epoch: 195 Batch: 2350
Training Loss: 0.012890563429670132
Epoch: 195 Batch: 2400
Training Loss: 0.012873942802349726
Epoch: 195 Batch: 2450
Training Loss: 0.012454978568213327
Epoch: 195 Batch: 2500
Training Loss: 0.01215663995742798
Epoch: 195 Batch: 2550
Training Loss: 0.01172792512996524
Epoch: 195 Batch: 2600
Training Loss: 0.01191028777223367
Epoch: 195 Batch: 2650
Training Loss: 0.01150745192788682
Epoch: 195 Batch: 2700
Training Loss: 0.011290583754027332
Epoch: 195 Batch: 2750
Training Loss: 0.011137925798242743
Epoch: 195 Batch: 2800
Training Loss: 0.011349372767976353
Epoch: 195 Batch: 2850
Training Loss: 0.01095854843917646
Epoch: 195 Batch: 2900
Training Loss: 0.010415301435980304
Epoch: 195 Batch: 2950
Training Loss: 0.010230882703247717
Epoch: 195 Batch: 3000
Training Loss: 0.010182933807373047
Epoch: 195 Batch: 3050
Training Loss: 0.009658989036669497
Epoch: 195 Batch: 3100
Training Loss: 0.009782378538962333
Epoch: 195 Batch: 3150
Training Loss: 0.009755966483600557
Epoch: 195 Batch: 3200
Training Loss: 0.009707127343863248
Epoch: 196 
 Validation Loss: 0.4689880046579573
---------------------------
Epoch: 196 Batch: 50
Training Loss: 0.605121127963066
Epoch: 196 Batch: 100
Training Loss: 0.3068897411227226
Epoch: 196 Batch: 150
Training Loss: 0.20834235628445943
Epoch: 196 Batch: 200
Training Loss: 0.1520811226963997
Epoch: 196 Batch: 250
Training Loss: 0.11535332262516022
Epoch: 196 Batch: 300
Training Loss: 0.10627436916033427
Epoch: 196 Batch: 350
Training Loss: 0.07927918859890529
Epoch: 196 Batch: 400
Training Loss: 0.07682712115347386
Epoch: 196 Batch: 450
Training Loss: 0.06658393899599711
Epoch: 196 Batch: 500
Training Loss: 0.06175242352485657
Epoch: 196 Batch: 550
Training Loss: 0.05384686973961917
Epoch: 196 Batch: 600
Training Loss: 0.049968801339467364
Epoch: 196 Batch: 650
Training Loss: 0.04678957861203414
Epoch: 196 Batch: 700
Training Loss: 0.04485824793577194
Epoch: 196 Batch: 750
Training Loss: 0.0414452440738678
Epoch: 196 Batch: 800
Training Loss: 0.03714670397341251
Epoch: 196 Batch: 850
Training Loss: 0.03595159905798295
Epoch: 196 Batch: 900
Training Loss: 0.033336133493317495
Epoch: 196 Batch: 950
Training Loss: 0.03226019548742395
Epoch: 196 Batch: 1000
Training Loss: 0.030344717085361482
Epoch: 196 Batch: 1050
Training Loss: 0.028409779611087982
Epoch: 196 Batch: 1100
Training Loss: 0.028949568759311328
Epoch: 196 Batch: 1150
Training Loss: 0.027056213228598885
Epoch: 196 Batch: 1200
Training Loss: 0.023386185417572657
Epoch: 196 Batch: 1250
Training Loss: 0.02337568507194519
Epoch: 196 Batch: 1300
Training Loss: 0.023057559453524078
Epoch: 196 Batch: 1350
Training Loss: 0.020856145178830183
Epoch: 196 Batch: 1400
Training Loss: 0.02124143387590136
Epoch: 196 Batch: 1450
Training Loss: 0.0212156768297327
Epoch: 196 Batch: 1500
Training Loss: 0.0206711688041687
Epoch: 196 Batch: 1550
Training Loss: 0.019993282633443032
Epoch: 196 Batch: 1600
Training Loss: 0.01926341276615858
Epoch: 196 Batch: 1650
Training Loss: 0.018899175651145704
Epoch: 196 Batch: 1700
Training Loss: 0.017814282915171453
Epoch: 196 Batch: 1750
Training Loss: 0.017206213014466423
Epoch: 196 Batch: 1800
Training Loss: 0.018238607661591636
Epoch: 196 Batch: 1850
Training Loss: 0.01609969606270661
Epoch: 196 Batch: 1900
Training Loss: 0.016079314837330265
Epoch: 196 Batch: 1950
Training Loss: 0.014996901016968947
Epoch: 196 Batch: 2000
Training Loss: 0.015129818350076676
Epoch: 196 Batch: 2050
Training Loss: 0.014781285277227076
Epoch: 196 Batch: 2100
Training Loss: 0.014575572950499398
Epoch: 196 Batch: 2150
Training Loss: 0.014168188765991565
Epoch: 196 Batch: 2200
Training Loss: 0.014211816367777911
Epoch: 196 Batch: 2250
Training Loss: 0.014077913562456766
Epoch: 196 Batch: 2300
Training Loss: 0.013623640031918235
Epoch: 196 Batch: 2350
Training Loss: 0.012853548044854023
Epoch: 196 Batch: 2400
Training Loss: 0.012695029936730861
Epoch: 196 Batch: 2450
Training Loss: 0.012251837861781217
Epoch: 196 Batch: 2500
Training Loss: 0.012141065418720245
Epoch: 196 Batch: 2550
Training Loss: 0.011898633323463739
Epoch: 196 Batch: 2600
Training Loss: 0.01136292516038968
Epoch: 196 Batch: 2650
Training Loss: 0.011524310224461105
Epoch: 196 Batch: 2700
Training Loss: 0.011353288922044966
Epoch: 196 Batch: 2750
Training Loss: 0.010832979950037869
Epoch: 196 Batch: 2800
Training Loss: 0.010437305165188654
Epoch: 196 Batch: 2850
Training Loss: 0.010956393762638694
Epoch: 196 Batch: 2900
Training Loss: 0.010245622560895723
Epoch: 196 Batch: 2950
Training Loss: 0.010342410267409632
Epoch: 196 Batch: 3000
Training Loss: 0.010553647577762604
Epoch: 196 Batch: 3050
Training Loss: 0.009642669431498793
Epoch: 196 Batch: 3100
Training Loss: 0.009900821610804527
Epoch: 196 Batch: 3150
Training Loss: 0.009502294565004015
Epoch: 196 Batch: 3200
Training Loss: 0.009698876915499567
Epoch: 197 
 Validation Loss: 0.46940827502144705
---------------------------
Epoch: 197 Batch: 50
Training Loss: 0.6015448200702668
Epoch: 197 Batch: 100
Training Loss: 0.2926081591844559
Epoch: 197 Batch: 150
Training Loss: 0.19979183495044708
Epoch: 197 Batch: 200
Training Loss: 0.15414753675460816
Epoch: 197 Batch: 250
Training Loss: 0.12415464425086975
Epoch: 197 Batch: 300
Training Loss: 0.10037661820650101
Epoch: 197 Batch: 350
Training Loss: 0.08734761553151267
Epoch: 197 Batch: 400
Training Loss: 0.07674861907958984
Epoch: 197 Batch: 450
Training Loss: 0.06777156015237172
Epoch: 197 Batch: 500
Training Loss: 0.0601311811208725
Epoch: 197 Batch: 550
Training Loss: 0.05417982459068298
Epoch: 197 Batch: 600
Training Loss: 0.051888354023297625
Epoch: 197 Batch: 650
Training Loss: 0.047391436191705556
Epoch: 197 Batch: 700
Training Loss: 0.04263243666716984
Epoch: 197 Batch: 750
Training Loss: 0.038974703113238016
Epoch: 197 Batch: 800
Training Loss: 0.03801349800080061
Epoch: 197 Batch: 850
Training Loss: 0.037199287239243005
Epoch: 197 Batch: 900
Training Loss: 0.03528581284814411
Epoch: 197 Batch: 950
Training Loss: 0.03094307563806835
Epoch: 197 Batch: 1000
Training Loss: 0.03034976628422737
Epoch: 197 Batch: 1050
Training Loss: 0.02986150100117638
Epoch: 197 Batch: 1100
Training Loss: 0.0279547812721946
Epoch: 197 Batch: 1150
Training Loss: 0.025956080400425456
Epoch: 197 Batch: 1200
Training Loss: 0.024008319303393363
Epoch: 197 Batch: 1250
Training Loss: 0.0242624559879303
Epoch: 197 Batch: 1300
Training Loss: 0.02359146535396576
Epoch: 197 Batch: 1350
Training Loss: 0.02198464556976601
Epoch: 197 Batch: 1400
Training Loss: 0.02174458267433303
Epoch: 197 Batch: 1450
Training Loss: 0.020058844295041314
Epoch: 197 Batch: 1500
Training Loss: 0.021475651462872822
Epoch: 197 Batch: 1550
Training Loss: 0.019073413879640642
Epoch: 197 Batch: 1600
Training Loss: 0.019496260732412337
Epoch: 197 Batch: 1650
Training Loss: 0.017963956702839245
Epoch: 197 Batch: 1700
Training Loss: 0.017579867524259232
Epoch: 197 Batch: 1750
Training Loss: 0.016667745232582094
Epoch: 197 Batch: 1800
Training Loss: 0.016157113909721375
Epoch: 197 Batch: 1850
Training Loss: 0.015951428993328198
Epoch: 197 Batch: 1900
Training Loss: 0.016185016506596615
Epoch: 197 Batch: 1950
Training Loss: 0.01566619908198332
Epoch: 197 Batch: 2000
Training Loss: 0.014847885861992836
Epoch: 197 Batch: 2050
Training Loss: 0.015031184117968489
Epoch: 197 Batch: 2100
Training Loss: 0.014338659956341699
Epoch: 197 Batch: 2150
Training Loss: 0.01411907505157382
Epoch: 197 Batch: 2200
Training Loss: 0.013423177166418596
Epoch: 197 Batch: 2250
Training Loss: 0.013521325336562263
Epoch: 197 Batch: 2300
Training Loss: 0.01343314334102299
Epoch: 197 Batch: 2350
Training Loss: 0.013154578817651627
Epoch: 197 Batch: 2400
Training Loss: 0.012748262907067934
Epoch: 197 Batch: 2450
Training Loss: 0.012649462089246633
Epoch: 197 Batch: 2500
Training Loss: 0.01187223082780838
Epoch: 197 Batch: 2550
Training Loss: 0.0118644913037618
Epoch: 197 Batch: 2600
Training Loss: 0.011649180134901634
Epoch: 197 Batch: 2650
Training Loss: 0.011498193920783276
Epoch: 197 Batch: 2700
Training Loss: 0.011028830221405736
Epoch: 197 Batch: 2750
Training Loss: 0.010792977484789761
Epoch: 197 Batch: 2800
Training Loss: 0.011083403751254082
Epoch: 197 Batch: 2850
Training Loss: 0.010820554745824714
Epoch: 197 Batch: 2900
Training Loss: 0.010420314546289115
Epoch: 197 Batch: 2950
Training Loss: 0.010470223709688349
Epoch: 197 Batch: 3000
Training Loss: 0.009690823187430699
Epoch: 197 Batch: 3050
Training Loss: 0.009895230433979973
Epoch: 197 Batch: 3100
Training Loss: 0.009992828349913321
Epoch: 197 Batch: 3150
Training Loss: 0.009555792288174705
Epoch: 197 Batch: 3200
Training Loss: 0.009385231770575047
Epoch: 198 
 Validation Loss: 0.4688222779168023
---------------------------
Epoch: 198 Batch: 50
Training Loss: 0.5932143300771713
Epoch: 198 Batch: 100
Training Loss: 0.3045745027065277
Epoch: 198 Batch: 150
Training Loss: 0.20613000849882762
Epoch: 198 Batch: 200
Training Loss: 0.1544959732890129
Epoch: 198 Batch: 250
Training Loss: 0.12169610500335694
Epoch: 198 Batch: 300
Training Loss: 0.10018897861242294
Epoch: 198 Batch: 350
Training Loss: 0.08691645562648773
Epoch: 198 Batch: 400
Training Loss: 0.07844386346638203
Epoch: 198 Batch: 450
Training Loss: 0.06289659155739678
Epoch: 198 Batch: 500
Training Loss: 0.06112336850166321
Epoch: 198 Batch: 550
Training Loss: 0.05818664279851046
Epoch: 198 Batch: 600
Training Loss: 0.05139202227195104
Epoch: 198 Batch: 650
Training Loss: 0.047709347284757175
Epoch: 198 Batch: 700
Training Loss: 0.04047369688749313
Epoch: 198 Batch: 750
Training Loss: 0.04094201389948527
Epoch: 198 Batch: 800
Training Loss: 0.03891400679945946
Epoch: 198 Batch: 850
Training Loss: 0.035860562114154594
Epoch: 198 Batch: 900
Training Loss: 0.03245952255196041
Epoch: 198 Batch: 950
Training Loss: 0.032586373059373154
Epoch: 198 Batch: 1000
Training Loss: 0.03134735342860222
Epoch: 198 Batch: 1050
Training Loss: 0.028851488402911594
Epoch: 198 Batch: 1100
Training Loss: 0.027170787778767674
Epoch: 198 Batch: 1150
Training Loss: 0.025767222953879315
Epoch: 198 Batch: 1200
Training Loss: 0.024321376929680505
Epoch: 198 Batch: 1250
Training Loss: 0.024198500990867615
Epoch: 198 Batch: 1300
Training Loss: 0.0244199986411975
Epoch: 198 Batch: 1350
Training Loss: 0.02158893351201658
Epoch: 198 Batch: 1400
Training Loss: 0.02154589504003525
Epoch: 198 Batch: 1450
Training Loss: 0.020849898971360307
Epoch: 198 Batch: 1500
Training Loss: 0.01972133990128835
Epoch: 198 Batch: 1550
Training Loss: 0.020516878635652602
Epoch: 198 Batch: 1600
Training Loss: 0.018740615360438823
Epoch: 198 Batch: 1650
Training Loss: 0.018326651327537767
Epoch: 198 Batch: 1700
Training Loss: 0.018007668730090645
Epoch: 198 Batch: 1750
Training Loss: 0.017480648636817933
Epoch: 198 Batch: 1800
Training Loss: 0.016749942087464862
Epoch: 198 Batch: 1850
Training Loss: 0.016721276109283034
Epoch: 198 Batch: 1900
Training Loss: 0.016376943964707225
Epoch: 198 Batch: 1950
Training Loss: 0.015600759769097352
Epoch: 198 Batch: 2000
Training Loss: 0.015228328332304954
Epoch: 198 Batch: 2050
Training Loss: 0.01527872300729519
Epoch: 198 Batch: 2100
Training Loss: 0.015000527458531516
Epoch: 198 Batch: 2150
Training Loss: 0.014063275012859078
Epoch: 198 Batch: 2200
Training Loss: 0.013554279424927452
Epoch: 198 Batch: 2250
Training Loss: 0.013666076752874587
Epoch: 198 Batch: 2300
Training Loss: 0.01258435924416003
Epoch: 198 Batch: 2350
Training Loss: 0.013223534492736166
Epoch: 198 Batch: 2400
Training Loss: 0.012991715297102928
Epoch: 198 Batch: 2450
Training Loss: 0.012668401331317669
Epoch: 198 Batch: 2500
Training Loss: 0.011776151752471924
Epoch: 198 Batch: 2550
Training Loss: 0.011871498028437297
Epoch: 198 Batch: 2600
Training Loss: 0.011762006466205304
Epoch: 198 Batch: 2650
Training Loss: 0.011738488516717587
Epoch: 198 Batch: 2700
Training Loss: 0.011229102666731234
Epoch: 198 Batch: 2750
Training Loss: 0.011319886142557318
Epoch: 198 Batch: 2800
Training Loss: 0.011211739235690662
Epoch: 198 Batch: 2850
Training Loss: 0.010578801025424087
Epoch: 198 Batch: 2900
Training Loss: 0.01023590475320816
Epoch: 198 Batch: 2950
Training Loss: 0.010057087358781847
Epoch: 198 Batch: 3000
Training Loss: 0.009943834493557612
Epoch: 198 Batch: 3050
Training Loss: 0.009866848890898658
Epoch: 198 Batch: 3100
Training Loss: 0.009822611376162499
Epoch: 198 Batch: 3150
Training Loss: 0.009639194872644212
Epoch: 198 Batch: 3200
Training Loss: 0.009434982417151332
Epoch: 199 
 Validation Loss: 0.46923426820172204
---------------------------
Epoch: 199 Batch: 50
Training Loss: 0.5641244721412658
Epoch: 199 Batch: 100
Training Loss: 0.3141124829649925
Epoch: 199 Batch: 150
Training Loss: 0.204862056573232
Epoch: 199 Batch: 200
Training Loss: 0.15290934726595878
Epoch: 199 Batch: 250
Training Loss: 0.12071193659305572
Epoch: 199 Batch: 300
Training Loss: 0.09946065187454224
Epoch: 199 Batch: 350
Training Loss: 0.08798206261226109
Epoch: 199 Batch: 400
Training Loss: 0.07605747707188129
Epoch: 199 Batch: 450
Training Loss: 0.06967069692081876
Epoch: 199 Batch: 500
Training Loss: 0.05886663711071014
Epoch: 199 Batch: 550
Training Loss: 0.05351016277616674
Epoch: 199 Batch: 600
Training Loss: 0.05175035372376442
Epoch: 199 Batch: 650
Training Loss: 0.048727709742692804
Epoch: 199 Batch: 700
Training Loss: 0.04374300488403865
Epoch: 199 Batch: 750
Training Loss: 0.03799593035380046
Epoch: 199 Batch: 800
Training Loss: 0.037868449836969374
Epoch: 199 Batch: 850
Training Loss: 0.03498765321338878
Epoch: 199 Batch: 900
Training Loss: 0.03365872061914868
Epoch: 199 Batch: 950
Training Loss: 0.03159178470310412
Epoch: 199 Batch: 1000
Training Loss: 0.03034082081913948
Epoch: 199 Batch: 1050
Training Loss: 0.029718471850667682
Epoch: 199 Batch: 1100
Training Loss: 0.02698823015798222
Epoch: 199 Batch: 1150
Training Loss: 0.0267977737084679
Epoch: 199 Batch: 1200
Training Loss: 0.024119175573190054
Epoch: 199 Batch: 1250
Training Loss: 0.024377517986297607
Epoch: 199 Batch: 1300
Training Loss: 0.0230225303539863
Epoch: 199 Batch: 1350
Training Loss: 0.023240941873303167
Epoch: 199 Batch: 1400
Training Loss: 0.022280015179089136
Epoch: 199 Batch: 1450
Training Loss: 0.019737340067995005
Epoch: 199 Batch: 1500
Training Loss: 0.020950484573841095
Epoch: 199 Batch: 1550
Training Loss: 0.020496898735723187
Epoch: 199 Batch: 1600
Training Loss: 0.01841843008995056
Epoch: 199 Batch: 1650
Training Loss: 0.01860331360137824
Epoch: 199 Batch: 1700
Training Loss: 0.018242629100294675
Epoch: 199 Batch: 1750
Training Loss: 0.017663509454045975
Epoch: 199 Batch: 1800
Training Loss: 0.01646109723382526
Epoch: 199 Batch: 1850
Training Loss: 0.01606109921996658
Epoch: 199 Batch: 1900
Training Loss: 0.016218436172133997
Epoch: 199 Batch: 1950
Training Loss: 0.015419614544281592
Epoch: 199 Batch: 2000
Training Loss: 0.014797364428639413
Epoch: 199 Batch: 2050
Training Loss: 0.014784073800575443
Epoch: 199 Batch: 2100
Training Loss: 0.014727391614800408
Epoch: 199 Batch: 2150
Training Loss: 0.014203659157420313
Epoch: 199 Batch: 2200
Training Loss: 0.013764487206935882
Epoch: 199 Batch: 2250
Training Loss: 0.013225700842009651
Epoch: 199 Batch: 2300
Training Loss: 0.013023828125518302
Epoch: 199 Batch: 2350
Training Loss: 0.012620448596934055
Epoch: 199 Batch: 2400
Training Loss: 0.012645380310714245
Epoch: 199 Batch: 2450
Training Loss: 0.012714125234253552
Epoch: 199 Batch: 2500
Training Loss: 0.012432142698764801
Epoch: 199 Batch: 2550
Training Loss: 0.012290006268258189
Epoch: 199 Batch: 2600
Training Loss: 0.011729410760677779
Epoch: 199 Batch: 2650
Training Loss: 0.011102067513285943
Epoch: 199 Batch: 2700
Training Loss: 0.01126813601564478
Epoch: 199 Batch: 2750
Training Loss: 0.010868586421012879
Epoch: 199 Batch: 2800
Training Loss: 0.011145695501140186
Epoch: 199 Batch: 2850
Training Loss: 0.010511477861488075
Epoch: 199 Batch: 2900
Training Loss: 0.010835934918502281
Epoch: 199 Batch: 2950
Training Loss: 0.009813244080139418
Epoch: 199 Batch: 3000
Training Loss: 0.009911756118138632
Epoch: 199 Batch: 3050
Training Loss: 0.009516182913154852
Epoch: 199 Batch: 3100
Training Loss: 0.009896122284473912
Epoch: 199 Batch: 3150
Training Loss: 0.009890293649264744
Epoch: 199 Batch: 3200
Training Loss: 0.009671053476631641
