Epoch: 0 
 Validation Loss: 4.625694009992811
---------------------------
Epoch: 0 Batch: 50
Training Loss: 4.139081311225891
Epoch: 0 Batch: 100
Training Loss: 2.5610805225372313
Epoch: 0 Batch: 150
Training Loss: 1.7856269574165344
Epoch: 0 Batch: 200
Training Loss: 1.5929224038124083
Epoch: 0 Batch: 250
Training Loss: 1.5984097254276275
Epoch: 0 Batch: 300
Training Loss: 1.551053296327591
Epoch: 0 Batch: 350
Training Loss: 1.4774455797672272
Epoch: 0 Batch: 400
Training Loss: 1.4691545391082763
Epoch: 0 Batch: 450
Training Loss: 1.5049839806556702
Epoch: 0 Batch: 500
Training Loss: 1.484018750190735
Epoch: 0 Batch: 550
Training Loss: 1.5029107177257537
Epoch: 0 Batch: 600
Training Loss: 1.4647939026355743
Epoch: 0 Batch: 650
Training Loss: 1.5027891135215758
Epoch: 0 Batch: 700
Training Loss: 1.484153025150299
Epoch: 0 Batch: 750
Training Loss: 1.4465348958969115
Epoch: 0 Batch: 800
Training Loss: 1.4089615416526795
Epoch: 0 Batch: 850
Training Loss: 1.4414311408996583
Epoch: 0 Batch: 900
Training Loss: 1.357962747812271
Epoch: 0 Batch: 950
Training Loss: 1.3490072941780091
Epoch: 0 Batch: 1000
Training Loss: 1.3862833428382872
Epoch: 0 Batch: 1050
Training Loss: 1.3024580729007722
Epoch: 0 Batch: 1100
Training Loss: 1.2518434381484986
Epoch: 0 Batch: 1150
Training Loss: 1.2351663780212403
Epoch: 0 Batch: 1200
Training Loss: 1.288153976202011
Epoch: 0 Batch: 1250
Training Loss: 1.260130089521408
Epoch: 0 Batch: 1300
Training Loss: 1.1992842185497283
Epoch: 0 Batch: 1350
Training Loss: 1.1980574798583985
Epoch: 0 Batch: 1400
Training Loss: 1.1729371786117553
Epoch: 0 Batch: 1450
Training Loss: 1.1646218955516816
Epoch: 0 Batch: 1500
Training Loss: 1.185749160051346
Epoch: 0 Batch: 1550
Training Loss: 1.1897579407691956
Epoch: 0 Batch: 1600
Training Loss: 1.1539564895629884
Epoch: 0 Batch: 1650
Training Loss: 1.171979811191559
Epoch: 0 Batch: 1700
Training Loss: 1.0770167899131775
Epoch: 0 Batch: 1750
Training Loss: 1.1061026680469512
Epoch: 0 Batch: 1800
Training Loss: 1.1103824615478515
Epoch: 0 Batch: 1850
Training Loss: 1.1069954204559327
Epoch: 0 Batch: 1900
Training Loss: 1.1511524653434753
Epoch: 0 Batch: 1950
Training Loss: 1.1595013916492463
Epoch: 0 Batch: 2000
Training Loss: 1.0727887988090514
Epoch: 0 Batch: 2050
Training Loss: 1.0406685209274291
Epoch: 0 Batch: 2100
Training Loss: 1.045559629201889
Epoch: 0 Batch: 2150
Training Loss: 1.0912602627277375
Epoch: 0 Batch: 2200
Training Loss: 1.069151213169098
Epoch: 0 Batch: 2250
Training Loss: 1.085048460960388
Epoch: 0 Batch: 2300
Training Loss: 1.0284013772010803
Epoch: 0 Batch: 2350
Training Loss: 1.092553024291992
Epoch: 0 Batch: 2400
Training Loss: 1.049079933166504
Epoch: 0 Batch: 2450
Training Loss: 1.1067311799526214
Epoch: 0 Batch: 2500
Training Loss: 1.059721964597702
Epoch: 0 Batch: 2550
Training Loss: 1.0446165895462036
Epoch: 0 Batch: 2600
Training Loss: 1.0748046875
Epoch: 0 Batch: 2650
Training Loss: 1.015329601764679
Epoch: 0 Batch: 2700
Training Loss: 1.075373854637146
Epoch: 0 Batch: 2750
Training Loss: 1.0165624022483826
Epoch: 0 Batch: 2800
Training Loss: 1.0423531174659728
Epoch: 0 Batch: 2850
Training Loss: 1.0654563891887665
Epoch: 0 Batch: 2900
Training Loss: 1.0645682895183564
Epoch: 0 Batch: 2950
Training Loss: 0.9744374024868011
Epoch: 0 Batch: 3000
Training Loss: 1.0382649612426758
Epoch: 0 Batch: 3050
Training Loss: 1.0465349555015564
Epoch: 0 Batch: 3100
Training Loss: 0.9618468010425567
Epoch: 0 Batch: 3150
Training Loss: 0.9801479804515839
Epoch: 0 Batch: 3200
Training Loss: 1.0040094006061553
Epoch: 0 Batch: 3250
Training Loss: 0.9863137650489807
Epoch: 0 Batch: 3300
Training Loss: 0.9596766698360443
Epoch: 0 Batch: 3350
Training Loss: 1.014420404434204
Epoch: 0 Batch: 3400
Training Loss: 0.9872836554050446
Epoch: 0 Batch: 3450
Training Loss: 0.9718252921104431
Epoch: 0 Batch: 3500
Training Loss: 1.019951013326645
Epoch: 1 
 Validation Loss: 0.8072880009810129
---------------------------
Epoch: 1 Batch: 50
Training Loss: 0.9667741000652313
Epoch: 1 Batch: 100
Training Loss: 0.9535732448101044
Epoch: 1 Batch: 150
Training Loss: 0.9294806802272797
Epoch: 1 Batch: 200
Training Loss: 0.977639583349228
Epoch: 1 Batch: 250
Training Loss: 0.9590137147903443
Epoch: 1 Batch: 300
Training Loss: 1.0156627297401428
Epoch: 1 Batch: 350
Training Loss: 0.9259197723865509
Epoch: 1 Batch: 400
Training Loss: 0.9131942820549012
Epoch: 1 Batch: 450
Training Loss: 0.9243576204776764
Epoch: 1 Batch: 500
Training Loss: 0.9629166865348816
Epoch: 1 Batch: 550
Training Loss: 0.9613310861587524
Epoch: 1 Batch: 600
Training Loss: 0.9286157250404358
Epoch: 1 Batch: 650
Training Loss: 0.9517369377613067
Epoch: 1 Batch: 700
Training Loss: 0.9116595435142517
Epoch: 1 Batch: 750
Training Loss: 0.9161381316184998
Epoch: 1 Batch: 800
Training Loss: 0.9767695903778076
Epoch: 1 Batch: 850
Training Loss: 0.9016490888595581
Epoch: 1 Batch: 900
Training Loss: 0.9309215247631073
Epoch: 1 Batch: 950
Training Loss: 0.8857323181629181
Epoch: 1 Batch: 1000
Training Loss: 0.9080580914020538
Epoch: 1 Batch: 1050
Training Loss: 0.9562281489372253
Epoch: 1 Batch: 1100
Training Loss: 0.9603093886375427
Epoch: 1 Batch: 1150
Training Loss: 0.9439129316806794
Epoch: 1 Batch: 1200
Training Loss: 0.9358876121044158
Epoch: 1 Batch: 1250
Training Loss: 0.8965730798244477
Epoch: 1 Batch: 1300
Training Loss: 0.9032750082015991
Epoch: 1 Batch: 1350
Training Loss: 0.8778581261634827
Epoch: 1 Batch: 1400
Training Loss: 0.9153325629234313
Epoch: 1 Batch: 1450
Training Loss: 0.9241498082876205
Epoch: 1 Batch: 1500
Training Loss: 0.8794307100772858
Epoch: 1 Batch: 1550
Training Loss: 0.8987816500663758
Epoch: 1 Batch: 1600
Training Loss: 0.9058365321159363
Epoch: 1 Batch: 1650
Training Loss: 0.8993391120433807
Epoch: 1 Batch: 1700
Training Loss: 0.9331551551818847
Epoch: 1 Batch: 1750
Training Loss: 0.8968791890144349
Epoch: 1 Batch: 1800
Training Loss: 0.91442453622818
Epoch: 1 Batch: 1850
Training Loss: 0.8788859009742737
Epoch: 1 Batch: 1900
Training Loss: 0.9037466442584992
Epoch: 1 Batch: 1950
Training Loss: 0.8463410758972167
Epoch: 1 Batch: 2000
Training Loss: 0.9069223415851593
Epoch: 1 Batch: 2050
Training Loss: 0.8917671370506287
Epoch: 1 Batch: 2100
Training Loss: 0.8910806477069855
Epoch: 1 Batch: 2150
Training Loss: 0.8931547164916992
Epoch: 1 Batch: 2200
Training Loss: 0.8553225362300872
Epoch: 1 Batch: 2250
Training Loss: 0.8966078102588654
Epoch: 1 Batch: 2300
Training Loss: 0.8692533850669861
Epoch: 1 Batch: 2350
Training Loss: 0.8686661469936371
Epoch: 1 Batch: 2400
Training Loss: 0.8682499861717224
Epoch: 1 Batch: 2450
Training Loss: 0.8687366533279419
Epoch: 1 Batch: 2500
Training Loss: 0.8763505566120148
Epoch: 1 Batch: 2550
Training Loss: 0.9034322285652161
Epoch: 1 Batch: 2600
Training Loss: 0.8621100521087647
Epoch: 1 Batch: 2650
Training Loss: 0.847497183084488
Epoch: 1 Batch: 2700
Training Loss: 0.8807841408252716
Epoch: 1 Batch: 2750
Training Loss: 0.8398293578624725
Epoch: 1 Batch: 2800
Training Loss: 0.8645589590072632
Epoch: 1 Batch: 2850
Training Loss: 0.8697527503967285
Epoch: 1 Batch: 2900
Training Loss: 0.8207556354999542
Epoch: 1 Batch: 2950
Training Loss: 0.8433365738391876
Epoch: 1 Batch: 3000
Training Loss: 0.8945526492595672
Epoch: 1 Batch: 3050
Training Loss: 0.8602265632152557
Epoch: 1 Batch: 3100
Training Loss: 0.8368359816074371
Epoch: 1 Batch: 3150
Training Loss: 0.8459333455562592
Epoch: 1 Batch: 3200
Training Loss: 0.8598337101936341
Epoch: 1 Batch: 3250
Training Loss: 0.8580729556083679
Epoch: 1 Batch: 3300
Training Loss: 0.8608728098869324
Epoch: 1 Batch: 3350
Training Loss: 0.8324609768390655
Epoch: 1 Batch: 3400
Training Loss: 0.858028244972229
Epoch: 1 Batch: 3450
Training Loss: 0.8687662625312805
Epoch: 1 Batch: 3500
Training Loss: 0.8520249915122986
Epoch: 2 
 Validation Loss: 0.7051531225442886
---------------------------
Epoch: 2 Batch: 50
Training Loss: 0.8678261935710907
Epoch: 2 Batch: 100
Training Loss: 0.8210947000980378
Epoch: 2 Batch: 150
Training Loss: 0.7611017942428588
Epoch: 2 Batch: 200
Training Loss: 0.8602770280838012
Epoch: 2 Batch: 250
Training Loss: 0.8453220534324646
Epoch: 2 Batch: 300
Training Loss: 0.8285511565208435
Epoch: 2 Batch: 350
Training Loss: 0.8063616275787353
Epoch: 2 Batch: 400
Training Loss: 0.8175513064861297
Epoch: 2 Batch: 450
Training Loss: 0.8120184159278869
Epoch: 2 Batch: 500
Training Loss: 0.8412902212142944
Epoch: 2 Batch: 550
Training Loss: 0.8202912068367004
Epoch: 2 Batch: 600
Training Loss: 0.8504931163787842
Epoch: 2 Batch: 650
Training Loss: 0.8553019952774048
Epoch: 2 Batch: 700
Training Loss: 0.7924127399921417
Epoch: 2 Batch: 750
Training Loss: 0.8393161165714264
Epoch: 2 Batch: 800
Training Loss: 0.7944712042808533
Epoch: 2 Batch: 850
Training Loss: 0.8437674677371979
Epoch: 2 Batch: 900
Training Loss: 0.8382863283157349
Epoch: 2 Batch: 950
Training Loss: 0.8058961021900177
Epoch: 2 Batch: 1000
Training Loss: 0.7971057164669036
Epoch: 2 Batch: 1050
Training Loss: 0.7861675226688385
Epoch: 2 Batch: 1100
Training Loss: 0.8474278330802918
Epoch: 2 Batch: 1150
Training Loss: 0.7833158272504807
Epoch: 2 Batch: 1200
Training Loss: 0.7909765589237213
Epoch: 2 Batch: 1250
Training Loss: 0.8286731028556824
Epoch: 2 Batch: 1300
Training Loss: 0.8159601652622223
Epoch: 2 Batch: 1350
Training Loss: 0.8238840854167938
Epoch: 2 Batch: 1400
Training Loss: 0.840272763967514
Epoch: 2 Batch: 1450
Training Loss: 0.8247395884990693
Epoch: 2 Batch: 1500
Training Loss: 0.8115253907442093
Epoch: 2 Batch: 1550
Training Loss: 0.8159423685073852
Epoch: 2 Batch: 1600
Training Loss: 0.8148085629940033
Epoch: 2 Batch: 1650
Training Loss: 0.836255087852478
Epoch: 2 Batch: 1700
Training Loss: 0.8026487010717392
Epoch: 2 Batch: 1750
Training Loss: 0.7768126440048218
Epoch: 2 Batch: 1800
Training Loss: 0.8239386606216431
Epoch: 2 Batch: 1850
Training Loss: 0.7965226590633392
Epoch: 2 Batch: 1900
Training Loss: 0.8150139439105988
Epoch: 2 Batch: 1950
Training Loss: 0.8206832563877106
Epoch: 2 Batch: 2000
Training Loss: 0.7698494791984558
Epoch: 2 Batch: 2050
Training Loss: 0.7998763787746429
Epoch: 2 Batch: 2100
Training Loss: 0.8014478492736816
Epoch: 2 Batch: 2150
Training Loss: 0.8061150145530701
Epoch: 2 Batch: 2200
Training Loss: 0.7992430686950683
Epoch: 2 Batch: 2250
Training Loss: 0.8106438004970551
Epoch: 2 Batch: 2300
Training Loss: 0.8181451177597046
Epoch: 2 Batch: 2350
Training Loss: 0.8115273773670196
Epoch: 2 Batch: 2400
Training Loss: 0.8103351092338562
Epoch: 2 Batch: 2450
Training Loss: 0.8111797404289246
Epoch: 2 Batch: 2500
Training Loss: 0.7798405647277832
Epoch: 2 Batch: 2550
Training Loss: 0.8092702853679657
Epoch: 2 Batch: 2600
Training Loss: 0.7923569315671921
Epoch: 2 Batch: 2650
Training Loss: 0.7530959618091583
Epoch: 2 Batch: 2700
Training Loss: 0.8122090947628021
Epoch: 2 Batch: 2750
Training Loss: 0.7748925399780273
Epoch: 2 Batch: 2800
Training Loss: 0.8119299626350402
Epoch: 2 Batch: 2850
Training Loss: 0.7794834893941879
Epoch: 2 Batch: 2900
Training Loss: 0.8105670267343521
Epoch: 2 Batch: 2950
Training Loss: 0.8249115514755249
Epoch: 2 Batch: 3000
Training Loss: 0.798260942697525
Epoch: 2 Batch: 3050
Training Loss: 0.817123054265976
Epoch: 2 Batch: 3100
Training Loss: 0.7482564771175384
Epoch: 2 Batch: 3150
Training Loss: 0.8027822840213775
Epoch: 2 Batch: 3200
Training Loss: 0.783854808807373
Epoch: 2 Batch: 3250
Training Loss: 0.7886070132255554
Epoch: 2 Batch: 3300
Training Loss: 0.7961879456043244
Epoch: 2 Batch: 3350
Training Loss: 0.8417981338500976
Epoch: 2 Batch: 3400
Training Loss: 0.7782163763046265
Epoch: 2 Batch: 3450
Training Loss: 0.7818918061256409
Epoch: 2 Batch: 3500
Training Loss: 0.7970778441429138
Epoch: 3 
 Validation Loss: 0.6621033267842399
---------------------------
Epoch: 3 Batch: 50
Training Loss: 0.8002720606327057
Epoch: 3 Batch: 100
Training Loss: 0.803816567659378
Epoch: 3 Batch: 150
Training Loss: 0.7803327775001526
Epoch: 3 Batch: 200
Training Loss: 0.798937222957611
Epoch: 3 Batch: 250
Training Loss: 0.8074297332763671
Epoch: 3 Batch: 300
Training Loss: 0.7809347975254058
Epoch: 3 Batch: 350
Training Loss: 0.7789395821094512
Epoch: 3 Batch: 400
Training Loss: 0.758010236620903
Epoch: 3 Batch: 450
Training Loss: 0.7819245427846908
Epoch: 3 Batch: 500
Training Loss: 0.7756419575214386
Epoch: 3 Batch: 550
Training Loss: 0.7881650638580322
Epoch: 3 Batch: 600
Training Loss: 0.771755485534668
Epoch: 3 Batch: 650
Training Loss: 0.7913539522886276
Epoch: 3 Batch: 700
Training Loss: 0.7367471146583557
Epoch: 3 Batch: 750
Training Loss: 0.778464435338974
Epoch: 3 Batch: 800
Training Loss: 0.7924115645885468
Epoch: 3 Batch: 850
Training Loss: 0.7668321341276169
Epoch: 3 Batch: 900
Training Loss: 0.7925744354724884
Epoch: 3 Batch: 950
Training Loss: 0.7580698227882385
Epoch: 3 Batch: 1000
Training Loss: 0.7473947918415069
Epoch: 3 Batch: 1050
Training Loss: 0.7805436682701111
Epoch: 3 Batch: 1100
Training Loss: 0.7868533861637116
Epoch: 3 Batch: 1150
Training Loss: 0.7656283128261566
Epoch: 3 Batch: 1200
Training Loss: 0.7788203895092011
Epoch: 3 Batch: 1250
Training Loss: 0.7515187156200409
Epoch: 3 Batch: 1300
Training Loss: 0.7599947535991669
Epoch: 3 Batch: 1350
Training Loss: 0.7832914471626282
Epoch: 3 Batch: 1400
Training Loss: 0.7549307477474213
Epoch: 3 Batch: 1450
Training Loss: 0.7775414490699768
Epoch: 3 Batch: 1500
Training Loss: 0.8306308758258819
Epoch: 3 Batch: 1550
Training Loss: 0.7538758873939514
Epoch: 3 Batch: 1600
Training Loss: 0.7695099294185639
Epoch: 3 Batch: 1650
Training Loss: 0.7483349233865738
Epoch: 3 Batch: 1700
Training Loss: 0.7699668335914612
Epoch: 3 Batch: 1750
Training Loss: 0.8125030720233917
Epoch: 3 Batch: 1800
Training Loss: 0.7634415543079376
Epoch: 3 Batch: 1850
Training Loss: 0.7705331432819367
Epoch: 3 Batch: 1900
Training Loss: 0.7670975518226624
Epoch: 3 Batch: 1950
Training Loss: 0.7251244658231735
Epoch: 3 Batch: 2000
Training Loss: 0.8211104476451874
Epoch: 3 Batch: 2050
Training Loss: 0.7644120776653289
Epoch: 3 Batch: 2100
Training Loss: 0.7449516153335571
Epoch: 3 Batch: 2150
Training Loss: 0.7747332566976547
Epoch: 3 Batch: 2200
Training Loss: 0.7938340783119202
Epoch: 3 Batch: 2250
Training Loss: 0.7843139815330505
Epoch: 3 Batch: 2300
Training Loss: 0.7544067233800889
Epoch: 3 Batch: 2350
Training Loss: 0.7566913372278213
Epoch: 3 Batch: 2400
Training Loss: 0.7673277634382248
Epoch: 3 Batch: 2450
Training Loss: 0.7694805681705474
Epoch: 3 Batch: 2500
Training Loss: 0.7683311188220978
Epoch: 3 Batch: 2550
Training Loss: 0.7687643873691559
Epoch: 3 Batch: 2600
Training Loss: 0.7615562862157822
Epoch: 3 Batch: 2650
Training Loss: 0.7395626002550125
Epoch: 3 Batch: 2700
Training Loss: 0.7568709444999695
Epoch: 3 Batch: 2750
Training Loss: 0.7408505010604859
Epoch: 3 Batch: 2800
Training Loss: 0.7663467109203339
Epoch: 3 Batch: 2850
Training Loss: 0.7320336759090423
Epoch: 3 Batch: 2900
Training Loss: 0.7363301229476928
Epoch: 3 Batch: 2950
Training Loss: 0.732670818567276
Epoch: 3 Batch: 3000
Training Loss: 0.7560834765434266
Epoch: 3 Batch: 3050
Training Loss: 0.7386624646186829
Epoch: 3 Batch: 3100
Training Loss: 0.7515030509233475
Epoch: 3 Batch: 3150
Training Loss: 0.7730796003341674
Epoch: 3 Batch: 3200
Training Loss: 0.7530751156806946
Epoch: 3 Batch: 3250
Training Loss: 0.7172440707683563
Epoch: 3 Batch: 3300
Training Loss: 0.77109976708889
Epoch: 3 Batch: 3350
Training Loss: 0.7188417357206345
Epoch: 3 Batch: 3400
Training Loss: 0.7510236656665802
Epoch: 3 Batch: 3450
Training Loss: 0.7421448606252671
Epoch: 3 Batch: 3500
Training Loss: 0.7601197624206543
Epoch: 4 
 Validation Loss: 0.6382813784811232
---------------------------
Epoch: 4 Batch: 50
Training Loss: 0.7985389006137847
Epoch: 4 Batch: 100
Training Loss: 0.7586610013246536
Epoch: 4 Batch: 150
Training Loss: 0.7566319888830185
Epoch: 4 Batch: 200
Training Loss: 0.7514411437511445
Epoch: 4 Batch: 250
Training Loss: 0.7910752618312835
Epoch: 4 Batch: 300
Training Loss: 0.7790325713157654
Epoch: 4 Batch: 350
Training Loss: 0.7553034329414368
Epoch: 4 Batch: 400
Training Loss: 0.720829565525055
Epoch: 4 Batch: 450
Training Loss: 0.7571332448720932
Epoch: 4 Batch: 500
Training Loss: 0.7518450433015823
Epoch: 4 Batch: 550
Training Loss: 0.7584867042303085
Epoch: 4 Batch: 600
Training Loss: 0.7616110342741013
Epoch: 4 Batch: 650
Training Loss: 0.7228213024139404
Epoch: 4 Batch: 700
Training Loss: 0.7632690078020096
Epoch: 4 Batch: 750
Training Loss: 0.7445972609519959
Epoch: 4 Batch: 800
Training Loss: 0.7561481392383576
Epoch: 4 Batch: 850
Training Loss: 0.7210738414525986
Epoch: 4 Batch: 900
Training Loss: 0.7263753175735473
Epoch: 4 Batch: 950
Training Loss: 0.7780505901575089
Epoch: 4 Batch: 1000
Training Loss: 0.7565091693401337
Epoch: 4 Batch: 1050
Training Loss: 0.7440126532316208
Epoch: 4 Batch: 1100
Training Loss: 0.7699458706378937
Epoch: 4 Batch: 1150
Training Loss: 0.7475186383724213
Epoch: 4 Batch: 1200
Training Loss: 0.722483029961586
Epoch: 4 Batch: 1250
Training Loss: 0.7271761298179626
Epoch: 4 Batch: 1300
Training Loss: 0.7500450980663299
Epoch: 4 Batch: 1350
Training Loss: 0.7662594598531723
Epoch: 4 Batch: 1400
Training Loss: 0.7799924808740616
Epoch: 4 Batch: 1450
Training Loss: 0.7444391053915024
Epoch: 4 Batch: 1500
Training Loss: 0.7424680733680725
Epoch: 4 Batch: 1550
Training Loss: 0.7573313421010971
Epoch: 4 Batch: 1600
Training Loss: 0.7078478038311005
Epoch: 4 Batch: 1650
Training Loss: 0.7313240426778793
Epoch: 4 Batch: 1700
Training Loss: 0.7604843610525132
Epoch: 4 Batch: 1750
Training Loss: 0.7610525250434875
Epoch: 4 Batch: 1800
Training Loss: 0.733442479968071
Epoch: 4 Batch: 1850
Training Loss: 0.756615201830864
Epoch: 4 Batch: 1900
Training Loss: 0.7197399246692657
Epoch: 4 Batch: 1950
Training Loss: 0.7382181900739669
Epoch: 4 Batch: 2000
Training Loss: 0.7131171828508377
Epoch: 4 Batch: 2050
Training Loss: 0.751447616815567
Epoch: 4 Batch: 2100
Training Loss: 0.7416411125659943
Epoch: 4 Batch: 2150
Training Loss: 0.7064474433660507
Epoch: 4 Batch: 2200
Training Loss: 0.7895622056722641
Epoch: 4 Batch: 2250
Training Loss: 0.7227368181943894
Epoch: 4 Batch: 2300
Training Loss: 0.7455269694328308
Epoch: 4 Batch: 2350
Training Loss: 0.7221568018198014
Epoch: 4 Batch: 2400
Training Loss: 0.7440723967552185
Epoch: 4 Batch: 2450
Training Loss: 0.7597167134284973
Epoch: 4 Batch: 2500
Training Loss: 0.6864630842208862
Epoch: 4 Batch: 2550
Training Loss: 0.7690642857551575
Epoch: 4 Batch: 2600
Training Loss: 0.756475477218628
Epoch: 4 Batch: 2650
Training Loss: 0.7499258106946946
Epoch: 4 Batch: 2700
Training Loss: 0.7489580595493317
Epoch: 4 Batch: 2750
Training Loss: 0.6912469673156738
Epoch: 4 Batch: 2800
Training Loss: 0.765089967250824
Epoch: 4 Batch: 2850
Training Loss: 0.7178889739513398
Epoch: 4 Batch: 2900
Training Loss: 0.7417311942577363
Epoch: 4 Batch: 2950
Training Loss: 0.7414755696058273
Epoch: 4 Batch: 3000
Training Loss: 0.7410151773691177
Epoch: 4 Batch: 3050
Training Loss: 0.7078330373764038
Epoch: 4 Batch: 3100
Training Loss: 0.7347532892227173
Epoch: 4 Batch: 3150
Training Loss: 0.7298586070537567
Epoch: 4 Batch: 3200
Training Loss: 0.7379103910923004
Epoch: 4 Batch: 3250
Training Loss: 0.7415076726675034
Epoch: 4 Batch: 3300
Training Loss: 0.7309651416540146
Epoch: 4 Batch: 3350
Training Loss: 0.7423657906055451
Epoch: 4 Batch: 3400
Training Loss: 0.7165154719352722
Epoch: 4 Batch: 3450
Training Loss: 0.7368179583549499
Epoch: 4 Batch: 3500
Training Loss: 0.7336077624559403
Epoch: 5 
 Validation Loss: 0.6227602346075906
---------------------------
Epoch: 5 Batch: 50
Training Loss: 0.7357248264551163
Epoch: 5 Batch: 100
Training Loss: 0.7431295293569565
Epoch: 5 Batch: 150
Training Loss: 0.738556696176529
Epoch: 5 Batch: 200
Training Loss: 0.7442557322978973
Epoch: 5 Batch: 250
Training Loss: 0.7440613341331482
Epoch: 5 Batch: 300
Training Loss: 0.7668737387657165
Epoch: 5 Batch: 350
Training Loss: 0.7455063593387604
Epoch: 5 Batch: 400
Training Loss: 0.6952224987745285
Epoch: 5 Batch: 450
Training Loss: 0.7479411673545837
Epoch: 5 Batch: 500
Training Loss: 0.6995438128709793
Epoch: 5 Batch: 550
Training Loss: 0.7691907334327698
Epoch: 5 Batch: 600
Training Loss: 0.7437083613872528
Epoch: 5 Batch: 650
Training Loss: 0.7381770795583725
Epoch: 5 Batch: 700
Training Loss: 0.7158477395772934
Epoch: 5 Batch: 750
Training Loss: 0.7215567636489868
Epoch: 5 Batch: 800
Training Loss: 0.775804483294487
Epoch: 5 Batch: 850
Training Loss: 0.7067458838224411
Epoch: 5 Batch: 900
Training Loss: 0.7218619292974472
Epoch: 5 Batch: 950
Training Loss: 0.7212834906578064
Epoch: 5 Batch: 1000
Training Loss: 0.7013370454311371
Epoch: 5 Batch: 1050
Training Loss: 0.7642802417278289
Epoch: 5 Batch: 1100
Training Loss: 0.7136779707670212
Epoch: 5 Batch: 1150
Training Loss: 0.7134073501825333
Epoch: 5 Batch: 1200
Training Loss: 0.6835100555419922
Epoch: 5 Batch: 1250
Training Loss: 0.7463206994533539
Epoch: 5 Batch: 1300
Training Loss: 0.7235947865247726
Epoch: 5 Batch: 1350
Training Loss: 0.7121141874790191
Epoch: 5 Batch: 1400
Training Loss: 0.7443699681758881
Epoch: 5 Batch: 1450
Training Loss: 0.722514346241951
Epoch: 5 Batch: 1500
Training Loss: 0.7207678818702697
Epoch: 5 Batch: 1550
Training Loss: 0.7522590351104737
Epoch: 5 Batch: 1600
Training Loss: 0.6824982064962387
Epoch: 5 Batch: 1650
Training Loss: 0.7423481512069702
Epoch: 5 Batch: 1700
Training Loss: 0.7133203989267349
Epoch: 5 Batch: 1750
Training Loss: 0.7224618542194366
Epoch: 5 Batch: 1800
Training Loss: 0.7101452958583832
Epoch: 5 Batch: 1850
Training Loss: 0.7159062057733536
Epoch: 5 Batch: 1900
Training Loss: 0.7276029098033905
Epoch: 5 Batch: 1950
Training Loss: 0.7439562571048737
Epoch: 5 Batch: 2000
Training Loss: 0.7072364735603333
Epoch: 5 Batch: 2050
Training Loss: 0.7277877968549729
Epoch: 5 Batch: 2100
Training Loss: 0.7188628035783767
Epoch: 5 Batch: 2150
Training Loss: 0.7011576068401336
Epoch: 5 Batch: 2200
Training Loss: 0.7006799185276031
Epoch: 5 Batch: 2250
Training Loss: 0.7054593765735626
Epoch: 5 Batch: 2300
Training Loss: 0.6903348422050476
Epoch: 5 Batch: 2350
Training Loss: 0.7367278265953064
Epoch: 5 Batch: 2400
Training Loss: 0.6984597671031952
Epoch: 5 Batch: 2450
Training Loss: 0.7409935343265533
Epoch: 5 Batch: 2500
Training Loss: 0.7345669209957123
Epoch: 5 Batch: 2550
Training Loss: 0.7169110095500946
Epoch: 5 Batch: 2600
Training Loss: 0.7273561716079712
Epoch: 5 Batch: 2650
Training Loss: 0.6861929190158844
Epoch: 5 Batch: 2700
Training Loss: 0.7280115801095962
Epoch: 5 Batch: 2750
Training Loss: 0.7070556581020355
Epoch: 5 Batch: 2800
Training Loss: 0.7349938017129898
Epoch: 5 Batch: 2850
Training Loss: 0.7522233080863953
Epoch: 5 Batch: 2900
Training Loss: 0.7443191659450531
Epoch: 5 Batch: 2950
Training Loss: 0.6898388159275055
Epoch: 5 Batch: 3000
Training Loss: 0.7427857857942581
Epoch: 5 Batch: 3050
Training Loss: 0.7135029458999633
Epoch: 5 Batch: 3100
Training Loss: 0.6874764013290405
Epoch: 5 Batch: 3150
Training Loss: 0.7127380436658859
Epoch: 5 Batch: 3200
Training Loss: 0.713704913854599
Epoch: 5 Batch: 3250
Training Loss: 0.7300092053413391
Epoch: 5 Batch: 3300
Training Loss: 0.7255682951211929
Epoch: 5 Batch: 3350
Training Loss: 0.7216407650709152
Epoch: 5 Batch: 3400
Training Loss: 0.7187485200166702
Epoch: 5 Batch: 3450
Training Loss: 0.712538270354271
Epoch: 5 Batch: 3500
Training Loss: 0.7267860388755798
Epoch: 6 
 Validation Loss: 0.6116553544998169
---------------------------
Epoch: 6 Batch: 50
Training Loss: 0.733971638083458
Epoch: 6 Batch: 100
Training Loss: 0.7070678889751434
Epoch: 6 Batch: 150
Training Loss: 0.7152295023202896
Epoch: 6 Batch: 200
Training Loss: 0.7173468315601349
Epoch: 6 Batch: 250
Training Loss: 0.7258495306968689
Epoch: 6 Batch: 300
Training Loss: 0.6883832406997681
Epoch: 6 Batch: 350
Training Loss: 0.7013119852542877
Epoch: 6 Batch: 400
Training Loss: 0.7266157650947571
Epoch: 6 Batch: 450
Training Loss: 0.6996087563037873
Epoch: 6 Batch: 500
Training Loss: 0.7400039076805115
Epoch: 6 Batch: 550
Training Loss: 0.7160533690452575
Epoch: 6 Batch: 600
Training Loss: 0.7063592994213104
Epoch: 6 Batch: 650
Training Loss: 0.6899646151065827
Epoch: 6 Batch: 700
Training Loss: 0.6883775049448013
Epoch: 6 Batch: 750
Training Loss: 0.7157853662967681
Epoch: 6 Batch: 800
Training Loss: 0.7345397645235061
Epoch: 6 Batch: 850
Training Loss: 0.6976722681522369
Epoch: 6 Batch: 900
Training Loss: 0.7227769750356674
Epoch: 6 Batch: 950
Training Loss: 0.6911222094297409
Epoch: 6 Batch: 1000
Training Loss: 0.7019834297895432
Epoch: 6 Batch: 1050
Training Loss: 0.7124829459190368
Epoch: 6 Batch: 1100
Training Loss: 0.7074380284547805
Epoch: 6 Batch: 1150
Training Loss: 0.7004145526885986
Epoch: 6 Batch: 1200
Training Loss: 0.689322629570961
Epoch: 6 Batch: 1250
Training Loss: 0.7153835886716843
Epoch: 6 Batch: 1300
Training Loss: 0.731564130783081
Epoch: 6 Batch: 1350
Training Loss: 0.7132656681537628
Epoch: 6 Batch: 1400
Training Loss: 0.686110919713974
Epoch: 6 Batch: 1450
Training Loss: 0.704471390247345
Epoch: 6 Batch: 1500
Training Loss: 0.6877982264757156
Epoch: 6 Batch: 1550
Training Loss: 0.7142447644472122
Epoch: 6 Batch: 1600
Training Loss: 0.7258141928911209
Epoch: 6 Batch: 1650
Training Loss: 0.7054077035188675
Epoch: 6 Batch: 1700
Training Loss: 0.7030465650558472
Epoch: 6 Batch: 1750
Training Loss: 0.7191143208742141
Epoch: 6 Batch: 1800
Training Loss: 0.6725631755590439
Epoch: 6 Batch: 1850
Training Loss: 0.7115297216176987
Epoch: 6 Batch: 1900
Training Loss: 0.7133900237083435
Epoch: 6 Batch: 1950
Training Loss: 0.7062643325328827
Epoch: 6 Batch: 2000
Training Loss: 0.7287304735183716
Epoch: 6 Batch: 2050
Training Loss: 0.724780056476593
Epoch: 6 Batch: 2100
Training Loss: 0.7016962969303131
Epoch: 6 Batch: 2150
Training Loss: 0.6992632633447647
Epoch: 6 Batch: 2200
Training Loss: 0.6980500113964081
Epoch: 6 Batch: 2250
Training Loss: 0.7108327782154084
Epoch: 6 Batch: 2300
Training Loss: 0.6810931044816971
Epoch: 6 Batch: 2350
Training Loss: 0.6660926592350006
Epoch: 6 Batch: 2400
Training Loss: 0.6792775946855545
Epoch: 6 Batch: 2450
Training Loss: 0.7210010582208634
Epoch: 6 Batch: 2500
Training Loss: 0.698521317243576
Epoch: 6 Batch: 2550
Training Loss: 0.7318790060281753
Epoch: 6 Batch: 2600
Training Loss: 0.7351148790121078
Epoch: 6 Batch: 2650
Training Loss: 0.7346422147750854
Epoch: 6 Batch: 2700
Training Loss: 0.7009266132116317
Epoch: 6 Batch: 2750
Training Loss: 0.7104854273796082
Epoch: 6 Batch: 2800
Training Loss: 0.7345406681299209
Epoch: 6 Batch: 2850
Training Loss: 0.7037501758337021
Epoch: 6 Batch: 2900
Training Loss: 0.7375498461723328
Epoch: 6 Batch: 2950
Training Loss: 0.744978199005127
Epoch: 6 Batch: 3000
Training Loss: 0.7063323140144349
Epoch: 6 Batch: 3050
Training Loss: 0.7210628700256347
Epoch: 6 Batch: 3100
Training Loss: 0.7003719985485077
Epoch: 6 Batch: 3150
Training Loss: 0.7327862155437469
Epoch: 6 Batch: 3200
Training Loss: 0.7078243780136109
Epoch: 6 Batch: 3250
Training Loss: 0.7122869855165481
Epoch: 6 Batch: 3300
Training Loss: 0.6919606471061707
Epoch: 6 Batch: 3350
Training Loss: 0.7058278107643128
Epoch: 6 Batch: 3400
Training Loss: 0.7485054504871368
Epoch: 6 Batch: 3450
Training Loss: 0.7001217585802079
Epoch: 6 Batch: 3500
Training Loss: 0.6878770732879639
Epoch: 7 
 Validation Loss: 0.6028600268893771
---------------------------
Epoch: 7 Batch: 50
Training Loss: 0.728748282790184
Epoch: 7 Batch: 100
Training Loss: 0.7138095170259475
Epoch: 7 Batch: 150
Training Loss: 0.6874295175075531
Epoch: 7 Batch: 200
Training Loss: 0.7012480705976486
Epoch: 7 Batch: 250
Training Loss: 0.7173759722709656
Epoch: 7 Batch: 300
Training Loss: 0.6890225338935853
Epoch: 7 Batch: 350
Training Loss: 0.702888258099556
Epoch: 7 Batch: 400
Training Loss: 0.7058153146505356
Epoch: 7 Batch: 450
Training Loss: 0.7147334450483322
Epoch: 7 Batch: 500
Training Loss: 0.7201201635599136
Epoch: 7 Batch: 550
Training Loss: 0.7043379986286163
Epoch: 7 Batch: 600
Training Loss: 0.7035936546325684
Epoch: 7 Batch: 650
Training Loss: 0.7044482606649399
Epoch: 7 Batch: 700
Training Loss: 0.7234227669239044
Epoch: 7 Batch: 750
Training Loss: 0.7172238826751709
Epoch: 7 Batch: 800
Training Loss: 0.7093769550323487
Epoch: 7 Batch: 850
Training Loss: 0.6925125974416733
Epoch: 7 Batch: 900
Training Loss: 0.7276769363880158
Epoch: 7 Batch: 950
Training Loss: 0.7151191824674606
Epoch: 7 Batch: 1000
Training Loss: 0.7141789823770524
Epoch: 7 Batch: 1050
Training Loss: 0.7276975411176682
Epoch: 7 Batch: 1100
Training Loss: 0.67846874833107
Epoch: 7 Batch: 1150
Training Loss: 0.7030637407302857
Epoch: 7 Batch: 1200
Training Loss: 0.6984510326385498
Epoch: 7 Batch: 1250
Training Loss: 0.6876138573884965
Epoch: 7 Batch: 1300
Training Loss: 0.6941301393508911
Epoch: 7 Batch: 1350
Training Loss: 0.7072777074575424
Epoch: 7 Batch: 1400
Training Loss: 0.6856081795692444
Epoch: 7 Batch: 1450
Training Loss: 0.7538241797685623
Epoch: 7 Batch: 1500
Training Loss: 0.691349019408226
Epoch: 7 Batch: 1550
Training Loss: 0.6950219136476516
Epoch: 7 Batch: 1600
Training Loss: 0.7211469972133636
Epoch: 7 Batch: 1650
Training Loss: 0.660561454296112
Epoch: 7 Batch: 1700
Training Loss: 0.7012489295005798
Epoch: 7 Batch: 1750
Training Loss: 0.7022260361909867
Epoch: 7 Batch: 1800
Training Loss: 0.6999677658081055
Epoch: 7 Batch: 1850
Training Loss: 0.7074610817432404
Epoch: 7 Batch: 1900
Training Loss: 0.7080300217866897
Epoch: 7 Batch: 1950
Training Loss: 0.7353127241134644
Epoch: 7 Batch: 2000
Training Loss: 0.6983005887269974
Epoch: 7 Batch: 2050
Training Loss: 0.7158633601665497
Epoch: 7 Batch: 2100
Training Loss: 0.7067411923408509
Epoch: 7 Batch: 2150
Training Loss: 0.658899012207985
Epoch: 7 Batch: 2200
Training Loss: 0.6934973937273026
Epoch: 7 Batch: 2250
Training Loss: 0.736815431714058
Epoch: 7 Batch: 2300
Training Loss: 0.6847461897134781
Epoch: 7 Batch: 2350
Training Loss: 0.6831798619031906
Epoch: 7 Batch: 2400
Training Loss: 0.6840278774499893
Epoch: 7 Batch: 2450
Training Loss: 0.7053424280881881
Epoch: 7 Batch: 2500
Training Loss: 0.6802345997095108
Epoch: 7 Batch: 2550
Training Loss: 0.6736292731761933
Epoch: 7 Batch: 2600
Training Loss: 0.6880388933420182
Epoch: 7 Batch: 2650
Training Loss: 0.7066101616621018
Epoch: 7 Batch: 2700
Training Loss: 0.6898893404006958
Epoch: 7 Batch: 2750
Training Loss: 0.6751126253604889
Epoch: 7 Batch: 2800
Training Loss: 0.6925052964687347
Epoch: 7 Batch: 2850
Training Loss: 0.7005005609989167
Epoch: 7 Batch: 2900
Training Loss: 0.7169633358716965
Epoch: 7 Batch: 2950
Training Loss: 0.6859833973646164
Epoch: 7 Batch: 3000
Training Loss: 0.7087923234701157
Epoch: 7 Batch: 3050
Training Loss: 0.6983673042058944
Epoch: 7 Batch: 3100
Training Loss: 0.6847203379869461
Epoch: 7 Batch: 3150
Training Loss: 0.6814562207460404
Epoch: 7 Batch: 3200
Training Loss: 0.7216357088088989
Epoch: 7 Batch: 3250
Training Loss: 0.6953849267959594
Epoch: 7 Batch: 3300
Training Loss: 0.697394204735756
Epoch: 7 Batch: 3350
Training Loss: 0.6807280319929123
Epoch: 7 Batch: 3400
Training Loss: 0.6770450764894486
Epoch: 7 Batch: 3450
Training Loss: 0.667374809384346
Epoch: 7 Batch: 3500
Training Loss: 0.6908078104257583
Epoch: 8 
 Validation Loss: 0.5959068040053049
---------------------------
Epoch: 8 Batch: 50
Training Loss: 0.7149562299251556
Epoch: 8 Batch: 100
Training Loss: 0.7143145322799682
Epoch: 8 Batch: 150
Training Loss: 0.6691891944408417
Epoch: 8 Batch: 200
Training Loss: 0.6935194998979568
Epoch: 8 Batch: 250
Training Loss: 0.7038560992479325
Epoch: 8 Batch: 300
Training Loss: 0.695749877691269
Epoch: 8 Batch: 350
Training Loss: 0.6850272858142853
Epoch: 8 Batch: 400
Training Loss: 0.6730164033174515
Epoch: 8 Batch: 450
Training Loss: 0.6768693917989731
Epoch: 8 Batch: 500
Training Loss: 0.7017763149738312
Epoch: 8 Batch: 550
Training Loss: 0.7130820542573929
Epoch: 8 Batch: 600
Training Loss: 0.687299752831459
Epoch: 8 Batch: 650
Training Loss: 0.6938352537155151
Epoch: 8 Batch: 700
Training Loss: 0.7378141939640045
Epoch: 8 Batch: 750
Training Loss: 0.7000297290086747
Epoch: 8 Batch: 800
Training Loss: 0.7000051015615463
Epoch: 8 Batch: 850
Training Loss: 0.7083665072917938
Epoch: 8 Batch: 900
Training Loss: 0.7061615246534347
Epoch: 8 Batch: 950
Training Loss: 0.715586918592453
Epoch: 8 Batch: 1000
Training Loss: 0.6969049149751663
Epoch: 8 Batch: 1050
Training Loss: 0.6782138651609421
Epoch: 8 Batch: 1100
Training Loss: 0.6475046640634536
Epoch: 8 Batch: 1150
Training Loss: 0.6940659999847412
Epoch: 8 Batch: 1200
Training Loss: 0.681760910153389
Epoch: 8 Batch: 1250
Training Loss: 0.713209273815155
Epoch: 8 Batch: 1300
Training Loss: 0.7076702094078064
Epoch: 8 Batch: 1350
Training Loss: 0.686105306148529
Epoch: 8 Batch: 1400
Training Loss: 0.6973025971651077
Epoch: 8 Batch: 1450
Training Loss: 0.7041138845682144
Epoch: 8 Batch: 1500
Training Loss: 0.6953931844234467
Epoch: 8 Batch: 1550
Training Loss: 0.7051665771007538
Epoch: 8 Batch: 1600
Training Loss: 0.6923426610231399
Epoch: 8 Batch: 1650
Training Loss: 0.7126533669233323
Epoch: 8 Batch: 1700
Training Loss: 0.668835341334343
Epoch: 8 Batch: 1750
Training Loss: 0.6866147166490555
Epoch: 8 Batch: 1800
Training Loss: 0.6613802468776703
Epoch: 8 Batch: 1850
Training Loss: 0.6999591022729874
Epoch: 8 Batch: 1900
Training Loss: 0.6674073052406311
Epoch: 8 Batch: 1950
Training Loss: 0.6673245894908905
Epoch: 8 Batch: 2000
Training Loss: 0.666790788769722
Epoch: 8 Batch: 2050
Training Loss: 0.7206184822320938
Epoch: 8 Batch: 2100
Training Loss: 0.6942105251550674
Epoch: 8 Batch: 2150
Training Loss: 0.6988916784524918
Epoch: 8 Batch: 2200
Training Loss: 0.6769239234924317
Epoch: 8 Batch: 2250
Training Loss: 0.7057695317268372
Epoch: 8 Batch: 2300
Training Loss: 0.7136383205652237
Epoch: 8 Batch: 2350
Training Loss: 0.6988803780078888
Epoch: 8 Batch: 2400
Training Loss: 0.6924888330698014
Epoch: 8 Batch: 2450
Training Loss: 0.7036677497625351
Epoch: 8 Batch: 2500
Training Loss: 0.6826705974340439
Epoch: 8 Batch: 2550
Training Loss: 0.6775274419784546
Epoch: 8 Batch: 2600
Training Loss: 0.6693982607126236
Epoch: 8 Batch: 2650
Training Loss: 0.6609086990356445
Epoch: 8 Batch: 2700
Training Loss: 0.6803974533081054
Epoch: 8 Batch: 2750
Training Loss: 0.6895441222190857
Epoch: 8 Batch: 2800
Training Loss: 0.6911893612146378
Epoch: 8 Batch: 2850
Training Loss: 0.6751756811141968
Epoch: 8 Batch: 2900
Training Loss: 0.6882016748189926
Epoch: 8 Batch: 2950
Training Loss: 0.6830867868661881
Epoch: 8 Batch: 3000
Training Loss: 0.7001488029956817
Epoch: 8 Batch: 3050
Training Loss: 0.7086700356006622
Epoch: 8 Batch: 3100
Training Loss: 0.642676267027855
Epoch: 8 Batch: 3150
Training Loss: 0.7041301482915878
Epoch: 8 Batch: 3200
Training Loss: 0.6887625545263291
Epoch: 8 Batch: 3250
Training Loss: 0.6904492962360382
Epoch: 8 Batch: 3300
Training Loss: 0.7015012377500534
Epoch: 8 Batch: 3350
Training Loss: 0.68532566010952
Epoch: 8 Batch: 3400
Training Loss: 0.6829209756851197
Epoch: 8 Batch: 3450
Training Loss: 0.6967733228206634
Epoch: 8 Batch: 3500
Training Loss: 0.7041303467750549
Epoch: 9 
 Validation Loss: 0.5902930514680015
---------------------------
Epoch: 9 Batch: 50
Training Loss: 0.7002562844753265
Epoch: 9 Batch: 100
Training Loss: 0.6809801459312439
Epoch: 9 Batch: 150
Training Loss: 0.7110474753379822
Epoch: 9 Batch: 200
Training Loss: 0.7066637378931045
Epoch: 9 Batch: 250
Training Loss: 0.6944724923372269
Epoch: 9 Batch: 300
Training Loss: 0.7069593381881714
Epoch: 9 Batch: 350
Training Loss: 0.6931827163696289
Epoch: 9 Batch: 400
Training Loss: 0.6705450463294983
Epoch: 9 Batch: 450
Training Loss: 0.6695438677072525
Epoch: 9 Batch: 500
Training Loss: 0.6608983778953552
Epoch: 9 Batch: 550
Training Loss: 0.7032205790281296
Epoch: 9 Batch: 600
Training Loss: 0.6756513094902039
Epoch: 9 Batch: 650
Training Loss: 0.7234635841846466
Epoch: 9 Batch: 700
Training Loss: 0.700564843416214
Epoch: 9 Batch: 750
Training Loss: 0.7184207159280777
Epoch: 9 Batch: 800
Training Loss: 0.687932745218277
Epoch: 9 Batch: 850
Training Loss: 0.6753319221735
Epoch: 9 Batch: 900
Training Loss: 0.7085602980852127
Epoch: 9 Batch: 950
Training Loss: 0.6921473008394241
Epoch: 9 Batch: 1000
Training Loss: 0.6765629887580872
Epoch: 9 Batch: 1050
Training Loss: 0.6632057279348373
Epoch: 9 Batch: 1100
Training Loss: 0.6787923139333725
Epoch: 9 Batch: 1150
Training Loss: 0.6544194996356965
Epoch: 9 Batch: 1200
Training Loss: 0.675934796333313
Epoch: 9 Batch: 1250
Training Loss: 0.7277300715446472
Epoch: 9 Batch: 1300
Training Loss: 0.6753519243001938
Epoch: 9 Batch: 1350
Training Loss: 0.6829283261299133
Epoch: 9 Batch: 1400
Training Loss: 0.6382810026407242
Epoch: 9 Batch: 1450
Training Loss: 0.7028205245733261
Epoch: 9 Batch: 1500
Training Loss: 0.6835691374540329
Epoch: 9 Batch: 1550
Training Loss: 0.6575379419326782
Epoch: 9 Batch: 1600
Training Loss: 0.6894260519742965
Epoch: 9 Batch: 1650
Training Loss: 0.6860480457544327
Epoch: 9 Batch: 1700
Training Loss: 0.6590837305784225
Epoch: 9 Batch: 1750
Training Loss: 0.681205325126648
Epoch: 9 Batch: 1800
Training Loss: 0.6773657089471817
Epoch: 9 Batch: 1850
Training Loss: 0.6953207695484162
Epoch: 9 Batch: 1900
Training Loss: 0.6669629728794098
Epoch: 9 Batch: 1950
Training Loss: 0.7222394347190857
Epoch: 9 Batch: 2000
Training Loss: 0.6922540700435639
Epoch: 9 Batch: 2050
Training Loss: 0.6689958381652832
Epoch: 9 Batch: 2100
Training Loss: 0.7067492872476577
Epoch: 9 Batch: 2150
Training Loss: 0.6978547078371048
Epoch: 9 Batch: 2200
Training Loss: 0.6705135852098465
Epoch: 9 Batch: 2250
Training Loss: 0.6795016640424728
Epoch: 9 Batch: 2300
Training Loss: 0.6758861708641052
Epoch: 9 Batch: 2350
Training Loss: 0.6748362159729004
Epoch: 9 Batch: 2400
Training Loss: 0.7127355456352233
Epoch: 9 Batch: 2450
Training Loss: 0.6781471878290176
Epoch: 9 Batch: 2500
Training Loss: 0.7163646513223648
Epoch: 9 Batch: 2550
Training Loss: 0.6987423002719879
Epoch: 9 Batch: 2600
Training Loss: 0.6878889191150666
Epoch: 9 Batch: 2650
Training Loss: 0.6825314915180206
Epoch: 9 Batch: 2700
Training Loss: 0.6693057799339295
Epoch: 9 Batch: 2750
Training Loss: 0.6836781400442123
Epoch: 9 Batch: 2800
Training Loss: 0.667306165099144
Epoch: 9 Batch: 2850
Training Loss: 0.676531366109848
Epoch: 9 Batch: 2900
Training Loss: 0.6721367424726487
Epoch: 9 Batch: 2950
Training Loss: 0.6861809915304184
Epoch: 9 Batch: 3000
Training Loss: 0.6600993245840072
Epoch: 9 Batch: 3050
Training Loss: 0.6946132117509842
Epoch: 9 Batch: 3100
Training Loss: 0.6858575254678726
Epoch: 9 Batch: 3150
Training Loss: 0.6564097595214844
Epoch: 9 Batch: 3200
Training Loss: 0.6402036148309708
Epoch: 9 Batch: 3250
Training Loss: 0.6803222131729126
Epoch: 9 Batch: 3300
Training Loss: 0.7009531104564667
Epoch: 9 Batch: 3350
Training Loss: 0.6997509598731995
Epoch: 9 Batch: 3400
Training Loss: 0.7006606882810593
Epoch: 9 Batch: 3450
Training Loss: 0.6892799186706543
Epoch: 9 Batch: 3500
Training Loss: 0.7169259440898895
Epoch: 10 
 Validation Loss: 0.5852663550111983
---------------------------
Epoch: 10 Batch: 50
Training Loss: 0.6914587670564651
Epoch: 10 Batch: 100
Training Loss: 0.6783938628435134
Epoch: 10 Batch: 150
Training Loss: 0.670062427520752
Epoch: 10 Batch: 200
Training Loss: 0.6641462188959122
Epoch: 10 Batch: 250
Training Loss: 0.6819962841272355
Epoch: 10 Batch: 300
Training Loss: 0.6938695919513702
Epoch: 10 Batch: 350
Training Loss: 0.6878424179553986
Epoch: 10 Batch: 400
Training Loss: 0.7003180778026581
Epoch: 10 Batch: 450
Training Loss: 0.7005570369958878
Epoch: 10 Batch: 500
Training Loss: 0.7126688301563263
Epoch: 10 Batch: 550
Training Loss: 0.6870220202207565
Epoch: 10 Batch: 600
Training Loss: 0.7021275317668915
Epoch: 10 Batch: 650
Training Loss: 0.6713308143615723
Epoch: 10 Batch: 700
Training Loss: 0.7155156135559082
Epoch: 10 Batch: 750
Training Loss: 0.6924717855453492
Epoch: 10 Batch: 800
Training Loss: 0.6647784030437469
Epoch: 10 Batch: 850
Training Loss: 0.7054462015628815
Epoch: 10 Batch: 900
Training Loss: 0.6964559090137482
Epoch: 10 Batch: 950
Training Loss: 0.6921227449178695
Epoch: 10 Batch: 1000
Training Loss: 0.6561870849132538
Epoch: 10 Batch: 1050
Training Loss: 0.6380986350774766
Epoch: 10 Batch: 1100
Training Loss: 0.7039887583255768
Epoch: 10 Batch: 1150
Training Loss: 0.6864601910114289
Epoch: 10 Batch: 1200
Training Loss: 0.6834561711549759
Epoch: 10 Batch: 1250
Training Loss: 0.7008417928218842
Epoch: 10 Batch: 1300
Training Loss: 0.6906811392307282
Epoch: 10 Batch: 1350
Training Loss: 0.6943217021226883
Epoch: 10 Batch: 1400
Training Loss: 0.6878032505512237
Epoch: 10 Batch: 1450
Training Loss: 0.6665408146381379
Epoch: 10 Batch: 1500
Training Loss: 0.6836796241998673
Epoch: 10 Batch: 1550
Training Loss: 0.6790817898511886
Epoch: 10 Batch: 1600
Training Loss: 0.6647168952226639
Epoch: 10 Batch: 1650
Training Loss: 0.6825673097372055
Epoch: 10 Batch: 1700
Training Loss: 0.6725744938850403
Epoch: 10 Batch: 1750
Training Loss: 0.6717126858234406
Epoch: 10 Batch: 1800
Training Loss: 0.6667154836654663
Epoch: 10 Batch: 1850
Training Loss: 0.6506787168979645
Epoch: 10 Batch: 1900
Training Loss: 0.6927668398618698
Epoch: 10 Batch: 1950
Training Loss: 0.6487486988306046
Epoch: 10 Batch: 2000
Training Loss: 0.6532646560668945
Epoch: 10 Batch: 2050
Training Loss: 0.6805699706077576
Epoch: 10 Batch: 2100
Training Loss: 0.6624042862653732
Epoch: 10 Batch: 2150
Training Loss: 0.6685297387838364
Epoch: 10 Batch: 2200
Training Loss: 0.6739727354049683
Epoch: 10 Batch: 2250
Training Loss: 0.6894295924901962
Epoch: 10 Batch: 2300
Training Loss: 0.6660661989450455
Epoch: 10 Batch: 2350
Training Loss: 0.6645635151863098
Epoch: 10 Batch: 2400
Training Loss: 0.6781347149610519
Epoch: 10 Batch: 2450
Training Loss: 0.6440095931291581
Epoch: 10 Batch: 2500
Training Loss: 0.688172977566719
Epoch: 10 Batch: 2550
Training Loss: 0.6273017275333405
Epoch: 10 Batch: 2600
Training Loss: 0.6882622557878494
Epoch: 10 Batch: 2650
Training Loss: 0.6483284693956375
Epoch: 10 Batch: 2700
Training Loss: 0.6893868046998978
Epoch: 10 Batch: 2750
Training Loss: 0.7356949913501739
Epoch: 10 Batch: 2800
Training Loss: 0.6473068106174469
Epoch: 10 Batch: 2850
Training Loss: 0.6795258802175522
Epoch: 10 Batch: 2900
Training Loss: 0.6994301426410675
Epoch: 10 Batch: 2950
Training Loss: 0.6732597994804382
Epoch: 10 Batch: 3000
Training Loss: 0.7043651306629181
Epoch: 10 Batch: 3050
Training Loss: 0.6475330406427383
Epoch: 10 Batch: 3100
Training Loss: 0.672989867925644
Epoch: 10 Batch: 3150
Training Loss: 0.6955462521314622
Epoch: 10 Batch: 3200
Training Loss: 0.6843741846084594
Epoch: 10 Batch: 3250
Training Loss: 0.6806619852781296
Epoch: 10 Batch: 3300
Training Loss: 0.7066035711765289
Epoch: 10 Batch: 3350
Training Loss: 0.6966292560100555
Epoch: 10 Batch: 3400
Training Loss: 0.690441967844963
Epoch: 10 Batch: 3450
Training Loss: 0.6814129483699799
Epoch: 10 Batch: 3500
Training Loss: 0.717734010219574
Epoch: 11 
 Validation Loss: 0.5813425547546811
---------------------------
Epoch: 11 Batch: 50
Training Loss: 0.6812824803590775
Epoch: 11 Batch: 100
Training Loss: 0.6702029395103455
Epoch: 11 Batch: 150
Training Loss: 0.6666600799560547
Epoch: 11 Batch: 200
Training Loss: 0.7069134885072708
Epoch: 11 Batch: 250
Training Loss: 0.7391616016626358
Epoch: 11 Batch: 300
Training Loss: 0.6780821144580841
Epoch: 11 Batch: 350
Training Loss: 0.6679769206047058
Epoch: 11 Batch: 400
Training Loss: 0.6827822011709214
Epoch: 11 Batch: 450
Training Loss: 0.6597186809778214
Epoch: 11 Batch: 500
Training Loss: 0.6592406243085861
Epoch: 11 Batch: 550
Training Loss: 0.6839806985855102
Epoch: 11 Batch: 600
Training Loss: 0.6667005300521851
Epoch: 11 Batch: 650
Training Loss: 0.6597259300947189
Epoch: 11 Batch: 700
Training Loss: 0.6727781909704208
Epoch: 11 Batch: 750
Training Loss: 0.6336424088478089
Epoch: 11 Batch: 800
Training Loss: 0.6480078583955765
Epoch: 11 Batch: 850
Training Loss: 0.6361827981472016
Epoch: 11 Batch: 900
Training Loss: 0.6493389737606049
Epoch: 11 Batch: 950
Training Loss: 0.6792524623870849
Epoch: 11 Batch: 1000
Training Loss: 0.6762406331300735
Epoch: 11 Batch: 1050
Training Loss: 0.6730291998386383
Epoch: 11 Batch: 1100
Training Loss: 0.7043489974737167
Epoch: 11 Batch: 1150
Training Loss: 0.6692310106754303
Epoch: 11 Batch: 1200
Training Loss: 0.7106618261337281
Epoch: 11 Batch: 1250
Training Loss: 0.6633839577436447
Epoch: 11 Batch: 1300
Training Loss: 0.6955830359458923
Epoch: 11 Batch: 1350
Training Loss: 0.691165024638176
Epoch: 11 Batch: 1400
Training Loss: 0.6798675310611725
Epoch: 11 Batch: 1450
Training Loss: 0.7024083185195923
Epoch: 11 Batch: 1500
Training Loss: 0.6883179599046707
Epoch: 11 Batch: 1550
Training Loss: 0.6728341799974441
Epoch: 11 Batch: 1600
Training Loss: 0.6667964273691177
Epoch: 11 Batch: 1650
Training Loss: 0.7246699434518814
Epoch: 11 Batch: 1700
Training Loss: 0.6604917579889298
Epoch: 11 Batch: 1750
Training Loss: 0.7250688660144806
Epoch: 11 Batch: 1800
Training Loss: 0.6720206969976426
Epoch: 11 Batch: 1850
Training Loss: 0.6992907327413559
Epoch: 11 Batch: 1900
Training Loss: 0.6745257222652435
Epoch: 11 Batch: 1950
Training Loss: 0.6456472909450531
Epoch: 11 Batch: 2000
Training Loss: 0.711356430053711
Epoch: 11 Batch: 2050
Training Loss: 0.6587403094768525
Epoch: 11 Batch: 2100
Training Loss: 0.6242489439249038
Epoch: 11 Batch: 2150
Training Loss: 0.6851740217208863
Epoch: 11 Batch: 2200
Training Loss: 0.6861955052614213
Epoch: 11 Batch: 2250
Training Loss: 0.6606952333450318
Epoch: 11 Batch: 2300
Training Loss: 0.6749631160497666
Epoch: 11 Batch: 2350
Training Loss: 0.6629606997966766
Epoch: 11 Batch: 2400
Training Loss: 0.6870038372278213
Epoch: 11 Batch: 2450
Training Loss: 0.7212725377082825
Epoch: 11 Batch: 2500
Training Loss: 0.6669472068548202
Epoch: 11 Batch: 2550
Training Loss: 0.6520917326211929
Epoch: 11 Batch: 2600
Training Loss: 0.6786084055900574
Epoch: 11 Batch: 2650
Training Loss: 0.6594095087051391
Epoch: 11 Batch: 2700
Training Loss: 0.6493705159425736
Epoch: 11 Batch: 2750
Training Loss: 0.6853246659040451
Epoch: 11 Batch: 2800
Training Loss: 0.691745936870575
Epoch: 11 Batch: 2850
Training Loss: 0.6727262049913406
Epoch: 11 Batch: 2900
Training Loss: 0.7004076218605042
Epoch: 11 Batch: 2950
Training Loss: 0.6840568900108337
Epoch: 11 Batch: 3000
Training Loss: 0.6691310876607894
Epoch: 11 Batch: 3050
Training Loss: 0.6940602618455887
Epoch: 11 Batch: 3100
Training Loss: 0.6830202054977417
Epoch: 11 Batch: 3150
Training Loss: 0.6599400222301484
Epoch: 11 Batch: 3200
Training Loss: 0.6944992196559906
Epoch: 11 Batch: 3250
Training Loss: 0.6517535150051117
Epoch: 11 Batch: 3300
Training Loss: 0.669686974287033
Epoch: 11 Batch: 3350
Training Loss: 0.640118567943573
Epoch: 11 Batch: 3400
Training Loss: 0.6564750528335571
Epoch: 11 Batch: 3450
Training Loss: 0.6789319396018982
Epoch: 11 Batch: 3500
Training Loss: 0.6715256845951081
Epoch: 12 
 Validation Loss: 0.5777269386582904
---------------------------
Epoch: 12 Batch: 50
Training Loss: 0.685318603515625
Epoch: 12 Batch: 100
Training Loss: 0.6528674125671386
Epoch: 12 Batch: 150
Training Loss: 0.6843303924798966
Epoch: 12 Batch: 200
Training Loss: 0.6851355814933777
Epoch: 12 Batch: 250
Training Loss: 0.6588284879922867
Epoch: 12 Batch: 300
Training Loss: 0.6919128996133804
Epoch: 12 Batch: 350
Training Loss: 0.7025412285327911
Epoch: 12 Batch: 400
Training Loss: 0.6402914273738861
Epoch: 12 Batch: 450
Training Loss: 0.6984476006031036
Epoch: 12 Batch: 500
Training Loss: 0.6615843796730041
Epoch: 12 Batch: 550
Training Loss: 0.6492689335346222
Epoch: 12 Batch: 600
Training Loss: 0.6515860712528229
Epoch: 12 Batch: 650
Training Loss: 0.684860748052597
Epoch: 12 Batch: 700
Training Loss: 0.6655755114555358
Epoch: 12 Batch: 750
Training Loss: 0.660250512957573
Epoch: 12 Batch: 800
Training Loss: 0.7062101191282273
Epoch: 12 Batch: 850
Training Loss: 0.6652360415458679
Epoch: 12 Batch: 900
Training Loss: 0.6969554048776626
Epoch: 12 Batch: 950
Training Loss: 0.6866608995199204
Epoch: 12 Batch: 1000
Training Loss: 0.6679482185840606
Epoch: 12 Batch: 1050
Training Loss: 0.6987976652383804
Epoch: 12 Batch: 1100
Training Loss: 0.6643816167116166
Epoch: 12 Batch: 1150
Training Loss: 0.688178802728653
Epoch: 12 Batch: 1200
Training Loss: 0.6635364055633545
Epoch: 12 Batch: 1250
Training Loss: 0.6589659631252289
Epoch: 12 Batch: 1300
Training Loss: 0.6735825401544571
Epoch: 12 Batch: 1350
Training Loss: 0.6807587593793869
Epoch: 12 Batch: 1400
Training Loss: 0.6892262905836105
Epoch: 12 Batch: 1450
Training Loss: 0.6787394320964814
Epoch: 12 Batch: 1500
Training Loss: 0.6657690405845642
Epoch: 12 Batch: 1550
Training Loss: 0.6958214402198791
Epoch: 12 Batch: 1600
Training Loss: 0.6632994788885117
Epoch: 12 Batch: 1650
Training Loss: 0.7093920600414276
Epoch: 12 Batch: 1700
Training Loss: 0.6509554278850556
Epoch: 12 Batch: 1750
Training Loss: 0.6803400641679764
Epoch: 12 Batch: 1800
Training Loss: 0.6694932264089585
Epoch: 12 Batch: 1850
Training Loss: 0.6442686241865158
Epoch: 12 Batch: 1900
Training Loss: 0.6686191552877426
Epoch: 12 Batch: 1950
Training Loss: 0.6630009973049164
Epoch: 12 Batch: 2000
Training Loss: 0.6543599981069564
Epoch: 12 Batch: 2050
Training Loss: 0.6756236845254898
Epoch: 12 Batch: 2100
Training Loss: 0.6808135265111923
Epoch: 12 Batch: 2150
Training Loss: 0.6529065704345703
Epoch: 12 Batch: 2200
Training Loss: 0.6416085880994796
Epoch: 12 Batch: 2250
Training Loss: 0.6804413902759552
Epoch: 12 Batch: 2300
Training Loss: 0.6632177674770355
Epoch: 12 Batch: 2350
Training Loss: 0.6845122259855271
Epoch: 12 Batch: 2400
Training Loss: 0.6868885040283204
Epoch: 12 Batch: 2450
Training Loss: 0.6812189817428589
Epoch: 12 Batch: 2500
Training Loss: 0.6679790621995926
Epoch: 12 Batch: 2550
Training Loss: 0.666963883638382
Epoch: 12 Batch: 2600
Training Loss: 0.6820249170064926
Epoch: 12 Batch: 2650
Training Loss: 0.6522351384162903
Epoch: 12 Batch: 2700
Training Loss: 0.7145854395627975
Epoch: 12 Batch: 2750
Training Loss: 0.6224224078655243
Epoch: 12 Batch: 2800
Training Loss: 0.7095328676700592
Epoch: 12 Batch: 2850
Training Loss: 0.6833053857088089
Epoch: 12 Batch: 2900
Training Loss: 0.6795648378133774
Epoch: 12 Batch: 2950
Training Loss: 0.6974024319648743
Epoch: 12 Batch: 3000
Training Loss: 0.6572957646846771
Epoch: 12 Batch: 3050
Training Loss: 0.6669093048572541
Epoch: 12 Batch: 3100
Training Loss: 0.6372146660089493
Epoch: 12 Batch: 3150
Training Loss: 0.656721779704094
Epoch: 12 Batch: 3200
Training Loss: 0.6460848605632782
Epoch: 12 Batch: 3250
Training Loss: 0.6687845760583877
Epoch: 12 Batch: 3300
Training Loss: 0.6621990692615509
Epoch: 12 Batch: 3350
Training Loss: 0.6685889106988907
Epoch: 12 Batch: 3400
Training Loss: 0.6754521608352662
Epoch: 12 Batch: 3450
Training Loss: 0.6486873322725296
Epoch: 12 Batch: 3500
Training Loss: 0.66977922976017
Epoch: 13 
 Validation Loss: 0.5742628557814492
---------------------------
Epoch: 13 Batch: 50
Training Loss: 0.6620007938146591
Epoch: 13 Batch: 100
Training Loss: 0.6535992836952209
Epoch: 13 Batch: 150
Training Loss: 0.6760449802875519
Epoch: 13 Batch: 200
Training Loss: 0.6591276347637176
Epoch: 13 Batch: 250
Training Loss: 0.6665166509151459
Epoch: 13 Batch: 300
Training Loss: 0.6924661409854889
Epoch: 13 Batch: 350
Training Loss: 0.6736018061637878
Epoch: 13 Batch: 400
Training Loss: 0.6772264564037322
Epoch: 13 Batch: 450
Training Loss: 0.634667643904686
Epoch: 13 Batch: 500
Training Loss: 0.6606393414735794
Epoch: 13 Batch: 550
Training Loss: 0.6694588309526444
Epoch: 13 Batch: 600
Training Loss: 0.6453732639551163
Epoch: 13 Batch: 650
Training Loss: 0.6688866257667542
Epoch: 13 Batch: 700
Training Loss: 0.6410807025432587
Epoch: 13 Batch: 750
Training Loss: 0.6840250074863434
Epoch: 13 Batch: 800
Training Loss: 0.6527089893817901
Epoch: 13 Batch: 850
Training Loss: 0.6544140326976776
Epoch: 13 Batch: 900
Training Loss: 0.6923239189386368
Epoch: 13 Batch: 950
Training Loss: 0.6723067218065262
Epoch: 13 Batch: 1000
Training Loss: 0.6867627674341201
Epoch: 13 Batch: 1050
Training Loss: 0.720443639755249
Epoch: 13 Batch: 1100
Training Loss: 0.6580527591705322
Epoch: 13 Batch: 1150
Training Loss: 0.6500254607200623
Epoch: 13 Batch: 1200
Training Loss: 0.6592534106969833
Epoch: 13 Batch: 1250
Training Loss: 0.6725039392709732
Epoch: 13 Batch: 1300
Training Loss: 0.6840366524457931
Epoch: 13 Batch: 1350
Training Loss: 0.6643248462677002
Epoch: 13 Batch: 1400
Training Loss: 0.6880855369567871
Epoch: 13 Batch: 1450
Training Loss: 0.6721518290042877
Epoch: 13 Batch: 1500
Training Loss: 0.6731593054533005
Epoch: 13 Batch: 1550
Training Loss: 0.6535471498966217
Epoch: 13 Batch: 1600
Training Loss: 0.6560163080692292
Epoch: 13 Batch: 1650
Training Loss: 0.6659647113084793
Epoch: 13 Batch: 1700
Training Loss: 0.6313208740949631
Epoch: 13 Batch: 1750
Training Loss: 0.6605160450935363
Epoch: 13 Batch: 1800
Training Loss: 0.6445176154375076
Epoch: 13 Batch: 1850
Training Loss: 0.6475066286325455
Epoch: 13 Batch: 1900
Training Loss: 0.6751932120323181
Epoch: 13 Batch: 1950
Training Loss: 0.6294847989082336
Epoch: 13 Batch: 2000
Training Loss: 0.6768449181318283
Epoch: 13 Batch: 2050
Training Loss: 0.6613360232114792
Epoch: 13 Batch: 2100
Training Loss: 0.656810411810875
Epoch: 13 Batch: 2150
Training Loss: 0.6549902778863906
Epoch: 13 Batch: 2200
Training Loss: 0.6850100779533386
Epoch: 13 Batch: 2250
Training Loss: 0.6740742027759552
Epoch: 13 Batch: 2300
Training Loss: 0.7095654714107513
Epoch: 13 Batch: 2350
Training Loss: 0.6465201008319855
Epoch: 13 Batch: 2400
Training Loss: 0.6780386155843735
Epoch: 13 Batch: 2450
Training Loss: 0.6600483709573746
Epoch: 13 Batch: 2500
Training Loss: 0.6837262624502182
Epoch: 13 Batch: 2550
Training Loss: 0.67541219830513
Epoch: 13 Batch: 2600
Training Loss: 0.674420719742775
Epoch: 13 Batch: 2650
Training Loss: 0.6511570370197296
Epoch: 13 Batch: 2700
Training Loss: 0.6639074379205704
Epoch: 13 Batch: 2750
Training Loss: 0.6605059933662415
Epoch: 13 Batch: 2800
Training Loss: 0.6721466666460038
Epoch: 13 Batch: 2850
Training Loss: 0.6752873647212982
Epoch: 13 Batch: 2900
Training Loss: 0.6390615403652191
Epoch: 13 Batch: 2950
Training Loss: 0.6886030280590058
Epoch: 13 Batch: 3000
Training Loss: 0.6706471586227417
Epoch: 13 Batch: 3050
Training Loss: 0.6572676509618759
Epoch: 13 Batch: 3100
Training Loss: 0.6734045970439911
Epoch: 13 Batch: 3150
Training Loss: 0.6478758782148362
Epoch: 13 Batch: 3200
Training Loss: 0.6613560056686402
Epoch: 13 Batch: 3250
Training Loss: 0.6835750305652618
Epoch: 13 Batch: 3300
Training Loss: 0.6473424774408341
Epoch: 13 Batch: 3350
Training Loss: 0.6595935189723968
Epoch: 13 Batch: 3400
Training Loss: 0.6559874260425568
Epoch: 13 Batch: 3450
Training Loss: 0.646983847618103
Epoch: 13 Batch: 3500
Training Loss: 0.687013903260231
Epoch: 14 
 Validation Loss: 0.5713201132085588
---------------------------
Epoch: 14 Batch: 50
Training Loss: 0.6733329451084137
Epoch: 14 Batch: 100
Training Loss: 0.6673275518417359
Epoch: 14 Batch: 150
Training Loss: 0.6537599575519562
Epoch: 14 Batch: 200
Training Loss: 0.6356078279018402
Epoch: 14 Batch: 250
Training Loss: 0.6693833446502686
Epoch: 14 Batch: 300
Training Loss: 0.6587180894613266
Epoch: 14 Batch: 350
Training Loss: 0.6259870278835297
Epoch: 14 Batch: 400
Training Loss: 0.6782680290937424
Epoch: 14 Batch: 450
Training Loss: 0.6484840160608292
Epoch: 14 Batch: 500
Training Loss: 0.6625229042768478
Epoch: 14 Batch: 550
Training Loss: 0.6688083148002625
Epoch: 14 Batch: 600
Training Loss: 0.6732457482814789
Epoch: 14 Batch: 650
Training Loss: 0.664875618815422
Epoch: 14 Batch: 700
Training Loss: 0.6569174575805664
Epoch: 14 Batch: 750
Training Loss: 0.6587848210334778
Epoch: 14 Batch: 800
Training Loss: 0.6740010976791382
Epoch: 14 Batch: 850
Training Loss: 0.672169623374939
Epoch: 14 Batch: 900
Training Loss: 0.655465475320816
Epoch: 14 Batch: 950
Training Loss: 0.7026843041181564
Epoch: 14 Batch: 1000
Training Loss: 0.6663889759778976
Epoch: 14 Batch: 1050
Training Loss: 0.6718899005651474
Epoch: 14 Batch: 1100
Training Loss: 0.6621809750795364
Epoch: 14 Batch: 1150
Training Loss: 0.6902848374843598
Epoch: 14 Batch: 1200
Training Loss: 0.6585494506359101
Epoch: 14 Batch: 1250
Training Loss: 0.6535399663448334
Epoch: 14 Batch: 1300
Training Loss: 0.6610485553741455
Epoch: 14 Batch: 1350
Training Loss: 0.659433234333992
Epoch: 14 Batch: 1400
Training Loss: 0.6546001929044724
Epoch: 14 Batch: 1450
Training Loss: 0.6732484221458435
Epoch: 14 Batch: 1500
Training Loss: 0.658129175901413
Epoch: 14 Batch: 1550
Training Loss: 0.6620565140247345
Epoch: 14 Batch: 1600
Training Loss: 0.6612159341573716
Epoch: 14 Batch: 1650
Training Loss: 0.6461598557233811
Epoch: 14 Batch: 1700
Training Loss: 0.6703741413354873
Epoch: 14 Batch: 1750
Training Loss: 0.6409803193807602
Epoch: 14 Batch: 1800
Training Loss: 0.6719926863908767
Epoch: 14 Batch: 1850
Training Loss: 0.6581442654132843
Epoch: 14 Batch: 1900
Training Loss: 0.6648877066373825
Epoch: 14 Batch: 1950
Training Loss: 0.6822572112083435
Epoch: 14 Batch: 2000
Training Loss: 0.6535145199298859
Epoch: 14 Batch: 2050
Training Loss: 0.7095933133363723
Epoch: 14 Batch: 2100
Training Loss: 0.664525848031044
Epoch: 14 Batch: 2150
Training Loss: 0.6865905022621155
Epoch: 14 Batch: 2200
Training Loss: 0.664183240532875
Epoch: 14 Batch: 2250
Training Loss: 0.6541236889362335
Epoch: 14 Batch: 2300
Training Loss: 0.6651387548446656
Epoch: 14 Batch: 2350
Training Loss: 0.6432332640886307
Epoch: 14 Batch: 2400
Training Loss: 0.6596078479290008
Epoch: 14 Batch: 2450
Training Loss: 0.6594906902313232
Epoch: 14 Batch: 2500
Training Loss: 0.6638424342870712
Epoch: 14 Batch: 2550
Training Loss: 0.6382390397787094
Epoch: 14 Batch: 2600
Training Loss: 0.7226277828216553
Epoch: 14 Batch: 2650
Training Loss: 0.6725127464532852
Epoch: 14 Batch: 2700
Training Loss: 0.6725916117429733
Epoch: 14 Batch: 2750
Training Loss: 0.6552544730901718
Epoch: 14 Batch: 2800
Training Loss: 0.6539500159025192
Epoch: 14 Batch: 2850
Training Loss: 0.6548406440019607
Epoch: 14 Batch: 2900
Training Loss: 0.6468318128585815
Epoch: 14 Batch: 2950
Training Loss: 0.66692995429039
Epoch: 14 Batch: 3000
Training Loss: 0.657346304655075
Epoch: 14 Batch: 3050
Training Loss: 0.6769468349218368
Epoch: 14 Batch: 3100
Training Loss: 0.6670600891113281
Epoch: 14 Batch: 3150
Training Loss: 0.6740602427721023
Epoch: 14 Batch: 3200
Training Loss: 0.6836062228679657
Epoch: 14 Batch: 3250
Training Loss: 0.6603612947463989
Epoch: 14 Batch: 3300
Training Loss: 0.7108388376235962
Epoch: 14 Batch: 3350
Training Loss: 0.6742677682638168
Epoch: 14 Batch: 3400
Training Loss: 0.666980403661728
Epoch: 14 Batch: 3450
Training Loss: 0.6450764352083206
Epoch: 14 Batch: 3500
Training Loss: 0.6562421083450317
Epoch: 15 
 Validation Loss: 0.5682755029863782
---------------------------
Epoch: 15 Batch: 50
Training Loss: 0.6769951039552689
Epoch: 15 Batch: 100
Training Loss: 0.6200314104557038
Epoch: 15 Batch: 150
Training Loss: 0.6596911031007767
Epoch: 15 Batch: 200
Training Loss: 0.6739317613840103
Epoch: 15 Batch: 250
Training Loss: 0.6441340410709381
Epoch: 15 Batch: 300
Training Loss: 0.6725107377767563
Epoch: 15 Batch: 350
Training Loss: 0.6772733920812607
Epoch: 15 Batch: 400
Training Loss: 0.6570515888929367
Epoch: 15 Batch: 450
Training Loss: 0.6594714331626892
Epoch: 15 Batch: 500
Training Loss: 0.6687134271860122
Epoch: 15 Batch: 550
Training Loss: 0.6556027668714524
Epoch: 15 Batch: 600
Training Loss: 0.6696225547790527
Epoch: 15 Batch: 650
Training Loss: 0.6687429600954056
Epoch: 15 Batch: 700
Training Loss: 0.6677355861663818
Epoch: 15 Batch: 750
Training Loss: 0.6544105833768845
Epoch: 15 Batch: 800
Training Loss: 0.6567068243026734
Epoch: 15 Batch: 850
Training Loss: 0.644334831237793
Epoch: 15 Batch: 900
Training Loss: 0.6666358387470246
Epoch: 15 Batch: 950
Training Loss: 0.6587638992071152
Epoch: 15 Batch: 1000
Training Loss: 0.6484704405069351
Epoch: 15 Batch: 1050
Training Loss: 0.6711576783657074
Epoch: 15 Batch: 1100
Training Loss: 0.6699405062198639
Epoch: 15 Batch: 1150
Training Loss: 0.7031853115558624
Epoch: 15 Batch: 1200
Training Loss: 0.6768654161691665
Epoch: 15 Batch: 1250
Training Loss: 0.6740136986970902
Epoch: 15 Batch: 1300
Training Loss: 0.6477860492467881
Epoch: 15 Batch: 1350
Training Loss: 0.6717600232362747
Epoch: 15 Batch: 1400
Training Loss: 0.6476859354972839
Epoch: 15 Batch: 1450
Training Loss: 0.6726166301965714
Epoch: 15 Batch: 1500
Training Loss: 0.6416751807928085
Epoch: 15 Batch: 1550
Training Loss: 0.6611993145942688
Epoch: 15 Batch: 1600
Training Loss: 0.6672578644752503
Epoch: 15 Batch: 1650
Training Loss: 0.6581068032979965
Epoch: 15 Batch: 1700
Training Loss: 0.6611636263132096
Epoch: 15 Batch: 1750
Training Loss: 0.6649153298139572
Epoch: 15 Batch: 1800
Training Loss: 0.6615806877613067
Epoch: 15 Batch: 1850
Training Loss: 0.6511340832710266
Epoch: 15 Batch: 1900
Training Loss: 0.6703353899717331
Epoch: 15 Batch: 1950
Training Loss: 0.6601356035470962
Epoch: 15 Batch: 2000
Training Loss: 0.6519132912158966
Epoch: 15 Batch: 2050
Training Loss: 0.6392879116535187
Epoch: 15 Batch: 2100
Training Loss: 0.6404878473281861
Epoch: 15 Batch: 2150
Training Loss: 0.6626436990499497
Epoch: 15 Batch: 2200
Training Loss: 0.6837071722745895
Epoch: 15 Batch: 2250
Training Loss: 0.6979633247852326
Epoch: 15 Batch: 2300
Training Loss: 0.6689942282438278
Epoch: 15 Batch: 2350
Training Loss: 0.6432705104351044
Epoch: 15 Batch: 2400
Training Loss: 0.6342694807052612
Epoch: 15 Batch: 2450
Training Loss: 0.653976446390152
Epoch: 15 Batch: 2500
Training Loss: 0.6395457172393799
Epoch: 15 Batch: 2550
Training Loss: 0.6702154231071472
Epoch: 15 Batch: 2600
Training Loss: 0.6679080718755722
Epoch: 15 Batch: 2650
Training Loss: 0.6543753075599671
Epoch: 15 Batch: 2700
Training Loss: 0.6561382120847702
Epoch: 15 Batch: 2750
Training Loss: 0.6528388595581055
Epoch: 15 Batch: 2800
Training Loss: 0.6620832633972168
Epoch: 15 Batch: 2850
Training Loss: 0.677483828663826
Epoch: 15 Batch: 2900
Training Loss: 0.6510616189241409
Epoch: 15 Batch: 2950
Training Loss: 0.6573427218198776
Epoch: 15 Batch: 3000
Training Loss: 0.6358979451656341
Epoch: 15 Batch: 3050
Training Loss: 0.6290801763534546
Epoch: 15 Batch: 3100
Training Loss: 0.6415902185440063
Epoch: 15 Batch: 3150
Training Loss: 0.6834113299846649
Epoch: 15 Batch: 3200
Training Loss: 0.6718445408344269
Epoch: 15 Batch: 3250
Training Loss: 0.646263222694397
Epoch: 15 Batch: 3300
Training Loss: 0.661789391040802
Epoch: 15 Batch: 3350
Training Loss: 0.6875375437736512
Epoch: 15 Batch: 3400
Training Loss: 0.655652813911438
Epoch: 15 Batch: 3450
Training Loss: 0.6531668645143509
Epoch: 15 Batch: 3500
Training Loss: 0.6501038610935211
Epoch: 16 
 Validation Loss: 0.5657870918512344
---------------------------
Epoch: 16 Batch: 50
Training Loss: 0.6895050078630447
Epoch: 16 Batch: 100
Training Loss: 0.6522064673900604
Epoch: 16 Batch: 150
Training Loss: 0.6258936858177185
Epoch: 16 Batch: 200
Training Loss: 0.6205329138040543
Epoch: 16 Batch: 250
Training Loss: 0.6683792501688004
Epoch: 16 Batch: 300
Training Loss: 0.6688627696037293
Epoch: 16 Batch: 350
Training Loss: 0.6703499734401703
Epoch: 16 Batch: 400
Training Loss: 0.6558686584234238
Epoch: 16 Batch: 450
Training Loss: 0.6577180421352387
Epoch: 16 Batch: 500
Training Loss: 0.6815764027833938
Epoch: 16 Batch: 550
Training Loss: 0.666022669672966
Epoch: 16 Batch: 600
Training Loss: 0.6366517198085785
Epoch: 16 Batch: 650
Training Loss: 0.6916922074556351
Epoch: 16 Batch: 700
Training Loss: 0.6346006435155869
Epoch: 16 Batch: 750
Training Loss: 0.6268664062023163
Epoch: 16 Batch: 800
Training Loss: 0.6496276342868805
Epoch: 16 Batch: 850
Training Loss: 0.625870276093483
Epoch: 16 Batch: 900
Training Loss: 0.6816039669513703
Epoch: 16 Batch: 950
Training Loss: 0.6825653886795044
Epoch: 16 Batch: 1000
Training Loss: 0.6335617607831955
Epoch: 16 Batch: 1050
Training Loss: 0.66068328499794
Epoch: 16 Batch: 1100
Training Loss: 0.665393933057785
Epoch: 16 Batch: 1150
Training Loss: 0.6405359190702439
Epoch: 16 Batch: 1200
Training Loss: 0.6496623486280442
Epoch: 16 Batch: 1250
Training Loss: 0.6675141042470932
Epoch: 16 Batch: 1300
Training Loss: 0.6956598818302154
Epoch: 16 Batch: 1350
Training Loss: 0.6520983999967576
Epoch: 16 Batch: 1400
Training Loss: 0.6438122296333313
Epoch: 16 Batch: 1450
Training Loss: 0.6692323541641235
Epoch: 16 Batch: 1500
Training Loss: 0.6824541759490966
Epoch: 16 Batch: 1550
Training Loss: 0.6817377990484238
Epoch: 16 Batch: 1600
Training Loss: 0.6621263909339905
Epoch: 16 Batch: 1650
Training Loss: 0.631360422372818
Epoch: 16 Batch: 1700
Training Loss: 0.6579456263780594
Epoch: 16 Batch: 1750
Training Loss: 0.6705236995220184
Epoch: 16 Batch: 1800
Training Loss: 0.6534432071447372
Epoch: 16 Batch: 1850
Training Loss: 0.6355841481685638
Epoch: 16 Batch: 1900
Training Loss: 0.6822678864002227
Epoch: 16 Batch: 1950
Training Loss: 0.6745354473590851
Epoch: 16 Batch: 2000
Training Loss: 0.6411166393756866
Epoch: 16 Batch: 2050
Training Loss: 0.6440356713533402
Epoch: 16 Batch: 2100
Training Loss: 0.6710129714012146
Epoch: 16 Batch: 2150
Training Loss: 0.6522724902629853
Epoch: 16 Batch: 2200
Training Loss: 0.6853815144300461
Epoch: 16 Batch: 2250
Training Loss: 0.6510029375553131
Epoch: 16 Batch: 2300
Training Loss: 0.637034398317337
Epoch: 16 Batch: 2350
Training Loss: 0.6583732014894486
Epoch: 16 Batch: 2400
Training Loss: 0.6900999879837036
Epoch: 16 Batch: 2450
Training Loss: 0.6574138557910919
Epoch: 16 Batch: 2500
Training Loss: 0.6609834092855453
Epoch: 16 Batch: 2550
Training Loss: 0.6577646452188491
Epoch: 16 Batch: 2600
Training Loss: 0.6692778205871582
Epoch: 16 Batch: 2650
Training Loss: 0.6379803854227066
Epoch: 16 Batch: 2700
Training Loss: 0.6888727957010269
Epoch: 16 Batch: 2750
Training Loss: 0.6575572210550308
Epoch: 16 Batch: 2800
Training Loss: 0.6656605815887451
Epoch: 16 Batch: 2850
Training Loss: 0.621061977148056
Epoch: 16 Batch: 2900
Training Loss: 0.6771757793426514
Epoch: 16 Batch: 2950
Training Loss: 0.6866749596595764
Epoch: 16 Batch: 3000
Training Loss: 0.6635093104839325
Epoch: 16 Batch: 3050
Training Loss: 0.6948037332296372
Epoch: 16 Batch: 3100
Training Loss: 0.6445748841762543
Epoch: 16 Batch: 3150
Training Loss: 0.6470499563217164
Epoch: 16 Batch: 3200
Training Loss: 0.6550243878364563
Epoch: 16 Batch: 3250
Training Loss: 0.6569188910722733
Epoch: 16 Batch: 3300
Training Loss: 0.6853146868944168
Epoch: 16 Batch: 3350
Training Loss: 0.6732491201162338
Epoch: 16 Batch: 3400
Training Loss: 0.6654252296686173
Epoch: 16 Batch: 3450
Training Loss: 0.6616866391897201
Epoch: 16 Batch: 3500
Training Loss: 0.645784912109375
Epoch: 17 
 Validation Loss: 0.563871810833613
---------------------------
Epoch: 17 Batch: 50
Training Loss: 0.6644266217947006
Epoch: 17 Batch: 100
Training Loss: 0.6351143598556519
Epoch: 17 Batch: 150
Training Loss: 0.6392952173948288
Epoch: 17 Batch: 200
Training Loss: 0.6371464776992798
Epoch: 17 Batch: 250
Training Loss: 0.6431971144676208
Epoch: 17 Batch: 300
Training Loss: 0.7161767101287841
Epoch: 17 Batch: 350
Training Loss: 0.6441271543502808
Epoch: 17 Batch: 400
Training Loss: 0.6482442504167557
Epoch: 17 Batch: 450
Training Loss: 0.6682577747106552
Epoch: 17 Batch: 500
Training Loss: 0.6762345892190933
Epoch: 17 Batch: 550
Training Loss: 0.6487511771917344
Epoch: 17 Batch: 600
Training Loss: 0.6390170270204544
Epoch: 17 Batch: 650
Training Loss: 0.6833145529031753
Epoch: 17 Batch: 700
Training Loss: 0.653625276684761
Epoch: 17 Batch: 750
Training Loss: 0.63392274081707
Epoch: 17 Batch: 800
Training Loss: 0.6405558532476425
Epoch: 17 Batch: 850
Training Loss: 0.6588243424892426
Epoch: 17 Batch: 900
Training Loss: 0.6592631655931472
Epoch: 17 Batch: 950
Training Loss: 0.6421455764770507
Epoch: 17 Batch: 1000
Training Loss: 0.6809691607952117
Epoch: 17 Batch: 1050
Training Loss: 0.6563624238967896
Epoch: 17 Batch: 1100
Training Loss: 0.6545978754758834
Epoch: 17 Batch: 1150
Training Loss: 0.6167011106014252
Epoch: 17 Batch: 1200
Training Loss: 0.6676860696077347
Epoch: 17 Batch: 1250
Training Loss: 0.6404345488548279
Epoch: 17 Batch: 1300
Training Loss: 0.6587590724229813
Epoch: 17 Batch: 1350
Training Loss: 0.677667944431305
Epoch: 17 Batch: 1400
Training Loss: 0.639300929903984
Epoch: 17 Batch: 1450
Training Loss: 0.6278713184595108
Epoch: 17 Batch: 1500
Training Loss: 0.6534275978803634
Epoch: 17 Batch: 1550
Training Loss: 0.6690019828081131
Epoch: 17 Batch: 1600
Training Loss: 0.6173810452222824
Epoch: 17 Batch: 1650
Training Loss: 0.632602419257164
Epoch: 17 Batch: 1700
Training Loss: 0.6405244046449661
Epoch: 17 Batch: 1750
Training Loss: 0.6343140131235123
Epoch: 17 Batch: 1800
Training Loss: 0.6433989053964615
Epoch: 17 Batch: 1850
Training Loss: 0.6800630128383637
Epoch: 17 Batch: 1900
Training Loss: 0.6581642180681229
Epoch: 17 Batch: 1950
Training Loss: 0.6410842001438141
Epoch: 17 Batch: 2000
Training Loss: 0.6778907805681229
Epoch: 17 Batch: 2050
Training Loss: 0.6495722818374634
Epoch: 17 Batch: 2100
Training Loss: 0.6855287665128708
Epoch: 17 Batch: 2150
Training Loss: 0.6223986625671387
Epoch: 17 Batch: 2200
Training Loss: 0.6756740170717239
Epoch: 17 Batch: 2250
Training Loss: 0.6851783674955368
Epoch: 17 Batch: 2300
Training Loss: 0.6568176656961441
Epoch: 17 Batch: 2350
Training Loss: 0.6467367631196975
Epoch: 17 Batch: 2400
Training Loss: 0.6777337092161179
Epoch: 17 Batch: 2450
Training Loss: 0.6866215389966964
Epoch: 17 Batch: 2500
Training Loss: 0.6864749109745025
Epoch: 17 Batch: 2550
Training Loss: 0.6756932210922241
Epoch: 17 Batch: 2600
Training Loss: 0.6422206693887711
Epoch: 17 Batch: 2650
Training Loss: 0.6555138623714447
Epoch: 17 Batch: 2700
Training Loss: 0.6267103737592697
Epoch: 17 Batch: 2750
Training Loss: 0.6462748903036117
Epoch: 17 Batch: 2800
Training Loss: 0.6613605898618699
Epoch: 17 Batch: 2850
Training Loss: 0.6822733932733536
Epoch: 17 Batch: 2900
Training Loss: 0.6435597217082978
Epoch: 17 Batch: 2950
Training Loss: 0.6370045423507691
Epoch: 17 Batch: 3000
Training Loss: 0.6394997465610505
Epoch: 17 Batch: 3050
Training Loss: 0.6529568582773209
Epoch: 17 Batch: 3100
Training Loss: 0.6201901465654374
Epoch: 17 Batch: 3150
Training Loss: 0.6530128228664398
Epoch: 17 Batch: 3200
Training Loss: 0.6544885754585266
Epoch: 17 Batch: 3250
Training Loss: 0.6672595220804215
Epoch: 17 Batch: 3300
Training Loss: 0.6450817465782166
Epoch: 17 Batch: 3350
Training Loss: 0.654750417470932
Epoch: 17 Batch: 3400
Training Loss: 0.6688874000310898
Epoch: 17 Batch: 3450
Training Loss: 0.6428411728143693
Epoch: 17 Batch: 3500
Training Loss: 0.6358956807851791
Epoch: 18 
 Validation Loss: 0.5622219244639078
---------------------------
Epoch: 18 Batch: 50
Training Loss: 0.6980179286003113
Epoch: 18 Batch: 100
Training Loss: 0.6796414589881897
Epoch: 18 Batch: 150
Training Loss: 0.660344717502594
Epoch: 18 Batch: 200
Training Loss: 0.6545752388238907
Epoch: 18 Batch: 250
Training Loss: 0.6805296486616135
Epoch: 18 Batch: 300
Training Loss: 0.6826156902313233
Epoch: 18 Batch: 350
Training Loss: 0.6414821147918701
Epoch: 18 Batch: 400
Training Loss: 0.6503752595186234
Epoch: 18 Batch: 450
Training Loss: 0.663938894867897
Epoch: 18 Batch: 500
Training Loss: 0.6295801794528961
Epoch: 18 Batch: 550
Training Loss: 0.6275858157873153
Epoch: 18 Batch: 600
Training Loss: 0.6583479505777359
Epoch: 18 Batch: 650
Training Loss: 0.638905150294304
Epoch: 18 Batch: 700
Training Loss: 0.650214005112648
Epoch: 18 Batch: 750
Training Loss: 0.6482851201295853
Epoch: 18 Batch: 800
Training Loss: 0.6379858297109604
Epoch: 18 Batch: 850
Training Loss: 0.6693582826852799
Epoch: 18 Batch: 900
Training Loss: 0.6442477238178254
Epoch: 18 Batch: 950
Training Loss: 0.6722900688648223
Epoch: 18 Batch: 1000
Training Loss: 0.6514522480964661
Epoch: 18 Batch: 1050
Training Loss: 0.658425070643425
Epoch: 18 Batch: 1100
Training Loss: 0.6573631602525711
Epoch: 18 Batch: 1150
Training Loss: 0.6435819816589355
Epoch: 18 Batch: 1200
Training Loss: 0.6454678738117218
Epoch: 18 Batch: 1250
Training Loss: 0.6697881090641021
Epoch: 18 Batch: 1300
Training Loss: 0.6578048557043076
Epoch: 18 Batch: 1350
Training Loss: 0.6536927604675293
Epoch: 18 Batch: 1400
Training Loss: 0.6299476605653763
Epoch: 18 Batch: 1450
Training Loss: 0.6277344089746475
Epoch: 18 Batch: 1500
Training Loss: 0.6443037986755371
Epoch: 18 Batch: 1550
Training Loss: 0.6952295434474945
Epoch: 18 Batch: 1600
Training Loss: 0.6374152952432632
Epoch: 18 Batch: 1650
Training Loss: 0.662567024230957
Epoch: 18 Batch: 1700
Training Loss: 0.6567500352859497
Epoch: 18 Batch: 1750
Training Loss: 0.638514112830162
Epoch: 18 Batch: 1800
Training Loss: 0.6798167788982391
Epoch: 18 Batch: 1850
Training Loss: 0.6438679724931717
Epoch: 18 Batch: 1900
Training Loss: 0.6482292121648788
Epoch: 18 Batch: 1950
Training Loss: 0.6219846737384797
Epoch: 18 Batch: 2000
Training Loss: 0.5900577455759048
Epoch: 18 Batch: 2050
Training Loss: 0.666445677280426
Epoch: 18 Batch: 2100
Training Loss: 0.6417172473669052
Epoch: 18 Batch: 2150
Training Loss: 0.6590107864141465
Epoch: 18 Batch: 2200
Training Loss: 0.6612558102607727
Epoch: 18 Batch: 2250
Training Loss: 0.6122008442878724
Epoch: 18 Batch: 2300
Training Loss: 0.6219401133060455
Epoch: 18 Batch: 2350
Training Loss: 0.6288629907369614
Epoch: 18 Batch: 2400
Training Loss: 0.6281367337703705
Epoch: 18 Batch: 2450
Training Loss: 0.6359547948837281
Epoch: 18 Batch: 2500
Training Loss: 0.6648845320940018
Epoch: 18 Batch: 2550
Training Loss: 0.6502693700790405
Epoch: 18 Batch: 2600
Training Loss: 0.6435974484682083
Epoch: 18 Batch: 2650
Training Loss: 0.6417945164442063
Epoch: 18 Batch: 2700
Training Loss: 0.6460177809000015
Epoch: 18 Batch: 2750
Training Loss: 0.6538596606254578
Epoch: 18 Batch: 2800
Training Loss: 0.655077406167984
Epoch: 18 Batch: 2850
Training Loss: 0.664721462726593
Epoch: 18 Batch: 2900
Training Loss: 0.677477051615715
Epoch: 18 Batch: 2950
Training Loss: 0.6569163727760315
Epoch: 18 Batch: 3000
Training Loss: 0.6527187544107437
Epoch: 18 Batch: 3050
Training Loss: 0.6519388842582703
Epoch: 18 Batch: 3100
Training Loss: 0.6572791159152984
Epoch: 18 Batch: 3150
Training Loss: 0.6649137073755265
Epoch: 18 Batch: 3200
Training Loss: 0.6367297720909119
Epoch: 18 Batch: 3250
Training Loss: 0.6418481630086899
Epoch: 18 Batch: 3300
Training Loss: 0.609432206749916
Epoch: 18 Batch: 3350
Training Loss: 0.6524032950401306
Epoch: 18 Batch: 3400
Training Loss: 0.6310077792406082
Epoch: 18 Batch: 3450
Training Loss: 0.6242465382814407
Epoch: 18 Batch: 3500
Training Loss: 0.6472479665279388
Epoch: 19 
 Validation Loss: 0.5599255926079221
---------------------------
Epoch: 19 Batch: 50
Training Loss: 0.6471753239631652
Epoch: 19 Batch: 100
Training Loss: 0.6465298569202423
Epoch: 19 Batch: 150
Training Loss: 0.6616915005445481
Epoch: 19 Batch: 200
Training Loss: 0.6633353507518769
Epoch: 19 Batch: 250
Training Loss: 0.6498792445659638
Epoch: 19 Batch: 300
Training Loss: 0.6222621762752533
Epoch: 19 Batch: 350
Training Loss: 0.6631955623626709
Epoch: 19 Batch: 400
Training Loss: 0.6338273602724075
Epoch: 19 Batch: 450
Training Loss: 0.6270689392089843
Epoch: 19 Batch: 500
Training Loss: 0.6681067341566086
Epoch: 19 Batch: 550
Training Loss: 0.6287644696235657
Epoch: 19 Batch: 600
Training Loss: 0.6510568606853485
Epoch: 19 Batch: 650
Training Loss: 0.6734984093904495
Epoch: 19 Batch: 700
Training Loss: 0.6266996532678604
Epoch: 19 Batch: 750
Training Loss: 0.6365422904491425
Epoch: 19 Batch: 800
Training Loss: 0.6512777268886566
Epoch: 19 Batch: 850
Training Loss: 0.6730173134803772
Epoch: 19 Batch: 900
Training Loss: 0.6374568492174149
Epoch: 19 Batch: 950
Training Loss: 0.6619498288631439
Epoch: 19 Batch: 1000
Training Loss: 0.6708282142877579
Epoch: 19 Batch: 1050
Training Loss: 0.6634541165828705
Epoch: 19 Batch: 1100
Training Loss: 0.6772932744026184
Epoch: 19 Batch: 1150
Training Loss: 0.6377854853868484
Epoch: 19 Batch: 1200
Training Loss: 0.6602169370651245
Epoch: 19 Batch: 1250
Training Loss: 0.6709760516881943
Epoch: 19 Batch: 1300
Training Loss: 0.662741362452507
Epoch: 19 Batch: 1350
Training Loss: 0.6608004075288773
Epoch: 19 Batch: 1400
Training Loss: 0.6697963798046112
Epoch: 19 Batch: 1450
Training Loss: 0.6858077502250671
Epoch: 19 Batch: 1500
Training Loss: 0.6638161009550094
Epoch: 19 Batch: 1550
Training Loss: 0.6306183749437332
Epoch: 19 Batch: 1600
Training Loss: 0.6447126311063767
Epoch: 19 Batch: 1650
Training Loss: 0.644719710946083
Epoch: 19 Batch: 1700
Training Loss: 0.6561447542905807
Epoch: 19 Batch: 1750
Training Loss: 0.6312383049726487
Epoch: 19 Batch: 1800
Training Loss: 0.6440173166990281
Epoch: 19 Batch: 1850
Training Loss: 0.6494628357887268
Epoch: 19 Batch: 1900
Training Loss: 0.6270628356933594
Epoch: 19 Batch: 1950
Training Loss: 0.6504219418764114
Epoch: 19 Batch: 2000
Training Loss: 0.6392926502227784
Epoch: 19 Batch: 2050
Training Loss: 0.6306712913513184
Epoch: 19 Batch: 2100
Training Loss: 0.6263879835605621
Epoch: 19 Batch: 2150
Training Loss: 0.6283252483606339
Epoch: 19 Batch: 2200
Training Loss: 0.6535512292385102
Epoch: 19 Batch: 2250
Training Loss: 0.6322617429494858
Epoch: 19 Batch: 2300
Training Loss: 0.6540067046880722
Epoch: 19 Batch: 2350
Training Loss: 0.6422733122110367
Epoch: 19 Batch: 2400
Training Loss: 0.6256461262702941
Epoch: 19 Batch: 2450
Training Loss: 0.6463373589515686
Epoch: 19 Batch: 2500
Training Loss: 0.6259509098529815
Epoch: 19 Batch: 2550
Training Loss: 0.6314333599805831
Epoch: 19 Batch: 2600
Training Loss: 0.6592135322093964
Epoch: 19 Batch: 2650
Training Loss: 0.6442668342590332
Epoch: 19 Batch: 2700
Training Loss: 0.6204573220014572
Epoch: 19 Batch: 2750
Training Loss: 0.6638338696956635
Epoch: 19 Batch: 2800
Training Loss: 0.6613313162326813
Epoch: 19 Batch: 2850
Training Loss: 0.6551299250125885
Epoch: 19 Batch: 2900
Training Loss: 0.6314579665660858
Epoch: 19 Batch: 2950
Training Loss: 0.6229520511627197
Epoch: 19 Batch: 3000
Training Loss: 0.6245859384536743
Epoch: 19 Batch: 3050
Training Loss: 0.6825826579332351
Epoch: 19 Batch: 3100
Training Loss: 0.6393907886743545
Epoch: 19 Batch: 3150
Training Loss: 0.660590346455574
Epoch: 19 Batch: 3200
Training Loss: 0.6657946884632111
Epoch: 19 Batch: 3250
Training Loss: 0.6702038383483887
Epoch: 19 Batch: 3300
Training Loss: 0.6399150431156159
Epoch: 19 Batch: 3350
Training Loss: 0.669967697262764
Epoch: 19 Batch: 3400
Training Loss: 0.6541706144809722
Epoch: 19 Batch: 3450
Training Loss: 0.6417676329612731
Epoch: 19 Batch: 3500
Training Loss: 0.6591472637653351
Epoch: 20 
 Validation Loss: 0.5578788287109799
---------------------------
Epoch: 20 Batch: 50
Training Loss: 0.64770811855793
Epoch: 20 Batch: 100
Training Loss: 0.6556920701265335
Epoch: 20 Batch: 150
Training Loss: 0.6206773442029953
Epoch: 20 Batch: 200
Training Loss: 0.6341481107473373
Epoch: 20 Batch: 250
Training Loss: 0.619353923201561
Epoch: 20 Batch: 300
Training Loss: 0.6827275383472443
Epoch: 20 Batch: 350
Training Loss: 0.6772695028781891
Epoch: 20 Batch: 400
Training Loss: 0.6631303954124451
Epoch: 20 Batch: 450
Training Loss: 0.6327629512548447
Epoch: 20 Batch: 500
Training Loss: 0.665494768023491
Epoch: 20 Batch: 550
Training Loss: 0.6500496739149093
Epoch: 20 Batch: 600
Training Loss: 0.6447867834568024
Epoch: 20 Batch: 650
Training Loss: 0.6866125100851059
Epoch: 20 Batch: 700
Training Loss: 0.6537819123268127
Epoch: 20 Batch: 750
Training Loss: 0.6507740360498429
Epoch: 20 Batch: 800
Training Loss: 0.6482626503705978
Epoch: 20 Batch: 850
Training Loss: 0.6660389643907547
Epoch: 20 Batch: 900
Training Loss: 0.625476536154747
Epoch: 20 Batch: 950
Training Loss: 0.6486815959215164
Epoch: 20 Batch: 1000
Training Loss: 0.6517873328924179
Epoch: 20 Batch: 1050
Training Loss: 0.6568180495500564
Epoch: 20 Batch: 1100
Training Loss: 0.6756481164693833
Epoch: 20 Batch: 1150
Training Loss: 0.6627938324213027
Epoch: 20 Batch: 1200
Training Loss: 0.6415258061885833
Epoch: 20 Batch: 1250
Training Loss: 0.6284892272949218
Epoch: 20 Batch: 1300
Training Loss: 0.6617739826440812
Epoch: 20 Batch: 1350
Training Loss: 0.6450153440237045
Epoch: 20 Batch: 1400
Training Loss: 0.6371048712730407
Epoch: 20 Batch: 1450
Training Loss: 0.6384622156620026
Epoch: 20 Batch: 1500
Training Loss: 0.6632710808515548
Epoch: 20 Batch: 1550
Training Loss: 0.6275132542848587
Epoch: 20 Batch: 1600
Training Loss: 0.6291772592067718
Epoch: 20 Batch: 1650
Training Loss: 0.6476254606246948
Epoch: 20 Batch: 1700
Training Loss: 0.640020244717598
Epoch: 20 Batch: 1750
Training Loss: 0.6586563712358475
Epoch: 20 Batch: 1800
Training Loss: 0.6463575845956803
Epoch: 20 Batch: 1850
Training Loss: 0.6337341231107712
Epoch: 20 Batch: 1900
Training Loss: 0.6455578821897506
Epoch: 20 Batch: 1950
Training Loss: 0.635701819062233
Epoch: 20 Batch: 2000
Training Loss: 0.6573414635658265
Epoch: 20 Batch: 2050
Training Loss: 0.6426591491699218
Epoch: 20 Batch: 2100
Training Loss: 0.6347048896551132
Epoch: 20 Batch: 2150
Training Loss: 0.6428191989660264
Epoch: 20 Batch: 2200
Training Loss: 0.6509584075212479
Epoch: 20 Batch: 2250
Training Loss: 0.6175852185487747
Epoch: 20 Batch: 2300
Training Loss: 0.6516262114048004
Epoch: 20 Batch: 2350
Training Loss: 0.638472483754158
Epoch: 20 Batch: 2400
Training Loss: 0.6689448326826095
Epoch: 20 Batch: 2450
Training Loss: 0.6835382431745529
Epoch: 20 Batch: 2500
Training Loss: 0.6893630963563919
Epoch: 20 Batch: 2550
Training Loss: 0.6369470244646073
Epoch: 20 Batch: 2600
Training Loss: 0.6475126719474793
Epoch: 20 Batch: 2650
Training Loss: 0.6865394014120102
Epoch: 20 Batch: 2700
Training Loss: 0.6288907867670059
Epoch: 20 Batch: 2750
Training Loss: 0.6713700151443481
Epoch: 20 Batch: 2800
Training Loss: 0.6367074108123779
Epoch: 20 Batch: 2850
Training Loss: 0.6639393508434296
Epoch: 20 Batch: 2900
Training Loss: 0.6193798011541367
Epoch: 20 Batch: 2950
Training Loss: 0.6466754180192947
Epoch: 20 Batch: 3000
Training Loss: 0.6256964635848999
Epoch: 20 Batch: 3050
Training Loss: 0.5989565825462342
Epoch: 20 Batch: 3100
Training Loss: 0.6387243098020554
Epoch: 20 Batch: 3150
Training Loss: 0.6429152971506119
Epoch: 20 Batch: 3200
Training Loss: 0.6290100812911987
Epoch: 20 Batch: 3250
Training Loss: 0.6372074466943741
Epoch: 20 Batch: 3300
Training Loss: 0.6291508692502975
Epoch: 20 Batch: 3350
Training Loss: 0.6500595325231552
Epoch: 20 Batch: 3400
Training Loss: 0.6238844108581543
Epoch: 20 Batch: 3450
Training Loss: 0.6445203137397766
Epoch: 20 Batch: 3500
Training Loss: 0.6412445437908173
Epoch: 21 
 Validation Loss: 0.5570903834369447
---------------------------
Epoch: 21 Batch: 50
Training Loss: 0.6385734564065934
Epoch: 21 Batch: 100
Training Loss: 0.6438836330175399
Epoch: 21 Batch: 150
Training Loss: 0.6453320044279098
Epoch: 21 Batch: 200
Training Loss: 0.6488234317302704
Epoch: 21 Batch: 250
Training Loss: 0.6522902762889862
Epoch: 21 Batch: 300
Training Loss: 0.6298965698480606
Epoch: 21 Batch: 350
Training Loss: 0.6336950337886811
Epoch: 21 Batch: 400
Training Loss: 0.6593081492185593
Epoch: 21 Batch: 450
Training Loss: 0.6420187532901764
Epoch: 21 Batch: 500
Training Loss: 0.6298748296499252
Epoch: 21 Batch: 550
Training Loss: 0.6713703632354736
Epoch: 21 Batch: 600
Training Loss: 0.6659876084327698
Epoch: 21 Batch: 650
Training Loss: 0.6880577033758164
Epoch: 21 Batch: 700
Training Loss: 0.6612975752353668
Epoch: 21 Batch: 750
Training Loss: 0.6642326837778092
Epoch: 21 Batch: 800
Training Loss: 0.6442270624637604
Epoch: 21 Batch: 850
Training Loss: 0.6558320504426957
Epoch: 21 Batch: 900
Training Loss: 0.6749998545646667
Epoch: 21 Batch: 950
Training Loss: 0.6296098107099533
Epoch: 21 Batch: 1000
Training Loss: 0.6664483094215393
Epoch: 21 Batch: 1050
Training Loss: 0.6450498479604722
Epoch: 21 Batch: 1100
Training Loss: 0.6675152784585953
Epoch: 21 Batch: 1150
Training Loss: 0.6497336006164551
Epoch: 21 Batch: 1200
Training Loss: 0.668452554345131
Epoch: 21 Batch: 1250
Training Loss: 0.644797215461731
Epoch: 21 Batch: 1300
Training Loss: 0.6328532660007476
Epoch: 21 Batch: 1350
Training Loss: 0.6567953377962112
Epoch: 21 Batch: 1400
Training Loss: 0.6676849621534348
Epoch: 21 Batch: 1450
Training Loss: 0.6481684148311615
Epoch: 21 Batch: 1500
Training Loss: 0.6817414134740829
Epoch: 21 Batch: 1550
Training Loss: 0.6710096722841263
Epoch: 21 Batch: 1600
Training Loss: 0.6619922626018524
Epoch: 21 Batch: 1650
Training Loss: 0.6660924661159515
Epoch: 21 Batch: 1700
Training Loss: 0.6643852323293686
Epoch: 21 Batch: 1750
Training Loss: 0.6465174698829651
Epoch: 21 Batch: 1800
Training Loss: 0.6624072140455246
Epoch: 21 Batch: 1850
Training Loss: 0.6383335119485856
Epoch: 21 Batch: 1900
Training Loss: 0.629940680861473
Epoch: 21 Batch: 1950
Training Loss: 0.6586337929964066
Epoch: 21 Batch: 2000
Training Loss: 0.6347213137149811
Epoch: 21 Batch: 2050
Training Loss: 0.6384346503019332
Epoch: 21 Batch: 2100
Training Loss: 0.6397555160522461
Epoch: 21 Batch: 2150
Training Loss: 0.6588928955793381
Epoch: 21 Batch: 2200
Training Loss: 0.6569748574495315
Epoch: 21 Batch: 2250
Training Loss: 0.6614919519424438
Epoch: 21 Batch: 2300
Training Loss: 0.6549581354856491
Epoch: 21 Batch: 2350
Training Loss: 0.6645366632938385
Epoch: 21 Batch: 2400
Training Loss: 0.6245759212970734
Epoch: 21 Batch: 2450
Training Loss: 0.5920516508817673
Epoch: 21 Batch: 2500
Training Loss: 0.6079780566692352
Epoch: 21 Batch: 2550
Training Loss: 0.6411625450849533
Epoch: 21 Batch: 2600
Training Loss: 0.6418802213668823
Epoch: 21 Batch: 2650
Training Loss: 0.6355294758081436
Epoch: 21 Batch: 2700
Training Loss: 0.6367546248435975
Epoch: 21 Batch: 2750
Training Loss: 0.6148946434259415
Epoch: 21 Batch: 2800
Training Loss: 0.6421982336044312
Epoch: 21 Batch: 2850
Training Loss: 0.6405041497945786
Epoch: 21 Batch: 2900
Training Loss: 0.6289586859941483
Epoch: 21 Batch: 2950
Training Loss: 0.6545157712697983
Epoch: 21 Batch: 3000
Training Loss: 0.690009880065918
Epoch: 21 Batch: 3050
Training Loss: 0.6057095378637314
Epoch: 21 Batch: 3100
Training Loss: 0.6190568119287491
Epoch: 21 Batch: 3150
Training Loss: 0.6240191292762757
Epoch: 21 Batch: 3200
Training Loss: 0.6468939888477325
Epoch: 21 Batch: 3250
Training Loss: 0.6327274197340012
Epoch: 21 Batch: 3300
Training Loss: 0.6543221569061279
Epoch: 21 Batch: 3350
Training Loss: 0.6705409872531891
Epoch: 21 Batch: 3400
Training Loss: 0.6561637085676193
Epoch: 21 Batch: 3450
Training Loss: 0.6573544937372208
Epoch: 21 Batch: 3500
Training Loss: 0.6526772564649582
Epoch: 22 
 Validation Loss: 0.5556240744060941
---------------------------
Epoch: 22 Batch: 50
Training Loss: 0.6555351036787033
Epoch: 22 Batch: 100
Training Loss: 0.6526265007257461
Epoch: 22 Batch: 150
Training Loss: 0.6477279502153397
Epoch: 22 Batch: 200
Training Loss: 0.6401872742176056
Epoch: 22 Batch: 250
Training Loss: 0.6766963839530945
Epoch: 22 Batch: 300
Training Loss: 0.6133388447761535
Epoch: 22 Batch: 350
Training Loss: 0.6357176613807678
Epoch: 22 Batch: 400
Training Loss: 0.658766292333603
Epoch: 22 Batch: 450
Training Loss: 0.6673203736543656
Epoch: 22 Batch: 500
Training Loss: 0.6176752412319183
Epoch: 22 Batch: 550
Training Loss: 0.6408047842979431
Epoch: 22 Batch: 600
Training Loss: 0.6415723615884781
Epoch: 22 Batch: 650
Training Loss: 0.653990638256073
Epoch: 22 Batch: 700
Training Loss: 0.6218323647975922
Epoch: 22 Batch: 750
Training Loss: 0.6551055854558945
Epoch: 22 Batch: 800
Training Loss: 0.6486573654413224
Epoch: 22 Batch: 850
Training Loss: 0.643574555516243
Epoch: 22 Batch: 900
Training Loss: 0.6481029945611954
Epoch: 22 Batch: 950
Training Loss: 0.6329618930816651
Epoch: 22 Batch: 1000
Training Loss: 0.6634997665882111
Epoch: 22 Batch: 1050
Training Loss: 0.6205686223506928
Epoch: 22 Batch: 1100
Training Loss: 0.6589338064193726
Epoch: 22 Batch: 1150
Training Loss: 0.638050594329834
Epoch: 22 Batch: 1200
Training Loss: 0.6684664106369018
Epoch: 22 Batch: 1250
Training Loss: 0.637138991355896
Epoch: 22 Batch: 1300
Training Loss: 0.6051213282346726
Epoch: 22 Batch: 1350
Training Loss: 0.6235414814949035
Epoch: 22 Batch: 1400
Training Loss: 0.5982040482759475
Epoch: 22 Batch: 1450
Training Loss: 0.6264164835214615
Epoch: 22 Batch: 1500
Training Loss: 0.6330002629756928
Epoch: 22 Batch: 1550
Training Loss: 0.6611846256256103
Epoch: 22 Batch: 1600
Training Loss: 0.6365866196155548
Epoch: 22 Batch: 1650
Training Loss: 0.6687525588274003
Epoch: 22 Batch: 1700
Training Loss: 0.6317320609092713
Epoch: 22 Batch: 1750
Training Loss: 0.6764882236719132
Epoch: 22 Batch: 1800
Training Loss: 0.6361941158771515
Epoch: 22 Batch: 1850
Training Loss: 0.6583087128400803
Epoch: 22 Batch: 1900
Training Loss: 0.6392541164159775
Epoch: 22 Batch: 1950
Training Loss: 0.6488435590267181
Epoch: 22 Batch: 2000
Training Loss: 0.6632113093137741
Epoch: 22 Batch: 2050
Training Loss: 0.6318590897321701
Epoch: 22 Batch: 2100
Training Loss: 0.6112825232744217
Epoch: 22 Batch: 2150
Training Loss: 0.6448409277200698
Epoch: 22 Batch: 2200
Training Loss: 0.6134756988286972
Epoch: 22 Batch: 2250
Training Loss: 0.6194874972105027
Epoch: 22 Batch: 2300
Training Loss: 0.6407415729761123
Epoch: 22 Batch: 2350
Training Loss: 0.6171637964248657
Epoch: 22 Batch: 2400
Training Loss: 0.6594316333532333
Epoch: 22 Batch: 2450
Training Loss: 0.6148882955312729
Epoch: 22 Batch: 2500
Training Loss: 0.6259611380100251
Epoch: 22 Batch: 2550
Training Loss: 0.6803559136390686
Epoch: 22 Batch: 2600
Training Loss: 0.6480403554439544
Epoch: 22 Batch: 2650
Training Loss: 0.6496834892034531
Epoch: 22 Batch: 2700
Training Loss: 0.6466912609338761
Epoch: 22 Batch: 2750
Training Loss: 0.645046626329422
Epoch: 22 Batch: 2800
Training Loss: 0.6557822179794311
Epoch: 22 Batch: 2850
Training Loss: 0.6482590121030808
Epoch: 22 Batch: 2900
Training Loss: 0.6319627332687378
Epoch: 22 Batch: 2950
Training Loss: 0.6259252959489823
Epoch: 22 Batch: 3000
Training Loss: 0.6265022897720337
Epoch: 22 Batch: 3050
Training Loss: 0.6492698794603348
Epoch: 22 Batch: 3100
Training Loss: 0.667415941953659
Epoch: 22 Batch: 3150
Training Loss: 0.673988184928894
Epoch: 22 Batch: 3200
Training Loss: 0.6605962306261063
Epoch: 22 Batch: 3250
Training Loss: 0.6282757008075714
Epoch: 22 Batch: 3300
Training Loss: 0.6562366133928299
Epoch: 22 Batch: 3350
Training Loss: 0.6372869092226029
Epoch: 22 Batch: 3400
Training Loss: 0.6495114940404892
Epoch: 22 Batch: 3450
Training Loss: 0.6347790414094925
Epoch: 22 Batch: 3500
Training Loss: 0.6441994386911393
Epoch: 23 
 Validation Loss: 0.5541145652532578
---------------------------
Epoch: 23 Batch: 50
Training Loss: 0.7026662176847458
Epoch: 23 Batch: 100
Training Loss: 0.6573685741424561
Epoch: 23 Batch: 150
Training Loss: 0.6296195673942566
Epoch: 23 Batch: 200
Training Loss: 0.6807763487100601
Epoch: 23 Batch: 250
Training Loss: 0.6295947170257569
Epoch: 23 Batch: 300
Training Loss: 0.6444376981258393
Epoch: 23 Batch: 350
Training Loss: 0.6404555654525756
Epoch: 23 Batch: 400
Training Loss: 0.5990263217687607
Epoch: 23 Batch: 450
Training Loss: 0.6436128824949264
Epoch: 23 Batch: 500
Training Loss: 0.6663708448410034
Epoch: 23 Batch: 550
Training Loss: 0.6634503602981567
Epoch: 23 Batch: 600
Training Loss: 0.666505395770073
Epoch: 23 Batch: 650
Training Loss: 0.6251286125183105
Epoch: 23 Batch: 700
Training Loss: 0.6519172191619873
Epoch: 23 Batch: 750
Training Loss: 0.6748971450328827
Epoch: 23 Batch: 800
Training Loss: 0.6189268177747727
Epoch: 23 Batch: 850
Training Loss: 0.6479336911439896
Epoch: 23 Batch: 900
Training Loss: 0.6317620170116425
Epoch: 23 Batch: 950
Training Loss: 0.668075698018074
Epoch: 23 Batch: 1000
Training Loss: 0.6635608053207398
Epoch: 23 Batch: 1050
Training Loss: 0.6428752893209457
Epoch: 23 Batch: 1100
Training Loss: 0.648729989528656
Epoch: 23 Batch: 1150
Training Loss: 0.6659012430906296
Epoch: 23 Batch: 1200
Training Loss: 0.6123461323976517
Epoch: 23 Batch: 1250
Training Loss: 0.6608784800767898
Epoch: 23 Batch: 1300
Training Loss: 0.6499037384986878
Epoch: 23 Batch: 1350
Training Loss: 0.609301740527153
Epoch: 23 Batch: 1400
Training Loss: 0.6788677632808685
Epoch: 23 Batch: 1450
Training Loss: 0.6742072665691375
Epoch: 23 Batch: 1500
Training Loss: 0.6082508927583694
Epoch: 23 Batch: 1550
Training Loss: 0.6531541925668717
Epoch: 23 Batch: 1600
Training Loss: 0.618556809425354
Epoch: 23 Batch: 1650
Training Loss: 0.6326319086551666
Epoch: 23 Batch: 1700
Training Loss: 0.6330172860622406
Epoch: 23 Batch: 1750
Training Loss: 0.6620124697685241
Epoch: 23 Batch: 1800
Training Loss: 0.6039914500713348
Epoch: 23 Batch: 1850
Training Loss: 0.6485179477930069
Epoch: 23 Batch: 1900
Training Loss: 0.6395505934953689
Epoch: 23 Batch: 1950
Training Loss: 0.6272033137083054
Epoch: 23 Batch: 2000
Training Loss: 0.662172241806984
Epoch: 23 Batch: 2050
Training Loss: 0.6804244947433472
Epoch: 23 Batch: 2100
Training Loss: 0.6688786149024963
Epoch: 23 Batch: 2150
Training Loss: 0.6235750657320023
Epoch: 23 Batch: 2200
Training Loss: 0.6496275132894516
Epoch: 23 Batch: 2250
Training Loss: 0.6285134649276733
Epoch: 23 Batch: 2300
Training Loss: 0.6425517320632934
Epoch: 23 Batch: 2350
Training Loss: 0.6339202052354813
Epoch: 23 Batch: 2400
Training Loss: 0.6327177250385284
Epoch: 23 Batch: 2450
Training Loss: 0.6250387954711915
Epoch: 23 Batch: 2500
Training Loss: 0.6350267904996872
Epoch: 23 Batch: 2550
Training Loss: 0.6721394699811936
Epoch: 23 Batch: 2600
Training Loss: 0.6436836850643158
Epoch: 23 Batch: 2650
Training Loss: 0.6570230752229691
Epoch: 23 Batch: 2700
Training Loss: 0.6325486010313034
Epoch: 23 Batch: 2750
Training Loss: 0.6466701650619506
Epoch: 23 Batch: 2800
Training Loss: 0.6459272778034211
Epoch: 23 Batch: 2850
Training Loss: 0.6521626728773117
Epoch: 23 Batch: 2900
Training Loss: 0.6326381939649582
Epoch: 23 Batch: 2950
Training Loss: 0.642571662068367
Epoch: 23 Batch: 3000
Training Loss: 0.6734096747636795
Epoch: 23 Batch: 3050
Training Loss: 0.6364628022909165
Epoch: 23 Batch: 3100
Training Loss: 0.6474020880460739
Epoch: 23 Batch: 3150
Training Loss: 0.6236264276504516
Epoch: 23 Batch: 3200
Training Loss: 0.6594585055112838
Epoch: 23 Batch: 3250
Training Loss: 0.6322040987014771
Epoch: 23 Batch: 3300
Training Loss: 0.6383952283859253
Epoch: 23 Batch: 3350
Training Loss: 0.6102028918266297
Epoch: 23 Batch: 3400
Training Loss: 0.6762000876665115
Epoch: 23 Batch: 3450
Training Loss: 0.647962772846222
Epoch: 23 Batch: 3500
Training Loss: 0.6562517040967941
Epoch: 24 
 Validation Loss: 0.5519478983349271
---------------------------
Epoch: 24 Batch: 50
Training Loss: 0.6851787739992141
Epoch: 24 Batch: 100
Training Loss: 0.6411885195970535
Epoch: 24 Batch: 150
Training Loss: 0.6377245247364044
Epoch: 24 Batch: 200
Training Loss: 0.6514295816421509
Epoch: 24 Batch: 250
Training Loss: 0.6420754098892212
Epoch: 24 Batch: 300
Training Loss: 0.6129925948381424
Epoch: 24 Batch: 350
Training Loss: 0.6502132195234299
Epoch: 24 Batch: 400
Training Loss: 0.6213810968399048
Epoch: 24 Batch: 450
Training Loss: 0.6275558763742447
Epoch: 24 Batch: 500
Training Loss: 0.6600616389513015
Epoch: 24 Batch: 550
Training Loss: 0.6536288797855377
Epoch: 24 Batch: 600
Training Loss: 0.6296120148897171
Epoch: 24 Batch: 650
Training Loss: 0.6679156082868576
Epoch: 24 Batch: 700
Training Loss: 0.653079047203064
Epoch: 24 Batch: 750
Training Loss: 0.6419597005844117
Epoch: 24 Batch: 800
Training Loss: 0.6470919358730316
Epoch: 24 Batch: 850
Training Loss: 0.6497256737947464
Epoch: 24 Batch: 900
Training Loss: 0.6101924294233322
Epoch: 24 Batch: 950
Training Loss: 0.6343550074100495
Epoch: 24 Batch: 1000
Training Loss: 0.622278304696083
Epoch: 24 Batch: 1050
Training Loss: 0.640171080827713
Epoch: 24 Batch: 1100
Training Loss: 0.5988268184661866
Epoch: 24 Batch: 1150
Training Loss: 0.6334701204299926
Epoch: 24 Batch: 1200
Training Loss: 0.6553131741285324
Epoch: 24 Batch: 1250
Training Loss: 0.6472586476802826
Epoch: 24 Batch: 1300
Training Loss: 0.6244476759433746
Epoch: 24 Batch: 1350
Training Loss: 0.6307835471630097
Epoch: 24 Batch: 1400
Training Loss: 0.6308148366212845
Epoch: 24 Batch: 1450
Training Loss: 0.6167869299650193
Epoch: 24 Batch: 1500
Training Loss: 0.6446787619590759
Epoch: 24 Batch: 1550
Training Loss: 0.6383322769403458
Epoch: 24 Batch: 1600
Training Loss: 0.6239339572191238
Epoch: 24 Batch: 1650
Training Loss: 0.6296074467897416
Epoch: 24 Batch: 1700
Training Loss: 0.6589707386493683
Epoch: 24 Batch: 1750
Training Loss: 0.6173205208778382
Epoch: 24 Batch: 1800
Training Loss: 0.6482535582780838
Epoch: 24 Batch: 1850
Training Loss: 0.691810651421547
Epoch: 24 Batch: 1900
Training Loss: 0.6350192081928253
Epoch: 24 Batch: 1950
Training Loss: 0.660545551776886
Epoch: 24 Batch: 2000
Training Loss: 0.6427105474472046
Epoch: 24 Batch: 2050
Training Loss: 0.6593144851922988
Epoch: 24 Batch: 2100
Training Loss: 0.6503458052873612
Epoch: 24 Batch: 2150
Training Loss: 0.6542091840505599
Epoch: 24 Batch: 2200
Training Loss: 0.6686335808038711
Epoch: 24 Batch: 2250
Training Loss: 0.6562982642650604
Epoch: 24 Batch: 2300
Training Loss: 0.638648905158043
Epoch: 24 Batch: 2350
Training Loss: 0.6780844181776047
Epoch: 24 Batch: 2400
Training Loss: 0.6524856120347977
Epoch: 24 Batch: 2450
Training Loss: 0.6347590434551239
Epoch: 24 Batch: 2500
Training Loss: 0.6364748722314835
Epoch: 24 Batch: 2550
Training Loss: 0.629267833828926
Epoch: 24 Batch: 2600
Training Loss: 0.6268702054023743
Epoch: 24 Batch: 2650
Training Loss: 0.6193421226739884
Epoch: 24 Batch: 2700
Training Loss: 0.6205478847026825
Epoch: 24 Batch: 2750
Training Loss: 0.6120760560035705
Epoch: 24 Batch: 2800
Training Loss: 0.6267100882530212
Epoch: 24 Batch: 2850
Training Loss: 0.6482940828800201
Epoch: 24 Batch: 2900
Training Loss: 0.6199810671806335
Epoch: 24 Batch: 2950
Training Loss: 0.6470962446928025
Epoch: 24 Batch: 3000
Training Loss: 0.6427395337820053
Epoch: 24 Batch: 3050
Training Loss: 0.6648283767700195
Epoch: 24 Batch: 3100
Training Loss: 0.677051711678505
Epoch: 24 Batch: 3150
Training Loss: 0.6271390837430953
Epoch: 24 Batch: 3200
Training Loss: 0.6313655960559845
Epoch: 24 Batch: 3250
Training Loss: 0.6554732424020767
Epoch: 24 Batch: 3300
Training Loss: 0.6222229731082917
Epoch: 24 Batch: 3350
Training Loss: 0.6398550230264664
Epoch: 24 Batch: 3400
Training Loss: 0.647103443145752
Epoch: 24 Batch: 3450
Training Loss: 0.6453891342878342
Epoch: 24 Batch: 3500
Training Loss: 0.6299667698144913
Epoch: 25 
 Validation Loss: 0.5514658457703061
---------------------------
Epoch: 25 Batch: 50
Training Loss: 0.6547841888666153
Epoch: 25 Batch: 100
Training Loss: 0.6399251699447632
Epoch: 25 Batch: 150
Training Loss: 0.6344732838869095
Epoch: 25 Batch: 200
Training Loss: 0.6308436143398285
Epoch: 25 Batch: 250
Training Loss: 0.6490106970071793
Epoch: 25 Batch: 300
Training Loss: 0.6459280908107757
Epoch: 25 Batch: 350
Training Loss: 0.6140158879756927
Epoch: 25 Batch: 400
Training Loss: 0.6476435762643814
Epoch: 25 Batch: 450
Training Loss: 0.6542790895700454
Epoch: 25 Batch: 500
Training Loss: 0.650616614818573
Epoch: 25 Batch: 550
Training Loss: 0.6228372412919998
Epoch: 25 Batch: 600
Training Loss: 0.6402961355447769
Epoch: 25 Batch: 650
Training Loss: 0.6084851992130279
Epoch: 25 Batch: 700
Training Loss: 0.639680209159851
Epoch: 25 Batch: 750
Training Loss: 0.6398747110366821
Epoch: 25 Batch: 800
Training Loss: 0.6307670950889588
Epoch: 25 Batch: 850
Training Loss: 0.6403119277954101
Epoch: 25 Batch: 900
Training Loss: 0.6514188647270203
Epoch: 25 Batch: 950
Training Loss: 0.6436402702331543
Epoch: 25 Batch: 1000
Training Loss: 0.6341208374500275
Epoch: 25 Batch: 1050
Training Loss: 0.6160631364583969
Epoch: 25 Batch: 1100
Training Loss: 0.6462200820446015
Epoch: 25 Batch: 1150
Training Loss: 0.6406971335411071
Epoch: 25 Batch: 1200
Training Loss: 0.6677413249015808
Epoch: 25 Batch: 1250
Training Loss: 0.6357037252187729
Epoch: 25 Batch: 1300
Training Loss: 0.6352119135856629
Epoch: 25 Batch: 1350
Training Loss: 0.6598003995418549
Epoch: 25 Batch: 1400
Training Loss: 0.655086373090744
Epoch: 25 Batch: 1450
Training Loss: 0.6203849524259567
Epoch: 25 Batch: 1500
Training Loss: 0.6497898137569428
Epoch: 25 Batch: 1550
Training Loss: 0.6422331911325455
Epoch: 25 Batch: 1600
Training Loss: 0.6388927894830704
Epoch: 25 Batch: 1650
Training Loss: 0.6623704367876053
Epoch: 25 Batch: 1700
Training Loss: 0.6652132982015609
Epoch: 25 Batch: 1750
Training Loss: 0.6528349775075912
Epoch: 25 Batch: 1800
Training Loss: 0.666975114941597
Epoch: 25 Batch: 1850
Training Loss: 0.6187410318851471
Epoch: 25 Batch: 1900
Training Loss: 0.6412131309509277
Epoch: 25 Batch: 1950
Training Loss: 0.5969269067049027
Epoch: 25 Batch: 2000
Training Loss: 0.662087494134903
Epoch: 25 Batch: 2050
Training Loss: 0.5977286690473557
Epoch: 25 Batch: 2100
Training Loss: 0.6217980742454529
Epoch: 25 Batch: 2150
Training Loss: 0.6357601976394653
Epoch: 25 Batch: 2200
Training Loss: 0.6525529080629349
Epoch: 25 Batch: 2250
Training Loss: 0.6632157158851624
Epoch: 25 Batch: 2300
Training Loss: 0.6534454756975174
Epoch: 25 Batch: 2350
Training Loss: 0.6388322031497955
Epoch: 25 Batch: 2400
Training Loss: 0.6615055131912232
Epoch: 25 Batch: 2450
Training Loss: 0.6360550594329833
Epoch: 25 Batch: 2500
Training Loss: 0.6337174499034881
Epoch: 25 Batch: 2550
Training Loss: 0.6535590833425522
Epoch: 25 Batch: 2600
Training Loss: 0.6300551360845565
Epoch: 25 Batch: 2650
Training Loss: 0.6302991139888764
Epoch: 25 Batch: 2700
Training Loss: 0.6544152116775512
Epoch: 25 Batch: 2750
Training Loss: 0.6484912472963333
Epoch: 25 Batch: 2800
Training Loss: 0.6334441012144089
Epoch: 25 Batch: 2850
Training Loss: 0.6152655595541
Epoch: 25 Batch: 2900
Training Loss: 0.618595399260521
Epoch: 25 Batch: 2950
Training Loss: 0.6419135427474976
Epoch: 25 Batch: 3000
Training Loss: 0.6631586480140687
Epoch: 25 Batch: 3050
Training Loss: 0.6426653903722763
Epoch: 25 Batch: 3100
Training Loss: 0.6286163985729217
Epoch: 25 Batch: 3150
Training Loss: 0.6398849385976791
Epoch: 25 Batch: 3200
Training Loss: 0.6409895944595337
Epoch: 25 Batch: 3250
Training Loss: 0.6308520007133483
Epoch: 25 Batch: 3300
Training Loss: 0.6589015227556229
Epoch: 25 Batch: 3350
Training Loss: 0.622654583454132
Epoch: 25 Batch: 3400
Training Loss: 0.630316852927208
Epoch: 25 Batch: 3450
Training Loss: 0.6263840842247009
Epoch: 25 Batch: 3500
Training Loss: 0.6063642764091491
Epoch: 26 
 Validation Loss: 0.5511894242631065
---------------------------
Epoch: 26 Batch: 50
Training Loss: 0.643034371137619
Epoch: 26 Batch: 100
Training Loss: 0.6268493515253067
Epoch: 26 Batch: 150
Training Loss: 0.664358274936676
Epoch: 26 Batch: 200
Training Loss: 0.6273373985290527
Epoch: 26 Batch: 250
Training Loss: 0.6412066054344178
Epoch: 26 Batch: 300
Training Loss: 0.6391539305448533
Epoch: 26 Batch: 350
Training Loss: 0.669268564581871
Epoch: 26 Batch: 400
Training Loss: 0.625414172410965
Epoch: 26 Batch: 450
Training Loss: 0.6326317429542542
Epoch: 26 Batch: 500
Training Loss: 0.6410755485296249
Epoch: 26 Batch: 550
Training Loss: 0.6284447211027145
Epoch: 26 Batch: 600
Training Loss: 0.6341822499036789
Epoch: 26 Batch: 650
Training Loss: 0.5952158635854721
Epoch: 26 Batch: 700
Training Loss: 0.6171830242872238
Epoch: 26 Batch: 750
Training Loss: 0.6249826353788376
Epoch: 26 Batch: 800
Training Loss: 0.6379739016294479
Epoch: 26 Batch: 850
Training Loss: 0.6493421339988709
Epoch: 26 Batch: 900
Training Loss: 0.6545147752761841
Epoch: 26 Batch: 950
Training Loss: 0.6406195086240768
Epoch: 26 Batch: 1000
Training Loss: 0.6257248955965042
Epoch: 26 Batch: 1050
Training Loss: 0.6555528354644775
Epoch: 26 Batch: 1100
Training Loss: 0.6452386963367462
Epoch: 26 Batch: 1150
Training Loss: 0.6317788118124008
Epoch: 26 Batch: 1200
Training Loss: 0.6624972742795944
Epoch: 26 Batch: 1250
Training Loss: 0.6412261039018631
Epoch: 26 Batch: 1300
Training Loss: 0.6209846383333206
Epoch: 26 Batch: 1350
Training Loss: 0.6317230427265167
Epoch: 26 Batch: 1400
Training Loss: 0.649298101067543
Epoch: 26 Batch: 1450
Training Loss: 0.6467534917593002
Epoch: 26 Batch: 1500
Training Loss: 0.6230650866031646
Epoch: 26 Batch: 1550
Training Loss: 0.6288267415761948
Epoch: 26 Batch: 1600
Training Loss: 0.6482381117343903
Epoch: 26 Batch: 1650
Training Loss: 0.61860396027565
Epoch: 26 Batch: 1700
Training Loss: 0.6456730103492737
Epoch: 26 Batch: 1750
Training Loss: 0.6248760080337524
Epoch: 26 Batch: 1800
Training Loss: 0.6115233778953553
Epoch: 26 Batch: 1850
Training Loss: 0.6470868337154388
Epoch: 26 Batch: 1900
Training Loss: 0.6247738325595855
Epoch: 26 Batch: 1950
Training Loss: 0.6118837946653366
Epoch: 26 Batch: 2000
Training Loss: 0.6383622562885285
Epoch: 26 Batch: 2050
Training Loss: 0.6105058693885803
Epoch: 26 Batch: 2100
Training Loss: 0.6500043958425522
Epoch: 26 Batch: 2150
Training Loss: 0.6257674211263656
Epoch: 26 Batch: 2200
Training Loss: 0.6397917932271957
Epoch: 26 Batch: 2250
Training Loss: 0.6471010732650757
Epoch: 26 Batch: 2300
Training Loss: 0.6368712490797043
Epoch: 26 Batch: 2350
Training Loss: 0.6251611518859863
Epoch: 26 Batch: 2400
Training Loss: 0.6436661696434021
Epoch: 26 Batch: 2450
Training Loss: 0.6101169615983963
Epoch: 26 Batch: 2500
Training Loss: 0.6604041367769241
Epoch: 26 Batch: 2550
Training Loss: 0.6306608760356903
Epoch: 26 Batch: 2600
Training Loss: 0.6367879962921142
Epoch: 26 Batch: 2650
Training Loss: 0.6215098488330841
Epoch: 26 Batch: 2700
Training Loss: 0.6680685079097748
Epoch: 26 Batch: 2750
Training Loss: 0.6535238802433014
Epoch: 26 Batch: 2800
Training Loss: 0.6520014625787734
Epoch: 26 Batch: 2850
Training Loss: 0.675159040093422
Epoch: 26 Batch: 2900
Training Loss: 0.642388054728508
Epoch: 26 Batch: 2950
Training Loss: 0.6217181384563446
Epoch: 26 Batch: 3000
Training Loss: 0.646931494474411
Epoch: 26 Batch: 3050
Training Loss: 0.6407326030731201
Epoch: 26 Batch: 3100
Training Loss: 0.6515978991985321
Epoch: 26 Batch: 3150
Training Loss: 0.6073607355356216
Epoch: 26 Batch: 3200
Training Loss: 0.6368399548530579
Epoch: 26 Batch: 3250
Training Loss: 0.6541008591651917
Epoch: 26 Batch: 3300
Training Loss: 0.6251075512170792
Epoch: 26 Batch: 3350
Training Loss: 0.6461947906017304
Epoch: 26 Batch: 3400
Training Loss: 0.6574896740913391
Epoch: 26 Batch: 3450
Training Loss: 0.6256249141693115
Epoch: 26 Batch: 3500
Training Loss: 0.6257076013088226
Epoch: 27 
 Validation Loss: 0.5494673202435175
---------------------------
Epoch: 27 Batch: 50
Training Loss: 0.6358726036548614
Epoch: 27 Batch: 100
Training Loss: 0.6576471877098083
Epoch: 27 Batch: 150
Training Loss: 0.6499811434745788
Epoch: 27 Batch: 200
Training Loss: 0.6532703024148941
Epoch: 27 Batch: 250
Training Loss: 0.644339045882225
Epoch: 27 Batch: 300
Training Loss: 0.621477962732315
Epoch: 27 Batch: 350
Training Loss: 0.655666007399559
Epoch: 27 Batch: 400
Training Loss: 0.6390912383794785
Epoch: 27 Batch: 450
Training Loss: 0.6532442712783814
Epoch: 27 Batch: 500
Training Loss: 0.616162423491478
Epoch: 27 Batch: 550
Training Loss: 0.6800953257083893
Epoch: 27 Batch: 600
Training Loss: 0.6454275232553482
Epoch: 27 Batch: 650
Training Loss: 0.6464381694793702
Epoch: 27 Batch: 700
Training Loss: 0.6308731389045715
Epoch: 27 Batch: 750
Training Loss: 0.6457573235034942
Epoch: 27 Batch: 800
Training Loss: 0.644557888507843
Epoch: 27 Batch: 850
Training Loss: 0.653650421500206
Epoch: 27 Batch: 900
Training Loss: 0.6448941892385482
Epoch: 27 Batch: 950
Training Loss: 0.6700175833702088
Epoch: 27 Batch: 1000
Training Loss: 0.679595775604248
Epoch: 27 Batch: 1050
Training Loss: 0.6459131646156311
Epoch: 27 Batch: 1100
Training Loss: 0.6257786875963212
Epoch: 27 Batch: 1150
Training Loss: 0.6278971803188323
Epoch: 27 Batch: 1200
Training Loss: 0.6190937948226929
Epoch: 27 Batch: 1250
Training Loss: 0.6172920286655426
Epoch: 27 Batch: 1300
Training Loss: 0.6386661100387573
Epoch: 27 Batch: 1350
Training Loss: 0.6230322545766831
Epoch: 27 Batch: 1400
Training Loss: 0.6376444715261459
Epoch: 27 Batch: 1450
Training Loss: 0.6459136807918548
Epoch: 27 Batch: 1500
Training Loss: 0.6409871298074722
Epoch: 27 Batch: 1550
Training Loss: 0.6538294649124146
Epoch: 27 Batch: 1600
Training Loss: 0.6664238858222962
Epoch: 27 Batch: 1650
Training Loss: 0.6427845072746277
Epoch: 27 Batch: 1700
Training Loss: 0.6355823427438736
Epoch: 27 Batch: 1750
Training Loss: 0.6240750867128372
Epoch: 27 Batch: 1800
Training Loss: 0.6220360141992569
Epoch: 27 Batch: 1850
Training Loss: 0.6290005451440811
Epoch: 27 Batch: 1900
Training Loss: 0.6351594394445419
Epoch: 27 Batch: 1950
Training Loss: 0.6566229897737503
Epoch: 27 Batch: 2000
Training Loss: 0.6064908468723297
Epoch: 27 Batch: 2050
Training Loss: 0.6228031647205353
Epoch: 27 Batch: 2100
Training Loss: 0.6213247686624527
Epoch: 27 Batch: 2150
Training Loss: 0.5982530033588409
Epoch: 27 Batch: 2200
Training Loss: 0.64282406270504
Epoch: 27 Batch: 2250
Training Loss: 0.642179474234581
Epoch: 27 Batch: 2300
Training Loss: 0.6772954183816909
Epoch: 27 Batch: 2350
Training Loss: 0.6145584636926651
Epoch: 27 Batch: 2400
Training Loss: 0.6066978412866593
Epoch: 27 Batch: 2450
Training Loss: 0.6085534393787384
Epoch: 27 Batch: 2500
Training Loss: 0.6764685887098313
Epoch: 27 Batch: 2550
Training Loss: 0.6227263975143432
Epoch: 27 Batch: 2600
Training Loss: 0.6441437768936157
Epoch: 27 Batch: 2650
Training Loss: 0.6042764723300934
Epoch: 27 Batch: 2700
Training Loss: 0.6424321526288986
Epoch: 27 Batch: 2750
Training Loss: 0.6432588803768158
Epoch: 27 Batch: 2800
Training Loss: 0.6301008445024491
Epoch: 27 Batch: 2850
Training Loss: 0.6272955930233002
Epoch: 27 Batch: 2900
Training Loss: 0.6198696541786194
Epoch: 27 Batch: 2950
Training Loss: 0.6412195378541946
Epoch: 27 Batch: 3000
Training Loss: 0.63392562687397
Epoch: 27 Batch: 3050
Training Loss: 0.6251444268226624
Epoch: 27 Batch: 3100
Training Loss: 0.6666746520996094
Epoch: 27 Batch: 3150
Training Loss: 0.6391784256696701
Epoch: 27 Batch: 3200
Training Loss: 0.6285626834630966
Epoch: 27 Batch: 3250
Training Loss: 0.6475205463171005
Epoch: 27 Batch: 3300
Training Loss: 0.6290229904651642
Epoch: 27 Batch: 3350
Training Loss: 0.6070596343278885
Epoch: 27 Batch: 3400
Training Loss: 0.6168788152933121
Epoch: 27 Batch: 3450
Training Loss: 0.6371676868200302
Epoch: 27 Batch: 3500
Training Loss: 0.6353800147771835
Epoch: 28 
 Validation Loss: 0.5481842408577601
---------------------------
Epoch: 28 Batch: 50
Training Loss: 0.6538076740503311
Epoch: 28 Batch: 100
Training Loss: 0.6395673644542694
Epoch: 28 Batch: 150
Training Loss: 0.6316656166315079
Epoch: 28 Batch: 200
Training Loss: 0.6506513720750808
Epoch: 28 Batch: 250
Training Loss: 0.6431221753358841
Epoch: 28 Batch: 300
Training Loss: 0.6018057453632355
Epoch: 28 Batch: 350
Training Loss: 0.6319238728284836
Epoch: 28 Batch: 400
Training Loss: 0.6626681393384933
Epoch: 28 Batch: 450
Training Loss: 0.6406770676374436
Epoch: 28 Batch: 500
Training Loss: 0.633878498673439
Epoch: 28 Batch: 550
Training Loss: 0.6327428025007248
Epoch: 28 Batch: 600
Training Loss: 0.6613968205451966
Epoch: 28 Batch: 650
Training Loss: 0.6502775555849075
Epoch: 28 Batch: 700
Training Loss: 0.6693755280971527
Epoch: 28 Batch: 750
Training Loss: 0.6322396379709244
Epoch: 28 Batch: 800
Training Loss: 0.6530314004421234
Epoch: 28 Batch: 850
Training Loss: 0.6448848778009415
Epoch: 28 Batch: 900
Training Loss: 0.6364556586742401
Epoch: 28 Batch: 950
Training Loss: 0.6770718616247177
Epoch: 28 Batch: 1000
Training Loss: 0.6274751454591752
Epoch: 28 Batch: 1050
Training Loss: 0.6464819818735122
Epoch: 28 Batch: 1100
Training Loss: 0.6596608448028565
Epoch: 28 Batch: 1150
Training Loss: 0.6546758896112442
Epoch: 28 Batch: 1200
Training Loss: 0.6184570044279099
Epoch: 28 Batch: 1250
Training Loss: 0.6394053679704667
Epoch: 28 Batch: 1300
Training Loss: 0.6280564284324646
Epoch: 28 Batch: 1350
Training Loss: 0.6337832254171372
Epoch: 28 Batch: 1400
Training Loss: 0.6040803533792496
Epoch: 28 Batch: 1450
Training Loss: 0.6169145810604095
Epoch: 28 Batch: 1500
Training Loss: 0.6326676338911057
Epoch: 28 Batch: 1550
Training Loss: 0.6455115377902985
Epoch: 28 Batch: 1600
Training Loss: 0.62539260327816
Epoch: 28 Batch: 1650
Training Loss: 0.6411121660470962
Epoch: 28 Batch: 1700
Training Loss: 0.6657538777589798
Epoch: 28 Batch: 1750
Training Loss: 0.631615184545517
Epoch: 28 Batch: 1800
Training Loss: 0.6277664691209793
Epoch: 28 Batch: 1850
Training Loss: 0.6340120500326156
Epoch: 28 Batch: 1900
Training Loss: 0.6354827708005906
Epoch: 28 Batch: 1950
Training Loss: 0.640029166340828
Epoch: 28 Batch: 2000
Training Loss: 0.6315255570411682
Epoch: 28 Batch: 2050
Training Loss: 0.6327649545669556
Epoch: 28 Batch: 2100
Training Loss: 0.6026897102594375
Epoch: 28 Batch: 2150
Training Loss: 0.6182532918453216
Epoch: 28 Batch: 2200
Training Loss: 0.6505931180715561
Epoch: 28 Batch: 2250
Training Loss: 0.646632085442543
Epoch: 28 Batch: 2300
Training Loss: 0.6402020472288131
Epoch: 28 Batch: 2350
Training Loss: 0.6469241040945053
Epoch: 28 Batch: 2400
Training Loss: 0.6225439715385437
Epoch: 28 Batch: 2450
Training Loss: 0.6342214155197143
Epoch: 28 Batch: 2500
Training Loss: 0.6332720804214478
Epoch: 28 Batch: 2550
Training Loss: 0.6186023777723313
Epoch: 28 Batch: 2600
Training Loss: 0.6384832674264908
Epoch: 28 Batch: 2650
Training Loss: 0.6358281296491622
Epoch: 28 Batch: 2700
Training Loss: 0.6615030997991562
Epoch: 28 Batch: 2750
Training Loss: 0.6096458160877227
Epoch: 28 Batch: 2800
Training Loss: 0.601689515709877
Epoch: 28 Batch: 2850
Training Loss: 0.6552902919054031
Epoch: 28 Batch: 2900
Training Loss: 0.6565728038549423
Epoch: 28 Batch: 2950
Training Loss: 0.6324496066570282
Epoch: 28 Batch: 3000
Training Loss: 0.6193616533279419
Epoch: 28 Batch: 3050
Training Loss: 0.5959162676334381
Epoch: 28 Batch: 3100
Training Loss: 0.6425198489427566
Epoch: 28 Batch: 3150
Training Loss: 0.6306640863418579
Epoch: 28 Batch: 3200
Training Loss: 0.5989915782213211
Epoch: 28 Batch: 3250
Training Loss: 0.6393439656496048
Epoch: 28 Batch: 3300
Training Loss: 0.6238122719526291
Epoch: 28 Batch: 3350
Training Loss: 0.6293092507123947
Epoch: 28 Batch: 3400
Training Loss: 0.6424897867441177
Epoch: 28 Batch: 3450
Training Loss: 0.6341175073385239
Epoch: 28 Batch: 3500
Training Loss: 0.6103044962882995
Epoch: 29 
 Validation Loss: 0.5476088027159373
---------------------------
Epoch: 29 Batch: 50
Training Loss: 0.6293094509840012
Epoch: 29 Batch: 100
Training Loss: 0.6413612079620361
Epoch: 29 Batch: 150
Training Loss: 0.6249683290719986
Epoch: 29 Batch: 200
Training Loss: 0.6608670049905777
Epoch: 29 Batch: 250
Training Loss: 0.6209539783000946
Epoch: 29 Batch: 300
Training Loss: 0.6007995337247849
Epoch: 29 Batch: 350
Training Loss: 0.6552993214130401
Epoch: 29 Batch: 400
Training Loss: 0.64978322327137
Epoch: 29 Batch: 450
Training Loss: 0.622265744805336
Epoch: 29 Batch: 500
Training Loss: 0.6366081649065017
Epoch: 29 Batch: 550
Training Loss: 0.6375169092416764
Epoch: 29 Batch: 600
Training Loss: 0.6215594857931137
Epoch: 29 Batch: 650
Training Loss: 0.6464281797409057
Epoch: 29 Batch: 700
Training Loss: 0.5973899698257447
Epoch: 29 Batch: 750
Training Loss: 0.6294965815544128
Epoch: 29 Batch: 800
Training Loss: 0.6344114965200425
Epoch: 29 Batch: 850
Training Loss: 0.6073051261901855
Epoch: 29 Batch: 900
Training Loss: 0.6144957977533341
Epoch: 29 Batch: 950
Training Loss: 0.627509949207306
Epoch: 29 Batch: 1000
Training Loss: 0.6638550454378128
Epoch: 29 Batch: 1050
Training Loss: 0.6607159924507141
Epoch: 29 Batch: 1100
Training Loss: 0.6180330246686936
Epoch: 29 Batch: 1150
Training Loss: 0.64435704767704
Epoch: 29 Batch: 1200
Training Loss: 0.6679808908700943
Epoch: 29 Batch: 1250
Training Loss: 0.6454403167963028
Epoch: 29 Batch: 1300
Training Loss: 0.6273552525043488
Epoch: 29 Batch: 1350
Training Loss: 0.6444195741415024
Epoch: 29 Batch: 1400
Training Loss: 0.646044065952301
Epoch: 29 Batch: 1450
Training Loss: 0.6355676287412644
Epoch: 29 Batch: 1500
Training Loss: 0.655291941165924
Epoch: 29 Batch: 1550
Training Loss: 0.6164797592163086
Epoch: 29 Batch: 1600
Training Loss: 0.6250472033023834
Epoch: 29 Batch: 1650
Training Loss: 0.6289358764886857
Epoch: 29 Batch: 1700
Training Loss: 0.645990082025528
Epoch: 29 Batch: 1750
Training Loss: 0.6249302768707276
Epoch: 29 Batch: 1800
Training Loss: 0.6370688217878342
Epoch: 29 Batch: 1850
Training Loss: 0.6244630646705628
Epoch: 29 Batch: 1900
Training Loss: 0.6453074896335602
Epoch: 29 Batch: 1950
Training Loss: 0.6576871514320374
Epoch: 29 Batch: 2000
Training Loss: 0.6300693190097809
Epoch: 29 Batch: 2050
Training Loss: 0.6281007742881775
Epoch: 29 Batch: 2100
Training Loss: 0.646581569314003
Epoch: 29 Batch: 2150
Training Loss: 0.6135957318544388
Epoch: 29 Batch: 2200
Training Loss: 0.6437672340869903
Epoch: 29 Batch: 2250
Training Loss: 0.6445491153001786
Epoch: 29 Batch: 2300
Training Loss: 0.6449764633178711
Epoch: 29 Batch: 2350
Training Loss: 0.6690894788503647
Epoch: 29 Batch: 2400
Training Loss: 0.6397639918327331
Epoch: 29 Batch: 2450
Training Loss: 0.6044950056076049
Epoch: 29 Batch: 2500
Training Loss: 0.6125567597150803
Epoch: 29 Batch: 2550
Training Loss: 0.6320256233215332
Epoch: 29 Batch: 2600
Training Loss: 0.6362257397174835
Epoch: 29 Batch: 2650
Training Loss: 0.6264650410413742
Epoch: 29 Batch: 2700
Training Loss: 0.6153196555376053
Epoch: 29 Batch: 2750
Training Loss: 0.6101822447776795
Epoch: 29 Batch: 2800
Training Loss: 0.6334075629711151
Epoch: 29 Batch: 2850
Training Loss: 0.6317964321374894
Epoch: 29 Batch: 2900
Training Loss: 0.6380915188789368
Epoch: 29 Batch: 2950
Training Loss: 0.6334060978889465
Epoch: 29 Batch: 3000
Training Loss: 0.5999898982048034
Epoch: 29 Batch: 3050
Training Loss: 0.6214000678062439
Epoch: 29 Batch: 3100
Training Loss: 0.6648802483081817
Epoch: 29 Batch: 3150
Training Loss: 0.6414397066831589
Epoch: 29 Batch: 3200
Training Loss: 0.6149960875511169
Epoch: 29 Batch: 3250
Training Loss: 0.5986323255300522
Epoch: 29 Batch: 3300
Training Loss: 0.6316592866182327
Epoch: 29 Batch: 3350
Training Loss: 0.6579928803443909
Epoch: 29 Batch: 3400
Training Loss: 0.6330029892921448
Epoch: 29 Batch: 3450
Training Loss: 0.626601984500885
Epoch: 29 Batch: 3500
Training Loss: 0.6317208015918732
Epoch: 30 
 Validation Loss: 0.5465045322974523
---------------------------
Epoch: 30 Batch: 50
Training Loss: 0.6483655148744583
Epoch: 30 Batch: 100
Training Loss: 0.6222137421369552
Epoch: 30 Batch: 150
Training Loss: 0.6216312336921692
Epoch: 30 Batch: 200
Training Loss: 0.6216274607181549
Epoch: 30 Batch: 250
Training Loss: 0.6389489156007767
Epoch: 30 Batch: 300
Training Loss: 0.6340949529409409
Epoch: 30 Batch: 350
Training Loss: 0.6448368322849274
Epoch: 30 Batch: 400
Training Loss: 0.6230317997932434
Epoch: 30 Batch: 450
Training Loss: 0.6355415904521942
Epoch: 30 Batch: 500
Training Loss: 0.6373618942499161
Epoch: 30 Batch: 550
Training Loss: 0.6089785701036453
Epoch: 30 Batch: 600
Training Loss: 0.6276555728912353
Epoch: 30 Batch: 650
Training Loss: 0.632030348777771
Epoch: 30 Batch: 700
Training Loss: 0.6238656413555145
Epoch: 30 Batch: 750
Training Loss: 0.6228728473186493
Epoch: 30 Batch: 800
Training Loss: 0.6702125936746597
Epoch: 30 Batch: 850
Training Loss: 0.6342909771203995
Epoch: 30 Batch: 900
Training Loss: 0.6045073676109314
Epoch: 30 Batch: 950
Training Loss: 0.6386977887153625
Epoch: 30 Batch: 1000
Training Loss: 0.6402266544103622
Epoch: 30 Batch: 1050
Training Loss: 0.6407205832004547
Epoch: 30 Batch: 1100
Training Loss: 0.6665804314613343
Epoch: 30 Batch: 1150
Training Loss: 0.6186423301696777
Epoch: 30 Batch: 1200
Training Loss: 0.6241571128368377
Epoch: 30 Batch: 1250
Training Loss: 0.6375080728530884
Epoch: 30 Batch: 1300
Training Loss: 0.6161217325925827
Epoch: 30 Batch: 1350
Training Loss: 0.6456963938474655
Epoch: 30 Batch: 1400
Training Loss: 0.6027861732244492
Epoch: 30 Batch: 1450
Training Loss: 0.6210963159799576
Epoch: 30 Batch: 1500
Training Loss: 0.6093029469251633
Epoch: 30 Batch: 1550
Training Loss: 0.6345761477947235
Epoch: 30 Batch: 1600
Training Loss: 0.6478928774595261
Epoch: 30 Batch: 1650
Training Loss: 0.6581136691570282
Epoch: 30 Batch: 1700
Training Loss: 0.6369629108905792
Epoch: 30 Batch: 1750
Training Loss: 0.618259329199791
Epoch: 30 Batch: 1800
Training Loss: 0.631992375254631
Epoch: 30 Batch: 1850
Training Loss: 0.6513520020246506
Epoch: 30 Batch: 1900
Training Loss: 0.6469907957315445
Epoch: 30 Batch: 1950
Training Loss: 0.6559786087274552
Epoch: 30 Batch: 2000
Training Loss: 0.6229059368371963
Epoch: 30 Batch: 2050
Training Loss: 0.6372502797842026
Epoch: 30 Batch: 2100
Training Loss: 0.6569133442640305
Epoch: 30 Batch: 2150
Training Loss: 0.6068253135681152
Epoch: 30 Batch: 2200
Training Loss: 0.6171438461542129
Epoch: 30 Batch: 2250
Training Loss: 0.6721301275491715
Epoch: 30 Batch: 2300
Training Loss: 0.6507649880647659
Epoch: 30 Batch: 2350
Training Loss: 0.6138087087869644
Epoch: 30 Batch: 2400
Training Loss: 0.6727611845731736
Epoch: 30 Batch: 2450
Training Loss: 0.6599134063720703
Epoch: 30 Batch: 2500
Training Loss: 0.6020820146799087
Epoch: 30 Batch: 2550
Training Loss: 0.6267327374219894
Epoch: 30 Batch: 2600
Training Loss: 0.6284362149238586
Epoch: 30 Batch: 2650
Training Loss: 0.6360066670179367
Epoch: 30 Batch: 2700
Training Loss: 0.6308556181192398
Epoch: 30 Batch: 2750
Training Loss: 0.6158792865276337
Epoch: 30 Batch: 2800
Training Loss: 0.6669466584920883
Epoch: 30 Batch: 2850
Training Loss: 0.6353626024723052
Epoch: 30 Batch: 2900
Training Loss: 0.6762770313024521
Epoch: 30 Batch: 2950
Training Loss: 0.6672586923837662
Epoch: 30 Batch: 3000
Training Loss: 0.6260065454244613
Epoch: 30 Batch: 3050
Training Loss: 0.6344268900156022
Epoch: 30 Batch: 3100
Training Loss: 0.6206753140687943
Epoch: 30 Batch: 3150
Training Loss: 0.6280278015136719
Epoch: 30 Batch: 3200
Training Loss: 0.6441377252340317
Epoch: 30 Batch: 3250
Training Loss: 0.6599757647514344
Epoch: 30 Batch: 3300
Training Loss: 0.6510679417848587
Epoch: 30 Batch: 3350
Training Loss: 0.623302172422409
Epoch: 30 Batch: 3400
Training Loss: 0.6276625680923462
Epoch: 30 Batch: 3450
Training Loss: 0.6157521170377731
Epoch: 30 Batch: 3500
Training Loss: 0.642167449593544
Epoch: 31 
 Validation Loss: 0.5458515932162603
---------------------------
Epoch: 31 Batch: 50
Training Loss: 0.652724159359932
Epoch: 31 Batch: 100
Training Loss: 0.6309106785058975
Epoch: 31 Batch: 150
Training Loss: 0.6508631402254105
Epoch: 31 Batch: 200
Training Loss: 0.6540134239196778
Epoch: 31 Batch: 250
Training Loss: 0.6338038349151611
Epoch: 31 Batch: 300
Training Loss: 0.6372383403778076
Epoch: 31 Batch: 350
Training Loss: 0.6460319375991821
Epoch: 31 Batch: 400
Training Loss: 0.6251147484779358
Epoch: 31 Batch: 450
Training Loss: 0.6361288702487946
Epoch: 31 Batch: 500
Training Loss: 0.6404489648342132
Epoch: 31 Batch: 550
Training Loss: 0.6537858337163925
Epoch: 31 Batch: 600
Training Loss: 0.6531257593631744
Epoch: 31 Batch: 650
Training Loss: 0.6515947782993317
Epoch: 31 Batch: 700
Training Loss: 0.6117652559280395
Epoch: 31 Batch: 750
Training Loss: 0.6525733160972595
Epoch: 31 Batch: 800
Training Loss: 0.63287189245224
Epoch: 31 Batch: 850
Training Loss: 0.630880628824234
Epoch: 31 Batch: 900
Training Loss: 0.6121875768899918
Epoch: 31 Batch: 950
Training Loss: 0.6385563105344773
Epoch: 31 Batch: 1000
Training Loss: 0.6323096549510956
Epoch: 31 Batch: 1050
Training Loss: 0.6194675636291503
Epoch: 31 Batch: 1100
Training Loss: 0.6160002589225769
Epoch: 31 Batch: 1150
Training Loss: 0.6382303309440612
Epoch: 31 Batch: 1200
Training Loss: 0.6259493070840836
Epoch: 31 Batch: 1250
Training Loss: 0.6119640654325486
Epoch: 31 Batch: 1300
Training Loss: 0.6345841991901398
Epoch: 31 Batch: 1350
Training Loss: 0.6433574688434601
Epoch: 31 Batch: 1400
Training Loss: 0.6315778660774231
Epoch: 31 Batch: 1450
Training Loss: 0.6219247829914093
Epoch: 31 Batch: 1500
Training Loss: 0.6318555116653443
Epoch: 31 Batch: 1550
Training Loss: 0.6287330889701843
Epoch: 31 Batch: 1600
Training Loss: 0.6293058943748474
Epoch: 31 Batch: 1650
Training Loss: 0.6299795913696289
Epoch: 31 Batch: 1700
Training Loss: 0.631389622092247
Epoch: 31 Batch: 1750
Training Loss: 0.6024609971046447
Epoch: 31 Batch: 1800
Training Loss: 0.6465998512506484
Epoch: 31 Batch: 1850
Training Loss: 0.6290638041496277
Epoch: 31 Batch: 1900
Training Loss: 0.6201296311616897
Epoch: 31 Batch: 1950
Training Loss: 0.6290612930059433
Epoch: 31 Batch: 2000
Training Loss: 0.639984085559845
Epoch: 31 Batch: 2050
Training Loss: 0.603505386710167
Epoch: 31 Batch: 2100
Training Loss: 0.6208340293169021
Epoch: 31 Batch: 2150
Training Loss: 0.6514560437202453
Epoch: 31 Batch: 2200
Training Loss: 0.6245504140853881
Epoch: 31 Batch: 2250
Training Loss: 0.6494652938842773
Epoch: 31 Batch: 2300
Training Loss: 0.6347947299480439
Epoch: 31 Batch: 2350
Training Loss: 0.6087539964914321
Epoch: 31 Batch: 2400
Training Loss: 0.6249153220653534
Epoch: 31 Batch: 2450
Training Loss: 0.6380740618705749
Epoch: 31 Batch: 2500
Training Loss: 0.611582048535347
Epoch: 31 Batch: 2550
Training Loss: 0.6619370073080063
Epoch: 31 Batch: 2600
Training Loss: 0.6270790570974349
Epoch: 31 Batch: 2650
Training Loss: 0.6361577183008194
Epoch: 31 Batch: 2700
Training Loss: 0.6165204232931137
Epoch: 31 Batch: 2750
Training Loss: 0.6335852354764938
Epoch: 31 Batch: 2800
Training Loss: 0.6009510868787765
Epoch: 31 Batch: 2850
Training Loss: 0.64812828540802
Epoch: 31 Batch: 2900
Training Loss: 0.6122205191850663
Epoch: 31 Batch: 2950
Training Loss: 0.6488969534635544
Epoch: 31 Batch: 3000
Training Loss: 0.6405592840909958
Epoch: 31 Batch: 3050
Training Loss: 0.6294741773605347
Epoch: 31 Batch: 3100
Training Loss: 0.6366893535852433
Epoch: 31 Batch: 3150
Training Loss: 0.6267795276641845
Epoch: 31 Batch: 3200
Training Loss: 0.6251740348339081
Epoch: 31 Batch: 3250
Training Loss: 0.6392578744888305
Epoch: 31 Batch: 3300
Training Loss: 0.6552336597442627
Epoch: 31 Batch: 3350
Training Loss: 0.6268786829710007
Epoch: 31 Batch: 3400
Training Loss: 0.6193162757158279
Epoch: 31 Batch: 3450
Training Loss: 0.6356782394647599
Epoch: 31 Batch: 3500
Training Loss: 0.6598471194505692
Epoch: 32 
 Validation Loss: 0.5452146199014452
---------------------------
Epoch: 32 Batch: 50
Training Loss: 0.6643208944797516
Epoch: 32 Batch: 100
Training Loss: 0.6355029934644699
Epoch: 32 Batch: 150
Training Loss: 0.6235298746824265
Epoch: 32 Batch: 200
Training Loss: 0.6146566289663314
Epoch: 32 Batch: 250
Training Loss: 0.6151467686891556
Epoch: 32 Batch: 300
Training Loss: 0.624522728919983
Epoch: 32 Batch: 350
Training Loss: 0.6200496339797974
Epoch: 32 Batch: 400
Training Loss: 0.6371752548217774
Epoch: 32 Batch: 450
Training Loss: 0.6432975083589554
Epoch: 32 Batch: 500
Training Loss: 0.6107506448030472
Epoch: 32 Batch: 550
Training Loss: 0.6385143971443177
Epoch: 32 Batch: 600
Training Loss: 0.6225451284646988
Epoch: 32 Batch: 650
Training Loss: 0.6535560011863708
Epoch: 32 Batch: 700
Training Loss: 0.6161263608932495
Epoch: 32 Batch: 750
Training Loss: 0.6412637799978256
Epoch: 32 Batch: 800
Training Loss: 0.611892819404602
Epoch: 32 Batch: 850
Training Loss: 0.6336974066495895
Epoch: 32 Batch: 900
Training Loss: 0.6268174022436142
Epoch: 32 Batch: 950
Training Loss: 0.6450755548477173
Epoch: 32 Batch: 1000
Training Loss: 0.6016697722673416
Epoch: 32 Batch: 1050
Training Loss: 0.6218121784925461
Epoch: 32 Batch: 1100
Training Loss: 0.6153669375181198
Epoch: 32 Batch: 1150
Training Loss: 0.6458757603168488
Epoch: 32 Batch: 1200
Training Loss: 0.6613618367910385
Epoch: 32 Batch: 1250
Training Loss: 0.6384022629261017
Epoch: 32 Batch: 1300
Training Loss: 0.6585127109289169
Epoch: 32 Batch: 1350
Training Loss: 0.6244065910577774
Epoch: 32 Batch: 1400
Training Loss: 0.6466970622539521
Epoch: 32 Batch: 1450
Training Loss: 0.5801790422201156
Epoch: 32 Batch: 1500
Training Loss: 0.6351210510730744
Epoch: 32 Batch: 1550
Training Loss: 0.6313994759321213
Epoch: 32 Batch: 1600
Training Loss: 0.635546504855156
Epoch: 32 Batch: 1650
Training Loss: 0.6262337332963943
Epoch: 32 Batch: 1700
Training Loss: 0.6254972106218338
Epoch: 32 Batch: 1750
Training Loss: 0.6449904584884644
Epoch: 32 Batch: 1800
Training Loss: 0.6320796030759811
Epoch: 32 Batch: 1850
Training Loss: 0.6132109385728836
Epoch: 32 Batch: 1900
Training Loss: 0.6335560601949691
Epoch: 32 Batch: 1950
Training Loss: 0.6620770412683487
Epoch: 32 Batch: 2000
Training Loss: 0.6326135617494583
Epoch: 32 Batch: 2050
Training Loss: 0.6251075732707977
Epoch: 32 Batch: 2100
Training Loss: 0.6432203871011734
Epoch: 32 Batch: 2150
Training Loss: 0.5930558520555497
Epoch: 32 Batch: 2200
Training Loss: 0.6401007664203644
Epoch: 32 Batch: 2250
Training Loss: 0.6626248228549957
Epoch: 32 Batch: 2300
Training Loss: 0.6243215477466584
Epoch: 32 Batch: 2350
Training Loss: 0.6082104754447937
Epoch: 32 Batch: 2400
Training Loss: 0.6225493693351746
Epoch: 32 Batch: 2450
Training Loss: 0.6318396198749542
Epoch: 32 Batch: 2500
Training Loss: 0.6530842769145966
Epoch: 32 Batch: 2550
Training Loss: 0.6555912268161773
Epoch: 32 Batch: 2600
Training Loss: 0.6401007789373397
Epoch: 32 Batch: 2650
Training Loss: 0.6251437413692474
Epoch: 32 Batch: 2700
Training Loss: 0.6094698804616928
Epoch: 32 Batch: 2750
Training Loss: 0.6465915495157242
Epoch: 32 Batch: 2800
Training Loss: 0.632334606051445
Epoch: 32 Batch: 2850
Training Loss: 0.6339443397521972
Epoch: 32 Batch: 2900
Training Loss: 0.6508860164880752
Epoch: 32 Batch: 2950
Training Loss: 0.6307449477910996
Epoch: 32 Batch: 3000
Training Loss: 0.6332284814119339
Epoch: 32 Batch: 3050
Training Loss: 0.6371122711896896
Epoch: 32 Batch: 3100
Training Loss: 0.6198689895868301
Epoch: 32 Batch: 3150
Training Loss: 0.6192692220211029
Epoch: 32 Batch: 3200
Training Loss: 0.5894308573007584
Epoch: 32 Batch: 3250
Training Loss: 0.619461829662323
Epoch: 32 Batch: 3300
Training Loss: 0.6310591059923172
Epoch: 32 Batch: 3350
Training Loss: 0.6070578229427338
Epoch: 32 Batch: 3400
Training Loss: 0.6092491805553436
Epoch: 32 Batch: 3450
Training Loss: 0.6099884217977524
Epoch: 32 Batch: 3500
Training Loss: 0.6327617061138153
Epoch: 33 
 Validation Loss: 0.5441028260522418
---------------------------
Epoch: 33 Batch: 50
Training Loss: 0.6497215592861175
Epoch: 33 Batch: 100
Training Loss: 0.6282765340805053
Epoch: 33 Batch: 150
Training Loss: 0.6436673647165299
Epoch: 33 Batch: 200
Training Loss: 0.6347254824638366
Epoch: 33 Batch: 250
Training Loss: 0.5981289035081864
Epoch: 33 Batch: 300
Training Loss: 0.6227888482809066
Epoch: 33 Batch: 350
Training Loss: 0.6177243888378143
Epoch: 33 Batch: 400
Training Loss: 0.6193682289123535
Epoch: 33 Batch: 450
Training Loss: 0.6165463656187058
Epoch: 33 Batch: 500
Training Loss: 0.634337636232376
Epoch: 33 Batch: 550
Training Loss: 0.6202004593610764
Epoch: 33 Batch: 600
Training Loss: 0.6606177145242691
Epoch: 33 Batch: 650
Training Loss: 0.6485034608840943
Epoch: 33 Batch: 700
Training Loss: 0.6497558617591858
Epoch: 33 Batch: 750
Training Loss: 0.6316575437784195
Epoch: 33 Batch: 800
Training Loss: 0.6541730612516403
Epoch: 33 Batch: 850
Training Loss: 0.6326919001340866
Epoch: 33 Batch: 900
Training Loss: 0.6178063172101974
Epoch: 33 Batch: 950
Training Loss: 0.645588047504425
Epoch: 33 Batch: 1000
Training Loss: 0.6448677319288254
Epoch: 33 Batch: 1050
Training Loss: 0.6256110900640488
Epoch: 33 Batch: 1100
Training Loss: 0.6240047991275788
Epoch: 33 Batch: 1150
Training Loss: 0.601154950261116
Epoch: 33 Batch: 1200
Training Loss: 0.6459524345397949
Epoch: 33 Batch: 1250
Training Loss: 0.6191871982812881
Epoch: 33 Batch: 1300
Training Loss: 0.6341048008203507
Epoch: 33 Batch: 1350
Training Loss: 0.6361874425411225
Epoch: 33 Batch: 1400
Training Loss: 0.5967539674043656
Epoch: 33 Batch: 1450
Training Loss: 0.6503749096393585
Epoch: 33 Batch: 1500
Training Loss: 0.6123482578992844
Epoch: 33 Batch: 1550
Training Loss: 0.6198381787538528
Epoch: 33 Batch: 1600
Training Loss: 0.6150877904891968
Epoch: 33 Batch: 1650
Training Loss: 0.5990347355604172
Epoch: 33 Batch: 1700
Training Loss: 0.6260809534788132
Epoch: 33 Batch: 1750
Training Loss: 0.6263295251131058
Epoch: 33 Batch: 1800
Training Loss: 0.6180355280637742
Epoch: 33 Batch: 1850
Training Loss: 0.650636630654335
Epoch: 33 Batch: 1900
Training Loss: 0.6148698472976685
Epoch: 33 Batch: 1950
Training Loss: 0.6318437284231186
Epoch: 33 Batch: 2000
Training Loss: 0.628033310174942
Epoch: 33 Batch: 2050
Training Loss: 0.5859727370738983
Epoch: 33 Batch: 2100
Training Loss: 0.6269717079401016
Epoch: 33 Batch: 2150
Training Loss: 0.6272199499607086
Epoch: 33 Batch: 2200
Training Loss: 0.6183700156211853
Epoch: 33 Batch: 2250
Training Loss: 0.6076416158676148
Epoch: 33 Batch: 2300
Training Loss: 0.5889724534749985
Epoch: 33 Batch: 2350
Training Loss: 0.6118954485654831
Epoch: 33 Batch: 2400
Training Loss: 0.6233809846639633
Epoch: 33 Batch: 2450
Training Loss: 0.6525081747770309
Epoch: 33 Batch: 2500
Training Loss: 0.6461498844623565
Epoch: 33 Batch: 2550
Training Loss: 0.6577572965621948
Epoch: 33 Batch: 2600
Training Loss: 0.6522950541973114
Epoch: 33 Batch: 2650
Training Loss: 0.6373612684011459
Epoch: 33 Batch: 2700
Training Loss: 0.6301998317241668
Epoch: 33 Batch: 2750
Training Loss: 0.6406329071521759
Epoch: 33 Batch: 2800
Training Loss: 0.6222798413038254
Epoch: 33 Batch: 2850
Training Loss: 0.6196164643764496
Epoch: 33 Batch: 2900
Training Loss: 0.6181436008214951
Epoch: 33 Batch: 2950
Training Loss: 0.645382684469223
Epoch: 33 Batch: 3000
Training Loss: 0.6305170893669129
Epoch: 33 Batch: 3050
Training Loss: 0.6308433341979981
Epoch: 33 Batch: 3100
Training Loss: 0.6637919002771377
Epoch: 33 Batch: 3150
Training Loss: 0.6273809111118317
Epoch: 33 Batch: 3200
Training Loss: 0.6225093281269074
Epoch: 33 Batch: 3250
Training Loss: 0.6336501836776733
Epoch: 33 Batch: 3300
Training Loss: 0.6185672837495804
Epoch: 33 Batch: 3350
Training Loss: 0.6461173993349075
Epoch: 33 Batch: 3400
Training Loss: 0.6307634085416793
Epoch: 33 Batch: 3450
Training Loss: 0.6196970528364182
Epoch: 33 Batch: 3500
Training Loss: 0.6160979908704758
Epoch: 34 
 Validation Loss: 0.5431556602319082
---------------------------
Epoch: 34 Batch: 50
Training Loss: 0.6249090558290482
Epoch: 34 Batch: 100
Training Loss: 0.6146631699800491
Epoch: 34 Batch: 150
Training Loss: 0.6237607419490814
Epoch: 34 Batch: 200
Training Loss: 0.6344996285438538
Epoch: 34 Batch: 250
Training Loss: 0.6401713913679123
Epoch: 34 Batch: 300
Training Loss: 0.6280928552150726
Epoch: 34 Batch: 350
Training Loss: 0.6103345519304275
Epoch: 34 Batch: 400
Training Loss: 0.6527797418832779
Epoch: 34 Batch: 450
Training Loss: 0.612259104847908
Epoch: 34 Batch: 500
Training Loss: 0.5982257837057113
Epoch: 34 Batch: 550
Training Loss: 0.6406299233436584
Epoch: 34 Batch: 600
Training Loss: 0.625659287571907
Epoch: 34 Batch: 650
Training Loss: 0.6468424481153489
Epoch: 34 Batch: 700
Training Loss: 0.6431480044126511
Epoch: 34 Batch: 750
Training Loss: 0.6401777726411819
Epoch: 34 Batch: 800
Training Loss: 0.6324085903167724
Epoch: 34 Batch: 850
Training Loss: 0.6086692088842391
Epoch: 34 Batch: 900
Training Loss: 0.6419410765171051
Epoch: 34 Batch: 950
Training Loss: 0.6295204573869705
Epoch: 34 Batch: 1000
Training Loss: 0.6242227464914322
Epoch: 34 Batch: 1050
Training Loss: 0.6336250346899033
Epoch: 34 Batch: 1100
Training Loss: 0.6603486341238022
Epoch: 34 Batch: 1150
Training Loss: 0.6478349071741104
Epoch: 34 Batch: 1200
Training Loss: 0.62810820043087
Epoch: 34 Batch: 1250
Training Loss: 0.5951939952373505
Epoch: 34 Batch: 1300
Training Loss: 0.6607009959220886
Epoch: 34 Batch: 1350
Training Loss: 0.6354967701435089
Epoch: 34 Batch: 1400
Training Loss: 0.6570120775699615
Epoch: 34 Batch: 1450
Training Loss: 0.5941860288381576
Epoch: 34 Batch: 1500
Training Loss: 0.6134284120798111
Epoch: 34 Batch: 1550
Training Loss: 0.6697791260480881
Epoch: 34 Batch: 1600
Training Loss: 0.6002163106203079
Epoch: 34 Batch: 1650
Training Loss: 0.6106737279891967
Epoch: 34 Batch: 1700
Training Loss: 0.6257577580213547
Epoch: 34 Batch: 1750
Training Loss: 0.6200466400384903
Epoch: 34 Batch: 1800
Training Loss: 0.6629261076450348
Epoch: 34 Batch: 1850
Training Loss: 0.6164473909139633
Epoch: 34 Batch: 1900
Training Loss: 0.6371814209222794
Epoch: 34 Batch: 1950
Training Loss: 0.6401706302165985
Epoch: 34 Batch: 2000
Training Loss: 0.6145805764198303
Epoch: 34 Batch: 2050
Training Loss: 0.6309099626541138
Epoch: 34 Batch: 2100
Training Loss: 0.6576493108272552
Epoch: 34 Batch: 2150
Training Loss: 0.6483680510520935
Epoch: 34 Batch: 2200
Training Loss: 0.6351068025827408
Epoch: 34 Batch: 2250
Training Loss: 0.6024143314361572
Epoch: 34 Batch: 2300
Training Loss: 0.6182475310564041
Epoch: 34 Batch: 2350
Training Loss: 0.6335659837722778
Epoch: 34 Batch: 2400
Training Loss: 0.6535410046577453
Epoch: 34 Batch: 2450
Training Loss: 0.6369778472185135
Epoch: 34 Batch: 2500
Training Loss: 0.6353811341524124
Epoch: 34 Batch: 2550
Training Loss: 0.6069802188873291
Epoch: 34 Batch: 2600
Training Loss: 0.6210631537437439
Epoch: 34 Batch: 2650
Training Loss: 0.5736983370780945
Epoch: 34 Batch: 2700
Training Loss: 0.6532101702690124
Epoch: 34 Batch: 2750
Training Loss: 0.6398495602607727
Epoch: 34 Batch: 2800
Training Loss: 0.6222121930122375
Epoch: 34 Batch: 2850
Training Loss: 0.6068014395236969
Epoch: 34 Batch: 2900
Training Loss: 0.656834117770195
Epoch: 34 Batch: 2950
Training Loss: 0.6384018296003342
Epoch: 34 Batch: 3000
Training Loss: 0.6359649699926376
Epoch: 34 Batch: 3050
Training Loss: 0.633422360420227
Epoch: 34 Batch: 3100
Training Loss: 0.6282792848348617
Epoch: 34 Batch: 3150
Training Loss: 0.6293682414293289
Epoch: 34 Batch: 3200
Training Loss: 0.6567653775215149
Epoch: 34 Batch: 3250
Training Loss: 0.6198641312122345
Epoch: 34 Batch: 3300
Training Loss: 0.6227636647224426
Epoch: 34 Batch: 3350
Training Loss: 0.634298461675644
Epoch: 34 Batch: 3400
Training Loss: 0.6353498256206512
Epoch: 34 Batch: 3450
Training Loss: 0.6240273433923721
Epoch: 34 Batch: 3500
Training Loss: 0.600044811964035
Epoch: 35 
 Validation Loss: 0.5420676353904936
---------------------------
Epoch: 35 Batch: 50
Training Loss: 0.6087096744775772
Epoch: 35 Batch: 100
Training Loss: 0.6242131823301316
Epoch: 35 Batch: 150
Training Loss: 0.6069974648952484
Epoch: 35 Batch: 200
Training Loss: 0.5956947004795075
Epoch: 35 Batch: 250
Training Loss: 0.6353823316097259
Epoch: 35 Batch: 300
Training Loss: 0.6144036954641342
Epoch: 35 Batch: 350
Training Loss: 0.6161428296566009
Epoch: 35 Batch: 400
Training Loss: 0.6157213312387466
Epoch: 35 Batch: 450
Training Loss: 0.6461596608161926
Epoch: 35 Batch: 500
Training Loss: 0.6090392547845841
Epoch: 35 Batch: 550
Training Loss: 0.6382581907510757
Epoch: 35 Batch: 600
Training Loss: 0.6280519986152648
Epoch: 35 Batch: 650
Training Loss: 0.631502195596695
Epoch: 35 Batch: 700
Training Loss: 0.6338157963752746
Epoch: 35 Batch: 750
Training Loss: 0.6196042948961258
Epoch: 35 Batch: 800
Training Loss: 0.6156088691949845
Epoch: 35 Batch: 850
Training Loss: 0.6550585985183716
Epoch: 35 Batch: 900
Training Loss: 0.607994304895401
Epoch: 35 Batch: 950
Training Loss: 0.6006662231683731
Epoch: 35 Batch: 1000
Training Loss: 0.6464573913812637
Epoch: 35 Batch: 1050
Training Loss: 0.6017835211753845
Epoch: 35 Batch: 1100
Training Loss: 0.6459336167573929
Epoch: 35 Batch: 1150
Training Loss: 0.6278142386674881
Epoch: 35 Batch: 1200
Training Loss: 0.5921679162979125
Epoch: 35 Batch: 1250
Training Loss: 0.6610745352506637
Epoch: 35 Batch: 1300
Training Loss: 0.5959007543325424
Epoch: 35 Batch: 1350
Training Loss: 0.6490799403190612
Epoch: 35 Batch: 1400
Training Loss: 0.6215038228034974
Epoch: 35 Batch: 1450
Training Loss: 0.6244581151008606
Epoch: 35 Batch: 1500
Training Loss: 0.6246706277132035
Epoch: 35 Batch: 1550
Training Loss: 0.6266337794065475
Epoch: 35 Batch: 1600
Training Loss: 0.6048901212215424
Epoch: 35 Batch: 1650
Training Loss: 0.6472384822368622
Epoch: 35 Batch: 1700
Training Loss: 0.6426262992620468
Epoch: 35 Batch: 1750
Training Loss: 0.6449064165353775
Epoch: 35 Batch: 1800
Training Loss: 0.6060328447818756
Epoch: 35 Batch: 1850
Training Loss: 0.6192995977401733
Epoch: 35 Batch: 1900
Training Loss: 0.6433057349920273
Epoch: 35 Batch: 1950
Training Loss: 0.6138221478462219
Epoch: 35 Batch: 2000
Training Loss: 0.6240941071510315
Epoch: 35 Batch: 2050
Training Loss: 0.6131307256221771
Epoch: 35 Batch: 2100
Training Loss: 0.6292352283000946
Epoch: 35 Batch: 2150
Training Loss: 0.6217332500219345
Epoch: 35 Batch: 2200
Training Loss: 0.6296320706605911
Epoch: 35 Batch: 2250
Training Loss: 0.6352933740615845
Epoch: 35 Batch: 2300
Training Loss: 0.6401489490270614
Epoch: 35 Batch: 2350
Training Loss: 0.6667473584413528
Epoch: 35 Batch: 2400
Training Loss: 0.6487054610252381
Epoch: 35 Batch: 2450
Training Loss: 0.6074659103155136
Epoch: 35 Batch: 2500
Training Loss: 0.6196882432699203
Epoch: 35 Batch: 2550
Training Loss: 0.6455218005180359
Epoch: 35 Batch: 2600
Training Loss: 0.638171951174736
Epoch: 35 Batch: 2650
Training Loss: 0.6381812953948974
Epoch: 35 Batch: 2700
Training Loss: 0.6583481413125992
Epoch: 35 Batch: 2750
Training Loss: 0.6334190154075623
Epoch: 35 Batch: 2800
Training Loss: 0.6251273167133331
Epoch: 35 Batch: 2850
Training Loss: 0.6327808809280395
Epoch: 35 Batch: 2900
Training Loss: 0.6143346244096756
Epoch: 35 Batch: 2950
Training Loss: 0.5995415037870407
Epoch: 35 Batch: 3000
Training Loss: 0.6146598446369171
Epoch: 35 Batch: 3050
Training Loss: 0.6272317510843277
Epoch: 35 Batch: 3100
Training Loss: 0.6539970248937607
Epoch: 35 Batch: 3150
Training Loss: 0.6245284563302994
Epoch: 35 Batch: 3200
Training Loss: 0.6119332271814346
Epoch: 35 Batch: 3250
Training Loss: 0.6347580409049988
Epoch: 35 Batch: 3300
Training Loss: 0.6197036814689636
Epoch: 35 Batch: 3350
Training Loss: 0.6180935609340668
Epoch: 35 Batch: 3400
Training Loss: 0.6462342184782028
Epoch: 35 Batch: 3450
Training Loss: 0.6170513820648194
Epoch: 35 Batch: 3500
Training Loss: 0.618284068107605
Epoch: 36 
 Validation Loss: 0.5417209565639496
---------------------------
Epoch: 36 Batch: 50
Training Loss: 0.6567051821947097
Epoch: 36 Batch: 100
Training Loss: 0.6362130469083787
Epoch: 36 Batch: 150
Training Loss: 0.6395020592212677
Epoch: 36 Batch: 200
Training Loss: 0.6560764467716217
Epoch: 36 Batch: 250
Training Loss: 0.6360983604192734
Epoch: 36 Batch: 300
Training Loss: 0.645788768529892
Epoch: 36 Batch: 350
Training Loss: 0.6347219568490982
Epoch: 36 Batch: 400
Training Loss: 0.6098173409700394
Epoch: 36 Batch: 450
Training Loss: 0.608721187710762
Epoch: 36 Batch: 500
Training Loss: 0.5996180403232575
Epoch: 36 Batch: 550
Training Loss: 0.6517186003923416
Epoch: 36 Batch: 600
Training Loss: 0.6308140164613724
Epoch: 36 Batch: 650
Training Loss: 0.641750316619873
Epoch: 36 Batch: 700
Training Loss: 0.6534617120027542
Epoch: 36 Batch: 750
Training Loss: 0.629653400182724
Epoch: 36 Batch: 800
Training Loss: 0.6296553248167038
Epoch: 36 Batch: 850
Training Loss: 0.6172847104072571
Epoch: 36 Batch: 900
Training Loss: 0.6444590777158737
Epoch: 36 Batch: 950
Training Loss: 0.5697681099176407
Epoch: 36 Batch: 1000
Training Loss: 0.6378642821311951
Epoch: 36 Batch: 1050
Training Loss: 0.5854252928495407
Epoch: 36 Batch: 1100
Training Loss: 0.6523691999912262
Epoch: 36 Batch: 1150
Training Loss: 0.6498036795854568
Epoch: 36 Batch: 1200
Training Loss: 0.6123961114883423
Epoch: 36 Batch: 1250
Training Loss: 0.611894503235817
Epoch: 36 Batch: 1300
Training Loss: 0.6139384752511978
Epoch: 36 Batch: 1350
Training Loss: 0.6336161816120147
Epoch: 36 Batch: 1400
Training Loss: 0.6084131467342376
Epoch: 36 Batch: 1450
Training Loss: 0.6194210070371627
Epoch: 36 Batch: 1500
Training Loss: 0.5931284594535827
Epoch: 36 Batch: 1550
Training Loss: 0.6187706631422043
Epoch: 36 Batch: 1600
Training Loss: 0.6284824895858765
Epoch: 36 Batch: 1650
Training Loss: 0.6148650884628296
Epoch: 36 Batch: 1700
Training Loss: 0.630384162068367
Epoch: 36 Batch: 1750
Training Loss: 0.6024900513887406
Epoch: 36 Batch: 1800
Training Loss: 0.6555555665493011
Epoch: 36 Batch: 1850
Training Loss: 0.636872575879097
Epoch: 36 Batch: 1900
Training Loss: 0.6006920397281647
Epoch: 36 Batch: 1950
Training Loss: 0.6190174633264541
Epoch: 36 Batch: 2000
Training Loss: 0.6459158843755722
Epoch: 36 Batch: 2050
Training Loss: 0.6153616034984588
Epoch: 36 Batch: 2100
Training Loss: 0.6317535322904587
Epoch: 36 Batch: 2150
Training Loss: 0.6518957090377807
Epoch: 36 Batch: 2200
Training Loss: 0.6155495285987854
Epoch: 36 Batch: 2250
Training Loss: 0.6590764534473419
Epoch: 36 Batch: 2300
Training Loss: 0.6258713877201081
Epoch: 36 Batch: 2350
Training Loss: 0.6236737883090973
Epoch: 36 Batch: 2400
Training Loss: 0.6186200708150864
Epoch: 36 Batch: 2450
Training Loss: 0.6543684190511704
Epoch: 36 Batch: 2500
Training Loss: 0.6221412515640259
Epoch: 36 Batch: 2550
Training Loss: 0.6155586111545562
Epoch: 36 Batch: 2600
Training Loss: 0.6238112831115723
Epoch: 36 Batch: 2650
Training Loss: 0.6217811554670334
Epoch: 36 Batch: 2700
Training Loss: 0.6204349392652512
Epoch: 36 Batch: 2750
Training Loss: 0.6352052736282349
Epoch: 36 Batch: 2800
Training Loss: 0.6269154453277588
Epoch: 36 Batch: 2850
Training Loss: 0.6214493769407272
Epoch: 36 Batch: 2900
Training Loss: 0.6112987536191941
Epoch: 36 Batch: 2950
Training Loss: 0.638528761267662
Epoch: 36 Batch: 3000
Training Loss: 0.614592564702034
Epoch: 36 Batch: 3050
Training Loss: 0.6642506265640259
Epoch: 36 Batch: 3100
Training Loss: 0.6039944493770599
Epoch: 36 Batch: 3150
Training Loss: 0.6356877613067627
Epoch: 36 Batch: 3200
Training Loss: 0.6067990893125534
Epoch: 36 Batch: 3250
Training Loss: 0.6311223155260086
Epoch: 36 Batch: 3300
Training Loss: 0.626757241487503
Epoch: 36 Batch: 3350
Training Loss: 0.6398605519533157
Epoch: 36 Batch: 3400
Training Loss: 0.6250917893648148
Epoch: 36 Batch: 3450
Training Loss: 0.6295235472917556
Epoch: 36 Batch: 3500
Training Loss: 0.6144765138626098
Epoch: 37 
 Validation Loss: 0.5422307153542837
---------------------------
Epoch: 37 Batch: 50
Training Loss: 0.6426398456096649
Epoch: 37 Batch: 100
Training Loss: 0.6478365433216094
Epoch: 37 Batch: 150
Training Loss: 0.638004047870636
Epoch: 37 Batch: 200
Training Loss: 0.6347210496664047
Epoch: 37 Batch: 250
Training Loss: 0.6454738938808441
Epoch: 37 Batch: 300
Training Loss: 0.6341319453716278
Epoch: 37 Batch: 350
Training Loss: 0.6399828457832336
Epoch: 37 Batch: 400
Training Loss: 0.6296449100971222
Epoch: 37 Batch: 450
Training Loss: 0.624670689702034
Epoch: 37 Batch: 500
Training Loss: 0.656107605099678
Epoch: 37 Batch: 550
Training Loss: 0.6286739534139634
Epoch: 37 Batch: 600
Training Loss: 0.6144160348176956
Epoch: 37 Batch: 650
Training Loss: 0.6375661021471024
Epoch: 37 Batch: 700
Training Loss: 0.6143804401159286
Epoch: 37 Batch: 750
Training Loss: 0.6392716079950332
Epoch: 37 Batch: 800
Training Loss: 0.6318997728824616
Epoch: 37 Batch: 850
Training Loss: 0.6368187195062638
Epoch: 37 Batch: 900
Training Loss: 0.6346094691753388
Epoch: 37 Batch: 950
Training Loss: 0.5953015899658203
Epoch: 37 Batch: 1000
Training Loss: 0.637767773270607
Epoch: 37 Batch: 1050
Training Loss: 0.6375010287761689
Epoch: 37 Batch: 1100
Training Loss: 0.6139180457592011
Epoch: 37 Batch: 1150
Training Loss: 0.6185254245996475
Epoch: 37 Batch: 1200
Training Loss: 0.6157631206512452
Epoch: 37 Batch: 1250
Training Loss: 0.6367114263772965
Epoch: 37 Batch: 1300
Training Loss: 0.6472698944807053
Epoch: 37 Batch: 1350
Training Loss: 0.6418184512853622
Epoch: 37 Batch: 1400
Training Loss: 0.634132885336876
Epoch: 37 Batch: 1450
Training Loss: 0.6139697182178497
Epoch: 37 Batch: 1500
Training Loss: 0.6206296658515931
Epoch: 37 Batch: 1550
Training Loss: 0.5863521635532379
Epoch: 37 Batch: 1600
Training Loss: 0.6478256475925446
Epoch: 37 Batch: 1650
Training Loss: 0.6424546957015991
Epoch: 37 Batch: 1700
Training Loss: 0.6154043483734131
Epoch: 37 Batch: 1750
Training Loss: 0.6017232531309128
Epoch: 37 Batch: 1800
Training Loss: 0.6338297724723816
Epoch: 37 Batch: 1850
Training Loss: 0.6110329180955887
Epoch: 37 Batch: 1900
Training Loss: 0.6046199399232864
Epoch: 37 Batch: 1950
Training Loss: 0.6269929963350296
Epoch: 37 Batch: 2000
Training Loss: 0.6270252782106399
Epoch: 37 Batch: 2050
Training Loss: 0.6143545407056809
Epoch: 37 Batch: 2100
Training Loss: 0.6136780554056167
Epoch: 37 Batch: 2150
Training Loss: 0.6205908781290055
Epoch: 37 Batch: 2200
Training Loss: 0.6296577101945877
Epoch: 37 Batch: 2250
Training Loss: 0.5976485508680344
Epoch: 37 Batch: 2300
Training Loss: 0.6347600305080414
Epoch: 37 Batch: 2350
Training Loss: 0.6153135377168656
Epoch: 37 Batch: 2400
Training Loss: 0.628238080739975
Epoch: 37 Batch: 2450
Training Loss: 0.5959532475471496
Epoch: 37 Batch: 2500
Training Loss: 0.6033864367008209
Epoch: 37 Batch: 2550
Training Loss: 0.60748839199543
Epoch: 37 Batch: 2600
Training Loss: 0.6232067894935608
Epoch: 37 Batch: 2650
Training Loss: 0.6106391441822052
Epoch: 37 Batch: 2700
Training Loss: 0.6104180520772934
Epoch: 37 Batch: 2750
Training Loss: 0.643836978673935
Epoch: 37 Batch: 2800
Training Loss: 0.6102080315351486
Epoch: 37 Batch: 2850
Training Loss: 0.6546011781692505
Epoch: 37 Batch: 2900
Training Loss: 0.6195265090465546
Epoch: 37 Batch: 2950
Training Loss: 0.6423979127407073
Epoch: 37 Batch: 3000
Training Loss: 0.6119729775190353
Epoch: 37 Batch: 3050
Training Loss: 0.6151995784044266
Epoch: 37 Batch: 3100
Training Loss: 0.6169598704576492
Epoch: 37 Batch: 3150
Training Loss: 0.6351542836427688
Epoch: 37 Batch: 3200
Training Loss: 0.6469108086824417
Epoch: 37 Batch: 3250
Training Loss: 0.6072521340847016
Epoch: 37 Batch: 3300
Training Loss: 0.6316701632738113
Epoch: 37 Batch: 3350
Training Loss: 0.6152694511413574
Epoch: 37 Batch: 3400
Training Loss: 0.6201936304569244
Epoch: 37 Batch: 3450
Training Loss: 0.652775286436081
Epoch: 37 Batch: 3500
Training Loss: 0.6249987596273422
Epoch: 38 
 Validation Loss: 0.540718651480145
---------------------------
Epoch: 38 Batch: 50
Training Loss: 0.6483665299415589
Epoch: 38 Batch: 100
Training Loss: 0.6268919402360916
Epoch: 38 Batch: 150
Training Loss: 0.6393127852678299
Epoch: 38 Batch: 200
Training Loss: 0.6180020439624786
Epoch: 38 Batch: 250
Training Loss: 0.6185055154561997
Epoch: 38 Batch: 300
Training Loss: 0.5989212739467621
Epoch: 38 Batch: 350
Training Loss: 0.6050830000638961
Epoch: 38 Batch: 400
Training Loss: 0.6312494021654129
Epoch: 38 Batch: 450
Training Loss: 0.6445098304748536
Epoch: 38 Batch: 500
Training Loss: 0.6243510729074478
Epoch: 38 Batch: 550
Training Loss: 0.6248812288045883
Epoch: 38 Batch: 600
Training Loss: 0.6246628928184509
Epoch: 38 Batch: 650
Training Loss: 0.6156170380115509
Epoch: 38 Batch: 700
Training Loss: 0.6146123296022415
Epoch: 38 Batch: 750
Training Loss: 0.6194504487514496
Epoch: 38 Batch: 800
Training Loss: 0.6246754789352417
Epoch: 38 Batch: 850
Training Loss: 0.6652123123407364
Epoch: 38 Batch: 900
Training Loss: 0.6414427131414413
Epoch: 38 Batch: 950
Training Loss: 0.6353949683904648
Epoch: 38 Batch: 1000
Training Loss: 0.6224887299537659
Epoch: 38 Batch: 1050
Training Loss: 0.6073132622241973
Epoch: 38 Batch: 1100
Training Loss: 0.6280677223205566
Epoch: 38 Batch: 1150
Training Loss: 0.6309250748157501
Epoch: 38 Batch: 1200
Training Loss: 0.6128900110721588
Epoch: 38 Batch: 1250
Training Loss: 0.6105978685617447
Epoch: 38 Batch: 1300
Training Loss: 0.6300369083881379
Epoch: 38 Batch: 1350
Training Loss: 0.6342151927947998
Epoch: 38 Batch: 1400
Training Loss: 0.6406095796823501
Epoch: 38 Batch: 1450
Training Loss: 0.6216368794441223
Epoch: 38 Batch: 1500
Training Loss: 0.6212863183021545
Epoch: 38 Batch: 1550
Training Loss: 0.6594577163457871
Epoch: 38 Batch: 1600
Training Loss: 0.6293696880340576
Epoch: 38 Batch: 1650
Training Loss: 0.613268632888794
Epoch: 38 Batch: 1700
Training Loss: 0.6323682785034179
Epoch: 38 Batch: 1750
Training Loss: 0.6294695967435837
Epoch: 38 Batch: 1800
Training Loss: 0.6103214550018311
Epoch: 38 Batch: 1850
Training Loss: 0.6121153676509857
Epoch: 38 Batch: 1900
Training Loss: 0.6183290827274323
Epoch: 38 Batch: 1950
Training Loss: 0.6270617926120758
Epoch: 38 Batch: 2000
Training Loss: 0.6422883826494217
Epoch: 38 Batch: 2050
Training Loss: 0.6400098735094071
Epoch: 38 Batch: 2100
Training Loss: 0.6158993417024612
Epoch: 38 Batch: 2150
Training Loss: 0.6318761557340622
Epoch: 38 Batch: 2200
Training Loss: 0.6304239404201507
Epoch: 38 Batch: 2250
Training Loss: 0.6372527670860291
Epoch: 38 Batch: 2300
Training Loss: 0.627602607011795
Epoch: 38 Batch: 2350
Training Loss: 0.6306882017850876
Epoch: 38 Batch: 2400
Training Loss: 0.6207220047712326
Epoch: 38 Batch: 2450
Training Loss: 0.6135298746824265
Epoch: 38 Batch: 2500
Training Loss: 0.6369018465280533
Epoch: 38 Batch: 2550
Training Loss: 0.6366016346216202
Epoch: 38 Batch: 2600
Training Loss: 0.6141165363788604
Epoch: 38 Batch: 2650
Training Loss: 0.6109329837560654
Epoch: 38 Batch: 2700
Training Loss: 0.6441542553901672
Epoch: 38 Batch: 2750
Training Loss: 0.6070078426599502
Epoch: 38 Batch: 2800
Training Loss: 0.6047215241193772
Epoch: 38 Batch: 2850
Training Loss: 0.6076162987947464
Epoch: 38 Batch: 2900
Training Loss: 0.6134928643703461
Epoch: 38 Batch: 2950
Training Loss: 0.6330067390203475
Epoch: 38 Batch: 3000
Training Loss: 0.6344780820608139
Epoch: 38 Batch: 3050
Training Loss: 0.6403332012891769
Epoch: 38 Batch: 3100
Training Loss: 0.6310648727416992
Epoch: 38 Batch: 3150
Training Loss: 0.6447076553106308
Epoch: 38 Batch: 3200
Training Loss: 0.6214447927474975
Epoch: 38 Batch: 3250
Training Loss: 0.6258026564121246
Epoch: 38 Batch: 3300
Training Loss: 0.6362639647722245
Epoch: 38 Batch: 3350
Training Loss: 0.6262151515483856
Epoch: 38 Batch: 3400
Training Loss: 0.627029018998146
Epoch: 38 Batch: 3450
Training Loss: 0.6330129247903824
Epoch: 38 Batch: 3500
Training Loss: 0.6329092460870743
Epoch: 39 
 Validation Loss: 0.5399202158053716
---------------------------
Epoch: 39 Batch: 50
Training Loss: 0.6353547841310501
Epoch: 39 Batch: 100
Training Loss: 0.598522527217865
Epoch: 39 Batch: 150
Training Loss: 0.6344970715045929
Epoch: 39 Batch: 200
Training Loss: 0.6330964410305023
Epoch: 39 Batch: 250
Training Loss: 0.6488389903306961
Epoch: 39 Batch: 300
Training Loss: 0.6281807345151901
Epoch: 39 Batch: 350
Training Loss: 0.6199085199832917
Epoch: 39 Batch: 400
Training Loss: 0.6296260941028595
Epoch: 39 Batch: 450
Training Loss: 0.6520676410198212
Epoch: 39 Batch: 500
Training Loss: 0.629432515501976
Epoch: 39 Batch: 550
Training Loss: 0.6170713472366333
Epoch: 39 Batch: 600
Training Loss: 0.6285592603683472
Epoch: 39 Batch: 650
Training Loss: 0.5865950232744217
Epoch: 39 Batch: 700
Training Loss: 0.6316822564601898
Epoch: 39 Batch: 750
Training Loss: 0.634108344912529
Epoch: 39 Batch: 800
Training Loss: 0.6556474179029464
Epoch: 39 Batch: 850
Training Loss: 0.6120423060655594
Epoch: 39 Batch: 900
Training Loss: 0.6312462949752807
Epoch: 39 Batch: 950
Training Loss: 0.6452697736024856
Epoch: 39 Batch: 1000
Training Loss: 0.6092126566171646
Epoch: 39 Batch: 1050
Training Loss: 0.6182315176725388
Epoch: 39 Batch: 1100
Training Loss: 0.6016882616281509
Epoch: 39 Batch: 1150
Training Loss: 0.61361847281456
Epoch: 39 Batch: 1200
Training Loss: 0.6337551993131637
Epoch: 39 Batch: 1250
Training Loss: 0.613867666721344
Epoch: 39 Batch: 1300
Training Loss: 0.6295546716451645
Epoch: 39 Batch: 1350
Training Loss: 0.6176817393302918
Epoch: 39 Batch: 1400
Training Loss: 0.6221522098779678
Epoch: 39 Batch: 1450
Training Loss: 0.6282996481657028
Epoch: 39 Batch: 1500
Training Loss: 0.6126582711935044
Epoch: 39 Batch: 1550
Training Loss: 0.6435734564065934
Epoch: 39 Batch: 1600
Training Loss: 0.6400446939468384
Epoch: 39 Batch: 1650
Training Loss: 0.6395668303966522
Epoch: 39 Batch: 1700
Training Loss: 0.6204953962564468
Epoch: 39 Batch: 1750
Training Loss: 0.6246305549144745
Epoch: 39 Batch: 1800
Training Loss: 0.6367286604642868
Epoch: 39 Batch: 1850
Training Loss: 0.6166392910480499
Epoch: 39 Batch: 1900
Training Loss: 0.6215486645698547
Epoch: 39 Batch: 1950
Training Loss: 0.6114822775125504
Epoch: 39 Batch: 2000
Training Loss: 0.5978623569011688
Epoch: 39 Batch: 2050
Training Loss: 0.6680072963237762
Epoch: 39 Batch: 2100
Training Loss: 0.6002658009529114
Epoch: 39 Batch: 2150
Training Loss: 0.6424865931272506
Epoch: 39 Batch: 2200
Training Loss: 0.6186203134059906
Epoch: 39 Batch: 2250
Training Loss: 0.599165957570076
Epoch: 39 Batch: 2300
Training Loss: 0.6231133580207825
Epoch: 39 Batch: 2350
Training Loss: 0.6341787415742874
Epoch: 39 Batch: 2400
Training Loss: 0.6103181797266006
Epoch: 39 Batch: 2450
Training Loss: 0.6006233316659927
Epoch: 39 Batch: 2500
Training Loss: 0.6143562990427017
Epoch: 39 Batch: 2550
Training Loss: 0.642796459197998
Epoch: 39 Batch: 2600
Training Loss: 0.6353745764493942
Epoch: 39 Batch: 2650
Training Loss: 0.6256150805950165
Epoch: 39 Batch: 2700
Training Loss: 0.6312620460987091
Epoch: 39 Batch: 2750
Training Loss: 0.6319072735309601
Epoch: 39 Batch: 2800
Training Loss: 0.5955516463518142
Epoch: 39 Batch: 2850
Training Loss: 0.6397712481021881
Epoch: 39 Batch: 2900
Training Loss: 0.6225936180353164
Epoch: 39 Batch: 2950
Training Loss: 0.6231598693132401
Epoch: 39 Batch: 3000
Training Loss: 0.6494440788030624
Epoch: 39 Batch: 3050
Training Loss: 0.6023917293548584
Epoch: 39 Batch: 3100
Training Loss: 0.6232927268743516
Epoch: 39 Batch: 3150
Training Loss: 0.6331384056806564
Epoch: 39 Batch: 3200
Training Loss: 0.595000609755516
Epoch: 39 Batch: 3250
Training Loss: 0.606711569428444
Epoch: 39 Batch: 3300
Training Loss: 0.6135941314697265
Epoch: 39 Batch: 3350
Training Loss: 0.6073000735044479
Epoch: 39 Batch: 3400
Training Loss: 0.6665053886175155
Epoch: 39 Batch: 3450
Training Loss: 0.6476886051893235
Epoch: 39 Batch: 3500
Training Loss: 0.629287868142128
Epoch: 40 
 Validation Loss: 0.5391323732005225
---------------------------
Epoch: 40 Batch: 50
Training Loss: 0.6721991151571274
Epoch: 40 Batch: 100
Training Loss: 0.6560699898004532
Epoch: 40 Batch: 150
Training Loss: 0.6051912951469421
Epoch: 40 Batch: 200
Training Loss: 0.642095091342926
Epoch: 40 Batch: 250
Training Loss: 0.6087505608797074
Epoch: 40 Batch: 300
Training Loss: 0.6440830343961715
Epoch: 40 Batch: 350
Training Loss: 0.6439128315448761
Epoch: 40 Batch: 400
Training Loss: 0.6236141860485077
Epoch: 40 Batch: 450
Training Loss: 0.6215955185890197
Epoch: 40 Batch: 500
Training Loss: 0.6338031679391861
Epoch: 40 Batch: 550
Training Loss: 0.629591521024704
Epoch: 40 Batch: 600
Training Loss: 0.6178066897392273
Epoch: 40 Batch: 650
Training Loss: 0.638106232881546
Epoch: 40 Batch: 700
Training Loss: 0.5808157628774643
Epoch: 40 Batch: 750
Training Loss: 0.6116440057754516
Epoch: 40 Batch: 800
Training Loss: 0.6151020908355713
Epoch: 40 Batch: 850
Training Loss: 0.6204262143373489
Epoch: 40 Batch: 900
Training Loss: 0.6074210661649704
Epoch: 40 Batch: 950
Training Loss: 0.6076968777179718
Epoch: 40 Batch: 1000
Training Loss: 0.6284106332063675
Epoch: 40 Batch: 1050
Training Loss: 0.6595600527524949
Epoch: 40 Batch: 1100
Training Loss: 0.6288222193717956
Epoch: 40 Batch: 1150
Training Loss: 0.6335404360294342
Epoch: 40 Batch: 1200
Training Loss: 0.6430851382017135
Epoch: 40 Batch: 1250
Training Loss: 0.6208563566207885
Epoch: 40 Batch: 1300
Training Loss: 0.6510041970014572
Epoch: 40 Batch: 1350
Training Loss: 0.6450652515888214
Epoch: 40 Batch: 1400
Training Loss: 0.650460416674614
Epoch: 40 Batch: 1450
Training Loss: 0.6150800722837448
Epoch: 40 Batch: 1500
Training Loss: 0.6405306762456894
Epoch: 40 Batch: 1550
Training Loss: 0.6038663786649704
Epoch: 40 Batch: 1600
Training Loss: 0.6304825353622436
Epoch: 40 Batch: 1650
Training Loss: 0.6255177664756775
Epoch: 40 Batch: 1700
Training Loss: 0.5903440594673157
Epoch: 40 Batch: 1750
Training Loss: 0.6427621883153916
Epoch: 40 Batch: 1800
Training Loss: 0.6336497604846955
Epoch: 40 Batch: 1850
Training Loss: 0.6472628223896026
Epoch: 40 Batch: 1900
Training Loss: 0.6541921693086624
Epoch: 40 Batch: 1950
Training Loss: 0.6004333132505417
Epoch: 40 Batch: 2000
Training Loss: 0.6232755404710769
Epoch: 40 Batch: 2050
Training Loss: 0.622298714518547
Epoch: 40 Batch: 2100
Training Loss: 0.5880428206920624
Epoch: 40 Batch: 2150
Training Loss: 0.6263294953107834
Epoch: 40 Batch: 2200
Training Loss: 0.6304171848297119
Epoch: 40 Batch: 2250
Training Loss: 0.6228814393281936
Epoch: 40 Batch: 2300
Training Loss: 0.5994696974754333
Epoch: 40 Batch: 2350
Training Loss: 0.6176520442962646
Epoch: 40 Batch: 2400
Training Loss: 0.6497081732749939
Epoch: 40 Batch: 2450
Training Loss: 0.609822605252266
Epoch: 40 Batch: 2500
Training Loss: 0.5931990647315979
Epoch: 40 Batch: 2550
Training Loss: 0.5907483506202698
Epoch: 40 Batch: 2600
Training Loss: 0.6542532348632812
Epoch: 40 Batch: 2650
Training Loss: 0.625283043384552
Epoch: 40 Batch: 2700
Training Loss: 0.6159807932376862
Epoch: 40 Batch: 2750
Training Loss: 0.6410583829879761
Epoch: 40 Batch: 2800
Training Loss: 0.64925237596035
Epoch: 40 Batch: 2850
Training Loss: 0.6204417717456817
Epoch: 40 Batch: 2900
Training Loss: 0.611155646443367
Epoch: 40 Batch: 2950
Training Loss: 0.6046501445770264
Epoch: 40 Batch: 3000
Training Loss: 0.6418946343660354
Epoch: 40 Batch: 3050
Training Loss: 0.627928050160408
Epoch: 40 Batch: 3100
Training Loss: 0.6094941753149032
Epoch: 40 Batch: 3150
Training Loss: 0.6293796002864838
Epoch: 40 Batch: 3200
Training Loss: 0.5981279975175857
Epoch: 40 Batch: 3250
Training Loss: 0.6198544913530349
Epoch: 40 Batch: 3300
Training Loss: 0.6041239982843399
Epoch: 40 Batch: 3350
Training Loss: 0.6243938374519348
Epoch: 40 Batch: 3400
Training Loss: 0.634809039235115
Epoch: 40 Batch: 3450
Training Loss: 0.6432939153909684
Epoch: 40 Batch: 3500
Training Loss: 0.6468002498149872
Epoch: 41 
 Validation Loss: 0.5387961996926202
---------------------------
Epoch: 41 Batch: 50
Training Loss: 0.6392839914560318
Epoch: 41 Batch: 100
Training Loss: 0.6159787386655807
Epoch: 41 Batch: 150
Training Loss: 0.617651903629303
Epoch: 41 Batch: 200
Training Loss: 0.6350791656970978
Epoch: 41 Batch: 250
Training Loss: 0.6146998816728592
Epoch: 41 Batch: 300
Training Loss: 0.6222345000505447
Epoch: 41 Batch: 350
Training Loss: 0.6208489102125168
Epoch: 41 Batch: 400
Training Loss: 0.6157851231098175
Epoch: 41 Batch: 450
Training Loss: 0.617123493552208
Epoch: 41 Batch: 500
Training Loss: 0.6325961458683014
Epoch: 41 Batch: 550
Training Loss: 0.6342989802360535
Epoch: 41 Batch: 600
Training Loss: 0.6169644510746002
Epoch: 41 Batch: 650
Training Loss: 0.6179799652099609
Epoch: 41 Batch: 700
Training Loss: 0.6761667203903198
Epoch: 41 Batch: 750
Training Loss: 0.6006114584207535
Epoch: 41 Batch: 800
Training Loss: 0.6178840982913971
Epoch: 41 Batch: 850
Training Loss: 0.6113924318552018
Epoch: 41 Batch: 900
Training Loss: 0.6081219488382339
Epoch: 41 Batch: 950
Training Loss: 0.6117157918214798
Epoch: 41 Batch: 1000
Training Loss: 0.645552186369896
Epoch: 41 Batch: 1050
Training Loss: 0.6314749592542648
Epoch: 41 Batch: 1100
Training Loss: 0.6153947526216507
Epoch: 41 Batch: 1150
Training Loss: 0.6340993797779083
Epoch: 41 Batch: 1200
Training Loss: 0.6400839591026306
Epoch: 41 Batch: 1250
Training Loss: 0.5996497160196305
Epoch: 41 Batch: 1300
Training Loss: 0.6359062379598618
Epoch: 41 Batch: 1350
Training Loss: 0.6332091516256333
Epoch: 41 Batch: 1400
Training Loss: 0.6132658690214157
Epoch: 41 Batch: 1450
Training Loss: 0.6000277453660965
Epoch: 41 Batch: 1500
Training Loss: 0.6469106620550156
Epoch: 41 Batch: 1550
Training Loss: 0.6244697427749634
Epoch: 41 Batch: 1600
Training Loss: 0.6095959287881851
Epoch: 41 Batch: 1650
Training Loss: 0.6083882963657379
Epoch: 41 Batch: 1700
Training Loss: 0.6191087996959687
Epoch: 41 Batch: 1750
Training Loss: 0.6560171580314637
Epoch: 41 Batch: 1800
Training Loss: 0.6137999761104583
Epoch: 41 Batch: 1850
Training Loss: 0.6113367569446564
Epoch: 41 Batch: 1900
Training Loss: 0.6506037682294845
Epoch: 41 Batch: 1950
Training Loss: 0.6308097350597381
Epoch: 41 Batch: 2000
Training Loss: 0.6094162690639496
Epoch: 41 Batch: 2050
Training Loss: 0.6015162873268127
Epoch: 41 Batch: 2100
Training Loss: 0.6457705187797547
Epoch: 41 Batch: 2150
Training Loss: 0.6293564093112946
Epoch: 41 Batch: 2200
Training Loss: 0.6347368317842483
Epoch: 41 Batch: 2250
Training Loss: 0.6036609417200088
Epoch: 41 Batch: 2300
Training Loss: 0.6010155475139618
Epoch: 41 Batch: 2350
Training Loss: 0.6167250514030457
Epoch: 41 Batch: 2400
Training Loss: 0.6137716847658158
Epoch: 41 Batch: 2450
Training Loss: 0.6089472377300262
Epoch: 41 Batch: 2500
Training Loss: 0.6242669886350631
Epoch: 41 Batch: 2550
Training Loss: 0.658163143992424
Epoch: 41 Batch: 2600
Training Loss: 0.6368320196866989
Epoch: 41 Batch: 2650
Training Loss: 0.6353165149688721
Epoch: 41 Batch: 2700
Training Loss: 0.619736739397049
Epoch: 41 Batch: 2750
Training Loss: 0.6475041800737381
Epoch: 41 Batch: 2800
Training Loss: 0.6191596472263337
Epoch: 41 Batch: 2850
Training Loss: 0.6534392356872558
Epoch: 41 Batch: 2900
Training Loss: 0.6368615263700486
Epoch: 41 Batch: 2950
Training Loss: 0.6292711287736893
Epoch: 41 Batch: 3000
Training Loss: 0.6033991241455078
Epoch: 41 Batch: 3050
Training Loss: 0.6266274714469909
Epoch: 41 Batch: 3100
Training Loss: 0.6120020687580109
Epoch: 41 Batch: 3150
Training Loss: 0.6264222824573517
Epoch: 41 Batch: 3200
Training Loss: 0.6381982064247131
Epoch: 41 Batch: 3250
Training Loss: 0.6173256188631058
Epoch: 41 Batch: 3300
Training Loss: 0.6391191726922989
Epoch: 41 Batch: 3350
Training Loss: 0.6543850338459015
Epoch: 41 Batch: 3400
Training Loss: 0.6153406041860581
Epoch: 41 Batch: 3450
Training Loss: 0.6274075090885163
Epoch: 41 Batch: 3500
Training Loss: 0.6130225414037704
Epoch: 42 
 Validation Loss: 0.53837707804309
---------------------------
Epoch: 42 Batch: 50
Training Loss: 0.6322337925434113
Epoch: 42 Batch: 100
Training Loss: 0.6347378873825074
Epoch: 42 Batch: 150
Training Loss: 0.6216635370254516
Epoch: 42 Batch: 200
Training Loss: 0.5987773633003235
Epoch: 42 Batch: 250
Training Loss: 0.6066321861743927
Epoch: 42 Batch: 300
Training Loss: 0.6638131994009018
Epoch: 42 Batch: 350
Training Loss: 0.6148429560661316
Epoch: 42 Batch: 400
Training Loss: 0.6166982823610305
Epoch: 42 Batch: 450
Training Loss: 0.6501223129034043
Epoch: 42 Batch: 500
Training Loss: 0.6196241515874863
Epoch: 42 Batch: 550
Training Loss: 0.6204646766185761
Epoch: 42 Batch: 600
Training Loss: 0.5890467584133148
Epoch: 42 Batch: 650
Training Loss: 0.6279397481679916
Epoch: 42 Batch: 700
Training Loss: 0.6334843987226486
Epoch: 42 Batch: 750
Training Loss: 0.643247030377388
Epoch: 42 Batch: 800
Training Loss: 0.6192941194772721
Epoch: 42 Batch: 850
Training Loss: 0.6175614124536515
Epoch: 42 Batch: 900
Training Loss: 0.6178108817338943
Epoch: 42 Batch: 950
Training Loss: 0.6223164999485016
Epoch: 42 Batch: 1000
Training Loss: 0.6385643357038497
Epoch: 42 Batch: 1050
Training Loss: 0.5940091598033905
Epoch: 42 Batch: 1100
Training Loss: 0.6479633283615113
Epoch: 42 Batch: 1150
Training Loss: 0.6324522864818573
Epoch: 42 Batch: 1200
Training Loss: 0.6706237107515335
Epoch: 42 Batch: 1250
Training Loss: 0.6149018359184265
Epoch: 42 Batch: 1300
Training Loss: 0.6216173219680786
Epoch: 42 Batch: 1350
Training Loss: 0.6172634947299958
Epoch: 42 Batch: 1400
Training Loss: 0.605777867436409
Epoch: 42 Batch: 1450
Training Loss: 0.624900552034378
Epoch: 42 Batch: 1500
Training Loss: 0.6391092443466186
Epoch: 42 Batch: 1550
Training Loss: 0.6420047426223755
Epoch: 42 Batch: 1600
Training Loss: 0.6169523978233338
Epoch: 42 Batch: 1650
Training Loss: 0.6342149502038956
Epoch: 42 Batch: 1700
Training Loss: 0.6170040023326874
Epoch: 42 Batch: 1750
Training Loss: 0.6214471757411957
Epoch: 42 Batch: 1800
Training Loss: 0.5930567145347595
Epoch: 42 Batch: 1850
Training Loss: 0.6554615366458892
Epoch: 42 Batch: 1900
Training Loss: 0.6309996557235718
Epoch: 42 Batch: 1950
Training Loss: 0.6275983422994613
Epoch: 42 Batch: 2000
Training Loss: 0.6235729420185089
Epoch: 42 Batch: 2050
Training Loss: 0.605840579867363
Epoch: 42 Batch: 2100
Training Loss: 0.6605897825956345
Epoch: 42 Batch: 2150
Training Loss: 0.6286711657047271
Epoch: 42 Batch: 2200
Training Loss: 0.619605987071991
Epoch: 42 Batch: 2250
Training Loss: 0.6363701117038727
Epoch: 42 Batch: 2300
Training Loss: 0.6120643037557602
Epoch: 42 Batch: 2350
Training Loss: 0.612518390417099
Epoch: 42 Batch: 2400
Training Loss: 0.6188297414779663
Epoch: 42 Batch: 2450
Training Loss: 0.6297747969627381
Epoch: 42 Batch: 2500
Training Loss: 0.6165446966886521
Epoch: 42 Batch: 2550
Training Loss: 0.6182370835542679
Epoch: 42 Batch: 2600
Training Loss: 0.6252190124988556
Epoch: 42 Batch: 2650
Training Loss: 0.6255082225799561
Epoch: 42 Batch: 2700
Training Loss: 0.6025648993253708
Epoch: 42 Batch: 2750
Training Loss: 0.6302688580751419
Epoch: 42 Batch: 2800
Training Loss: 0.6196618914604187
Epoch: 42 Batch: 2850
Training Loss: 0.6040442115068436
Epoch: 42 Batch: 2900
Training Loss: 0.6181689602136612
Epoch: 42 Batch: 2950
Training Loss: 0.5888722306489944
Epoch: 42 Batch: 3000
Training Loss: 0.59604787170887
Epoch: 42 Batch: 3050
Training Loss: 0.6300786155462265
Epoch: 42 Batch: 3100
Training Loss: 0.6309914892911911
Epoch: 42 Batch: 3150
Training Loss: 0.6065800225734711
Epoch: 42 Batch: 3200
Training Loss: 0.6443330204486847
Epoch: 42 Batch: 3250
Training Loss: 0.616308889389038
Epoch: 42 Batch: 3300
Training Loss: 0.6403147977590561
Epoch: 42 Batch: 3350
Training Loss: 0.615436304807663
Epoch: 42 Batch: 3400
Training Loss: 0.6099501091241837
Epoch: 42 Batch: 3450
Training Loss: 0.6140797907114028
Epoch: 42 Batch: 3500
Training Loss: 0.6267732530832291
Epoch: 43 
 Validation Loss: 0.5379840781291326
---------------------------
Epoch: 43 Batch: 50
Training Loss: 0.6035993617773056
Epoch: 43 Batch: 100
Training Loss: 0.6136728030443191
Epoch: 43 Batch: 150
Training Loss: 0.6598941087722778
Epoch: 43 Batch: 200
Training Loss: 0.6113104629516601
Epoch: 43 Batch: 250
Training Loss: 0.6150492227077484
Epoch: 43 Batch: 300
Training Loss: 0.6113208681344986
Epoch: 43 Batch: 350
Training Loss: 0.5852921479940414
Epoch: 43 Batch: 400
Training Loss: 0.6208113712072373
Epoch: 43 Batch: 450
Training Loss: 0.5878912049531937
Epoch: 43 Batch: 500
Training Loss: 0.6316233366727829
Epoch: 43 Batch: 550
Training Loss: 0.6321857243776321
Epoch: 43 Batch: 600
Training Loss: 0.6561066973209381
Epoch: 43 Batch: 650
Training Loss: 0.6399863487482071
Epoch: 43 Batch: 700
Training Loss: 0.6259967523813248
Epoch: 43 Batch: 750
Training Loss: 0.6092787706851959
Epoch: 43 Batch: 800
Training Loss: 0.6314499992132186
Epoch: 43 Batch: 850
Training Loss: 0.6119968402385711
Epoch: 43 Batch: 900
Training Loss: 0.6413207006454468
Epoch: 43 Batch: 950
Training Loss: 0.6311438155174255
Epoch: 43 Batch: 1000
Training Loss: 0.6178170657157898
Epoch: 43 Batch: 1050
Training Loss: 0.6307164841890335
Epoch: 43 Batch: 1100
Training Loss: 0.6132019966840744
Epoch: 43 Batch: 1150
Training Loss: 0.6374120616912842
Epoch: 43 Batch: 1200
Training Loss: 0.6090610778331756
Epoch: 43 Batch: 1250
Training Loss: 0.6252007222175598
Epoch: 43 Batch: 1300
Training Loss: 0.624027761220932
Epoch: 43 Batch: 1350
Training Loss: 0.6408659672737121
Epoch: 43 Batch: 1400
Training Loss: 0.5878247272968292
Epoch: 43 Batch: 1450
Training Loss: 0.6227329421043396
Epoch: 43 Batch: 1500
Training Loss: 0.5966259837150574
Epoch: 43 Batch: 1550
Training Loss: 0.6259995049238205
Epoch: 43 Batch: 1600
Training Loss: 0.6152445358037949
Epoch: 43 Batch: 1650
Training Loss: 0.623118240237236
Epoch: 43 Batch: 1700
Training Loss: 0.5893465960025788
Epoch: 43 Batch: 1750
Training Loss: 0.6329100030660629
Epoch: 43 Batch: 1800
Training Loss: 0.6012593561410904
Epoch: 43 Batch: 1850
Training Loss: 0.6299902009963989
Epoch: 43 Batch: 1900
Training Loss: 0.6216951143741608
Epoch: 43 Batch: 1950
Training Loss: 0.6214429306983947
Epoch: 43 Batch: 2000
Training Loss: 0.6264078062772751
Epoch: 43 Batch: 2050
Training Loss: 0.6299871289730072
Epoch: 43 Batch: 2100
Training Loss: 0.6305256921052933
Epoch: 43 Batch: 2150
Training Loss: 0.6187738186120987
Epoch: 43 Batch: 2200
Training Loss: 0.6398273950815201
Epoch: 43 Batch: 2250
Training Loss: 0.5991987133026123
Epoch: 43 Batch: 2300
Training Loss: 0.6153987634181977
Epoch: 43 Batch: 2350
Training Loss: 0.5710933721065521
Epoch: 43 Batch: 2400
Training Loss: 0.6015256577730179
Epoch: 43 Batch: 2450
Training Loss: 0.6185751730203628
Epoch: 43 Batch: 2500
Training Loss: 0.6378468608856201
Epoch: 43 Batch: 2550
Training Loss: 0.6041025578975677
Epoch: 43 Batch: 2600
Training Loss: 0.61538822889328
Epoch: 43 Batch: 2650
Training Loss: 0.6012329632043838
Epoch: 43 Batch: 2700
Training Loss: 0.6193218636512756
Epoch: 43 Batch: 2750
Training Loss: 0.6201327699422836
Epoch: 43 Batch: 2800
Training Loss: 0.6511469465494156
Epoch: 43 Batch: 2850
Training Loss: 0.6188966530561447
Epoch: 43 Batch: 2900
Training Loss: 0.6121846598386764
Epoch: 43 Batch: 2950
Training Loss: 0.6195239371061325
Epoch: 43 Batch: 3000
Training Loss: 0.6091577166318893
Epoch: 43 Batch: 3050
Training Loss: 0.6263644272089004
Epoch: 43 Batch: 3100
Training Loss: 0.6187896400690078
Epoch: 43 Batch: 3150
Training Loss: 0.6348010993003845
Epoch: 43 Batch: 3200
Training Loss: 0.6001814377307891
Epoch: 43 Batch: 3250
Training Loss: 0.6152546203136444
Epoch: 43 Batch: 3300
Training Loss: 0.6040861564874649
Epoch: 43 Batch: 3350
Training Loss: 0.6352597177028656
Epoch: 43 Batch: 3400
Training Loss: 0.6332676702737808
Epoch: 43 Batch: 3450
Training Loss: 0.6202093583345413
Epoch: 43 Batch: 3500
Training Loss: 0.6214451617002488
Epoch: 44 
 Validation Loss: 0.5371158967415491
---------------------------
Epoch: 44 Batch: 50
Training Loss: 0.6572229105234146
Epoch: 44 Batch: 100
Training Loss: 0.5873788291215897
Epoch: 44 Batch: 150
Training Loss: 0.6351166206598282
Epoch: 44 Batch: 200
Training Loss: 0.6136861312389373
Epoch: 44 Batch: 250
Training Loss: 0.6162510788440705
Epoch: 44 Batch: 300
Training Loss: 0.611999973654747
Epoch: 44 Batch: 350
Training Loss: 0.6393881857395172
Epoch: 44 Batch: 400
Training Loss: 0.6127592319250107
Epoch: 44 Batch: 450
Training Loss: 0.6150366413593292
Epoch: 44 Batch: 500
Training Loss: 0.623513303399086
Epoch: 44 Batch: 550
Training Loss: 0.6218746894598007
Epoch: 44 Batch: 600
Training Loss: 0.6023950028419495
Epoch: 44 Batch: 650
Training Loss: 0.6720423233509064
Epoch: 44 Batch: 700
Training Loss: 0.6546439397335052
Epoch: 44 Batch: 750
Training Loss: 0.6089984840154647
Epoch: 44 Batch: 800
Training Loss: 0.6216381233930588
Epoch: 44 Batch: 850
Training Loss: 0.6193850362300872
Epoch: 44 Batch: 900
Training Loss: 0.6344976729154587
Epoch: 44 Batch: 950
Training Loss: 0.6152549254894256
Epoch: 44 Batch: 1000
Training Loss: 0.6361065924167633
Epoch: 44 Batch: 1050
Training Loss: 0.6111626607179642
Epoch: 44 Batch: 1100
Training Loss: 0.6204942923784256
Epoch: 44 Batch: 1150
Training Loss: 0.6496834290027619
Epoch: 44 Batch: 1200
Training Loss: 0.6142395913600922
Epoch: 44 Batch: 1250
Training Loss: 0.6104983937740326
Epoch: 44 Batch: 1300
Training Loss: 0.6232508772611618
Epoch: 44 Batch: 1350
Training Loss: 0.6247502076625824
Epoch: 44 Batch: 1400
Training Loss: 0.6351044690608978
Epoch: 44 Batch: 1450
Training Loss: 0.6207223761081696
Epoch: 44 Batch: 1500
Training Loss: 0.6392614090442658
Epoch: 44 Batch: 1550
Training Loss: 0.6279527807235717
Epoch: 44 Batch: 1600
Training Loss: 0.5975936788320542
Epoch: 44 Batch: 1650
Training Loss: 0.6319733542203904
Epoch: 44 Batch: 1700
Training Loss: 0.6150003117322922
Epoch: 44 Batch: 1750
Training Loss: 0.614666286110878
Epoch: 44 Batch: 1800
Training Loss: 0.6411381888389588
Epoch: 44 Batch: 1850
Training Loss: 0.6041631931066513
Epoch: 44 Batch: 1900
Training Loss: 0.6476290112733841
Epoch: 44 Batch: 1950
Training Loss: 0.636290368437767
Epoch: 44 Batch: 2000
Training Loss: 0.6174213498830795
Epoch: 44 Batch: 2050
Training Loss: 0.5938005822896958
Epoch: 44 Batch: 2100
Training Loss: 0.6394008070230484
Epoch: 44 Batch: 2150
Training Loss: 0.6032881313562393
Epoch: 44 Batch: 2200
Training Loss: 0.6145818966627121
Epoch: 44 Batch: 2250
Training Loss: 0.6298369812965393
Epoch: 44 Batch: 2300
Training Loss: 0.6555855518579483
Epoch: 44 Batch: 2350
Training Loss: 0.6216397279500961
Epoch: 44 Batch: 2400
Training Loss: 0.6008783513307572
Epoch: 44 Batch: 2450
Training Loss: 0.5870007646083831
Epoch: 44 Batch: 2500
Training Loss: 0.616637436747551
Epoch: 44 Batch: 2550
Training Loss: 0.6338919699192047
Epoch: 44 Batch: 2600
Training Loss: 0.598986246585846
Epoch: 44 Batch: 2650
Training Loss: 0.6343499249219895
Epoch: 44 Batch: 2700
Training Loss: 0.5931699341535568
Epoch: 44 Batch: 2750
Training Loss: 0.6120548552274704
Epoch: 44 Batch: 2800
Training Loss: 0.6351121640205384
Epoch: 44 Batch: 2850
Training Loss: 0.6570655155181885
Epoch: 44 Batch: 2900
Training Loss: 0.611329174041748
Epoch: 44 Batch: 2950
Training Loss: 0.5999219858646393
Epoch: 44 Batch: 3000
Training Loss: 0.6100175958871842
Epoch: 44 Batch: 3050
Training Loss: 0.5965305817127228
Epoch: 44 Batch: 3100
Training Loss: 0.6223906934261322
Epoch: 44 Batch: 3150
Training Loss: 0.6237969392538071
Epoch: 44 Batch: 3200
Training Loss: 0.6035084921121597
Epoch: 44 Batch: 3250
Training Loss: 0.6283850461244583
Epoch: 44 Batch: 3300
Training Loss: 0.6066707462072373
Epoch: 44 Batch: 3350
Training Loss: 0.6304915857315063
Epoch: 44 Batch: 3400
Training Loss: 0.6059891021251679
Epoch: 44 Batch: 3450
Training Loss: 0.6315514594316483
Epoch: 44 Batch: 3500
Training Loss: 0.5930690944194794
Epoch: 45 
 Validation Loss: 0.5373042043712404
---------------------------
Epoch: 45 Batch: 50
Training Loss: 0.5990762203931809
Epoch: 45 Batch: 100
Training Loss: 0.6419635599851609
Epoch: 45 Batch: 150
Training Loss: 0.651133120059967
Epoch: 45 Batch: 200
Training Loss: 0.6230650973320008
Epoch: 45 Batch: 250
Training Loss: 0.5942285412549972
Epoch: 45 Batch: 300
Training Loss: 0.6054604291915894
Epoch: 45 Batch: 350
Training Loss: 0.5778046983480454
Epoch: 45 Batch: 400
Training Loss: 0.6362172496318818
Epoch: 45 Batch: 450
Training Loss: 0.6323041445016861
Epoch: 45 Batch: 500
Training Loss: 0.60219018638134
Epoch: 45 Batch: 550
Training Loss: 0.6053510743379593
Epoch: 45 Batch: 600
Training Loss: 0.6183314722776413
Epoch: 45 Batch: 650
Training Loss: 0.611405319571495
Epoch: 45 Batch: 700
Training Loss: 0.6223162317276001
Epoch: 45 Batch: 750
Training Loss: 0.641366645693779
Epoch: 45 Batch: 800
Training Loss: 0.6207004380226135
Epoch: 45 Batch: 850
Training Loss: 0.6222318190336228
Epoch: 45 Batch: 900
Training Loss: 0.6764644432067871
Epoch: 45 Batch: 950
Training Loss: 0.6191566318273545
Epoch: 45 Batch: 1000
Training Loss: 0.6218157351016999
Epoch: 45 Batch: 1050
Training Loss: 0.6297615259885788
Epoch: 45 Batch: 1100
Training Loss: 0.6093341821432113
Epoch: 45 Batch: 1150
Training Loss: 0.6402022790908813
Epoch: 45 Batch: 1200
Training Loss: 0.5984354990720749
Epoch: 45 Batch: 1250
Training Loss: 0.616939160823822
Epoch: 45 Batch: 1300
Training Loss: 0.6225965338945388
Epoch: 45 Batch: 1350
Training Loss: 0.6440357190370559
Epoch: 45 Batch: 1400
Training Loss: 0.5984193247556686
Epoch: 45 Batch: 1450
Training Loss: 0.6274848175048828
Epoch: 45 Batch: 1500
Training Loss: 0.6399353665113449
Epoch: 45 Batch: 1550
Training Loss: 0.6298535388708114
Epoch: 45 Batch: 1600
Training Loss: 0.6077045756578445
Epoch: 45 Batch: 1650
Training Loss: 0.6135444605350494
Epoch: 45 Batch: 1700
Training Loss: 0.6292183023691177
Epoch: 45 Batch: 1750
Training Loss: 0.6086514979600907
Epoch: 45 Batch: 1800
Training Loss: 0.6332827758789062
Epoch: 45 Batch: 1850
Training Loss: 0.6139048969745636
Epoch: 45 Batch: 1900
Training Loss: 0.6206642705202102
Epoch: 45 Batch: 1950
Training Loss: 0.5809511810541153
Epoch: 45 Batch: 2000
Training Loss: 0.6237251567840576
Epoch: 45 Batch: 2050
Training Loss: 0.6226307517290115
Epoch: 45 Batch: 2100
Training Loss: 0.5964876073598862
Epoch: 45 Batch: 2150
Training Loss: 0.6200461208820343
Epoch: 45 Batch: 2200
Training Loss: 0.6075661319494248
Epoch: 45 Batch: 2250
Training Loss: 0.6073243725299835
Epoch: 45 Batch: 2300
Training Loss: 0.608008564710617
Epoch: 45 Batch: 2350
Training Loss: 0.6169338625669479
Epoch: 45 Batch: 2400
Training Loss: 0.6474465179443359
Epoch: 45 Batch: 2450
Training Loss: 0.6446388202905655
Epoch: 45 Batch: 2500
Training Loss: 0.606765348315239
Epoch: 45 Batch: 2550
Training Loss: 0.6338539481163025
Epoch: 45 Batch: 2600
Training Loss: 0.6256400245428085
Epoch: 45 Batch: 2650
Training Loss: 0.6095841079950333
Epoch: 45 Batch: 2700
Training Loss: 0.6134103673696518
Epoch: 45 Batch: 2750
Training Loss: 0.6279139637947082
Epoch: 45 Batch: 2800
Training Loss: 0.6263438725471496
Epoch: 45 Batch: 2850
Training Loss: 0.6071963459253311
Epoch: 45 Batch: 2900
Training Loss: 0.6280094331502915
Epoch: 45 Batch: 2950
Training Loss: 0.6093553858995437
Epoch: 45 Batch: 3000
Training Loss: 0.6179546016454697
Epoch: 45 Batch: 3050
Training Loss: 0.601593896150589
Epoch: 45 Batch: 3100
Training Loss: 0.6263561689853668
Epoch: 45 Batch: 3150
Training Loss: 0.6268697786331177
Epoch: 45 Batch: 3200
Training Loss: 0.616866569519043
Epoch: 45 Batch: 3250
Training Loss: 0.6221123671531678
Epoch: 45 Batch: 3300
Training Loss: 0.6206714695692063
Epoch: 45 Batch: 3350
Training Loss: 0.6026614862680435
Epoch: 45 Batch: 3400
Training Loss: 0.6098289453983307
Epoch: 45 Batch: 3450
Training Loss: 0.6233332532644272
Epoch: 45 Batch: 3500
Training Loss: 0.6172713309526443
Epoch: 46 
 Validation Loss: 0.5364826089806027
---------------------------
Epoch: 46 Batch: 50
Training Loss: 0.6390556114912033
Epoch: 46 Batch: 100
Training Loss: 0.6552672636508942
Epoch: 46 Batch: 150
Training Loss: 0.6327990454435348
Epoch: 46 Batch: 200
Training Loss: 0.6216400015354157
Epoch: 46 Batch: 250
Training Loss: 0.6253811424970627
Epoch: 46 Batch: 300
Training Loss: 0.6419191360473633
Epoch: 46 Batch: 350
Training Loss: 0.6260580468177795
Epoch: 46 Batch: 400
Training Loss: 0.621714535355568
Epoch: 46 Batch: 450
Training Loss: 0.6241732841730118
Epoch: 46 Batch: 500
Training Loss: 0.6370736944675446
Epoch: 46 Batch: 550
Training Loss: 0.6433950912952423
Epoch: 46 Batch: 600
Training Loss: 0.6031176835298538
Epoch: 46 Batch: 650
Training Loss: 0.6569775038957596
Epoch: 46 Batch: 700
Training Loss: 0.6221943819522857
Epoch: 46 Batch: 750
Training Loss: 0.6364741170406342
Epoch: 46 Batch: 800
Training Loss: 0.6321301460266113
Epoch: 46 Batch: 850
Training Loss: 0.6292415004968643
Epoch: 46 Batch: 900
Training Loss: 0.6195629900693893
Epoch: 46 Batch: 950
Training Loss: 0.6208988517522812
Epoch: 46 Batch: 1000
Training Loss: 0.6089649248123169
Epoch: 46 Batch: 1050
Training Loss: 0.6189711028337479
Epoch: 46 Batch: 1100
Training Loss: 0.624656749367714
Epoch: 46 Batch: 1150
Training Loss: 0.6444681853055954
Epoch: 46 Batch: 1200
Training Loss: 0.6365978616476059
Epoch: 46 Batch: 1250
Training Loss: 0.6383346170186996
Epoch: 46 Batch: 1300
Training Loss: 0.626450686454773
Epoch: 46 Batch: 1350
Training Loss: 0.6254832834005356
Epoch: 46 Batch: 1400
Training Loss: 0.6123929053544999
Epoch: 46 Batch: 1450
Training Loss: 0.5646669340133667
Epoch: 46 Batch: 1500
Training Loss: 0.6392786753177643
Epoch: 46 Batch: 1550
Training Loss: 0.6311717522144318
Epoch: 46 Batch: 1600
Training Loss: 0.6167154741287232
Epoch: 46 Batch: 1650
Training Loss: 0.6576148211956024
Epoch: 46 Batch: 1700
Training Loss: 0.630510715842247
Epoch: 46 Batch: 1750
Training Loss: 0.6316497761011124
Epoch: 46 Batch: 1800
Training Loss: 0.6206499475240708
Epoch: 46 Batch: 1850
Training Loss: 0.6094547963142395
Epoch: 46 Batch: 1900
Training Loss: 0.6143484365940094
Epoch: 46 Batch: 1950
Training Loss: 0.6307195311784745
Epoch: 46 Batch: 2000
Training Loss: 0.6662316960096359
Epoch: 46 Batch: 2050
Training Loss: 0.6263005864620209
Epoch: 46 Batch: 2100
Training Loss: 0.6122782272100449
Epoch: 46 Batch: 2150
Training Loss: 0.5795517462491989
Epoch: 46 Batch: 2200
Training Loss: 0.5798443704843521
Epoch: 46 Batch: 2250
Training Loss: 0.6158673268556595
Epoch: 46 Batch: 2300
Training Loss: 0.6247385901212692
Epoch: 46 Batch: 2350
Training Loss: 0.6121636003255844
Epoch: 46 Batch: 2400
Training Loss: 0.6317086035013199
Epoch: 46 Batch: 2450
Training Loss: 0.6098356086015702
Epoch: 46 Batch: 2500
Training Loss: 0.6124382764101028
Epoch: 46 Batch: 2550
Training Loss: 0.6161951619386673
Epoch: 46 Batch: 2600
Training Loss: 0.6013988775014877
Epoch: 46 Batch: 2650
Training Loss: 0.6114049905538559
Epoch: 46 Batch: 2700
Training Loss: 0.5923290514945984
Epoch: 46 Batch: 2750
Training Loss: 0.6044029867649079
Epoch: 46 Batch: 2800
Training Loss: 0.6332282567024231
Epoch: 46 Batch: 2850
Training Loss: 0.6209700733423233
Epoch: 46 Batch: 2900
Training Loss: 0.59651780128479
Epoch: 46 Batch: 2950
Training Loss: 0.6218011599779129
Epoch: 46 Batch: 3000
Training Loss: 0.635169872045517
Epoch: 46 Batch: 3050
Training Loss: 0.6227064192295074
Epoch: 46 Batch: 3100
Training Loss: 0.5932877320051193
Epoch: 46 Batch: 3150
Training Loss: 0.5876272028684616
Epoch: 46 Batch: 3200
Training Loss: 0.6193318009376526
Epoch: 46 Batch: 3250
Training Loss: 0.6283466446399689
Epoch: 46 Batch: 3300
Training Loss: 0.608111725449562
Epoch: 46 Batch: 3350
Training Loss: 0.6413667374849319
Epoch: 46 Batch: 3400
Training Loss: 0.612650665640831
Epoch: 46 Batch: 3450
Training Loss: 0.6353942281007767
Epoch: 46 Batch: 3500
Training Loss: 0.6148764258623123
Epoch: 47 
 Validation Loss: 0.5360231565104591
---------------------------
Epoch: 47 Batch: 50
Training Loss: 0.6405585151910782
Epoch: 47 Batch: 100
Training Loss: 0.5959195649623871
Epoch: 47 Batch: 150
Training Loss: 0.5886113584041596
Epoch: 47 Batch: 200
Training Loss: 0.6312082952260971
Epoch: 47 Batch: 250
Training Loss: 0.6094344598054886
Epoch: 47 Batch: 300
Training Loss: 0.5922933262586594
Epoch: 47 Batch: 350
Training Loss: 0.6187853741645813
Epoch: 47 Batch: 400
Training Loss: 0.6186856526136398
Epoch: 47 Batch: 450
Training Loss: 0.6322548681497574
Epoch: 47 Batch: 500
Training Loss: 0.6221018922328949
Epoch: 47 Batch: 550
Training Loss: 0.6353450679779052
Epoch: 47 Batch: 600
Training Loss: 0.6201031190156937
Epoch: 47 Batch: 650
Training Loss: 0.6105996406078339
Epoch: 47 Batch: 700
Training Loss: 0.6177468657493591
Epoch: 47 Batch: 750
Training Loss: 0.5953756469488144
Epoch: 47 Batch: 800
Training Loss: 0.6189794594049454
Epoch: 47 Batch: 850
Training Loss: 0.629210141301155
Epoch: 47 Batch: 900
Training Loss: 0.6051938772201538
Epoch: 47 Batch: 950
Training Loss: 0.6468489319086075
Epoch: 47 Batch: 1000
Training Loss: 0.6240861767530441
Epoch: 47 Batch: 1050
Training Loss: 0.6308737760782241
Epoch: 47 Batch: 1100
Training Loss: 0.6112461340427399
Epoch: 47 Batch: 1150
Training Loss: 0.6226063328981399
Epoch: 47 Batch: 1200
Training Loss: 0.5960016620159149
Epoch: 47 Batch: 1250
Training Loss: 0.6417675399780274
Epoch: 47 Batch: 1300
Training Loss: 0.6291417211294175
Epoch: 47 Batch: 1350
Training Loss: 0.6116291487216949
Epoch: 47 Batch: 1400
Training Loss: 0.598829106092453
Epoch: 47 Batch: 1450
Training Loss: 0.6371051013469696
Epoch: 47 Batch: 1500
Training Loss: 0.6231509798765182
Epoch: 47 Batch: 1550
Training Loss: 0.6345326673984527
Epoch: 47 Batch: 1600
Training Loss: 0.6405946397781372
Epoch: 47 Batch: 1650
Training Loss: 0.6294836574792861
Epoch: 47 Batch: 1700
Training Loss: 0.6326921051740646
Epoch: 47 Batch: 1750
Training Loss: 0.640786612033844
Epoch: 47 Batch: 1800
Training Loss: 0.6338083946704864
Epoch: 47 Batch: 1850
Training Loss: 0.6153308480978013
Epoch: 47 Batch: 1900
Training Loss: 0.6026628828048706
Epoch: 47 Batch: 1950
Training Loss: 0.5992076337337494
Epoch: 47 Batch: 2000
Training Loss: 0.6154798835515976
Epoch: 47 Batch: 2050
Training Loss: 0.6037919747829438
Epoch: 47 Batch: 2100
Training Loss: 0.5763580673933029
Epoch: 47 Batch: 2150
Training Loss: 0.6286494755744934
Epoch: 47 Batch: 2200
Training Loss: 0.6061171704530716
Epoch: 47 Batch: 2250
Training Loss: 0.6236316585540771
Epoch: 47 Batch: 2300
Training Loss: 0.6347369015216827
Epoch: 47 Batch: 2350
Training Loss: 0.6316605830192565
Epoch: 47 Batch: 2400
Training Loss: 0.6288449621200561
Epoch: 47 Batch: 2450
Training Loss: 0.6015861415863037
Epoch: 47 Batch: 2500
Training Loss: 0.637691221833229
Epoch: 47 Batch: 2550
Training Loss: 0.6355015367269516
Epoch: 47 Batch: 2600
Training Loss: 0.5901532232761383
Epoch: 47 Batch: 2650
Training Loss: 0.6080001223087311
Epoch: 47 Batch: 2700
Training Loss: 0.5980541259050369
Epoch: 47 Batch: 2750
Training Loss: 0.5992267203330993
Epoch: 47 Batch: 2800
Training Loss: 0.6233934772014618
Epoch: 47 Batch: 2850
Training Loss: 0.6212299251556397
Epoch: 47 Batch: 2900
Training Loss: 0.6461893701553345
Epoch: 47 Batch: 2950
Training Loss: 0.6394037419557571
Epoch: 47 Batch: 3000
Training Loss: 0.6260052609443665
Epoch: 47 Batch: 3050
Training Loss: 0.6207265412807464
Epoch: 47 Batch: 3100
Training Loss: 0.5937834626436234
Epoch: 47 Batch: 3150
Training Loss: 0.630566548705101
Epoch: 47 Batch: 3200
Training Loss: 0.6453331488370896
Epoch: 47 Batch: 3250
Training Loss: 0.6470597124099732
Epoch: 47 Batch: 3300
Training Loss: 0.6138409072160721
Epoch: 47 Batch: 3350
Training Loss: 0.6138232725858689
Epoch: 47 Batch: 3400
Training Loss: 0.5959277206659317
Epoch: 47 Batch: 3450
Training Loss: 0.6361055314540863
Epoch: 47 Batch: 3500
Training Loss: 0.6239009416103363
Epoch: 48 
 Validation Loss: 0.5350744989183214
---------------------------
Epoch: 48 Batch: 50
Training Loss: 0.6155416309833527
Epoch: 48 Batch: 100
Training Loss: 0.5978165811300278
Epoch: 48 Batch: 150
Training Loss: 0.6566808634996414
Epoch: 48 Batch: 200
Training Loss: 0.5943316090106964
Epoch: 48 Batch: 250
Training Loss: 0.6439130771160125
Epoch: 48 Batch: 300
Training Loss: 0.6291643691062927
Epoch: 48 Batch: 350
Training Loss: 0.6368998032808304
Epoch: 48 Batch: 400
Training Loss: 0.6173977708816528
Epoch: 48 Batch: 450
Training Loss: 0.5882985335588455
Epoch: 48 Batch: 500
Training Loss: 0.6033180940151215
Epoch: 48 Batch: 550
Training Loss: 0.6225511145591736
Epoch: 48 Batch: 600
Training Loss: 0.5923871654272079
Epoch: 48 Batch: 650
Training Loss: 0.6295447140932083
Epoch: 48 Batch: 700
Training Loss: 0.6350503975152969
Epoch: 48 Batch: 750
Training Loss: 0.6348811304569244
Epoch: 48 Batch: 800
Training Loss: 0.6071031153202057
Epoch: 48 Batch: 850
Training Loss: 0.6319418263435364
Epoch: 48 Batch: 900
Training Loss: 0.5889923250675202
Epoch: 48 Batch: 950
Training Loss: 0.5992769515514373
Epoch: 48 Batch: 1000
Training Loss: 0.5939198398590088
Epoch: 48 Batch: 1050
Training Loss: 0.6002665662765503
Epoch: 48 Batch: 1100
Training Loss: 0.60368821144104
Epoch: 48 Batch: 1150
Training Loss: 0.6181576937437058
Epoch: 48 Batch: 1200
Training Loss: 0.5966274732351303
Epoch: 48 Batch: 1250
Training Loss: 0.6159832519292832
Epoch: 48 Batch: 1300
Training Loss: 0.6458192420005798
Epoch: 48 Batch: 1350
Training Loss: 0.6200333362817765
Epoch: 48 Batch: 1400
Training Loss: 0.6103278756141662
Epoch: 48 Batch: 1450
Training Loss: 0.6105563336610794
Epoch: 48 Batch: 1500
Training Loss: 0.6302330964803695
Epoch: 48 Batch: 1550
Training Loss: 0.6059611058235168
Epoch: 48 Batch: 1600
Training Loss: 0.618623241186142
Epoch: 48 Batch: 1650
Training Loss: 0.6547762191295624
Epoch: 48 Batch: 1700
Training Loss: 0.6446830976009369
Epoch: 48 Batch: 1750
Training Loss: 0.5963748890161514
Epoch: 48 Batch: 1800
Training Loss: 0.6236613464355468
Epoch: 48 Batch: 1850
Training Loss: 0.6206371170282364
Epoch: 48 Batch: 1900
Training Loss: 0.6266884046792984
Epoch: 48 Batch: 1950
Training Loss: 0.6169980305433274
Epoch: 48 Batch: 2000
Training Loss: 0.5986511224508285
Epoch: 48 Batch: 2050
Training Loss: 0.623655168414116
Epoch: 48 Batch: 2100
Training Loss: 0.6417320656776428
Epoch: 48 Batch: 2150
Training Loss: 0.6137593913078309
Epoch: 48 Batch: 2200
Training Loss: 0.6122874426841736
Epoch: 48 Batch: 2250
Training Loss: 0.5952637994289398
Epoch: 48 Batch: 2300
Training Loss: 0.5872194993495942
Epoch: 48 Batch: 2350
Training Loss: 0.6303286784887314
Epoch: 48 Batch: 2400
Training Loss: 0.603651316165924
Epoch: 48 Batch: 2450
Training Loss: 0.6032020717859268
Epoch: 48 Batch: 2500
Training Loss: 0.6201714932918548
Epoch: 48 Batch: 2550
Training Loss: 0.619417941570282
Epoch: 48 Batch: 2600
Training Loss: 0.5892948824167251
Epoch: 48 Batch: 2650
Training Loss: 0.6097549438476563
Epoch: 48 Batch: 2700
Training Loss: 0.6288589853048324
Epoch: 48 Batch: 2750
Training Loss: 0.6198790609836579
Epoch: 48 Batch: 2800
Training Loss: 0.6094539546966553
Epoch: 48 Batch: 2850
Training Loss: 0.6292381757497787
Epoch: 48 Batch: 2900
Training Loss: 0.6093333196640015
Epoch: 48 Batch: 2950
Training Loss: 0.6048449075222015
Epoch: 48 Batch: 3000
Training Loss: 0.6433713138103485
Epoch: 48 Batch: 3050
Training Loss: 0.6173623883724213
Epoch: 48 Batch: 3100
Training Loss: 0.5835617500543594
Epoch: 48 Batch: 3150
Training Loss: 0.6338323932886124
Epoch: 48 Batch: 3200
Training Loss: 0.6184903657436371
Epoch: 48 Batch: 3250
Training Loss: 0.6065932065248489
Epoch: 48 Batch: 3300
Training Loss: 0.5956227397918701
Epoch: 48 Batch: 3350
Training Loss: 0.6376636958122254
Epoch: 48 Batch: 3400
Training Loss: 0.6230910444259643
Epoch: 48 Batch: 3450
Training Loss: 0.6187075209617615
Epoch: 48 Batch: 3500
Training Loss: 0.6265761941671372
Epoch: 49 
 Validation Loss: 0.5345137036508985
---------------------------
Epoch: 49 Batch: 50
Training Loss: 0.6685435163974762
Epoch: 49 Batch: 100
Training Loss: 0.6080345469713211
Epoch: 49 Batch: 150
Training Loss: 0.6329582363367081
Epoch: 49 Batch: 200
Training Loss: 0.6482722580432891
Epoch: 49 Batch: 250
Training Loss: 0.6053845292329788
Epoch: 49 Batch: 300
Training Loss: 0.6010398614406586
Epoch: 49 Batch: 350
Training Loss: 0.6360003703832626
Epoch: 49 Batch: 400
Training Loss: 0.6186156219244003
Epoch: 49 Batch: 450
Training Loss: 0.5969843566417694
Epoch: 49 Batch: 500
Training Loss: 0.5976671916246414
Epoch: 49 Batch: 550
Training Loss: 0.642783955335617
Epoch: 49 Batch: 600
Training Loss: 0.6396019095182419
Epoch: 49 Batch: 650
Training Loss: 0.6330198931694031
Epoch: 49 Batch: 700
Training Loss: 0.5975100880861283
Epoch: 49 Batch: 750
Training Loss: 0.6341762626171112
Epoch: 49 Batch: 800
Training Loss: 0.5821884554624558
Epoch: 49 Batch: 850
Training Loss: 0.6148240751028061
Epoch: 49 Batch: 900
Training Loss: 0.6168214106559753
Epoch: 49 Batch: 950
Training Loss: 0.6004188615083694
Epoch: 49 Batch: 1000
Training Loss: 0.6102580827474594
Epoch: 49 Batch: 1050
Training Loss: 0.6239270102977753
Epoch: 49 Batch: 1100
Training Loss: 0.607524841427803
Epoch: 49 Batch: 1150
Training Loss: 0.6130984985828399
Epoch: 49 Batch: 1200
Training Loss: 0.6172703135013581
Epoch: 49 Batch: 1250
Training Loss: 0.622946310043335
Epoch: 49 Batch: 1300
Training Loss: 0.6067621630430221
Epoch: 49 Batch: 1350
Training Loss: 0.5927862644195556
Epoch: 49 Batch: 1400
Training Loss: 0.6252265822887421
Epoch: 49 Batch: 1450
Training Loss: 0.63465786755085
Epoch: 49 Batch: 1500
Training Loss: 0.6169249874353409
Epoch: 49 Batch: 1550
Training Loss: 0.6260836845636368
Epoch: 49 Batch: 1600
Training Loss: 0.6350747764110565
Epoch: 49 Batch: 1650
Training Loss: 0.6267784059047699
Epoch: 49 Batch: 1700
Training Loss: 0.6230977445840835
Epoch: 49 Batch: 1750
Training Loss: 0.6455905693769455
Epoch: 49 Batch: 1800
Training Loss: 0.60494455575943
Epoch: 49 Batch: 1850
Training Loss: 0.6375510263442993
Epoch: 49 Batch: 1900
Training Loss: 0.6362570780515671
Epoch: 49 Batch: 1950
Training Loss: 0.6041474115848541
Epoch: 49 Batch: 2000
Training Loss: 0.6417551851272583
Epoch: 49 Batch: 2050
Training Loss: 0.5991454327106476
Epoch: 49 Batch: 2100
Training Loss: 0.6424388390779495
Epoch: 49 Batch: 2150
Training Loss: 0.5972047889232636
Epoch: 49 Batch: 2200
Training Loss: 0.6368318724632264
Epoch: 49 Batch: 2250
Training Loss: 0.6341989129781723
Epoch: 49 Batch: 2300
Training Loss: 0.6281995272636414
Epoch: 49 Batch: 2350
Training Loss: 0.6365932428836822
Epoch: 49 Batch: 2400
Training Loss: 0.6325076192617416
Epoch: 49 Batch: 2450
Training Loss: 0.6100150036811829
Epoch: 49 Batch: 2500
Training Loss: 0.6010356068611145
Epoch: 49 Batch: 2550
Training Loss: 0.5957410079240799
Epoch: 49 Batch: 2600
Training Loss: 0.5921757584810257
Epoch: 49 Batch: 2650
Training Loss: 0.612037091255188
Epoch: 49 Batch: 2700
Training Loss: 0.5914827555418014
Epoch: 49 Batch: 2750
Training Loss: 0.630319077372551
Epoch: 49 Batch: 2800
Training Loss: 0.6126678937673569
Epoch: 49 Batch: 2850
Training Loss: 0.602221450805664
Epoch: 49 Batch: 2900
Training Loss: 0.6070806527137756
Epoch: 49 Batch: 2950
Training Loss: 0.6043014049530029
Epoch: 49 Batch: 3000
Training Loss: 0.6146045434474945
Epoch: 49 Batch: 3050
Training Loss: 0.5822225201129914
Epoch: 49 Batch: 3100
Training Loss: 0.6273687565326691
Epoch: 49 Batch: 3150
Training Loss: 0.6336466723680496
Epoch: 49 Batch: 3200
Training Loss: 0.6250195729732514
Epoch: 49 Batch: 3250
Training Loss: 0.6062365520000458
Epoch: 49 Batch: 3300
Training Loss: 0.6479616987705231
Epoch: 49 Batch: 3350
Training Loss: 0.6046847426891326
Epoch: 49 Batch: 3400
Training Loss: 0.6278320980072022
Epoch: 49 Batch: 3450
Training Loss: 0.6239119827747345
Epoch: 49 Batch: 3500
Training Loss: 0.6255899524688721
Epoch: 50 
 Validation Loss: 0.5350555827220281
---------------------------
Epoch: 50 Batch: 50
Training Loss: 0.6149517214298248
Epoch: 50 Batch: 100
Training Loss: 0.6168563544750214
Epoch: 50 Batch: 150
Training Loss: 0.5972686344385147
Epoch: 50 Batch: 200
Training Loss: 0.6289436602592469
Epoch: 50 Batch: 250
Training Loss: 0.6500557208061218
Epoch: 50 Batch: 300
Training Loss: 0.6224520415067673
Epoch: 50 Batch: 350
Training Loss: 0.6156324237585068
Epoch: 50 Batch: 400
Training Loss: 0.6036325585842133
Epoch: 50 Batch: 450
Training Loss: 0.6449103957414627
Epoch: 50 Batch: 500
Training Loss: 0.6267945826053619
Epoch: 50 Batch: 550
Training Loss: 0.5996591871976853
