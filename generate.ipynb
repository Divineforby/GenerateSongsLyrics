{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dataloader import data_split,onehot\n",
    "from models import GenLSTM\n",
    "from configs import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files to generate...\n",
      "Using cuda for compute...\n",
      "Beginning text generation...\n",
      "Finished text generation...\n"
     ]
    }
   ],
   "source": [
    "# Load the state_dict of a model given path\n",
    "def load_model(path):\n",
    "    \n",
    "    # Make a fresh model\n",
    "    model = GenLSTM(input_size = len(SongData.vocab), output_size = len(SongData.vocab) )\n",
    "    \n",
    "    # Load in saved model params at checkpoint path\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Takes a pre-trained model and generate new text\n",
    "def generateText(data, model):\n",
    "    \n",
    "    # Number of texts to generate\n",
    "    # Max length of text\n",
    "    numText = 10\n",
    "    maxLen = 500\n",
    "    \n",
    "    # Start the sentence with the SOS ordinal, ordinal value is 0\n",
    "    # One-hot the values for generation\n",
    "    text = torch.tensor(np.zeros((numText, 1))).long()\n",
    "    \n",
    "    EOSreached = False\n",
    "    currSeqIndex = 0\n",
    "    hc = None\n",
    "    Temp = cfg['temperature']\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Put into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # No need for gradient\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # While we haven't reached the EOS run latest character through model with the previous hidden and cell state\n",
    "        # Softmax and sample output and use that as input\n",
    "        while (not EOSreached and currSeqIndex < maxLen):\n",
    "            \n",
    "            # Onehot the batch at the current sequence index\n",
    "            oh_input = onehot(text[:,currSeqIndex:currSeqIndex+1], len(data.encode))\n",
    "            oh_input = oh_input.to(device)\n",
    "            \n",
    "            # Pass characters at current sequence index through model\n",
    "            # First character we don't have hc\n",
    "            if not hc:\n",
    "                output, hc = model(oh_input)\n",
    "            # Every subsequent character we pass in the previous hc\n",
    "            else:\n",
    "                output, hc = model(oh_input, hc)\n",
    "                \n",
    "            # Convert outputs to probabilities using softmax with temperature\n",
    "            probs = torch.nn.functional.softmax(output/Temp, dim=2)\n",
    "            \n",
    "            # Create distribution and sample for the next indices\n",
    "            sampled = torch.distributions.Categorical(probs).sample()\n",
    "            \n",
    "            # Join sample with the current body of characters\n",
    "            text = torch.cat((text, sampled.cpu()), dim=1)\n",
    "            \n",
    "            currSeqIndex += 1\n",
    "            \n",
    "            # Check whether each rows contain any 1s(EOS) and that must be true for all rows\n",
    "            if (np.array(text) == 1).any(axis=1).all():\n",
    "                EOSreached = True\n",
    "                \n",
    "    # Decode the ordinals to characters\n",
    "    generated = [''.join([data.decode[c] for c in gen]) for gen in np.array(text)]\n",
    "    \n",
    "    # For each string remove everything after the first EOS(\\3)\n",
    "    generated = [text[0:text.find('\\3')+1] for text in generated]\n",
    "    \n",
    "    print(\"Finished text generation...\", flush=True)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Loading necessary files to generate...\", flush=True)\n",
    "    # Get dataset to know meta-data\n",
    "    data = pd.read_csv('songdata.csv')\n",
    "    SongData.init_vocab(data)\n",
    "    train_set, val_set = data_split(data)\n",
    "    \n",
    "    # Get path\n",
    "    checkpointPath = 'model_checkpoints/training_session0/'\n",
    "    savedModePath = 'LSTMmodel_E_1_B_1338.mdl'\n",
    "    \n",
    "    # Load in trained model\n",
    "    model = load_model(data, os.path.join(checkpointPath, savedModePath))\n",
    "    \n",
    "    \n",
    "    useCuda = cfg['use_cuda']\n",
    "    # Check for cuda and set default compute device\n",
    "    if ( torch.cuda.is_available() and useCuda ):\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(\"Using %s for compute...\" % device.type, flush=True)\n",
    "    \n",
    "    \n",
    "    print(\"Beginning text generation...\", flush=True)\n",
    "    \n",
    "    # Generate text\n",
    "    gen = generateText(train_set, model)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptorch]",
   "language": "python",
   "name": "conda-env-ptorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
