{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GenLSTM\n",
    "from dataloader import SongData, Pad, onehot\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "from configs import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from CSV...\n",
      "Dividing whole data into training and validation sets...\n",
      "Beginning model training...\n",
      "Using cuda for compute...\n",
      "In epoch 0...\n",
      "Loss at 50 batch of epoch 0 is 3.031362\n",
      "Validation Loss at 50 batch of epoch 0 is 1.572025\n",
      "Loss at 100 batch of epoch 0 is 1.773006\n",
      "Validation Loss at 100 batch of epoch 0 is 1.371832\n",
      "Loss at 150 batch of epoch 0 is 1.507325\n",
      "Validation Loss at 150 batch of epoch 0 is 1.368706\n",
      "Loss at 200 batch of epoch 0 is 1.471900\n",
      "Validation Loss at 200 batch of epoch 0 is 1.340961\n",
      "Loss at 250 batch of epoch 0 is 1.440752\n",
      "Validation Loss at 250 batch of epoch 0 is 1.332919\n",
      "Loss at 300 batch of epoch 0 is 1.542848\n",
      "Validation Loss at 300 batch of epoch 0 is 1.331082\n",
      "Loss at 350 batch of epoch 0 is 1.530759\n",
      "Validation Loss at 350 batch of epoch 0 is 1.324907\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e542d6af8193>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e542d6af8193>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_set, val_set)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# Push data and labels onto device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;31m# Reset gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make new folder to keep sessions checkpoints\n",
    "def makeSessionDir():\n",
    "    \n",
    "    # Get all the indices of the sessions\n",
    "    sessionIndices = [int(name[-1]) for name in os.listdir(path='model_checkpoints/') if 'training_session' in name]\n",
    "    \n",
    "    # If none exists we make the index 0\n",
    "    if ( not sessionIndices ):\n",
    "        newIndex = 0\n",
    "    else:\n",
    "        newIndex = max(sessionIndices) + 1\n",
    "        \n",
    "    # Make the new folder and return the path\n",
    "    path = './model_checkpoints/training_session' + str(newIndex)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    return path\n",
    "    \n",
    "# Calculate Validation Loss\n",
    "def validation(model, val_set, device, LossFcn):\n",
    "    # No need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Config\n",
    "        batchSize = cfg['validation_batch_size']\n",
    "\n",
    "        # Validation set data loader\n",
    "        validation_loader = DataLoader(val_set, batch_size=batchSize, collate_fn=Pad)\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Go into evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate loss w.r.t the entire validation set\n",
    "        for idx, (data, labels) in enumerate(validation_loader):\n",
    "\n",
    "            # One_hot \n",
    "            data = onehot(data, len(val_set.encode))\n",
    "\n",
    "            # Push data and labels onto device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            # Run the data through the model\n",
    "            Output = model(data)\n",
    "\n",
    "            # Reshape output and loss to interpret timesteps as another sample\n",
    "            Output, labels = Output.view(Output.shape[0]*Output.shape[1], -1), labels.view(labels.shape[0]*labels.shape[1])\n",
    "\n",
    "            # Compute Loss and add to average \n",
    "            Loss = LossFcn(Output, labels) \n",
    "            val_loss += float(Loss.item())\n",
    "\n",
    "        # Average loss over entire set\n",
    "        val_loss /= len(validation_loader)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Takes a fresh model and data set\n",
    "# Train the model and save checkpoints \n",
    "def train(model, train_set, val_set):\n",
    "    \n",
    "    # Training options\n",
    "    maxEpochs = np.arange(cfg['max_epochs'])\n",
    "    lr = cfg['lr']\n",
    "    l2_decay = cfg['l2_penalty']\n",
    "    batchSize = cfg['batch_size']\n",
    "    useCuda = cfg['use_cuda']\n",
    "    \n",
    "    # Check for cuda and set default compute device\n",
    "    if ( torch.cuda.is_available() and useCuda ):\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    print(\"Using %s for compute...\" % device.type)\n",
    "  \n",
    "        \n",
    "    # Send model to chose device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create training set data loader, loss, and optimizer\n",
    "    train_loader = DataLoader(train_set, batch_size=batchSize, collate_fn=Pad, shuffle=True)\n",
    "    LossFcn = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(params=model.parameters(), lr = lr, weight_decay=l2_decay)\n",
    "    \n",
    "    # Loss collection and model chkpt every Nth batch\n",
    "    N = 50\n",
    "  \n",
    "    # Training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Make new dir to keep this training session\n",
    "    checkpointPath = makeSessionDir()\n",
    "    \n",
    "    # Losses every Nth batch for the epoch\n",
    "    epoch_losses = []\n",
    "    epoch_val_losses = []\n",
    "    \n",
    "    # Early stopping boolean\n",
    "    # If the values in the early_stopping_range is monotonically increasing\n",
    "    early_stopping = False\n",
    "    early_stopping_range = 3\n",
    "    \n",
    "    # For each epoch train the model on the data\n",
    "    for e in maxEpochs:\n",
    "        \n",
    "        print(\"In epoch %d...\" % e)\n",
    "        \n",
    "        batch_loss = 0\n",
    "        \n",
    "        # For each batch in the loader send all \n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "             \n",
    "            # One_hot the inputs using the number of possible encoding values\n",
    "            data = onehot(data, len(train_set.encode))\n",
    "            \n",
    "            # Push data and labels onto device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Run the data through the model\n",
    "            Output = model(data)\n",
    "            \n",
    "            # Reshape output and loss to interpret timesteps as another sample\n",
    "            Output, labels = Output.view(Output.shape[0]*Output.shape[1], -1), labels.view(labels.shape[0]*labels.shape[1])\n",
    "                          \n",
    "            # Compute Loss and add to average \n",
    "            Loss = LossFcn(Output, labels) \n",
    "            batch_loss += float(Loss.item())        \n",
    "                          \n",
    "            # Take gradient and update\n",
    "            Loss.backward()\n",
    "            optim.step()\n",
    "                          \n",
    "            # Calculate average batch losses every nth batch\n",
    "            if ( idx % N == 0 and idx > 0  ):\n",
    "                \n",
    "                # Checkpoint, save model and optimizer \n",
    "                modelPath = os.path.join( checkpointPath, 'LSTMmodel_E_{0}_B_{1}'.format(e,idx) )\n",
    "                optimPath = os.path.join( checkpointPath, 'LSTMoptim_E_{0}_B_{1}'.format(e,idx) )\n",
    "                \n",
    "                torch.save(model.state_dict(), modelPath )\n",
    "                torch.save(optim.state_dict(), optimPath )\n",
    "                # Calculate average training loss of N batches\n",
    "                avg_loss = batch_loss/N\n",
    "                batch_loss = 0\n",
    "                epoch_losses.append(avg_loss)\n",
    "                          \n",
    "                print(\"Loss at %d batch of epoch %d is %f\" % (idx, e, avg_loss) )\n",
    "                \n",
    "                # Try to clear some unused variable and run validation \n",
    "                del Loss, Output, data, labels\n",
    "                val_loss = validation(model, val_set, device, LossFcn)\n",
    "                \n",
    "                # reset to training mode\n",
    "                model.train()\n",
    "         \n",
    "                epoch_val_losses.append(val_loss)\n",
    "                \n",
    "                print(\"Validation Loss at %d batch of epoch %d is %f\" % (idx, e, val_loss))\n",
    "            \n",
    "            # Check for early stopping with monotonicity \n",
    "            # Only check when we have at least 3 values to check\n",
    "            if (len(epoch_val_losses[early_stopping_range::-1]) > early_stopping_range and monotonicIncr(epoch_val_losses[3::-1])):\n",
    "                early_stopping = True\n",
    "            \n",
    "            # If early stopping we stop going through the data set\n",
    "            if (early_stopping):\n",
    "                break\n",
    "                \n",
    "        # Break out of epoch loop\n",
    "        if (early_stopping):\n",
    "            break\n",
    "    \n",
    "    # Write out the loss arrays \n",
    "    with open(os.path.join(checkpointPath, 'train_losses.pkl'), \"wb+\") as tlossfile:\n",
    "        pkl.dump(epoch_losses, tlossfile, pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(os.path.join(checkpointPath, 'val_losses.pkl'), \"wb+\") as vlossfile:\n",
    "        pkl.dump(epoch_losses, vlossfile, pkl.HIGHEST_PROTOCOL)\n",
    "                          \n",
    "    print(\"Finished training for %d epochs...\" % e)\n",
    "        \n",
    "# Check whether the list is monotonically increasing\n",
    "def monotonicIncr(lst):\n",
    "\n",
    "    # Monotonic increasing means subsequent values are always larger\n",
    "    # Differences are then always <= 0 \n",
    "    diff = np.array(lst[::-1])\n",
    "    diff = (diff < 0)\n",
    "    \n",
    "    # If all the differences are > 0 we are monotonically decreasing\n",
    "    return diff.all()\n",
    "    \n",
    "# Splits up full dataset to train and validation\n",
    "# Returns pytorch Dataset of splits\n",
    "def data_split(data):\n",
    "    \n",
    "    # Sample 10% from the dataset for validation\n",
    "    val = data.sample(frac = .01)\n",
    "    \n",
    "    # Remove the sampled rows from the original\n",
    "    train = data.drop(val.index)\n",
    "    \n",
    "    # Reset both indices for uniformity\n",
    "    val = val.reset_index(drop=True)\n",
    "    train = train.reset_index(drop=True)\n",
    "    \n",
    "    # Make song datasets from both dataframes\n",
    "    val_set = SongData(val)\n",
    "    train_set = SongData(train)\n",
    "    \n",
    "    return train_set, val_set\n",
    "    \n",
    "    \n",
    "    \n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Reading data from CSV...\")\n",
    "    \n",
    "    # Read in data and create train_val split\n",
    "    data = pd.read_csv('songdata.csv')\n",
    "    \n",
    "    print(\"Dividing whole data into training and validation sets...\")\n",
    "    \n",
    "    # Split data to training and validation set\n",
    "    train_set, val_set = data_split(data)\n",
    "    \n",
    "    # Make fresh model to train\n",
    "    # Both input and output are size of possible characters\n",
    "    # We are inputting characters at t and asking to predict character t+1 \n",
    "    model = GenLSTM(input_size = len(train_set.encode), output_size = len(train_set.encode) )\n",
    "    \n",
    "    print(\"Beginning model training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    train(model, train_set, val_set)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptorch]",
   "language": "python",
   "name": "conda-env-ptorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
